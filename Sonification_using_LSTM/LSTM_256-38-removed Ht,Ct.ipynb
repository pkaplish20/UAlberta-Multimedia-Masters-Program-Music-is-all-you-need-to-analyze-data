{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "#import torch.utils.tensorboard as tb\n",
    "from Preprocessing.preprocessing import PreprocessingTrainingData\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as  plt\n",
    "import os\n",
    "import logging\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "import pandas as pd\n",
    "from Postprocessing.postprocessing import PostProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static parameters\n",
    "train_batch_size = 170\n",
    "val_batch_size = 170\n",
    "sequence_length=50\n",
    "test_batch_size = 1\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layer = 2\n",
    "output_size = 38\n",
    "clip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from preprocessing.py\n",
    "dataset_path = os.path.join(os.path.abspath('..'),'Dataset\\\\Clementi dataset\\\\Clementi dataset' )\n",
    "network_input,network_output,max_midi_number,min_midi_number,int_to_note = PreprocessingTrainingData().preprocess_notes(dataset_path)\n",
    "network_input, network_output = network_input.cuda(), network_output.cuda()\n",
    "\n",
    "# print(network_input)\n",
    "#print(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(network_output.max())\n",
    "print(network_output.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "89\n",
      "50\n",
      "{0: 50, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61, 11: 62, 12: 63, 13: 64, 14: 65, 15: 66, 16: 67, 17: 68, 18: 69, 19: 70, 20: 71, 21: 72, 22: 73, 23: 74, 24: 75, 25: 76, 26: 77, 27: 78, 28: 79, 29: 80, 30: 81, 31: 82, 32: 83, 33: 84, 34: 85, 35: 86, 36: 88, 37: 89}\n"
     ]
    }
   ],
   "source": [
    "print(network_input.max())\n",
    "print(network_input.min())\n",
    "print(max_midi_number)\n",
    "print(min_midi_number)\n",
    "print(int_to_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata is highly unbalanced\\n# '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data is highly unbalanced\n",
    "# '''\n",
    "# sns.distplot(torch.tensor(network_output).cpu())\n",
    "# xx = pd.DataFrame(torch.tensor(network_output).cpu())\n",
    "# xx.groupby(0).size().to_frame(name='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8500, 50, 1])\n",
      "torch.Size([8500])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "to make batch of equal sizes\n",
    "Quick Fix\n",
    "'''\n",
    "network_input = network_input[: -117]\n",
    "network_output = network_output[: -117]\n",
    "\n",
    "print(network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bakchodi Normalization\n",
    "# network_input=network_input.cpu().numpy().tolist()\n",
    "# for i in range(len(network_input)):\n",
    "#     for j in range(len(network_input[i])):\n",
    "#         network_input[i][j][0]=((network_input[i][j][0])*(max_midi_number-min_midi_number)+min_midi_number)/max_midi_number\n",
    "# network_input=torch.Tensor(network_input).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_input[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create Stacked LSTM model\n",
    "'''\n",
    "class Stacked_LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size = input_size, hidden_size = hidden_size, batch_first = True)\n",
    "        self.lstm2 = nn.LSTM(input_size = hidden_size, hidden_size = output_size,batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(output_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, batch_size):\n",
    "        \n",
    "        output, _ = self.lstm1(x)        \n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        output, _ = self.lstm2(output)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        output = output.contiguous().view(-1, 38)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        #print('Linear Output :-',output.shape)\n",
    "        \n",
    "        #output = F.softmax(output, dim = 1)\n",
    "        #print('SOFTMAX OUTPUT :--', output)\n",
    "        \n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1)\n",
    "        #print('Reshape to batch size first :-',output.shape)\n",
    "        \n",
    "        output = output[:, -self.output_size:] # get last batch of labels\n",
    "        #print('Final Output :-',output)\n",
    "        #print('RESHAPE SIZE :-', output.shape)\n",
    "        \n",
    "        return output\n",
    "\n",
    "#initialize the weights of LSTM using Xavier initialization    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Divide the dataset into train/val \n",
    "'''\n",
    "train_size = 0.8\n",
    "indices = list(range(len(network_input)))\n",
    "split = int(np.floor(train_size*len(network_input)))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SequentialSampler(train_idx)\n",
    "val_sampler = SequentialSampler(val_idx)\n",
    "\n",
    "dataset = TensorDataset(network_input,network_output)\n",
    "train_loader = DataLoader(dataset, batch_size= train_batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size= val_batch_size,sampler= val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm1): LSTM(1, 256, batch_first=True)\n",
       "  (lstm2): LSTM(256, 38, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=38, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optimizer\n",
    "\n",
    "model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer.AdamW(model.parameters())\n",
    "#optimizer = optimizer.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "#make sure to transfer model to GPU after initializing optimizer\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 3.3446208 \tVal Loss:3.0100807 \tTrain Acc: 7.808824% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from    inf to 3.010081, saving the model weights\n",
      "Epoch: 1\tTrain Loss: 3.1425091 \tVal Loss:2.9826850 \tTrain Acc: 9.132353% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 3.010081 to 2.982685, saving the model weights\n",
      "Epoch: 2\tTrain Loss: 3.1131256 \tVal Loss:2.9613094 \tTrain Acc: 8.294118% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.982685 to 2.961309, saving the model weights\n",
      "Epoch: 3\tTrain Loss: 3.0987655 \tVal Loss:2.9609893 \tTrain Acc: 9.205883% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.961309 to 2.960989, saving the model weights\n",
      "Epoch: 4\tTrain Loss: 3.0920197 \tVal Loss:2.9544968 \tTrain Acc: 8.838236% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.960989 to 2.954497, saving the model weights\n",
      "Epoch: 5\tTrain Loss: 3.0762066 \tVal Loss:2.9481595 \tTrain Acc: 9.57353% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.954497 to 2.948160, saving the model weights\n",
      "Epoch: 6\tTrain Loss: 3.0694336 \tVal Loss:2.9459568 \tTrain Acc: 9.470588% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.948160 to 2.945957, saving the model weights\n",
      "Epoch: 7\tTrain Loss: 3.0760107 \tVal Loss:2.9479998 \tTrain Acc: 8.838236% \tVal Acc: 11.4117651%\n",
      "Epoch: 8\tTrain Loss: 3.0668140 \tVal Loss:2.9438443 \tTrain Acc: 10.11765% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.945957 to 2.943844, saving the model weights\n",
      "Epoch: 9\tTrain Loss: 3.0617232 \tVal Loss:2.9381823 \tTrain Acc: 10.16176% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.943844 to 2.938182, saving the model weights\n",
      "Epoch: 10\tTrain Loss: 3.0571600 \tVal Loss:2.9385442 \tTrain Acc: 10.04412% \tVal Acc: 11.4117651%\n",
      "Epoch: 11\tTrain Loss: 3.0575083 \tVal Loss:2.9371056 \tTrain Acc: 9.205883% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.938182 to 2.937106, saving the model weights\n",
      "Epoch: 12\tTrain Loss: 3.0501337 \tVal Loss:2.9353057 \tTrain Acc: 9.985294% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.937106 to 2.935306, saving the model weights\n",
      "Epoch: 13\tTrain Loss: 3.0478803 \tVal Loss:2.9341406 \tTrain Acc: 10.07353% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.935306 to 2.934141, saving the model weights\n",
      "Epoch: 14\tTrain Loss: 3.0502628 \tVal Loss:2.9348997 \tTrain Acc: 9.691177% \tVal Acc: 11.4117651%\n",
      "Epoch: 15\tTrain Loss: 3.0491362 \tVal Loss:2.9330247 \tTrain Acc: 10.17647% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.934141 to 2.933025, saving the model weights\n",
      "Epoch: 16\tTrain Loss: 3.0434509 \tVal Loss:2.9455805 \tTrain Acc: 10.48529% \tVal Acc: 11.4117651%\n",
      "Epoch: 17\tTrain Loss: 3.0457466 \tVal Loss:2.9287678 \tTrain Acc: 9.882353% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.933025 to 2.928768, saving the model weights\n",
      "Epoch: 18\tTrain Loss: 3.0346341 \tVal Loss:2.9292116 \tTrain Acc: 10.60294% \tVal Acc: 11.4117651%\n",
      "Epoch: 19\tTrain Loss: 3.0369641 \tVal Loss:2.9338475 \tTrain Acc: 10.48529% \tVal Acc: 11.4117651%\n",
      "Epoch: 20\tTrain Loss: 3.0222581 \tVal Loss:2.9059349 \tTrain Acc: 10.27941% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.928768 to 2.905935, saving the model weights\n",
      "Epoch: 21\tTrain Loss: 2.9964928 \tVal Loss:2.9018550 \tTrain Acc: 11.07353% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.905935 to 2.901855, saving the model weights\n",
      "Epoch: 22\tTrain Loss: 2.9707708 \tVal Loss:2.8484907 \tTrain Acc: 11.10294% \tVal Acc: 11.5882357%\n",
      "Validation Loss decreased from 2.901855 to 2.848491, saving the model weights\n",
      "Epoch: 23\tTrain Loss: 2.9335804 \tVal Loss:2.8113679 \tTrain Acc: 11.14706% \tVal Acc: 12.0588239%\n",
      "Validation Loss decreased from 2.848491 to 2.811368, saving the model weights\n",
      "Epoch: 24\tTrain Loss: 2.8464071 \tVal Loss:2.7088981 \tTrain Acc: 12.85294% \tVal Acc: 13.2352944%\n",
      "Validation Loss decreased from 2.811368 to 2.708898, saving the model weights\n",
      "Epoch: 25\tTrain Loss: 2.7618611 \tVal Loss:2.6413613 \tTrain Acc: 13.82353% \tVal Acc: 13.9411769%\n",
      "Validation Loss decreased from 2.708898 to 2.641361, saving the model weights\n",
      "Epoch: 26\tTrain Loss: 2.7094171 \tVal Loss:2.5878699 \tTrain Acc: 14.16177% \tVal Acc: 13.2352944%\n",
      "Validation Loss decreased from 2.641361 to 2.587870, saving the model weights\n",
      "Epoch: 27\tTrain Loss: 2.6519558 \tVal Loss:2.5609137 \tTrain Acc: 15.16177% \tVal Acc: 12.5882357%\n",
      "Validation Loss decreased from 2.587870 to 2.560914, saving the model weights\n",
      "Epoch: 28\tTrain Loss: 2.6294358 \tVal Loss:2.5218344 \tTrain Acc: 15.22059% \tVal Acc: 14.5294122%\n",
      "Validation Loss decreased from 2.560914 to 2.521834, saving the model weights\n",
      "Epoch: 29\tTrain Loss: 2.6120412 \tVal Loss:2.4990490 \tTrain Acc: 14.92647% \tVal Acc: 14.1176473%\n",
      "Validation Loss decreased from 2.521834 to 2.499049, saving the model weights\n",
      "Epoch: 30\tTrain Loss: 2.5806996 \tVal Loss:2.4762186 \tTrain Acc: 15.72059% \tVal Acc: 15.5882358%\n",
      "Validation Loss decreased from 2.499049 to 2.476219, saving the model weights\n",
      "Epoch: 31\tTrain Loss: 2.5597700 \tVal Loss:2.4676915 \tTrain Acc: 16.20588% \tVal Acc: 14.1764710%\n",
      "Validation Loss decreased from 2.476219 to 2.467691, saving the model weights\n",
      "Epoch: 32\tTrain Loss: 2.5484717 \tVal Loss:2.4568775 \tTrain Acc: 15.67647% \tVal Acc: 13.8823533%\n",
      "Validation Loss decreased from 2.467691 to 2.456878, saving the model weights\n",
      "Epoch: 33\tTrain Loss: 2.5329491 \tVal Loss:2.4455275 \tTrain Acc: 16.52941% \tVal Acc: 14.7647063%\n",
      "Validation Loss decreased from 2.456878 to 2.445528, saving the model weights\n",
      "Epoch: 34\tTrain Loss: 2.5160800 \tVal Loss:2.4445130 \tTrain Acc: 16.29412% \tVal Acc: 14.8235298%\n",
      "Validation Loss decreased from 2.445528 to 2.444513, saving the model weights\n",
      "Epoch: 35\tTrain Loss: 2.5151165 \tVal Loss:2.4294557 \tTrain Acc: 16.92647% \tVal Acc: 16.1764710%\n",
      "Validation Loss decreased from 2.444513 to 2.429456, saving the model weights\n",
      "Epoch: 36\tTrain Loss: 2.5028842 \tVal Loss:2.4251547 \tTrain Acc: 16.47059% \tVal Acc: 15.9411769%\n",
      "Validation Loss decreased from 2.429456 to 2.425155, saving the model weights\n",
      "Epoch: 37\tTrain Loss: 2.4995809 \tVal Loss:2.4187500 \tTrain Acc: 17.60294% \tVal Acc: 15.7058828%\n",
      "Validation Loss decreased from 2.425155 to 2.418750, saving the model weights\n",
      "Epoch: 38\tTrain Loss: 2.4890766 \tVal Loss:2.4097068 \tTrain Acc: 17.44118% \tVal Acc: 16.3529416%\n",
      "Validation Loss decreased from 2.418750 to 2.409707, saving the model weights\n",
      "Epoch: 39\tTrain Loss: 2.4803643 \tVal Loss:2.4081442 \tTrain Acc: 17.26471% \tVal Acc: 16.3529415%\n",
      "Validation Loss decreased from 2.409707 to 2.408144, saving the model weights\n",
      "Epoch: 40\tTrain Loss: 2.4810936 \tVal Loss:2.4112100 \tTrain Acc: 17.11765% \tVal Acc: 16.0000004%\n",
      "Epoch: 41\tTrain Loss: 2.4710097 \tVal Loss:2.4028848 \tTrain Acc: 17.22059% \tVal Acc: 16.6470592%\n",
      "Validation Loss decreased from 2.408144 to 2.402885, saving the model weights\n",
      "Epoch: 42\tTrain Loss: 2.4693212 \tVal Loss:2.3960891 \tTrain Acc: 16.86765% \tVal Acc: 16.5882356%\n",
      "Validation Loss decreased from 2.402885 to 2.396089, saving the model weights\n",
      "Epoch: 43\tTrain Loss: 2.4614630 \tVal Loss:2.4155279 \tTrain Acc: 17.75% \tVal Acc: 15.9411769%\n",
      "Epoch: 44\tTrain Loss: 2.4636399 \tVal Loss:2.3975023 \tTrain Acc: 17.66177% \tVal Acc: 16.2352946%\n",
      "Epoch: 45\tTrain Loss: 2.4531770 \tVal Loss:2.4044553 \tTrain Acc: 18.29412% \tVal Acc: 15.8823533%\n",
      "Epoch: 46\tTrain Loss: 2.4481769 \tVal Loss:2.3928148 \tTrain Acc: 18.39706% \tVal Acc: 15.4705887%\n",
      "Validation Loss decreased from 2.396089 to 2.392815, saving the model weights\n",
      "Epoch: 47\tTrain Loss: 2.4423847 \tVal Loss:2.4008396 \tTrain Acc: 17.61765% \tVal Acc: 16.8823534%\n",
      "Epoch: 48\tTrain Loss: 2.4402667 \tVal Loss:2.4028750 \tTrain Acc: 18.48529% \tVal Acc: 15.7647063%\n",
      "Epoch: 49\tTrain Loss: 2.4374239 \tVal Loss:2.3981264 \tTrain Acc: 18.60294% \tVal Acc: 16.1764711%\n",
      "Epoch: 50\tTrain Loss: 2.4259158 \tVal Loss:2.3925696 \tTrain Acc: 18.64706% \tVal Acc: 16.4117652%\n",
      "Validation Loss decreased from 2.392815 to 2.392570, saving the model weights\n",
      "Epoch: 51\tTrain Loss: 2.4291615 \tVal Loss:2.3862245 \tTrain Acc: 19.36765% \tVal Acc: 17.3529416%\n",
      "Validation Loss decreased from 2.392570 to 2.386224, saving the model weights\n",
      "Epoch: 52\tTrain Loss: 2.4274164 \tVal Loss:2.3741406 \tTrain Acc: 18.33824% \tVal Acc: 18.2352944%\n",
      "Validation Loss decreased from 2.386224 to 2.374141, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 53\tTrain Loss: 2.4192681 \tVal Loss:2.3860807 \tTrain Acc: 19.25% \tVal Acc: 17.7647062%\n",
      "Epoch: 54\tTrain Loss: 2.4192419 \tVal Loss:2.3764142 \tTrain Acc: 19.41177% \tVal Acc: 17.8235298%\n",
      "Epoch: 55\tTrain Loss: 2.4108529 \tVal Loss:2.3820433 \tTrain Acc: 19.69118% \tVal Acc: 18.5294122%\n",
      "Epoch: 56\tTrain Loss: 2.4039810 \tVal Loss:2.3763202 \tTrain Acc: 19.58824% \tVal Acc: 17.9411768%\n",
      "Epoch: 57\tTrain Loss: 2.4039505 \tVal Loss:2.3679344 \tTrain Acc: 19.38235% \tVal Acc: 17.1176475%\n",
      "Validation Loss decreased from 2.374141 to 2.367934, saving the model weights\n",
      "Epoch: 58\tTrain Loss: 2.3987224 \tVal Loss:2.3630337 \tTrain Acc: 19.63235% \tVal Acc: 18.8235298%\n",
      "Validation Loss decreased from 2.367934 to 2.363034, saving the model weights\n",
      "Epoch: 59\tTrain Loss: 2.4040757 \tVal Loss:2.3608697 \tTrain Acc: 19.91177% \tVal Acc: 19.4705886%\n",
      "Validation Loss decreased from 2.363034 to 2.360870, saving the model weights\n",
      "Epoch: 60\tTrain Loss: 2.3933937 \tVal Loss:2.3607222 \tTrain Acc: 20.02941% \tVal Acc: 17.7058828%\n",
      "Validation Loss decreased from 2.360870 to 2.360722, saving the model weights\n",
      "Epoch: 61\tTrain Loss: 2.3881538 \tVal Loss:2.3581607 \tTrain Acc: 20.57353% \tVal Acc: 19.0000004%\n",
      "Validation Loss decreased from 2.360722 to 2.358161, saving the model weights\n",
      "Epoch: 62\tTrain Loss: 2.3869537 \tVal Loss:2.3660314 \tTrain Acc: 20.77941% \tVal Acc: 17.6470593%\n",
      "Epoch: 63\tTrain Loss: 2.3799000 \tVal Loss:2.3627938 \tTrain Acc: 20.89706% \tVal Acc: 17.8235298%\n",
      "Epoch: 64\tTrain Loss: 2.3792395 \tVal Loss:2.3386244 \tTrain Acc: 20.92647% \tVal Acc: 20.1176473%\n",
      "Validation Loss decreased from 2.358161 to 2.338624, saving the model weights\n",
      "Epoch: 65\tTrain Loss: 2.3707810 \tVal Loss:2.3270766 \tTrain Acc: 20.91177% \tVal Acc: 21.6470592%\n",
      "Validation Loss decreased from 2.338624 to 2.327077, saving the model weights\n",
      "Epoch: 66\tTrain Loss: 2.3503215 \tVal Loss:2.3240070 \tTrain Acc: 21.30882% \tVal Acc: 20.6470592%\n",
      "Validation Loss decreased from 2.327077 to 2.324007, saving the model weights\n",
      "Epoch: 67\tTrain Loss: 2.3569928 \tVal Loss:2.3310549 \tTrain Acc: 21.11765% \tVal Acc: 18.8235298%\n",
      "Epoch: 68\tTrain Loss: 2.3490107 \tVal Loss:2.3055298 \tTrain Acc: 22.01471% \tVal Acc: 23.3529416%\n",
      "Validation Loss decreased from 2.324007 to 2.305530, saving the model weights\n",
      "Epoch: 69\tTrain Loss: 2.3480483 \tVal Loss:2.3194510 \tTrain Acc: 22.23529% \tVal Acc: 19.8823532%\n",
      "Epoch: 70\tTrain Loss: 2.3489737 \tVal Loss:2.3434854 \tTrain Acc: 22.25% \tVal Acc: 19.1764709%\n",
      "Epoch: 71\tTrain Loss: 2.3669970 \tVal Loss:2.3098978 \tTrain Acc: 21.55882% \tVal Acc: 19.8823532%\n",
      "Epoch: 72\tTrain Loss: 2.3398514 \tVal Loss:2.2968623 \tTrain Acc: 22.30882% \tVal Acc: 21.1176474%\n",
      "Validation Loss decreased from 2.305530 to 2.296862, saving the model weights\n",
      "Epoch: 73\tTrain Loss: 2.3332909 \tVal Loss:2.2838686 \tTrain Acc: 21.91177% \tVal Acc: 23.0000003%\n",
      "Validation Loss decreased from 2.296862 to 2.283869, saving the model weights\n",
      "Epoch: 74\tTrain Loss: 2.3198489 \tVal Loss:2.2902074 \tTrain Acc: 22.51471% \tVal Acc: 20.5882356%\n",
      "Epoch: 75\tTrain Loss: 2.3194934 \tVal Loss:2.2840937 \tTrain Acc: 22.85294% \tVal Acc: 21.0588239%\n",
      "Epoch: 76\tTrain Loss: 2.3112068 \tVal Loss:2.2845820 \tTrain Acc: 23.27941% \tVal Acc: 22.6470593%\n",
      "Epoch: 77\tTrain Loss: 2.3157968 \tVal Loss:2.2691779 \tTrain Acc: 23.23529% \tVal Acc: 21.8823534%\n",
      "Validation Loss decreased from 2.283869 to 2.269178, saving the model weights\n",
      "Epoch: 78\tTrain Loss: 2.3082878 \tVal Loss:2.2878778 \tTrain Acc: 23.70588% \tVal Acc: 20.4117651%\n",
      "Epoch: 79\tTrain Loss: 2.3097538 \tVal Loss:2.2702471 \tTrain Acc: 23.64706% \tVal Acc: 20.8823532%\n",
      "Epoch: 80\tTrain Loss: 2.2931235 \tVal Loss:2.2623898 \tTrain Acc: 23.42647% \tVal Acc: 22.6470593%\n",
      "Validation Loss decreased from 2.269178 to 2.262390, saving the model weights\n",
      "Epoch: 81\tTrain Loss: 2.2958744 \tVal Loss:2.2591401 \tTrain Acc: 23.13235% \tVal Acc: 22.5882356%\n",
      "Validation Loss decreased from 2.262390 to 2.259140, saving the model weights\n",
      "Epoch: 82\tTrain Loss: 2.2892137 \tVal Loss:2.2637569 \tTrain Acc: 24.32353% \tVal Acc: 21.0588238%\n",
      "Epoch: 83\tTrain Loss: 2.2906213 \tVal Loss:2.3039107 \tTrain Acc: 24.05882% \tVal Acc: 18.4705887%\n",
      "Epoch: 84\tTrain Loss: 2.2869512 \tVal Loss:2.2410040 \tTrain Acc: 24.16177% \tVal Acc: 23.9411768%\n",
      "Validation Loss decreased from 2.259140 to 2.241004, saving the model weights\n",
      "Epoch: 85\tTrain Loss: 2.2723587 \tVal Loss:2.2419289 \tTrain Acc: 24.95588% \tVal Acc: 22.3529416%\n",
      "Epoch: 86\tTrain Loss: 2.2722909 \tVal Loss:2.2463516 \tTrain Acc: 24.14706% \tVal Acc: 23.0000004%\n",
      "Epoch: 87\tTrain Loss: 2.2655339 \tVal Loss:2.2822916 \tTrain Acc: 25.16177% \tVal Acc: 20.3529416%\n",
      "Epoch: 88\tTrain Loss: 2.2688589 \tVal Loss:2.2476197 \tTrain Acc: 25.07353% \tVal Acc: 20.5882357%\n",
      "Epoch: 89\tTrain Loss: 2.2535992 \tVal Loss:2.2647500 \tTrain Acc: 24.97059% \tVal Acc: 20.4705887%\n",
      "Epoch: 90\tTrain Loss: 2.2640133 \tVal Loss:2.2406402 \tTrain Acc: 24.86765% \tVal Acc: 21.1764711%\n",
      "Validation Loss decreased from 2.241004 to 2.240640, saving the model weights\n",
      "Epoch: 91\tTrain Loss: 2.2499383 \tVal Loss:2.2276091 \tTrain Acc: 25.0% \tVal Acc: 23.5294123%\n",
      "Validation Loss decreased from 2.240640 to 2.227609, saving the model weights\n",
      "Epoch: 92\tTrain Loss: 2.2475189 \tVal Loss:2.2311963 \tTrain Acc: 25.35294% \tVal Acc: 22.5294122%\n",
      "Epoch: 93\tTrain Loss: 2.2432225 \tVal Loss:2.2169546 \tTrain Acc: 25.02941% \tVal Acc: 24.5882359%\n",
      "Validation Loss decreased from 2.227609 to 2.216955, saving the model weights\n",
      "Epoch: 94\tTrain Loss: 2.2506240 \tVal Loss:2.2351936 \tTrain Acc: 25.47059% \tVal Acc: 21.2941180%\n",
      "Epoch: 95\tTrain Loss: 2.2329929 \tVal Loss:2.2010189 \tTrain Acc: 25.94118% \tVal Acc: 25.2352947%\n",
      "Validation Loss decreased from 2.216955 to 2.201019, saving the model weights\n",
      "Epoch: 96\tTrain Loss: 2.2760851 \tVal Loss:2.2351822 \tTrain Acc: 25.61765% \tVal Acc: 23.7058829%\n",
      "Epoch: 97\tTrain Loss: 2.2368360 \tVal Loss:2.1906646 \tTrain Acc: 26.16177% \tVal Acc: 26.2352951%\n",
      "Validation Loss decreased from 2.201019 to 2.190665, saving the model weights\n",
      "Epoch: 98\tTrain Loss: 2.2247464 \tVal Loss:2.2127952 \tTrain Acc: 25.86765% \tVal Acc: 24.6470593%\n",
      "Epoch: 99\tTrain Loss: 2.2166982 \tVal Loss:2.2139649 \tTrain Acc: 26.32353% \tVal Acc: 23.1764710%\n",
      "Epoch: 100\tTrain Loss: 2.2270018 \tVal Loss:2.1770921 \tTrain Acc: 26.32353% \tVal Acc: 25.8823536%\n",
      "Validation Loss decreased from 2.190665 to 2.177092, saving the model weights\n",
      "Epoch: 101\tTrain Loss: 2.2127331 \tVal Loss:2.1823319 \tTrain Acc: 26.92647% \tVal Acc: 27.1764714%\n",
      "Epoch: 102\tTrain Loss: 2.2124094 \tVal Loss:2.1950380 \tTrain Acc: 27.11765% \tVal Acc: 25.7647064%\n",
      "Epoch: 103\tTrain Loss: 2.2004285 \tVal Loss:2.1646727 \tTrain Acc: 27.58824% \tVal Acc: 27.0588246%\n",
      "Validation Loss decreased from 2.177092 to 2.164673, saving the model weights\n",
      "Epoch: 104\tTrain Loss: 2.1832608 \tVal Loss:2.1610584 \tTrain Acc: 27.57353% \tVal Acc: 26.7058833%\n",
      "Validation Loss decreased from 2.164673 to 2.161058, saving the model weights\n",
      "Epoch: 105\tTrain Loss: 2.1846935 \tVal Loss:2.1679203 \tTrain Acc: 28.63235% \tVal Acc: 27.1764715%\n",
      "Epoch: 106\tTrain Loss: 2.1999004 \tVal Loss:2.1634011 \tTrain Acc: 27.19118% \tVal Acc: 28.1764717%\n",
      "Epoch: 107\tTrain Loss: 2.1818785 \tVal Loss:2.1265399 \tTrain Acc: 27.58824% \tVal Acc: 28.6470598%\n",
      "Validation Loss decreased from 2.161058 to 2.126540, saving the model weights\n",
      "Epoch: 108\tTrain Loss: 2.1617574 \tVal Loss:2.1315571 \tTrain Acc: 28.35294% \tVal Acc: 27.1176480%\n",
      "Epoch: 109\tTrain Loss: 2.1605356 \tVal Loss:2.1264268 \tTrain Acc: 28.20588% \tVal Acc: 27.7058834%\n",
      "Validation Loss decreased from 2.126540 to 2.126427, saving the model weights\n",
      "Epoch: 110\tTrain Loss: 2.1752684 \tVal Loss:2.1131918 \tTrain Acc: 27.64706% \tVal Acc: 28.7647069%\n",
      "Validation Loss decreased from 2.126427 to 2.113192, saving the model weights\n",
      "Epoch: 111\tTrain Loss: 2.1586002 \tVal Loss:2.0847730 \tTrain Acc: 28.02941% \tVal Acc: 30.8235304%\n",
      "Validation Loss decreased from 2.113192 to 2.084773, saving the model weights\n",
      "Epoch: 112\tTrain Loss: 2.1416092 \tVal Loss:2.0776226 \tTrain Acc: 29.63235% \tVal Acc: 30.3529423%\n",
      "Validation Loss decreased from 2.084773 to 2.077623, saving the model weights\n",
      "Epoch: 113\tTrain Loss: 2.1479237 \tVal Loss:2.0929632 \tTrain Acc: 28.58824% \tVal Acc: 29.1176480%\n",
      "Epoch: 114\tTrain Loss: 2.1405324 \tVal Loss:2.0620183 \tTrain Acc: 29.36765% \tVal Acc: 31.4705892%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased from 2.077623 to 2.062018, saving the model weights\n",
      "Epoch: 115\tTrain Loss: 2.1362804 \tVal Loss:2.0760272 \tTrain Acc: 29.44118% \tVal Acc: 30.0000010%\n",
      "Epoch: 116\tTrain Loss: 2.1228648 \tVal Loss:2.1012772 \tTrain Acc: 29.75% \tVal Acc: 28.6470594%\n",
      "Epoch: 117\tTrain Loss: 2.0996368 \tVal Loss:2.0902095 \tTrain Acc: 30.88235% \tVal Acc: 29.2941184%\n",
      "Epoch: 118\tTrain Loss: 2.1206163 \tVal Loss:2.0577907 \tTrain Acc: 29.95588% \tVal Acc: 30.8235304%\n",
      "Validation Loss decreased from 2.062018 to 2.057791, saving the model weights\n",
      "Epoch: 119\tTrain Loss: 2.0887333 \tVal Loss:2.0695171 \tTrain Acc: 31.19118% \tVal Acc: 31.2941188%\n",
      "Epoch: 120\tTrain Loss: 2.0975074 \tVal Loss:2.0909365 \tTrain Acc: 30.33824% \tVal Acc: 30.0588246%\n",
      "Epoch: 121\tTrain Loss: 2.0828688 \tVal Loss:2.0675716 \tTrain Acc: 31.11765% \tVal Acc: 31.5294126%\n",
      "Epoch: 122\tTrain Loss: 2.0837955 \tVal Loss:2.1294379 \tTrain Acc: 31.47059% \tVal Acc: 27.9411773%\n",
      "Epoch: 123\tTrain Loss: 2.1034702 \tVal Loss:2.0936185 \tTrain Acc: 30.44118% \tVal Acc: 29.5882362%\n",
      "Epoch: 124\tTrain Loss: 2.0744972 \tVal Loss:2.1905544 \tTrain Acc: 31.41177% \tVal Acc: 25.2352947%\n",
      "Epoch: 125\tTrain Loss: 2.0679051 \tVal Loss:2.1256976 \tTrain Acc: 31.70588% \tVal Acc: 28.7647066%\n",
      "Epoch: 126\tTrain Loss: 2.0681841 \tVal Loss:2.1803988 \tTrain Acc: 30.97059% \tVal Acc: 26.7647067%\n",
      "Epoch: 127\tTrain Loss: 2.0779020 \tVal Loss:2.1390361 \tTrain Acc: 31.39706% \tVal Acc: 26.8235299%\n",
      "Epoch: 128\tTrain Loss: 2.0821173 \tVal Loss:2.1565616 \tTrain Acc: 30.9853% \tVal Acc: 25.2941182%\n",
      "Epoch: 129\tTrain Loss: 2.0985522 \tVal Loss:2.1532921 \tTrain Acc: 30.32353% \tVal Acc: 28.2352947%\n",
      "Epoch: 130\tTrain Loss: 2.1066703 \tVal Loss:2.1938491 \tTrain Acc: 30.33824% \tVal Acc: 26.7647064%\n",
      "Epoch: 131\tTrain Loss: 2.1768240 \tVal Loss:2.1877994 \tTrain Acc: 28.19118% \tVal Acc: 27.5882359%\n",
      "Epoch: 132\tTrain Loss: 2.1966986 \tVal Loss:2.1039346 \tTrain Acc: 27.35294% \tVal Acc: 30.0588244%\n",
      "Epoch: 133\tTrain Loss: 2.1738770 \tVal Loss:2.0081900 \tTrain Acc: 28.16177% \tVal Acc: 34.6470597%\n",
      "Validation Loss decreased from 2.057791 to 2.008190, saving the model weights\n",
      "Epoch: 134\tTrain Loss: 2.1117564 \tVal Loss:2.0029937 \tTrain Acc: 30.47059% \tVal Acc: 34.6470597%\n",
      "Validation Loss decreased from 2.008190 to 2.002994, saving the model weights\n",
      "Epoch: 135\tTrain Loss: 2.0473500 \tVal Loss:1.9882712 \tTrain Acc: 32.27941% \tVal Acc: 36.0588244%\n",
      "Validation Loss decreased from 2.002994 to 1.988271, saving the model weights\n",
      "Epoch: 136\tTrain Loss: 2.0258693 \tVal Loss:1.9515525 \tTrain Acc: 33.51471% \tVal Acc: 35.6470597%\n",
      "Validation Loss decreased from 1.988271 to 1.951553, saving the model weights\n",
      "Epoch: 137\tTrain Loss: 2.0005995 \tVal Loss:1.9261940 \tTrain Acc: 34.58824% \tVal Acc: 37.7647066%\n",
      "Validation Loss decreased from 1.951553 to 1.926194, saving the model weights\n",
      "Epoch: 138\tTrain Loss: 1.9778553 \tVal Loss:1.9293817 \tTrain Acc: 34.85294% \tVal Acc: 36.9411772%\n",
      "Epoch: 139\tTrain Loss: 1.9675372 \tVal Loss:1.9174939 \tTrain Acc: 35.20588% \tVal Acc: 39.0588242%\n",
      "Validation Loss decreased from 1.926194 to 1.917494, saving the model weights\n",
      "Epoch: 140\tTrain Loss: 1.9859723 \tVal Loss:1.9358289 \tTrain Acc: 35.32353% \tVal Acc: 37.6470596%\n",
      "Epoch: 141\tTrain Loss: 1.9901633 \tVal Loss:1.9383849 \tTrain Acc: 34.66177% \tVal Acc: 38.2352948%\n",
      "Epoch: 142\tTrain Loss: 1.9652686 \tVal Loss:1.8917887 \tTrain Acc: 35.01471% \tVal Acc: 38.6470595%\n",
      "Validation Loss decreased from 1.917494 to 1.891789, saving the model weights\n",
      "Epoch: 143\tTrain Loss: 1.9635034 \tVal Loss:1.9133943 \tTrain Acc: 35.25% \tVal Acc: 39.9411771%\n",
      "Epoch: 144\tTrain Loss: 1.9603260 \tVal Loss:1.8629585 \tTrain Acc: 34.67647% \tVal Acc: 39.6470594%\n",
      "Validation Loss decreased from 1.891789 to 1.862959, saving the model weights\n",
      "Epoch: 145\tTrain Loss: 1.9387778 \tVal Loss:1.8990481 \tTrain Acc: 36.35294% \tVal Acc: 38.7058830%\n",
      "Epoch: 146\tTrain Loss: 1.9539898 \tVal Loss:1.9370428 \tTrain Acc: 34.89706% \tVal Acc: 37.0588243%\n",
      "Epoch: 147\tTrain Loss: 1.9539221 \tVal Loss:1.9164717 \tTrain Acc: 35.7353% \tVal Acc: 38.8235301%\n",
      "Epoch: 148\tTrain Loss: 1.9444027 \tVal Loss:1.9517982 \tTrain Acc: 35.89706% \tVal Acc: 36.4117655%\n",
      "Epoch: 149\tTrain Loss: 1.9499130 \tVal Loss:1.9808077 \tTrain Acc: 35.07353% \tVal Acc: 35.0588244%\n",
      "Epoch: 150\tTrain Loss: 1.9339792 \tVal Loss:1.9392868 \tTrain Acc: 36.10294% \tVal Acc: 36.7058831%\n",
      "Epoch: 151\tTrain Loss: 1.9352556 \tVal Loss:1.8766601 \tTrain Acc: 35.60294% \tVal Acc: 38.4705889%\n",
      "Epoch: 152\tTrain Loss: 1.9062874 \tVal Loss:1.8496487 \tTrain Acc: 37.36765% \tVal Acc: 39.4705889%\n",
      "Validation Loss decreased from 1.862959 to 1.849649, saving the model weights\n",
      "Epoch: 153\tTrain Loss: 1.8896437 \tVal Loss:1.8154018 \tTrain Acc: 37.32353% \tVal Acc: 41.4117655%\n",
      "Validation Loss decreased from 1.849649 to 1.815402, saving the model weights\n",
      "Epoch: 154\tTrain Loss: 1.8680629 \tVal Loss:1.8149474 \tTrain Acc: 38.94118% \tVal Acc: 40.1764712%\n",
      "Validation Loss decreased from 1.815402 to 1.814947, saving the model weights\n",
      "Epoch: 155\tTrain Loss: 1.8504450 \tVal Loss:1.7969316 \tTrain Acc: 39.14706% \tVal Acc: 41.8823540%\n",
      "Validation Loss decreased from 1.814947 to 1.796932, saving the model weights\n",
      "Epoch: 156\tTrain Loss: 1.8604740 \tVal Loss:1.7814710 \tTrain Acc: 38.72059% \tVal Acc: 42.1176475%\n",
      "Validation Loss decreased from 1.796932 to 1.781471, saving the model weights\n",
      "Epoch: 157\tTrain Loss: 1.8507224 \tVal Loss:1.7743610 \tTrain Acc: 38.95588% \tVal Acc: 44.2941180%\n",
      "Validation Loss decreased from 1.781471 to 1.774361, saving the model weights\n",
      "Epoch: 158\tTrain Loss: 1.8140034 \tVal Loss:1.7130888 \tTrain Acc: 40.47059% \tVal Acc: 45.6470594%\n",
      "Validation Loss decreased from 1.774361 to 1.713089, saving the model weights\n",
      "Epoch: 159\tTrain Loss: 1.7964749 \tVal Loss:1.7371220 \tTrain Acc: 41.20588% \tVal Acc: 43.5294124%\n",
      "Epoch: 160\tTrain Loss: 1.8015329 \tVal Loss:1.6992090 \tTrain Acc: 41.22059% \tVal Acc: 46.7647070%\n",
      "Validation Loss decreased from 1.713089 to 1.699209, saving the model weights\n",
      "Epoch: 161\tTrain Loss: 1.7725877 \tVal Loss:1.6704240 \tTrain Acc: 41.88235% \tVal Acc: 47.0000008%\n",
      "Validation Loss decreased from 1.699209 to 1.670424, saving the model weights\n",
      "Epoch: 162\tTrain Loss: 1.7393079 \tVal Loss:1.6457801 \tTrain Acc: 43.36765% \tVal Acc: 48.1176481%\n",
      "Validation Loss decreased from 1.670424 to 1.645780, saving the model weights\n",
      "Epoch: 163\tTrain Loss: 1.7338560 \tVal Loss:1.6308293 \tTrain Acc: 43.16177% \tVal Acc: 47.7647063%\n",
      "Validation Loss decreased from 1.645780 to 1.630829, saving the model weights\n",
      "Epoch: 164\tTrain Loss: 1.7182096 \tVal Loss:1.6145517 \tTrain Acc: 43.85294% \tVal Acc: 49.2352945%\n",
      "Validation Loss decreased from 1.630829 to 1.614552, saving the model weights\n",
      "Epoch: 165\tTrain Loss: 1.7139051 \tVal Loss:1.5897109 \tTrain Acc: 43.79412% \tVal Acc: 50.4117656%\n",
      "Validation Loss decreased from 1.614552 to 1.589711, saving the model weights\n",
      "Epoch: 166\tTrain Loss: 1.7011323 \tVal Loss:1.6142030 \tTrain Acc: 44.39706% \tVal Acc: 48.2352951%\n",
      "Epoch: 167\tTrain Loss: 1.6961534 \tVal Loss:1.6041986 \tTrain Acc: 44.20588% \tVal Acc: 48.8235298%\n",
      "Epoch: 168\tTrain Loss: 1.7014202 \tVal Loss:1.5814977 \tTrain Acc: 44.44118% \tVal Acc: 50.5294120%\n",
      "Validation Loss decreased from 1.589711 to 1.581498, saving the model weights\n",
      "Epoch: 169\tTrain Loss: 1.6789504 \tVal Loss:1.5458869 \tTrain Acc: 44.75% \tVal Acc: 51.3529420%\n",
      "Validation Loss decreased from 1.581498 to 1.545887, saving the model weights\n",
      "Epoch: 170\tTrain Loss: 1.6424527 \tVal Loss:1.5332859 \tTrain Acc: 47.25% \tVal Acc: 50.0588247%\n",
      "Validation Loss decreased from 1.545887 to 1.533286, saving the model weights\n",
      "Epoch: 171\tTrain Loss: 1.6397604 \tVal Loss:1.4852541 \tTrain Acc: 47.33824% \tVal Acc: 53.2352945%\n",
      "Validation Loss decreased from 1.533286 to 1.485254, saving the model weights\n",
      "Epoch: 172\tTrain Loss: 1.6263810 \tVal Loss:1.5371168 \tTrain Acc: 46.54412% \tVal Acc: 49.9411777%\n",
      "Epoch: 173\tTrain Loss: 1.6331709 \tVal Loss:1.5034894 \tTrain Acc: 46.94118% \tVal Acc: 52.6470605%\n",
      "Epoch: 174\tTrain Loss: 1.6203465 \tVal Loss:1.4761157 \tTrain Acc: 47.11765% \tVal Acc: 54.8823541%\n",
      "Validation Loss decreased from 1.485254 to 1.476116, saving the model weights\n",
      "Epoch: 175\tTrain Loss: 1.6057883 \tVal Loss:1.4517048 \tTrain Acc: 47.48529% \tVal Acc: 53.8823530%\n",
      "Validation Loss decreased from 1.476116 to 1.451705, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 176\tTrain Loss: 1.6232459 \tVal Loss:1.4733730 \tTrain Acc: 47.16177% \tVal Acc: 54.1764709%\n",
      "Epoch: 177\tTrain Loss: 1.6004480 \tVal Loss:1.4083323 \tTrain Acc: 48.38235% \tVal Acc: 56.8823540%\n",
      "Validation Loss decreased from 1.451705 to 1.408332, saving the model weights\n",
      "Epoch: 178\tTrain Loss: 1.5890505 \tVal Loss:1.4231391 \tTrain Acc: 48.20588% \tVal Acc: 56.7647079%\n",
      "Epoch: 179\tTrain Loss: 1.5604677 \tVal Loss:1.3765319 \tTrain Acc: 49.63235% \tVal Acc: 56.6470593%\n",
      "Validation Loss decreased from 1.408332 to 1.376532, saving the model weights\n",
      "Epoch: 180\tTrain Loss: 1.5621078 \tVal Loss:1.3764940 \tTrain Acc: 48.98529% \tVal Acc: 57.1176475%\n",
      "Validation Loss decreased from 1.376532 to 1.376494, saving the model weights\n",
      "Epoch: 181\tTrain Loss: 1.5526067 \tVal Loss:1.4249021 \tTrain Acc: 49.04412% \tVal Acc: 56.2941182%\n",
      "Epoch: 182\tTrain Loss: 1.5859152 \tVal Loss:1.3543179 \tTrain Acc: 48.32353% \tVal Acc: 59.4117650%\n",
      "Validation Loss decreased from 1.376494 to 1.354318, saving the model weights\n",
      "Epoch: 183\tTrain Loss: 1.5292625 \tVal Loss:1.4318548 \tTrain Acc: 50.23529% \tVal Acc: 56.0000011%\n",
      "Epoch: 184\tTrain Loss: 1.5257465 \tVal Loss:1.3866401 \tTrain Acc: 51.0% \tVal Acc: 58.8235307%\n",
      "Epoch: 185\tTrain Loss: 1.5013291 \tVal Loss:1.3368043 \tTrain Acc: 51.58824% \tVal Acc: 60.8235297%\n",
      "Validation Loss decreased from 1.354318 to 1.336804, saving the model weights\n",
      "Epoch: 186\tTrain Loss: 1.4901908 \tVal Loss:1.2956797 \tTrain Acc: 52.26471% \tVal Acc: 61.5294129%\n",
      "Validation Loss decreased from 1.336804 to 1.295680, saving the model weights\n",
      "Epoch: 187\tTrain Loss: 1.4783175 \tVal Loss:1.3282948 \tTrain Acc: 51.98529% \tVal Acc: 57.7647069%\n",
      "Epoch: 188\tTrain Loss: 1.4782361 \tVal Loss:1.3136674 \tTrain Acc: 52.20588% \tVal Acc: 60.1764715%\n",
      "Epoch: 189\tTrain Loss: 1.4542810 \tVal Loss:1.3203533 \tTrain Acc: 52.54412% \tVal Acc: 60.2352950%\n",
      "Epoch: 190\tTrain Loss: 1.4444628 \tVal Loss:1.3766537 \tTrain Acc: 52.92647% \tVal Acc: 58.4705886%\n",
      "Epoch: 191\tTrain Loss: 1.4249116 \tVal Loss:1.3739267 \tTrain Acc: 53.79412% \tVal Acc: 56.8235302%\n",
      "Epoch: 192\tTrain Loss: 1.4105631 \tVal Loss:1.3001524 \tTrain Acc: 54.36765% \tVal Acc: 59.6470594%\n",
      "Epoch: 193\tTrain Loss: 1.4368042 \tVal Loss:1.4214045 \tTrain Acc: 53.57353% \tVal Acc: 54.9411771%\n",
      "Epoch: 194\tTrain Loss: 1.4213557 \tVal Loss:1.2364865 \tTrain Acc: 54.63235% \tVal Acc: 62.5294125%\n",
      "Validation Loss decreased from 1.295680 to 1.236487, saving the model weights\n",
      "Epoch: 195\tTrain Loss: 1.3652057 \tVal Loss:1.2829827 \tTrain Acc: 56.30882% \tVal Acc: 60.9411770%\n",
      "Epoch: 196\tTrain Loss: 1.3504293 \tVal Loss:1.3695398 \tTrain Acc: 56.36765% \tVal Acc: 57.5294131%\n",
      "Epoch: 197\tTrain Loss: 1.3448849 \tVal Loss:1.3898846 \tTrain Acc: 56.55882% \tVal Acc: 54.8823532%\n",
      "Epoch: 198\tTrain Loss: 1.3834850 \tVal Loss:1.3881348 \tTrain Acc: 54.98529% \tVal Acc: 55.7058832%\n",
      "Epoch: 199\tTrain Loss: 1.3657610 \tVal Loss:1.2758417 \tTrain Acc: 55.26471% \tVal Acc: 60.3529418%\n",
      "Epoch: 200\tTrain Loss: 1.3364049 \tVal Loss:1.2501947 \tTrain Acc: 56.94118% \tVal Acc: 61.0588247%\n",
      "Epoch: 201\tTrain Loss: 1.3167314 \tVal Loss:1.2036833 \tTrain Acc: 58.52941% \tVal Acc: 62.3529416%\n",
      "Validation Loss decreased from 1.236487 to 1.203683, saving the model weights\n",
      "Epoch: 202\tTrain Loss: 1.3155003 \tVal Loss:1.1774898 \tTrain Acc: 57.88235% \tVal Acc: 64.2352951%\n",
      "Validation Loss decreased from 1.203683 to 1.177490, saving the model weights\n",
      "Epoch: 203\tTrain Loss: 1.2832423 \tVal Loss:1.2850122 \tTrain Acc: 58.89706% \tVal Acc: 60.3529420%\n",
      "Epoch: 204\tTrain Loss: 1.3148424 \tVal Loss:1.2943903 \tTrain Acc: 57.42647% \tVal Acc: 58.1176475%\n",
      "Epoch: 205\tTrain Loss: 1.3123022 \tVal Loss:1.2036514 \tTrain Acc: 57.82353% \tVal Acc: 64.7647071%\n",
      "Epoch: 206\tTrain Loss: 1.3038938 \tVal Loss:1.1865793 \tTrain Acc: 58.02941% \tVal Acc: 62.9411775%\n",
      "Epoch: 207\tTrain Loss: 1.2936883 \tVal Loss:1.2014267 \tTrain Acc: 58.30882% \tVal Acc: 63.6470592%\n",
      "Epoch: 208\tTrain Loss: 1.2524443 \tVal Loss:1.0332161 \tTrain Acc: 60.10294% \tVal Acc: 69.1764712%\n",
      "Validation Loss decreased from 1.177490 to 1.033216, saving the model weights\n",
      "Epoch: 209\tTrain Loss: 1.2536045 \tVal Loss:1.1403418 \tTrain Acc: 59.89706% \tVal Acc: 65.6470585%\n",
      "Epoch: 210\tTrain Loss: 1.2445335 \tVal Loss:1.0909380 \tTrain Acc: 59.05882% \tVal Acc: 68.2352942%\n",
      "Epoch: 211\tTrain Loss: 1.2341109 \tVal Loss:1.0441101 \tTrain Acc: 60.45588% \tVal Acc: 68.6470586%\n",
      "Epoch: 212\tTrain Loss: 1.2336436 \tVal Loss:1.0746005 \tTrain Acc: 60.07353% \tVal Acc: 66.3529420%\n",
      "Epoch: 213\tTrain Loss: 1.2252990 \tVal Loss:1.0501528 \tTrain Acc: 60.4853% \tVal Acc: 69.4705886%\n",
      "Epoch: 214\tTrain Loss: 1.2180544 \tVal Loss:1.0760696 \tTrain Acc: 61.02941% \tVal Acc: 66.9411778%\n",
      "Epoch: 215\tTrain Loss: 1.2370149 \tVal Loss:1.0583717 \tTrain Acc: 59.86765% \tVal Acc: 68.4705889%\n",
      "Epoch: 216\tTrain Loss: 1.1931191 \tVal Loss:1.0001559 \tTrain Acc: 61.35294% \tVal Acc: 71.4705884%\n",
      "Validation Loss decreased from 1.033216 to 1.000156, saving the model weights\n",
      "Epoch: 217\tTrain Loss: 1.1714404 \tVal Loss:1.0621722 \tTrain Acc: 62.70588% \tVal Acc: 67.5882357%\n",
      "Epoch: 218\tTrain Loss: 1.1967227 \tVal Loss:1.1613379 \tTrain Acc: 61.60294% \tVal Acc: 64.4117653%\n",
      "Epoch: 219\tTrain Loss: 1.1886945 \tVal Loss:1.0120676 \tTrain Acc: 62.52941% \tVal Acc: 69.2352939%\n",
      "Epoch: 220\tTrain Loss: 1.1336635 \tVal Loss:0.9943356 \tTrain Acc: 63.77941% \tVal Acc: 70.2352953%\n",
      "Validation Loss decreased from 1.000156 to 0.994336, saving the model weights\n",
      "Epoch: 221\tTrain Loss: 1.1426766 \tVal Loss:1.0528781 \tTrain Acc: 63.29412% \tVal Acc: 69.1176474%\n",
      "Epoch: 222\tTrain Loss: 1.1427690 \tVal Loss:0.9649349 \tTrain Acc: 63.51471% \tVal Acc: 71.3529414%\n",
      "Validation Loss decreased from 0.994336 to 0.964935, saving the model weights\n",
      "Epoch: 223\tTrain Loss: 1.1114644 \tVal Loss:1.0881631 \tTrain Acc: 64.16177% \tVal Acc: 67.4705881%\n",
      "Epoch: 224\tTrain Loss: 1.0969666 \tVal Loss:1.0345511 \tTrain Acc: 65.10294% \tVal Acc: 68.1764704%\n",
      "Epoch: 225\tTrain Loss: 1.0785563 \tVal Loss:1.0487259 \tTrain Acc: 65.38235% \tVal Acc: 69.7058827%\n",
      "Epoch: 226\tTrain Loss: 1.0873803 \tVal Loss:0.9708940 \tTrain Acc: 65.54412% \tVal Acc: 71.2352943%\n",
      "Epoch: 227\tTrain Loss: 1.0762927 \tVal Loss:0.8585821 \tTrain Acc: 65.17647% \tVal Acc: 75.7647055%\n",
      "Validation Loss decreased from 0.964935 to 0.858582, saving the model weights\n",
      "Epoch: 228\tTrain Loss: 1.0522578 \tVal Loss:0.8937549 \tTrain Acc: 66.36765% \tVal Acc: 73.6470592%\n",
      "Epoch: 229\tTrain Loss: 1.0421405 \tVal Loss:0.7959099 \tTrain Acc: 66.60294% \tVal Acc: 77.3529416%\n",
      "Validation Loss decreased from 0.858582 to 0.795910, saving the model weights\n",
      "Epoch: 230\tTrain Loss: 1.0146588 \tVal Loss:0.8592922 \tTrain Acc: 67.52941% \tVal Acc: 74.2352939%\n",
      "Epoch: 231\tTrain Loss: 0.9802095 \tVal Loss:0.8687362 \tTrain Acc: 69.26471% \tVal Acc: 73.8235301%\n",
      "Epoch: 232\tTrain Loss: 0.9973439 \tVal Loss:0.8259794 \tTrain Acc: 67.63235% \tVal Acc: 76.6470587%\n",
      "Epoch: 233\tTrain Loss: 0.9860658 \tVal Loss:0.8002834 \tTrain Acc: 67.69118% \tVal Acc: 76.1176461%\n",
      "Epoch: 234\tTrain Loss: 0.9693424 \tVal Loss:0.7904827 \tTrain Acc: 68.95588% \tVal Acc: 76.8235290%\n",
      "Validation Loss decreased from 0.795910 to 0.790483, saving the model weights\n",
      "Epoch: 235\tTrain Loss: 0.9665049 \tVal Loss:0.8048465 \tTrain Acc: 69.36765% \tVal Acc: 77.4705881%\n",
      "Epoch: 236\tTrain Loss: 0.9707128 \tVal Loss:0.7945916 \tTrain Acc: 68.60294% \tVal Acc: 75.8235294%\n",
      "Epoch: 237\tTrain Loss: 0.9692719 \tVal Loss:0.7701026 \tTrain Acc: 68.88235% \tVal Acc: 77.7058822%\n",
      "Validation Loss decreased from 0.790483 to 0.770103, saving the model weights\n",
      "Epoch: 238\tTrain Loss: 0.9243514 \tVal Loss:0.7120840 \tTrain Acc: 70.30882% \tVal Acc: 79.6470582%\n",
      "Validation Loss decreased from 0.770103 to 0.712084, saving the model weights\n",
      "Epoch: 239\tTrain Loss: 0.9445243 \tVal Loss:0.7475956 \tTrain Acc: 70.69118% \tVal Acc: 78.6470586%\n",
      "Epoch: 240\tTrain Loss: 0.9084457 \tVal Loss:0.6775945 \tTrain Acc: 71.47059% \tVal Acc: 81.7058820%\n",
      "Validation Loss decreased from 0.712084 to 0.677594, saving the model weights\n",
      "Epoch: 241\tTrain Loss: 0.9024073 \tVal Loss:0.6332376 \tTrain Acc: 71.66176% \tVal Acc: 82.5882357%\n",
      "Validation Loss decreased from 0.677594 to 0.633238, saving the model weights\n",
      "Epoch: 242\tTrain Loss: 0.9280695 \tVal Loss:0.6116383 \tTrain Acc: 70.92647% \tVal Acc: 83.4117639%\n",
      "Validation Loss decreased from 0.633238 to 0.611638, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 243\tTrain Loss: 0.9137364 \tVal Loss:0.6426551 \tTrain Acc: 71.27941% \tVal Acc: 83.1764698%\n",
      "Epoch: 244\tTrain Loss: 0.9285711 \tVal Loss:0.6597871 \tTrain Acc: 70.22059% \tVal Acc: 81.7058814%\n",
      "Epoch: 245\tTrain Loss: 0.8892465 \tVal Loss:0.6947488 \tTrain Acc: 71.92647% \tVal Acc: 81.2352931%\n",
      "Epoch: 246\tTrain Loss: 0.8780314 \tVal Loss:0.7436654 \tTrain Acc: 72.08824% \tVal Acc: 78.6470592%\n",
      "Epoch: 247\tTrain Loss: 0.8708497 \tVal Loss:0.8776915 \tTrain Acc: 72.41177% \tVal Acc: 73.6470592%\n",
      "Epoch: 248\tTrain Loss: 0.8849610 \tVal Loss:0.6272041 \tTrain Acc: 71.32353% \tVal Acc: 83.5882342%\n",
      "Epoch: 249\tTrain Loss: 0.8393839 \tVal Loss:0.6309623 \tTrain Acc: 73.17647% \tVal Acc: 82.3529398%\n",
      "Epoch: 250\tTrain Loss: 0.8417565 \tVal Loss:0.6534748 \tTrain Acc: 72.57353% \tVal Acc: 80.9411764%\n",
      "Epoch: 251\tTrain Loss: 0.8353992 \tVal Loss:0.6027658 \tTrain Acc: 73.72059% \tVal Acc: 83.9999998%\n",
      "Validation Loss decreased from 0.611638 to 0.602766, saving the model weights\n",
      "Epoch: 252\tTrain Loss: 0.8086177 \tVal Loss:0.6136790 \tTrain Acc: 75.02941% \tVal Acc: 82.4117643%\n",
      "Epoch: 253\tTrain Loss: 0.7909505 \tVal Loss:0.6444208 \tTrain Acc: 74.63235% \tVal Acc: 82.5294113%\n",
      "Epoch: 254\tTrain Loss: 0.8168636 \tVal Loss:0.6513623 \tTrain Acc: 73.94118% \tVal Acc: 80.7647061%\n",
      "Epoch: 255\tTrain Loss: 0.8296738 \tVal Loss:0.6752645 \tTrain Acc: 73.48529% \tVal Acc: 81.3529408%\n",
      "Epoch: 256\tTrain Loss: 0.8116549 \tVal Loss:0.5386050 \tTrain Acc: 74.19118% \tVal Acc: 85.4117644%\n",
      "Validation Loss decreased from 0.602766 to 0.538605, saving the model weights\n",
      "Epoch: 257\tTrain Loss: 0.7966253 \tVal Loss:0.5559980 \tTrain Acc: 74.42647% \tVal Acc: 85.1764697%\n",
      "Epoch: 258\tTrain Loss: 0.8180666 \tVal Loss:0.5587447 \tTrain Acc: 74.51471% \tVal Acc: 84.4117635%\n",
      "Epoch: 259\tTrain Loss: 0.7960467 \tVal Loss:0.5013456 \tTrain Acc: 74.97059% \tVal Acc: 86.1764699%\n",
      "Validation Loss decreased from 0.538605 to 0.501346, saving the model weights\n",
      "Epoch: 260\tTrain Loss: 0.7621692 \tVal Loss:0.5039279 \tTrain Acc: 76.55882% \tVal Acc: 86.2941164%\n",
      "Epoch: 261\tTrain Loss: 0.7579523 \tVal Loss:0.4646709 \tTrain Acc: 75.85294% \tVal Acc: 88.2941169%\n",
      "Validation Loss decreased from 0.501346 to 0.464671, saving the model weights\n",
      "Epoch: 262\tTrain Loss: 0.7566896 \tVal Loss:0.4561670 \tTrain Acc: 76.11765% \tVal Acc: 87.7647054%\n",
      "Validation Loss decreased from 0.464671 to 0.456167, saving the model weights\n",
      "Epoch: 263\tTrain Loss: 0.7680420 \tVal Loss:0.5036888 \tTrain Acc: 75.94118% \tVal Acc: 86.5294111%\n",
      "Epoch: 264\tTrain Loss: 0.8013755 \tVal Loss:0.5292362 \tTrain Acc: 74.70588% \tVal Acc: 85.4117632%\n",
      "Epoch: 265\tTrain Loss: 0.7699863 \tVal Loss:0.5630452 \tTrain Acc: 75.97059% \tVal Acc: 83.7647057%\n",
      "Epoch: 266\tTrain Loss: 0.7488996 \tVal Loss:0.4901023 \tTrain Acc: 76.20588% \tVal Acc: 87.0588231%\n",
      "Epoch: 267\tTrain Loss: 0.7683368 \tVal Loss:0.5053591 \tTrain Acc: 75.51471% \tVal Acc: 86.4117640%\n",
      "Epoch: 268\tTrain Loss: 0.7755450 \tVal Loss:0.4452830 \tTrain Acc: 75.91176% \tVal Acc: 88.2352936%\n",
      "Validation Loss decreased from 0.456167 to 0.445283, saving the model weights\n",
      "Epoch: 269\tTrain Loss: 0.7607193 \tVal Loss:0.4217531 \tTrain Acc: 75.79412% \tVal Acc: 88.3529407%\n",
      "Validation Loss decreased from 0.445283 to 0.421753, saving the model weights\n",
      "Epoch: 270\tTrain Loss: 0.7383051 \tVal Loss:0.4119261 \tTrain Acc: 76.63235% \tVal Acc: 88.9411759%\n",
      "Validation Loss decreased from 0.421753 to 0.411926, saving the model weights\n",
      "Epoch: 271\tTrain Loss: 0.7276249 \tVal Loss:0.4339766 \tTrain Acc: 77.41176% \tVal Acc: 88.8823521%\n",
      "Epoch: 272\tTrain Loss: 0.7539580 \tVal Loss:0.4340653 \tTrain Acc: 75.97059% \tVal Acc: 88.1176460%\n",
      "Epoch: 273\tTrain Loss: 0.7645392 \tVal Loss:0.4271212 \tTrain Acc: 75.80882% \tVal Acc: 89.5882344%\n",
      "Epoch: 274\tTrain Loss: 0.7700151 \tVal Loss:0.4215022 \tTrain Acc: 75.67647% \tVal Acc: 88.2352936%\n",
      "Epoch: 275\tTrain Loss: 0.7383038 \tVal Loss:0.4286631 \tTrain Acc: 76.79412% \tVal Acc: 88.4705871%\n",
      "Epoch: 276\tTrain Loss: 0.7455573 \tVal Loss:0.4552333 \tTrain Acc: 77.04412% \tVal Acc: 87.5882351%\n",
      "Epoch: 277\tTrain Loss: 0.7463372 \tVal Loss:0.4615747 \tTrain Acc: 76.35294% \tVal Acc: 86.8235290%\n",
      "Epoch: 278\tTrain Loss: 0.7788371 \tVal Loss:0.4313348 \tTrain Acc: 75.45588% \tVal Acc: 87.9411769%\n",
      "Epoch: 279\tTrain Loss: 0.7752421 \tVal Loss:0.5278946 \tTrain Acc: 75.27941% \tVal Acc: 85.1764709%\n",
      "Epoch: 280\tTrain Loss: 0.7781651 \tVal Loss:0.4877238 \tTrain Acc: 75.33824% \tVal Acc: 85.6470585%\n",
      "Epoch: 281\tTrain Loss: 0.7417241 \tVal Loss:0.4352405 \tTrain Acc: 76.10294% \tVal Acc: 87.9411757%\n",
      "Epoch: 282\tTrain Loss: 0.7406647 \tVal Loss:0.5432618 \tTrain Acc: 76.91176% \tVal Acc: 84.5882344%\n",
      "Epoch: 283\tTrain Loss: 0.7464878 \tVal Loss:0.5938425 \tTrain Acc: 76.30882% \tVal Acc: 82.2352940%\n",
      "Epoch: 284\tTrain Loss: 0.7674848 \tVal Loss:0.5391355 \tTrain Acc: 75.55882% \tVal Acc: 84.3529403%\n",
      "Epoch: 285\tTrain Loss: 0.7327184 \tVal Loss:0.4247431 \tTrain Acc: 76.91176% \tVal Acc: 88.1764692%\n",
      "Epoch: 286\tTrain Loss: 0.7405043 \tVal Loss:0.5738344 \tTrain Acc: 76.60294% \tVal Acc: 82.7647048%\n",
      "Epoch: 287\tTrain Loss: 0.7239798 \tVal Loss:0.7025092 \tTrain Acc: 77.35294% \tVal Acc: 79.7647053%\n",
      "Epoch: 288\tTrain Loss: 0.7438475 \tVal Loss:0.4507824 \tTrain Acc: 76.02941% \tVal Acc: 87.2941160%\n",
      "Epoch: 289\tTrain Loss: 0.7021804 \tVal Loss:0.5512920 \tTrain Acc: 77.76471% \tVal Acc: 83.8235301%\n",
      "Epoch: 290\tTrain Loss: 0.6820814 \tVal Loss:0.6221098 \tTrain Acc: 78.85294% \tVal Acc: 81.5294117%\n",
      "Epoch: 291\tTrain Loss: 0.6786815 \tVal Loss:0.6365786 \tTrain Acc: 78.5% \tVal Acc: 80.9999996%\n",
      "Epoch: 292\tTrain Loss: 0.6554614 \tVal Loss:0.5124977 \tTrain Acc: 78.97059% \tVal Acc: 85.4117644%\n",
      "Epoch: 293\tTrain Loss: 0.6360820 \tVal Loss:0.4933611 \tTrain Acc: 79.86765% \tVal Acc: 85.4705876%\n",
      "Epoch: 294\tTrain Loss: 0.6464072 \tVal Loss:0.3523114 \tTrain Acc: 79.75% \tVal Acc: 90.4117644%\n",
      "Validation Loss decreased from 0.411926 to 0.352311, saving the model weights\n",
      "Epoch: 295\tTrain Loss: 0.6118539 \tVal Loss:0.3072227 \tTrain Acc: 80.85294% \tVal Acc: 92.1176463%\n",
      "Validation Loss decreased from 0.352311 to 0.307223, saving the model weights\n",
      "Epoch: 296\tTrain Loss: 0.5828198 \tVal Loss:0.2601286 \tTrain Acc: 81.70588% \tVal Acc: 94.3529403%\n",
      "Validation Loss decreased from 0.307223 to 0.260129, saving the model weights\n",
      "Epoch: 297\tTrain Loss: 0.5516855 \tVal Loss:0.2623125 \tTrain Acc: 83.05882% \tVal Acc: 93.7058806%\n",
      "Epoch: 298\tTrain Loss: 0.5444698 \tVal Loss:0.2527209 \tTrain Acc: 83.26471% \tVal Acc: 93.8823515%\n",
      "Validation Loss decreased from 0.260129 to 0.252721, saving the model weights\n",
      "Epoch: 299\tTrain Loss: 0.5338613 \tVal Loss:0.2339857 \tTrain Acc: 83.14706% \tVal Acc: 95.0588220%\n",
      "Validation Loss decreased from 0.252721 to 0.233986, saving the model weights\n",
      "Epoch: 300\tTrain Loss: 0.5170083 \tVal Loss:0.2465846 \tTrain Acc: 84.17647% \tVal Acc: 94.0588224%\n",
      "Epoch: 301\tTrain Loss: 0.5094139 \tVal Loss:0.2189044 \tTrain Acc: 84.26471% \tVal Acc: 94.4705862%\n",
      "Validation Loss decreased from 0.233986 to 0.218904, saving the model weights\n",
      "Epoch: 302\tTrain Loss: 0.4837133 \tVal Loss:0.2020367 \tTrain Acc: 84.89706% \tVal Acc: 95.5882335%\n",
      "Validation Loss decreased from 0.218904 to 0.202037, saving the model weights\n",
      "Epoch: 303\tTrain Loss: 0.4705123 \tVal Loss:0.2009742 \tTrain Acc: 85.61765% \tVal Acc: 95.4705876%\n",
      "Validation Loss decreased from 0.202037 to 0.200974, saving the model weights\n",
      "Epoch: 304\tTrain Loss: 0.4683032 \tVal Loss:0.2083412 \tTrain Acc: 86.22059% \tVal Acc: 94.9411750%\n",
      "Epoch: 305\tTrain Loss: 0.4689798 \tVal Loss:0.2327173 \tTrain Acc: 85.92647% \tVal Acc: 93.9999986%\n",
      "Epoch: 306\tTrain Loss: 0.4896448 \tVal Loss:0.2108846 \tTrain Acc: 84.55882% \tVal Acc: 94.8235285%\n",
      "Epoch: 307\tTrain Loss: 0.4609367 \tVal Loss:0.1996793 \tTrain Acc: 85.72059% \tVal Acc: 95.6470579%\n",
      "Validation Loss decreased from 0.200974 to 0.199679, saving the model weights\n",
      "Epoch: 308\tTrain Loss: 0.4636847 \tVal Loss:0.2007232 \tTrain Acc: 85.45588% \tVal Acc: 94.3529403%\n",
      "Epoch: 309\tTrain Loss: 0.4585000 \tVal Loss:0.1748479 \tTrain Acc: 86.0% \tVal Acc: 96.1176455%\n",
      "Validation Loss decreased from 0.199679 to 0.174848, saving the model weights\n",
      "Epoch: 310\tTrain Loss: 0.4465612 \tVal Loss:0.1847379 \tTrain Acc: 86.52941% \tVal Acc: 95.7647043%\n",
      "Epoch: 311\tTrain Loss: 0.4539660 \tVal Loss:0.1697925 \tTrain Acc: 85.77941% \tVal Acc: 96.4117640%\n",
      "Validation Loss decreased from 0.174848 to 0.169793, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 312\tTrain Loss: 0.4479332 \tVal Loss:0.1842815 \tTrain Acc: 86.16176% \tVal Acc: 95.7647043%\n",
      "Epoch: 313\tTrain Loss: 0.4360884 \tVal Loss:0.1687448 \tTrain Acc: 86.5147% \tVal Acc: 96.3529396%\n",
      "Validation Loss decreased from 0.169793 to 0.168745, saving the model weights\n",
      "Epoch: 314\tTrain Loss: 0.4381494 \tVal Loss:0.1629275 \tTrain Acc: 86.52941% \tVal Acc: 96.4705867%\n",
      "Validation Loss decreased from 0.168745 to 0.162928, saving the model weights\n",
      "Epoch: 315\tTrain Loss: 0.4093408 \tVal Loss:0.1735010 \tTrain Acc: 87.67647% \tVal Acc: 95.8235288%\n",
      "Epoch: 316\tTrain Loss: 0.4247738 \tVal Loss:0.1652310 \tTrain Acc: 86.97059% \tVal Acc: 96.0588223%\n",
      "Epoch: 317\tTrain Loss: 0.4314558 \tVal Loss:0.1914353 \tTrain Acc: 87.01471% \tVal Acc: 95.2352923%\n",
      "Epoch: 318\tTrain Loss: 0.4357536 \tVal Loss:0.1770897 \tTrain Acc: 86.44118% \tVal Acc: 95.5294096%\n",
      "Epoch: 319\tTrain Loss: 0.4261527 \tVal Loss:0.1877720 \tTrain Acc: 86.77941% \tVal Acc: 94.9411762%\n",
      "Epoch: 320\tTrain Loss: 0.4394156 \tVal Loss:0.1899088 \tTrain Acc: 86.02941% \tVal Acc: 95.1176465%\n",
      "Epoch: 321\tTrain Loss: 0.4183589 \tVal Loss:0.1824910 \tTrain Acc: 86.91176% \tVal Acc: 94.9999982%\n",
      "Epoch: 322\tTrain Loss: 0.4255960 \tVal Loss:0.1626719 \tTrain Acc: 86.79412% \tVal Acc: 95.9999990%\n",
      "Validation Loss decreased from 0.162928 to 0.162672, saving the model weights\n",
      "Epoch: 323\tTrain Loss: 0.4049220 \tVal Loss:0.1780942 \tTrain Acc: 87.36765% \tVal Acc: 96.4117634%\n",
      "Epoch: 324\tTrain Loss: 0.4065761 \tVal Loss:0.1978671 \tTrain Acc: 87.19118% \tVal Acc: 94.8235279%\n",
      "Epoch: 325\tTrain Loss: 0.4160671 \tVal Loss:0.1718234 \tTrain Acc: 87.30882% \tVal Acc: 95.9411746%\n",
      "Epoch: 326\tTrain Loss: 0.4248404 \tVal Loss:0.1797064 \tTrain Acc: 86.75% \tVal Acc: 95.2941161%\n",
      "Epoch: 327\tTrain Loss: 0.4442845 \tVal Loss:0.1553420 \tTrain Acc: 85.73529% \tVal Acc: 96.0588217%\n",
      "Validation Loss decreased from 0.162672 to 0.155342, saving the model weights\n",
      "Epoch: 328\tTrain Loss: 0.3884306 \tVal Loss:0.1487589 \tTrain Acc: 88.04412% \tVal Acc: 96.6470575%\n",
      "Validation Loss decreased from 0.155342 to 0.148759, saving the model weights\n",
      "Epoch: 329\tTrain Loss: 0.3836570 \tVal Loss:0.1396270 \tTrain Acc: 88.11765% \tVal Acc: 96.8235284%\n",
      "Validation Loss decreased from 0.148759 to 0.139627, saving the model weights\n",
      "Epoch: 330\tTrain Loss: 0.3897155 \tVal Loss:0.1482339 \tTrain Acc: 87.72059% \tVal Acc: 96.8823510%\n",
      "Epoch: 331\tTrain Loss: 0.3814596 \tVal Loss:0.1557276 \tTrain Acc: 88.54412% \tVal Acc: 96.5882343%\n",
      "Epoch: 332\tTrain Loss: 0.3924591 \tVal Loss:0.1827262 \tTrain Acc: 87.72059% \tVal Acc: 95.7647049%\n",
      "Epoch: 333\tTrain Loss: 0.3889223 \tVal Loss:0.1539040 \tTrain Acc: 87.60294% \tVal Acc: 96.5882343%\n",
      "Epoch: 334\tTrain Loss: 0.3892572 \tVal Loss:0.1309100 \tTrain Acc: 87.57353% \tVal Acc: 96.9999987%\n",
      "Validation Loss decreased from 0.139627 to 0.130910, saving the model weights\n",
      "Epoch: 335\tTrain Loss: 0.3952937 \tVal Loss:0.1313071 \tTrain Acc: 87.66176% \tVal Acc: 97.5294101%\n",
      "Epoch: 336\tTrain Loss: 0.3841252 \tVal Loss:0.1364999 \tTrain Acc: 88.23529% \tVal Acc: 96.8823516%\n",
      "Epoch: 337\tTrain Loss: 0.3830356 \tVal Loss:0.1324076 \tTrain Acc: 87.80882% \tVal Acc: 96.9999993%\n",
      "Epoch: 338\tTrain Loss: 0.3614821 \tVal Loss:0.1195710 \tTrain Acc: 88.80882% \tVal Acc: 97.3529404%\n",
      "Validation Loss decreased from 0.130910 to 0.119571, saving the model weights\n",
      "Epoch: 339\tTrain Loss: 0.3574314 \tVal Loss:0.1331481 \tTrain Acc: 89.20588% \tVal Acc: 96.7058802%\n",
      "Epoch: 340\tTrain Loss: 0.3485897 \tVal Loss:0.1128871 \tTrain Acc: 89.27941% \tVal Acc: 97.3529398%\n",
      "Validation Loss decreased from 0.119571 to 0.112887, saving the model weights\n",
      "Epoch: 341\tTrain Loss: 0.3421906 \tVal Loss:0.1173541 \tTrain Acc: 89.36765% \tVal Acc: 96.9999993%\n",
      "Epoch: 342\tTrain Loss: 0.3389385 \tVal Loss:0.1012661 \tTrain Acc: 89.51471% \tVal Acc: 98.2352930%\n",
      "Validation Loss decreased from 0.112887 to 0.101266, saving the model weights\n",
      "Epoch: 343\tTrain Loss: 0.3290026 \tVal Loss:0.1046227 \tTrain Acc: 89.69118% \tVal Acc: 97.7647048%\n",
      "Epoch: 344\tTrain Loss: 0.3260309 \tVal Loss:0.1004592 \tTrain Acc: 89.86765% \tVal Acc: 97.5882339%\n",
      "Validation Loss decreased from 0.101266 to 0.100459, saving the model weights\n",
      "Epoch: 345\tTrain Loss: 0.3205660 \tVal Loss:0.0939576 \tTrain Acc: 90.29412% \tVal Acc: 97.5882334%\n",
      "Validation Loss decreased from 0.100459 to 0.093958, saving the model weights\n",
      "Epoch: 346\tTrain Loss: 0.3177826 \tVal Loss:0.0920069 \tTrain Acc: 90.41176% \tVal Acc: 97.8235281%\n",
      "Validation Loss decreased from 0.093958 to 0.092007, saving the model weights\n",
      "Epoch: 347\tTrain Loss: 0.3175542 \tVal Loss:0.0950805 \tTrain Acc: 89.95588% \tVal Acc: 97.7647054%\n",
      "Epoch: 348\tTrain Loss: 0.3298077 \tVal Loss:0.0947364 \tTrain Acc: 89.89706% \tVal Acc: 98.1176454%\n",
      "Epoch: 349\tTrain Loss: 0.3385986 \tVal Loss:0.0989862 \tTrain Acc: 89.48529% \tVal Acc: 97.9999983%\n",
      "Epoch: 350\tTrain Loss: 0.3141995 \tVal Loss:0.1004435 \tTrain Acc: 90.35294% \tVal Acc: 97.7058804%\n",
      "Epoch: 351\tTrain Loss: 0.2944880 \tVal Loss:0.0893775 \tTrain Acc: 91.17647% \tVal Acc: 97.9411751%\n",
      "Validation Loss decreased from 0.092007 to 0.089377, saving the model weights\n",
      "Epoch: 352\tTrain Loss: 0.3113200 \tVal Loss:0.0924789 \tTrain Acc: 90.33823% \tVal Acc: 97.4705869%\n",
      "Epoch: 353\tTrain Loss: 0.3239569 \tVal Loss:0.1139347 \tTrain Acc: 89.70588% \tVal Acc: 97.0588231%\n",
      "Epoch: 354\tTrain Loss: 0.3576652 \tVal Loss:0.1268627 \tTrain Acc: 89.20588% \tVal Acc: 96.8235272%\n",
      "Epoch: 355\tTrain Loss: 0.3695048 \tVal Loss:0.1419773 \tTrain Acc: 88.41176% \tVal Acc: 96.0588223%\n",
      "Epoch: 356\tTrain Loss: 0.3976063 \tVal Loss:0.1571943 \tTrain Acc: 87.77941% \tVal Acc: 96.3529402%\n",
      "Epoch: 357\tTrain Loss: 0.3948722 \tVal Loss:0.1710139 \tTrain Acc: 87.45588% \tVal Acc: 95.7058805%\n",
      "Epoch: 358\tTrain Loss: 0.3676292 \tVal Loss:0.1343523 \tTrain Acc: 88.04412% \tVal Acc: 96.8823522%\n",
      "Epoch: 359\tTrain Loss: 0.3683785 \tVal Loss:0.1376839 \tTrain Acc: 88.72059% \tVal Acc: 96.3529396%\n",
      "Epoch: 360\tTrain Loss: 0.3534904 \tVal Loss:0.1110987 \tTrain Acc: 88.91176% \tVal Acc: 96.9999993%\n",
      "Epoch: 361\tTrain Loss: 0.3330610 \tVal Loss:0.1001429 \tTrain Acc: 89.61765% \tVal Acc: 97.6470572%\n",
      "Epoch: 362\tTrain Loss: 0.2851924 \tVal Loss:0.0859890 \tTrain Acc: 91.30882% \tVal Acc: 98.1764692%\n",
      "Validation Loss decreased from 0.089377 to 0.085989, saving the model weights\n",
      "Epoch: 363\tTrain Loss: 0.2730355 \tVal Loss:0.0671559 \tTrain Acc: 91.61765% \tVal Acc: 98.5294098%\n",
      "Validation Loss decreased from 0.085989 to 0.067156, saving the model weights\n",
      "Epoch: 364\tTrain Loss: 0.2640979 \tVal Loss:0.0589286 \tTrain Acc: 92.42647% \tVal Acc: 98.8235283%\n",
      "Validation Loss decreased from 0.067156 to 0.058929, saving the model weights\n",
      "Epoch: 365\tTrain Loss: 0.2611384 \tVal Loss:0.0588081 \tTrain Acc: 92.39706% \tVal Acc: 98.7647051%\n",
      "Validation Loss decreased from 0.058929 to 0.058808, saving the model weights\n",
      "Epoch: 366\tTrain Loss: 0.2508308 \tVal Loss:0.0615928 \tTrain Acc: 92.61765% \tVal Acc: 98.8235283%\n",
      "Epoch: 367\tTrain Loss: 0.2523701 \tVal Loss:0.0625370 \tTrain Acc: 92.17647% \tVal Acc: 98.5882336%\n",
      "Epoch: 368\tTrain Loss: 0.2512425 \tVal Loss:0.0557215 \tTrain Acc: 92.70588% \tVal Acc: 99.0588224%\n",
      "Validation Loss decreased from 0.058808 to 0.055721, saving the model weights\n",
      "Epoch: 369\tTrain Loss: 0.2713398 \tVal Loss:0.0655270 \tTrain Acc: 92.0% \tVal Acc: 98.7647051%\n",
      "Epoch: 370\tTrain Loss: 0.2662986 \tVal Loss:0.0635048 \tTrain Acc: 91.88235% \tVal Acc: 98.9411753%\n",
      "Epoch: 371\tTrain Loss: 0.2955237 \tVal Loss:0.0811315 \tTrain Acc: 90.58823% \tVal Acc: 97.9999983%\n",
      "Epoch: 372\tTrain Loss: 0.3834498 \tVal Loss:0.1756746 \tTrain Acc: 87.38235% \tVal Acc: 94.9411756%\n",
      "Epoch: 373\tTrain Loss: 0.5471606 \tVal Loss:0.6257724 \tTrain Acc: 83.52941% \tVal Acc: 84.2352939%\n",
      "Epoch: 374\tTrain Loss: 1.0140630 \tVal Loss:0.6985379 \tTrain Acc: 70.70588% \tVal Acc: 80.4705882%\n",
      "Epoch: 375\tTrain Loss: 0.6950383 \tVal Loss:0.2342682 \tTrain Acc: 78.5% \tVal Acc: 94.0588218%\n",
      "Epoch: 376\tTrain Loss: 0.4325012 \tVal Loss:0.1606489 \tTrain Acc: 86.61765% \tVal Acc: 95.7058805%\n",
      "Epoch: 377\tTrain Loss: 0.3476242 \tVal Loss:0.0966809 \tTrain Acc: 89.14706% \tVal Acc: 97.9999983%\n",
      "Epoch: 378\tTrain Loss: 0.2886784 \tVal Loss:0.0791086 \tTrain Acc: 91.38235% \tVal Acc: 98.5294104%\n",
      "Epoch: 379\tTrain Loss: 0.2700528 \tVal Loss:0.0606249 \tTrain Acc: 91.60294% \tVal Acc: 98.9411753%\n",
      "Epoch: 380\tTrain Loss: 0.2508229 \tVal Loss:0.0574518 \tTrain Acc: 92.38235% \tVal Acc: 99.1176462%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 381\tTrain Loss: 0.2364316 \tVal Loss:0.0578845 \tTrain Acc: 93.13235% \tVal Acc: 98.9411753%\n",
      "Epoch: 382\tTrain Loss: 0.2310744 \tVal Loss:0.0487817 \tTrain Acc: 92.89706% \tVal Acc: 99.2352921%\n",
      "Validation Loss decreased from 0.055721 to 0.048782, saving the model weights\n",
      "Epoch: 383\tTrain Loss: 0.2221326 \tVal Loss:0.0462122 \tTrain Acc: 93.42647% \tVal Acc: 99.2941171%\n",
      "Validation Loss decreased from 0.048782 to 0.046212, saving the model weights\n",
      "Epoch: 384\tTrain Loss: 0.2228993 \tVal Loss:0.0461014 \tTrain Acc: 93.54412% \tVal Acc: 99.2941165%\n",
      "Validation Loss decreased from 0.046212 to 0.046101, saving the model weights\n",
      "Epoch: 385\tTrain Loss: 0.2218558 \tVal Loss:0.0415609 \tTrain Acc: 93.79412% \tVal Acc: 99.3529403%\n",
      "Validation Loss decreased from 0.046101 to 0.041561, saving the model weights\n",
      "Epoch: 386\tTrain Loss: 0.2041023 \tVal Loss:0.0401895 \tTrain Acc: 94.04412% \tVal Acc: 99.3529397%\n",
      "Validation Loss decreased from 0.041561 to 0.040189, saving the model weights\n",
      "Epoch: 387\tTrain Loss: 0.2064176 \tVal Loss:0.0392217 \tTrain Acc: 93.85294% \tVal Acc: 99.2941171%\n",
      "Validation Loss decreased from 0.040189 to 0.039222, saving the model weights\n",
      "Epoch: 388\tTrain Loss: 0.1958083 \tVal Loss:0.0379623 \tTrain Acc: 94.22059% \tVal Acc: 99.4117635%\n",
      "Validation Loss decreased from 0.039222 to 0.037962, saving the model weights\n",
      "Epoch: 389\tTrain Loss: 0.1839084 \tVal Loss:0.0377731 \tTrain Acc: 94.5147% \tVal Acc: 99.4117630%\n",
      "Validation Loss decreased from 0.037962 to 0.037773, saving the model weights\n",
      "Epoch: 390\tTrain Loss: 0.1907875 \tVal Loss:0.0392193 \tTrain Acc: 94.32353% \tVal Acc: 99.2941165%\n",
      "Epoch: 391\tTrain Loss: 0.1930283 \tVal Loss:0.0431456 \tTrain Acc: 94.38235% \tVal Acc: 99.2352927%\n",
      "Epoch: 392\tTrain Loss: 0.2008934 \tVal Loss:0.0352232 \tTrain Acc: 93.89706% \tVal Acc: 99.4117641%\n",
      "Validation Loss decreased from 0.037773 to 0.035223, saving the model weights\n",
      "Epoch: 393\tTrain Loss: 0.1863185 \tVal Loss:0.0379988 \tTrain Acc: 94.47059% \tVal Acc: 99.2941165%\n",
      "Epoch: 394\tTrain Loss: 0.1852003 \tVal Loss:0.0365759 \tTrain Acc: 94.44118% \tVal Acc: 99.3529397%\n",
      "Epoch: 395\tTrain Loss: 0.1958744 \tVal Loss:0.0368537 \tTrain Acc: 94.29412% \tVal Acc: 99.1764688%\n",
      "Epoch: 396\tTrain Loss: 0.1889437 \tVal Loss:0.0328922 \tTrain Acc: 94.38235% \tVal Acc: 99.4705868%\n",
      "Validation Loss decreased from 0.035223 to 0.032892, saving the model weights\n",
      "Epoch: 397\tTrain Loss: 0.1969868 \tVal Loss:0.0357442 \tTrain Acc: 93.80882% \tVal Acc: 99.3529397%\n",
      "Epoch: 398\tTrain Loss: 0.1997890 \tVal Loss:0.0420488 \tTrain Acc: 94.27941% \tVal Acc: 98.9411753%\n",
      "Epoch: 399\tTrain Loss: 0.2080268 \tVal Loss:0.0429643 \tTrain Acc: 93.67647% \tVal Acc: 99.2352927%\n",
      "Epoch: 400\tTrain Loss: 0.2211293 \tVal Loss:0.0399717 \tTrain Acc: 93.16176% \tVal Acc: 99.2352933%\n",
      "Epoch: 401\tTrain Loss: 0.2194381 \tVal Loss:0.0406094 \tTrain Acc: 93.35294% \tVal Acc: 99.4117635%\n",
      "Epoch: 402\tTrain Loss: 0.2257936 \tVal Loss:0.0416739 \tTrain Acc: 93.29412% \tVal Acc: 99.1176450%\n",
      "Epoch: 403\tTrain Loss: 0.2397205 \tVal Loss:0.0607706 \tTrain Acc: 92.51471% \tVal Acc: 98.5294104%\n",
      "Epoch: 404\tTrain Loss: 0.2264635 \tVal Loss:0.0559552 \tTrain Acc: 93.02941% \tVal Acc: 99.0588224%\n",
      "Epoch: 405\tTrain Loss: 0.2225901 \tVal Loss:0.0472979 \tTrain Acc: 92.92647% \tVal Acc: 98.9411753%\n",
      "Epoch: 406\tTrain Loss: 0.2362879 \tVal Loss:0.0545087 \tTrain Acc: 92.86765% \tVal Acc: 98.8235283%\n",
      "Epoch: 407\tTrain Loss: 0.2383632 \tVal Loss:0.0690873 \tTrain Acc: 92.38235% \tVal Acc: 98.4705871%\n",
      "Epoch: 408\tTrain Loss: 0.2374424 \tVal Loss:0.0521830 \tTrain Acc: 92.82353% \tVal Acc: 98.8823509%\n",
      "Epoch: 409\tTrain Loss: 0.2323650 \tVal Loss:0.0703537 \tTrain Acc: 92.67647% \tVal Acc: 98.1764692%\n",
      "Epoch: 410\tTrain Loss: 0.2429612 \tVal Loss:0.0597128 \tTrain Acc: 92.82353% \tVal Acc: 98.5294098%\n",
      "Epoch: 411\tTrain Loss: 0.2545082 \tVal Loss:0.0735426 \tTrain Acc: 92.41176% \tVal Acc: 98.1176454%\n",
      "Epoch: 412\tTrain Loss: 0.2840650 \tVal Loss:0.0639314 \tTrain Acc: 91.72059% \tVal Acc: 98.2352930%\n",
      "Epoch: 413\tTrain Loss: 0.2644363 \tVal Loss:0.0731158 \tTrain Acc: 91.55882% \tVal Acc: 98.4117633%\n",
      "Epoch: 414\tTrain Loss: 0.2337032 \tVal Loss:0.0423200 \tTrain Acc: 92.72059% \tVal Acc: 98.9999980%\n",
      "Epoch: 415\tTrain Loss: 0.1973231 \tVal Loss:0.0400255 \tTrain Acc: 94.23529% \tVal Acc: 99.1176456%\n",
      "Epoch: 416\tTrain Loss: 0.1857889 \tVal Loss:0.0578860 \tTrain Acc: 94.77941% \tVal Acc: 98.5882336%\n",
      "Epoch: 417\tTrain Loss: 0.2010303 \tVal Loss:0.0316573 \tTrain Acc: 93.80882% \tVal Acc: 99.4117630%\n",
      "Validation Loss decreased from 0.032892 to 0.031657, saving the model weights\n",
      "Epoch: 418\tTrain Loss: 0.1754491 \tVal Loss:0.0280780 \tTrain Acc: 95.0% \tVal Acc: 99.5294112%\n",
      "Validation Loss decreased from 0.031657 to 0.028078, saving the model weights\n",
      "Epoch: 419\tTrain Loss: 0.1680777 \tVal Loss:0.0358251 \tTrain Acc: 95.22059% \tVal Acc: 99.1764700%\n",
      "Epoch: 420\tTrain Loss: 0.1602408 \tVal Loss:0.0325161 \tTrain Acc: 95.10294% \tVal Acc: 99.4117635%\n",
      "Epoch: 421\tTrain Loss: 0.1742295 \tVal Loss:0.0276892 \tTrain Acc: 94.86765% \tVal Acc: 99.6470582%\n",
      "Validation Loss decreased from 0.028078 to 0.027689, saving the model weights\n",
      "Epoch: 422\tTrain Loss: 0.1781056 \tVal Loss:0.0306255 \tTrain Acc: 94.89706% \tVal Acc: 99.4705868%\n",
      "Epoch: 423\tTrain Loss: 0.2549098 \tVal Loss:0.1935224 \tTrain Acc: 92.72059% \tVal Acc: 95.5294108%\n",
      "Epoch: 424\tTrain Loss: 0.4925226 \tVal Loss:0.2405587 \tTrain Acc: 85.01471% \tVal Acc: 93.6470580%\n",
      "Epoch: 425\tTrain Loss: 0.5018414 \tVal Loss:0.2636811 \tTrain Acc: 84.52941% \tVal Acc: 92.0588225%\n",
      "Epoch: 426\tTrain Loss: 0.4352569 \tVal Loss:0.2019602 \tTrain Acc: 86.41176% \tVal Acc: 94.5294112%\n",
      "Epoch: 427\tTrain Loss: 0.3611986 \tVal Loss:0.0988203 \tTrain Acc: 88.55882% \tVal Acc: 97.4117631%\n",
      "Epoch: 428\tTrain Loss: 0.2637759 \tVal Loss:0.0633213 \tTrain Acc: 92.05882% \tVal Acc: 98.1176454%\n",
      "Epoch: 429\tTrain Loss: 0.2236365 \tVal Loss:0.0467934 \tTrain Acc: 93.22059% \tVal Acc: 99.1176456%\n",
      "Epoch: 430\tTrain Loss: 0.1913295 \tVal Loss:0.0309041 \tTrain Acc: 94.73529% \tVal Acc: 99.5294106%\n",
      "Epoch: 431\tTrain Loss: 0.1694953 \tVal Loss:0.0251360 \tTrain Acc: 94.86765% \tVal Acc: 99.4705874%\n",
      "Validation Loss decreased from 0.027689 to 0.025136, saving the model weights\n",
      "Epoch: 432\tTrain Loss: 0.1585173 \tVal Loss:0.0236319 \tTrain Acc: 95.42647% \tVal Acc: 99.5294112%\n",
      "Validation Loss decreased from 0.025136 to 0.023632, saving the model weights\n",
      "Epoch: 433\tTrain Loss: 0.1621287 \tVal Loss:0.0263769 \tTrain Acc: 95.22059% \tVal Acc: 99.4117641%\n",
      "Epoch: 434\tTrain Loss: 0.1606399 \tVal Loss:0.0235919 \tTrain Acc: 95.27941% \tVal Acc: 99.6470582%\n",
      "Validation Loss decreased from 0.023632 to 0.023592, saving the model weights\n",
      "Epoch: 435\tTrain Loss: 0.1528995 \tVal Loss:0.0243040 \tTrain Acc: 95.58823% \tVal Acc: 99.5882344%\n",
      "Epoch: 436\tTrain Loss: 0.1527872 \tVal Loss:0.0208170 \tTrain Acc: 95.64706% \tVal Acc: 99.5294112%\n",
      "Validation Loss decreased from 0.023592 to 0.020817, saving the model weights\n",
      "Epoch: 437\tTrain Loss: 0.1447799 \tVal Loss:0.0210385 \tTrain Acc: 95.72059% \tVal Acc: 99.5882344%\n",
      "Epoch: 438\tTrain Loss: 0.1331872 \tVal Loss:0.0225861 \tTrain Acc: 96.20588% \tVal Acc: 99.5294106%\n",
      "Epoch: 439\tTrain Loss: 0.1363567 \tVal Loss:0.0202991 \tTrain Acc: 96.20588% \tVal Acc: 99.6470582%\n",
      "Validation Loss decreased from 0.020817 to 0.020299, saving the model weights\n",
      "Epoch: 440\tTrain Loss: 0.1381100 \tVal Loss:0.0198272 \tTrain Acc: 95.92647% \tVal Acc: 99.5294106%\n",
      "Validation Loss decreased from 0.020299 to 0.019827, saving the model weights\n",
      "Epoch: 441\tTrain Loss: 0.1372164 \tVal Loss:0.0257731 \tTrain Acc: 96.07353% \tVal Acc: 99.2941171%\n",
      "Epoch: 442\tTrain Loss: 0.1450026 \tVal Loss:0.0258773 \tTrain Acc: 95.69118% \tVal Acc: 99.4117641%\n",
      "Epoch: 443\tTrain Loss: 0.1532968 \tVal Loss:0.0301962 \tTrain Acc: 95.33823% \tVal Acc: 99.2352933%\n",
      "Epoch: 444\tTrain Loss: 0.1453289 \tVal Loss:0.0187724 \tTrain Acc: 95.42647% \tVal Acc: 99.5882344%\n",
      "Validation Loss decreased from 0.019827 to 0.018772, saving the model weights\n",
      "Epoch: 445\tTrain Loss: 0.1334671 \tVal Loss:0.0284981 \tTrain Acc: 96.14706% \tVal Acc: 99.2941171%\n",
      "Epoch: 446\tTrain Loss: 0.1439920 \tVal Loss:0.0214713 \tTrain Acc: 95.66176% \tVal Acc: 99.5882344%\n",
      "Epoch: 447\tTrain Loss: 0.1640312 \tVal Loss:0.0294501 \tTrain Acc: 95.17647% \tVal Acc: 99.3529403%\n",
      "Epoch: 448\tTrain Loss: 0.1673131 \tVal Loss:0.0375446 \tTrain Acc: 94.91176% \tVal Acc: 99.1176462%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 449\tTrain Loss: 0.1617241 \tVal Loss:0.0433864 \tTrain Acc: 95.22059% \tVal Acc: 98.9411747%\n",
      "Epoch: 450\tTrain Loss: 0.1910868 \tVal Loss:0.0628820 \tTrain Acc: 94.42647% \tVal Acc: 98.7647045%\n",
      "Epoch: 451\tTrain Loss: 0.2038781 \tVal Loss:0.0413725 \tTrain Acc: 93.69118% \tVal Acc: 98.9999992%\n",
      "Epoch: 452\tTrain Loss: 0.2059736 \tVal Loss:0.0337959 \tTrain Acc: 93.72059% \tVal Acc: 99.1764694%\n",
      "Epoch: 453\tTrain Loss: 0.1894423 \tVal Loss:0.0326292 \tTrain Acc: 93.91176% \tVal Acc: 98.9999986%\n",
      "Epoch: 454\tTrain Loss: 0.1813995 \tVal Loss:0.0368283 \tTrain Acc: 94.36765% \tVal Acc: 99.1176462%\n",
      "Epoch: 455\tTrain Loss: 0.1834255 \tVal Loss:0.0399858 \tTrain Acc: 94.23529% \tVal Acc: 98.9411747%\n",
      "Epoch: 456\tTrain Loss: 0.2151810 \tVal Loss:0.0434600 \tTrain Acc: 93.29412% \tVal Acc: 99.1764700%\n",
      "Epoch: 457\tTrain Loss: 0.2270973 \tVal Loss:0.0441765 \tTrain Acc: 93.36765% \tVal Acc: 98.8823527%\n",
      "Epoch: 458\tTrain Loss: 0.2130835 \tVal Loss:0.0364071 \tTrain Acc: 94.17647% \tVal Acc: 98.9999992%\n",
      "Epoch: 459\tTrain Loss: 0.2494710 \tVal Loss:0.0619476 \tTrain Acc: 92.69118% \tVal Acc: 98.1764686%\n",
      "Epoch: 460\tTrain Loss: 0.2812675 \tVal Loss:0.1247798 \tTrain Acc: 91.30882% \tVal Acc: 96.6470575%\n",
      "Epoch: 461\tTrain Loss: 0.3799888 \tVal Loss:0.1246718 \tTrain Acc: 88.58823% \tVal Acc: 96.1764693%\n",
      "Epoch: 462\tTrain Loss: 0.3452184 \tVal Loss:0.1105579 \tTrain Acc: 89.27941% \tVal Acc: 96.7058808%\n",
      "Epoch: 463\tTrain Loss: 0.2775868 \tVal Loss:0.0568550 \tTrain Acc: 91.47059% \tVal Acc: 98.2941157%\n",
      "Epoch: 464\tTrain Loss: 0.2209293 \tVal Loss:0.0390191 \tTrain Acc: 93.10294% \tVal Acc: 99.1764700%\n",
      "Epoch: 465\tTrain Loss: 0.2041628 \tVal Loss:0.0359592 \tTrain Acc: 94.11765% \tVal Acc: 99.0588230%\n",
      "Epoch: 466\tTrain Loss: 0.1776938 \tVal Loss:0.0238081 \tTrain Acc: 94.89706% \tVal Acc: 99.5294106%\n",
      "Epoch: 467\tTrain Loss: 0.1520087 \tVal Loss:0.0198314 \tTrain Acc: 95.07353% \tVal Acc: 99.5882344%\n",
      "Epoch: 468\tTrain Loss: 0.1422094 \tVal Loss:0.0182510 \tTrain Acc: 95.83823% \tVal Acc: 99.5294112%\n",
      "Validation Loss decreased from 0.018772 to 0.018251, saving the model weights\n",
      "Epoch: 469\tTrain Loss: 0.1325335 \tVal Loss:0.0175478 \tTrain Acc: 96.30882% \tVal Acc: 99.4705874%\n",
      "Validation Loss decreased from 0.018251 to 0.017548, saving the model weights\n",
      "Epoch: 470\tTrain Loss: 0.1376360 \tVal Loss:0.0274245 \tTrain Acc: 95.82353% \tVal Acc: 99.3529403%\n",
      "Epoch: 471\tTrain Loss: 0.1367058 \tVal Loss:0.0812391 \tTrain Acc: 96.0147% \tVal Acc: 98.1764698%\n",
      "Epoch: 472\tTrain Loss: 0.1614627 \tVal Loss:0.0303617 \tTrain Acc: 95.14706% \tVal Acc: 99.4705874%\n",
      "Epoch: 473\tTrain Loss: 0.1394393 \tVal Loss:0.0322325 \tTrain Acc: 96.05882% \tVal Acc: 99.4117635%\n",
      "Epoch: 474\tTrain Loss: 0.1349589 \tVal Loss:0.0170318 \tTrain Acc: 95.97059% \tVal Acc: 99.5882344%\n",
      "Validation Loss decreased from 0.017548 to 0.017032, saving the model weights\n",
      "Epoch: 475\tTrain Loss: 0.1202780 \tVal Loss:0.0195145 \tTrain Acc: 96.52941% \tVal Acc: 99.4117635%\n",
      "Epoch: 476\tTrain Loss: 0.1195309 \tVal Loss:0.0164656 \tTrain Acc: 96.63235% \tVal Acc: 99.6470577%\n",
      "Validation Loss decreased from 0.017032 to 0.016466, saving the model weights\n",
      "Epoch: 477\tTrain Loss: 0.1210684 \tVal Loss:0.0152830 \tTrain Acc: 96.55882% \tVal Acc: 99.5294112%\n",
      "Validation Loss decreased from 0.016466 to 0.015283, saving the model weights\n",
      "Epoch: 478\tTrain Loss: 0.1167944 \tVal Loss:0.0216014 \tTrain Acc: 96.75% \tVal Acc: 99.3529403%\n",
      "Epoch: 479\tTrain Loss: 0.1078898 \tVal Loss:0.0173575 \tTrain Acc: 97.04412% \tVal Acc: 99.5882344%\n",
      "Epoch: 480\tTrain Loss: 0.1159698 \tVal Loss:0.0126095 \tTrain Acc: 96.91176% \tVal Acc: 99.7058815%\n",
      "Validation Loss decreased from 0.015283 to 0.012610, saving the model weights\n",
      "Epoch: 481\tTrain Loss: 0.1093389 \tVal Loss:0.0221067 \tTrain Acc: 96.94117% \tVal Acc: 99.4117635%\n",
      "Epoch: 482\tTrain Loss: 0.1055673 \tVal Loss:0.0157252 \tTrain Acc: 97.11765% \tVal Acc: 99.5294106%\n",
      "Epoch: 483\tTrain Loss: 0.1108464 \tVal Loss:0.0175618 \tTrain Acc: 96.73529% \tVal Acc: 99.4117641%\n",
      "Epoch: 484\tTrain Loss: 0.1051147 \tVal Loss:0.0132929 \tTrain Acc: 97.05882% \tVal Acc: 99.7058815%\n",
      "Epoch: 485\tTrain Loss: 0.1172003 \tVal Loss:0.0218336 \tTrain Acc: 96.66176% \tVal Acc: 99.2941165%\n",
      "Epoch: 486\tTrain Loss: 0.1437236 \tVal Loss:0.0300871 \tTrain Acc: 95.97059% \tVal Acc: 99.3529397%\n",
      "Epoch: 487\tTrain Loss: 0.1500152 \tVal Loss:0.0415558 \tTrain Acc: 95.60294% \tVal Acc: 99.0588224%\n",
      "Epoch: 488\tTrain Loss: 0.1499356 \tVal Loss:0.0358492 \tTrain Acc: 95.58823% \tVal Acc: 99.0588218%\n",
      "Epoch: 489\tTrain Loss: 0.1495349 \tVal Loss:0.0314528 \tTrain Acc: 95.54412% \tVal Acc: 99.1176456%\n",
      "Epoch: 490\tTrain Loss: 0.1590311 \tVal Loss:0.0307802 \tTrain Acc: 95.17647% \tVal Acc: 98.9411753%\n",
      "Epoch: 491\tTrain Loss: 0.1778355 \tVal Loss:0.0426765 \tTrain Acc: 94.79412% \tVal Acc: 98.7647045%\n",
      "Epoch: 492\tTrain Loss: 0.2581471 \tVal Loss:0.2173698 \tTrain Acc: 92.0147% \tVal Acc: 95.2352923%\n",
      "Epoch: 493\tTrain Loss: 0.3194881 \tVal Loss:0.1541327 \tTrain Acc: 90.44118% \tVal Acc: 95.3529394%\n",
      "Epoch: 494\tTrain Loss: 0.3335077 \tVal Loss:0.0991938 \tTrain Acc: 89.44118% \tVal Acc: 97.4705869%\n",
      "Epoch: 495\tTrain Loss: 0.3108793 \tVal Loss:0.1037477 \tTrain Acc: 90.54412% \tVal Acc: 97.1176463%\n",
      "Epoch: 496\tTrain Loss: 0.2585618 \tVal Loss:0.0324046 \tTrain Acc: 91.95588% \tVal Acc: 99.2352933%\n",
      "Epoch: 497\tTrain Loss: 0.1834874 \tVal Loss:0.0198198 \tTrain Acc: 94.30882% \tVal Acc: 99.6470582%\n",
      "Epoch: 498\tTrain Loss: 0.1487552 \tVal Loss:0.0189221 \tTrain Acc: 95.29412% \tVal Acc: 99.6470577%\n",
      "Epoch: 499\tTrain Loss: 0.1318534 \tVal Loss:0.0172202 \tTrain Acc: 96.02941% \tVal Acc: 99.5882350%\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        '''\n",
    "        Creating new variables for the hidden state, otherwise\n",
    "        we'd backprop through the entire training history\n",
    "        '''\n",
    "        \n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "       \n",
    "        # get the output from the model\n",
    "        output = model.forward(inputs, train_batch_size)\n",
    "        #print('OUTPUT', output)\n",
    "        \n",
    "        \n",
    "        #print('Labels Shape :-', (torch.max(labels, 1)[1]).shape)\n",
    "    \n",
    "        # calculate the loss and perform backprop\n",
    "        #print('Labels Long :-', labels.long())\n",
    "        loss = criterion(output,labels.long())\n",
    "        #print('LOSS IS :-', loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #calculate training accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        #logging.debug(' top probab {} top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        #print(train_loss)\n",
    "              \n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "                \n",
    "        output = model.forward(inputs, val_batch_size)\n",
    "       \n",
    "        loss = criterion(output,labels.long())\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        #calculate validation accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        \n",
    "        #logging.debug(output)\n",
    "        #logging.debug('VALIDATION top probab {} VALIDATION top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        #print('Top Class:- ',top_class)\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        #print('Equals:- ', equals)\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    #Averaging losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    val_accuracy = val_accuracy/len(val_loader)\n",
    "    train_accuracy = train_accuracy/len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}\\tTrain Loss: {:.7f} \\tVal Loss:{:.7f} \\tTrain Acc: {:.7}% \\tVal Acc: {:.7f}%'.format(e, train_loss, val_loss, train_accuracy*100,val_accuracy*100))\n",
    "    \n",
    "    #saving the model if validation loss is decreased\n",
    "    if val_loss <= min_val_loss:\n",
    "        print('Validation Loss decreased from {:6f} to {:6f}, saving the model weights'.format(min_val_loss, val_loss))\n",
    "        torch.save(model.state_dict(), 'lstm_state_256-38_our_normalization.pt')\n",
    "        min_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUSIC GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm1): LSTM(1, 256, batch_first=True)\n",
       "  (lstm2): LSTM(256, 38, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=38, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights\n",
    "test_model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "test_model.load_state_dict(torch.load('lstm_state_256-38_our_normalization.pt'))\n",
    "test_model.eval()\n",
    "test_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population database\n",
    "#testing_data = np.ones(200)*1\n",
    "testing_data = list(range(50,90))\n",
    "testing_data.extend(testing_data[::-1])\n",
    "testing_data_rev = testing_data[::-1]\n",
    "testing_data_rev.extend(testing_data)\n",
    "testing_data_rev.extend(testing_data_rev)\n",
    "testing_data = testing_data_rev\n",
    "\n",
    "\n",
    "testing_data = np.asarray(testing_data)\n",
    "testing_data = testing_data.reshape(testing_data.shape[0],1)\n",
    "\n",
    "initial_seq = [network_input[0][1:].cpu().numpy().tolist()]\n",
    "\n",
    "testing_data_unnorm = testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "testing_data=testing_data.tolist()\n",
    "for i in range(len(testing_data)):\n",
    "    list1.extend(testing_data[i])\n",
    "\n",
    "#list1\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    list1[i]=(list1[i]-50)/(89-50)\n",
    "#     list1[i]=(list1[i])/(89)\n",
    "\n",
    "list1 = np.asarray(list1)\n",
    "list1 = list1.reshape(list1.shape[0],1)\n",
    "testing_data = list1\n",
    "#list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting\n",
    "def prediction_with_influence(influence,int2note,initial_seq, max_note, min_note,test_batch_size = 1):\n",
    "\n",
    "    predicted_notes = []\n",
    "    initial_seq[0].extend([[0]]*len(testing_data))\n",
    "    test_seq = torch.Tensor(initial_seq).cuda()\n",
    "    \n",
    "    for i in range(len(influence)):\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = float(influence[i])\n",
    "        \n",
    "        test_slice = test_seq[0][i : i + sequence_length]        \n",
    "        test_slice = test_slice.view(1, test_slice.shape[0], test_slice.shape[1])\n",
    "        \n",
    "        test_output = test_model.forward(test_slice, test_batch_size)\n",
    "    \n",
    "        test_output = F.softmax(test_output, dim = 1)\n",
    "        top_p, top_class = test_output.topk(1,dim =1)\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = (int2note[top_class.item()] - min_note)/(max_note - min_note)\n",
    "#         test_seq[0][sequence_length - 1 + i][0] = int2note[top_class.item()]/max_note\n",
    "        \n",
    "        predicted_notes.append(int2note[top_class.item()])\n",
    "        \n",
    "    return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_notes_lst = prediction_with_influence(testing_data,int_to_note,initial_seq, max_midi_number, min_midi_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_notes_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a7e7c511c8>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(list(np.ones(200)*89))\n",
    "\n",
    "#plt.plot(list(np.ones(200)*50))\n",
    "#plt.plot(list(np.ones(20)*50))\n",
    "plt.plot(testing_data_unnorm)\n",
    "plt.plot(predicted_notes_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({72: 19,\n",
       "         71: 15,\n",
       "         69: 15,\n",
       "         68: 1,\n",
       "         66: 16,\n",
       "         64: 25,\n",
       "         67: 20,\n",
       "         62: 26,\n",
       "         65: 16,\n",
       "         74: 21,\n",
       "         79: 30,\n",
       "         84: 16,\n",
       "         81: 15,\n",
       "         83: 11,\n",
       "         78: 4,\n",
       "         86: 4,\n",
       "         76: 9,\n",
       "         70: 5,\n",
       "         61: 3,\n",
       "         57: 2,\n",
       "         53: 1,\n",
       "         55: 6,\n",
       "         60: 20,\n",
       "         77: 12,\n",
       "         75: 2,\n",
       "         82: 1,\n",
       "         59: 4,\n",
       "         73: 1})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(predicted_notes_lst)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PostProcessing().generate_midi_file('hello.midi', predicted_notes_lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
