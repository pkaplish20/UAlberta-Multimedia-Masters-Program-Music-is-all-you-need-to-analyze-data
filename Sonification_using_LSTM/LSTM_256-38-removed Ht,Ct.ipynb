{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "#import torch.utils.tensorboard as tb\n",
    "from Preprocessing.preprocessing import PreprocessingTrainingData\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as  plt\n",
    "import os\n",
    "import logging\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "import pandas as pd\n",
    "from Postprocessing.postprocessing import PostProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static parameters\n",
    "train_batch_size = 170\n",
    "val_batch_size = 170\n",
    "sequence_length=50\n",
    "test_batch_size = 1\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layer = 2\n",
    "output_size = 38\n",
    "clip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from preprocessing.py\n",
    "dataset_path = os.path.join(os.path.abspath('..'),'Dataset\\\\Clementi dataset\\\\Clementi dataset' )\n",
    "network_input,network_output,max_midi_number,min_midi_number,int_to_note = PreprocessingTrainingData().preprocess_notes(dataset_path)\n",
    "network_input, network_output = network_input.cuda(), network_output.cuda()\n",
    "\n",
    "# print(network_input)\n",
    "#print(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(network_output.max())\n",
    "print(network_output.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "89\n",
      "50\n",
      "{0: 50, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61, 11: 62, 12: 63, 13: 64, 14: 65, 15: 66, 16: 67, 17: 68, 18: 69, 19: 70, 20: 71, 21: 72, 22: 73, 23: 74, 24: 75, 25: 76, 26: 77, 27: 78, 28: 79, 29: 80, 30: 81, 31: 82, 32: 83, 33: 84, 34: 85, 35: 86, 36: 88, 37: 89}\n"
     ]
    }
   ],
   "source": [
    "print(network_input.max())\n",
    "print(network_input.min())\n",
    "print(max_midi_number)\n",
    "print(min_midi_number)\n",
    "print(int_to_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata is highly unbalanced\\n# '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data is highly unbalanced\n",
    "# '''\n",
    "# sns.distplot(torch.tensor(network_output).cpu())\n",
    "# xx = pd.DataFrame(torch.tensor(network_output).cpu())\n",
    "# xx.groupby(0).size().to_frame(name='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8500, 50, 1])\n",
      "torch.Size([8500])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "to make batch of equal sizes\n",
    "Quick Fix\n",
    "'''\n",
    "network_input = network_input[: -117]\n",
    "network_output = network_output[: -117]\n",
    "\n",
    "print(network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bakchodi Normalization\n",
    "# network_input=network_input.cpu().numpy().tolist()\n",
    "# for i in range(len(network_input)):\n",
    "#     for j in range(len(network_input[i])):\n",
    "#         network_input[i][j][0]=((network_input[i][j][0])*(max_midi_number-min_midi_number)+min_midi_number)/max_midi_number\n",
    "# network_input=torch.Tensor(network_input).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_input[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create Stacked LSTM model\n",
    "'''\n",
    "class Stacked_LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size = input_size, hidden_size = hidden_size, batch_first = True)\n",
    "        self.lstm2 = nn.LSTM(input_size = hidden_size, hidden_size = output_size,batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(output_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, batch_size):\n",
    "        \n",
    "        output, _ = self.lstm1(x)        \n",
    "        #print(output.shape)\n",
    "        \n",
    "        #output = self.dropout(output)\n",
    "        \n",
    "        output, _ = self.lstm2(output)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        output = output.contiguous().view(-1, 38)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        #print('Linear Output :-',output.shape)\n",
    "        \n",
    "        #output = F.softmax(output, dim = 1)\n",
    "        #print('SOFTMAX OUTPUT :--', output)\n",
    "        \n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1)\n",
    "        #print('Reshape to batch size first :-',output.shape)\n",
    "        \n",
    "        output = output[:, -self.output_size:] # get last batch of labels\n",
    "        #print('Final Output :-',output)\n",
    "        #print('RESHAPE SIZE :-', output.shape)\n",
    "        \n",
    "        return output\n",
    "\n",
    "#initialize the weights of LSTM using Xavier initialization    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Divide the dataset into train/val \n",
    "'''\n",
    "train_size = 0.8\n",
    "indices = list(range(len(network_input)))\n",
    "split = int(np.floor(train_size*len(network_input)))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SequentialSampler(train_idx)\n",
    "val_sampler = SequentialSampler(val_idx)\n",
    "\n",
    "dataset = TensorDataset(network_input,network_output)\n",
    "train_loader = DataLoader(dataset, batch_size= train_batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size= val_batch_size,sampler= val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm1): LSTM(1, 256, batch_first=True)\n",
       "  (lstm2): LSTM(256, 38, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=38, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optimizer\n",
    "\n",
    "model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer.AdamW(model.parameters())\n",
    "#optimizer = optimizer.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "#make sure to transfer model to GPU after initializing optimizer\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 3.3668325 \tVal Loss:3.0439595 \tTrain Acc: 8.352941% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from    inf to 3.043960, saving the model weights\n",
      "Epoch: 1\tTrain Loss: 3.1634846 \tVal Loss:2.9785109 \tTrain Acc: 9.029412% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 3.043960 to 2.978511, saving the model weights\n",
      "Epoch: 2\tTrain Loss: 3.1245804 \tVal Loss:2.9572137 \tTrain Acc: 8.764706% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.978511 to 2.957214, saving the model weights\n",
      "Epoch: 3\tTrain Loss: 3.1104905 \tVal Loss:2.9583088 \tTrain Acc: 9.279412% \tVal Acc: 11.4117651%\n",
      "Epoch: 4\tTrain Loss: 3.0969946 \tVal Loss:2.9501348 \tTrain Acc: 8.647059% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.957214 to 2.950135, saving the model weights\n",
      "Epoch: 5\tTrain Loss: 3.0861625 \tVal Loss:2.9469243 \tTrain Acc: 9.161765% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.950135 to 2.946924, saving the model weights\n",
      "Epoch: 6\tTrain Loss: 3.0828948 \tVal Loss:2.9477134 \tTrain Acc: 9.235294% \tVal Acc: 11.4117651%\n",
      "Epoch: 7\tTrain Loss: 3.0738877 \tVal Loss:2.9427830 \tTrain Acc: 10.04412% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.946924 to 2.942783, saving the model weights\n",
      "Epoch: 8\tTrain Loss: 3.0694848 \tVal Loss:2.9424737 \tTrain Acc: 9.176471% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.942783 to 2.942474, saving the model weights\n",
      "Epoch: 9\tTrain Loss: 3.0658311 \tVal Loss:2.9403424 \tTrain Acc: 9.25% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.942474 to 2.940342, saving the model weights\n",
      "Epoch: 10\tTrain Loss: 3.0578734 \tVal Loss:2.9382515 \tTrain Acc: 9.735294% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.940342 to 2.938252, saving the model weights\n",
      "Epoch: 11\tTrain Loss: 3.0589468 \tVal Loss:2.9383711 \tTrain Acc: 8.82353% \tVal Acc: 11.4117651%\n",
      "Epoch: 12\tTrain Loss: 3.0630292 \tVal Loss:2.9381985 \tTrain Acc: 9.470589% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.938252 to 2.938199, saving the model weights\n",
      "Epoch: 13\tTrain Loss: 3.0585464 \tVal Loss:2.9379051 \tTrain Acc: 9.735294% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.938199 to 2.937905, saving the model weights\n",
      "Epoch: 14\tTrain Loss: 3.0510495 \tVal Loss:2.9365609 \tTrain Acc: 9.75% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.937905 to 2.936561, saving the model weights\n",
      "Epoch: 15\tTrain Loss: 3.0485995 \tVal Loss:2.9349343 \tTrain Acc: 10.23529% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.936561 to 2.934934, saving the model weights\n",
      "Epoch: 16\tTrain Loss: 3.0518357 \tVal Loss:2.9355152 \tTrain Acc: 9.779412% \tVal Acc: 11.4117651%\n",
      "Epoch: 17\tTrain Loss: 3.0468791 \tVal Loss:2.9366726 \tTrain Acc: 10.13235% \tVal Acc: 11.4117651%\n",
      "Epoch: 18\tTrain Loss: 3.0425687 \tVal Loss:2.9352565 \tTrain Acc: 9.764706% \tVal Acc: 11.4117651%\n",
      "Epoch: 19\tTrain Loss: 3.0445927 \tVal Loss:2.9336963 \tTrain Acc: 10.17647% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.934934 to 2.933696, saving the model weights\n",
      "Epoch: 20\tTrain Loss: 3.0459268 \tVal Loss:2.9345988 \tTrain Acc: 10.11765% \tVal Acc: 11.4117651%\n",
      "Epoch: 21\tTrain Loss: 3.0412629 \tVal Loss:2.9344532 \tTrain Acc: 10.0% \tVal Acc: 11.4117651%\n",
      "Epoch: 22\tTrain Loss: 3.0419397 \tVal Loss:2.9331069 \tTrain Acc: 10.11765% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.933696 to 2.933107, saving the model weights\n",
      "Epoch: 23\tTrain Loss: 3.0422016 \tVal Loss:2.9314128 \tTrain Acc: 9.838236% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.933107 to 2.931413, saving the model weights\n",
      "Epoch: 24\tTrain Loss: 3.0370057 \tVal Loss:2.9303188 \tTrain Acc: 10.54412% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.931413 to 2.930319, saving the model weights\n",
      "Epoch: 25\tTrain Loss: 3.0397719 \tVal Loss:2.9311381 \tTrain Acc: 10.41176% \tVal Acc: 11.4117651%\n",
      "Epoch: 26\tTrain Loss: 3.0422610 \tVal Loss:2.9312253 \tTrain Acc: 10.13235% \tVal Acc: 11.4117651%\n",
      "Epoch: 27\tTrain Loss: 3.0355926 \tVal Loss:2.9322818 \tTrain Acc: 10.57353% \tVal Acc: 11.4117651%\n",
      "Epoch: 28\tTrain Loss: 3.0355557 \tVal Loss:2.9307675 \tTrain Acc: 10.85294% \tVal Acc: 11.4117651%\n",
      "Epoch: 29\tTrain Loss: 3.0343629 \tVal Loss:2.9305130 \tTrain Acc: 10.67647% \tVal Acc: 11.4117651%\n",
      "Epoch: 30\tTrain Loss: 3.0387700 \tVal Loss:2.9297520 \tTrain Acc: 10.80882% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.930319 to 2.929752, saving the model weights\n",
      "Epoch: 31\tTrain Loss: 3.0347314 \tVal Loss:2.9306056 \tTrain Acc: 10.45588% \tVal Acc: 11.4117651%\n",
      "Epoch: 32\tTrain Loss: 3.0359338 \tVal Loss:2.9291285 \tTrain Acc: 10.42647% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.929752 to 2.929128, saving the model weights\n",
      "Epoch: 33\tTrain Loss: 3.0323071 \tVal Loss:2.9299157 \tTrain Acc: 10.61765% \tVal Acc: 11.4117651%\n",
      "Epoch: 34\tTrain Loss: 3.0332964 \tVal Loss:2.9306633 \tTrain Acc: 10.97059% \tVal Acc: 11.4117651%\n",
      "Epoch: 35\tTrain Loss: 3.0318221 \tVal Loss:2.9297588 \tTrain Acc: 10.61765% \tVal Acc: 11.4117651%\n",
      "Epoch: 36\tTrain Loss: 3.0304542 \tVal Loss:2.9287207 \tTrain Acc: 10.60294% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.929128 to 2.928721, saving the model weights\n",
      "Epoch: 37\tTrain Loss: 3.0346594 \tVal Loss:2.9279540 \tTrain Acc: 10.45588% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.928721 to 2.927954, saving the model weights\n",
      "Epoch: 38\tTrain Loss: 3.0293452 \tVal Loss:2.9278936 \tTrain Acc: 10.88235% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.927954 to 2.927894, saving the model weights\n",
      "Epoch: 39\tTrain Loss: 3.0305715 \tVal Loss:2.9248718 \tTrain Acc: 10.58824% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.927894 to 2.924872, saving the model weights\n",
      "Epoch: 40\tTrain Loss: 3.0302026 \tVal Loss:2.9242274 \tTrain Acc: 10.61765% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.924872 to 2.924227, saving the model weights\n",
      "Epoch: 41\tTrain Loss: 3.0314185 \tVal Loss:2.9307634 \tTrain Acc: 10.69118% \tVal Acc: 11.4117651%\n",
      "Epoch: 42\tTrain Loss: 3.0320799 \tVal Loss:2.9251529 \tTrain Acc: 10.77941% \tVal Acc: 11.4117651%\n",
      "Epoch: 43\tTrain Loss: 3.0272775 \tVal Loss:2.9214636 \tTrain Acc: 10.86765% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.924227 to 2.921464, saving the model weights\n",
      "Epoch: 44\tTrain Loss: 3.0296045 \tVal Loss:2.9199898 \tTrain Acc: 10.63235% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.921464 to 2.919990, saving the model weights\n",
      "Epoch: 45\tTrain Loss: 3.0256293 \tVal Loss:2.9257647 \tTrain Acc: 10.92647% \tVal Acc: 11.4117651%\n",
      "Epoch: 46\tTrain Loss: 3.0319195 \tVal Loss:2.9563117 \tTrain Acc: 10.83824% \tVal Acc: 11.4117651%\n",
      "Epoch: 47\tTrain Loss: 3.0369577 \tVal Loss:2.9356074 \tTrain Acc: 10.0% \tVal Acc: 11.4117651%\n",
      "Epoch: 48\tTrain Loss: 3.0284843 \tVal Loss:2.9257022 \tTrain Acc: 10.73529% \tVal Acc: 11.4117651%\n",
      "Epoch: 49\tTrain Loss: 3.0268194 \tVal Loss:2.9273314 \tTrain Acc: 10.76471% \tVal Acc: 11.4117651%\n",
      "Epoch: 50\tTrain Loss: 3.0212328 \tVal Loss:2.9124358 \tTrain Acc: 10.79412% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.919990 to 2.912436, saving the model weights\n",
      "Epoch: 51\tTrain Loss: 3.0182223 \tVal Loss:2.9250016 \tTrain Acc: 11.16176% \tVal Acc: 11.4117651%\n",
      "Epoch: 52\tTrain Loss: 3.0106400 \tVal Loss:2.9264353 \tTrain Acc: 11.05882% \tVal Acc: 11.4117651%\n",
      "Epoch: 53\tTrain Loss: 3.0337630 \tVal Loss:2.9207157 \tTrain Acc: 10.67647% \tVal Acc: 11.4117651%\n",
      "Epoch: 54\tTrain Loss: 3.0178378 \tVal Loss:2.9267647 \tTrain Acc: 10.51471% \tVal Acc: 11.4117651%\n",
      "Epoch: 55\tTrain Loss: 3.0064583 \tVal Loss:2.8922228 \tTrain Acc: 10.82353% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.912436 to 2.892223, saving the model weights\n",
      "Epoch: 56\tTrain Loss: 3.0030270 \tVal Loss:2.9424367 \tTrain Acc: 10.5% \tVal Acc: 11.4117651%\n",
      "Epoch: 57\tTrain Loss: 3.0268684 \tVal Loss:2.9190662 \tTrain Acc: 10.55882% \tVal Acc: 11.4117651%\n",
      "Epoch: 58\tTrain Loss: 3.0064294 \tVal Loss:2.9304924 \tTrain Acc: 10.91176% \tVal Acc: 11.4117651%\n",
      "Epoch: 59\tTrain Loss: 2.9761804 \tVal Loss:2.8412629 \tTrain Acc: 11.13235% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.892223 to 2.841263, saving the model weights\n",
      "Epoch: 60\tTrain Loss: 2.9337205 \tVal Loss:2.8113410 \tTrain Acc: 11.22059% \tVal Acc: 11.7647062%\n",
      "Validation Loss decreased from 2.841263 to 2.811341, saving the model weights\n",
      "Epoch: 61\tTrain Loss: 2.8726111 \tVal Loss:2.8016212 \tTrain Acc: 12.70588% \tVal Acc: 10.7058827%\n",
      "Validation Loss decreased from 2.811341 to 2.801621, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62\tTrain Loss: 2.8406813 \tVal Loss:2.7188312 \tTrain Acc: 13.10294% \tVal Acc: 12.4705886%\n",
      "Validation Loss decreased from 2.801621 to 2.718831, saving the model weights\n",
      "Epoch: 63\tTrain Loss: 2.8018362 \tVal Loss:2.6632511 \tTrain Acc: 13.25% \tVal Acc: 14.0588240%\n",
      "Validation Loss decreased from 2.718831 to 2.663251, saving the model weights\n",
      "Epoch: 64\tTrain Loss: 2.7548085 \tVal Loss:2.6274626 \tTrain Acc: 14.36765% \tVal Acc: 13.7647063%\n",
      "Validation Loss decreased from 2.663251 to 2.627463, saving the model weights\n",
      "Epoch: 65\tTrain Loss: 2.7029635 \tVal Loss:2.5922425 \tTrain Acc: 14.61765% \tVal Acc: 13.2352944%\n",
      "Validation Loss decreased from 2.627463 to 2.592242, saving the model weights\n",
      "Epoch: 66\tTrain Loss: 2.6538521 \tVal Loss:2.5720674 \tTrain Acc: 15.69118% \tVal Acc: 13.2941179%\n",
      "Validation Loss decreased from 2.592242 to 2.572067, saving the model weights\n",
      "Epoch: 67\tTrain Loss: 2.6302821 \tVal Loss:2.5501688 \tTrain Acc: 15.77941% \tVal Acc: 12.4705885%\n",
      "Validation Loss decreased from 2.572067 to 2.550169, saving the model weights\n",
      "Epoch: 68\tTrain Loss: 2.6082671 \tVal Loss:2.5339581 \tTrain Acc: 16.38235% \tVal Acc: 13.2352944%\n",
      "Validation Loss decreased from 2.550169 to 2.533958, saving the model weights\n",
      "Epoch: 69\tTrain Loss: 2.5906697 \tVal Loss:2.5142643 \tTrain Acc: 16.45588% \tVal Acc: 13.5882355%\n",
      "Validation Loss decreased from 2.533958 to 2.514264, saving the model weights\n",
      "Epoch: 70\tTrain Loss: 2.5674299 \tVal Loss:2.4987696 \tTrain Acc: 16.22059% \tVal Acc: 13.3529415%\n",
      "Validation Loss decreased from 2.514264 to 2.498770, saving the model weights\n",
      "Epoch: 71\tTrain Loss: 2.5534914 \tVal Loss:2.4928671 \tTrain Acc: 16.39706% \tVal Acc: 14.1764709%\n",
      "Validation Loss decreased from 2.498770 to 2.492867, saving the model weights\n",
      "Epoch: 72\tTrain Loss: 2.5451095 \tVal Loss:2.4779612 \tTrain Acc: 16.60294% \tVal Acc: 14.5294122%\n",
      "Validation Loss decreased from 2.492867 to 2.477961, saving the model weights\n",
      "Epoch: 73\tTrain Loss: 2.5278051 \tVal Loss:2.4720471 \tTrain Acc: 16.41177% \tVal Acc: 15.5294123%\n",
      "Validation Loss decreased from 2.477961 to 2.472047, saving the model weights\n",
      "Epoch: 74\tTrain Loss: 2.5185136 \tVal Loss:2.4626508 \tTrain Acc: 17.38235% \tVal Acc: 15.4705886%\n",
      "Validation Loss decreased from 2.472047 to 2.462651, saving the model weights\n",
      "Epoch: 75\tTrain Loss: 2.5094044 \tVal Loss:2.4582660 \tTrain Acc: 17.07353% \tVal Acc: 15.4117651%\n",
      "Validation Loss decreased from 2.462651 to 2.458266, saving the model weights\n",
      "Epoch: 76\tTrain Loss: 2.4993415 \tVal Loss:2.4579005 \tTrain Acc: 16.95588% \tVal Acc: 15.4117651%\n",
      "Validation Loss decreased from 2.458266 to 2.457901, saving the model weights\n",
      "Epoch: 77\tTrain Loss: 2.4926057 \tVal Loss:2.4440747 \tTrain Acc: 17.58824% \tVal Acc: 15.5294120%\n",
      "Validation Loss decreased from 2.457901 to 2.444075, saving the model weights\n",
      "Epoch: 78\tTrain Loss: 2.4834244 \tVal Loss:2.4525893 \tTrain Acc: 17.25% \tVal Acc: 15.2352946%\n",
      "Epoch: 79\tTrain Loss: 2.4837467 \tVal Loss:2.4300285 \tTrain Acc: 17.38235% \tVal Acc: 16.1764710%\n",
      "Validation Loss decreased from 2.444075 to 2.430029, saving the model weights\n",
      "Epoch: 80\tTrain Loss: 2.4685714 \tVal Loss:2.4254121 \tTrain Acc: 18.04412% \tVal Acc: 15.6470592%\n",
      "Validation Loss decreased from 2.430029 to 2.425412, saving the model weights\n",
      "Epoch: 81\tTrain Loss: 2.4680103 \tVal Loss:2.4444308 \tTrain Acc: 17.36765% \tVal Acc: 15.0588239%\n",
      "Epoch: 82\tTrain Loss: 2.4714515 \tVal Loss:2.4245822 \tTrain Acc: 17.5% \tVal Acc: 16.4705887%\n",
      "Validation Loss decreased from 2.425412 to 2.424582, saving the model weights\n",
      "Epoch: 83\tTrain Loss: 2.4576977 \tVal Loss:2.4159865 \tTrain Acc: 18.29412% \tVal Acc: 16.0000005%\n",
      "Validation Loss decreased from 2.424582 to 2.415986, saving the model weights\n",
      "Epoch: 84\tTrain Loss: 2.4499010 \tVal Loss:2.4264603 \tTrain Acc: 18.47059% \tVal Acc: 15.7058828%\n",
      "Epoch: 85\tTrain Loss: 2.4527677 \tVal Loss:2.4257816 \tTrain Acc: 18.60294% \tVal Acc: 16.0588239%\n",
      "Epoch: 86\tTrain Loss: 2.4474363 \tVal Loss:2.4116369 \tTrain Acc: 17.86765% \tVal Acc: 17.1764710%\n",
      "Validation Loss decreased from 2.415986 to 2.411637, saving the model weights\n",
      "Epoch: 87\tTrain Loss: 2.4475046 \tVal Loss:2.4225107 \tTrain Acc: 18.02941% \tVal Acc: 15.8235298%\n",
      "Epoch: 88\tTrain Loss: 2.4452985 \tVal Loss:2.4138849 \tTrain Acc: 17.48529% \tVal Acc: 17.1176475%\n",
      "Epoch: 89\tTrain Loss: 2.4376437 \tVal Loss:2.4185596 \tTrain Acc: 18.02941% \tVal Acc: 15.5882356%\n",
      "Epoch: 90\tTrain Loss: 2.4404087 \tVal Loss:2.4196610 \tTrain Acc: 18.05882% \tVal Acc: 16.8235298%\n",
      "Epoch: 91\tTrain Loss: 2.4466206 \tVal Loss:2.4039217 \tTrain Acc: 18.17647% \tVal Acc: 16.4705886%\n",
      "Validation Loss decreased from 2.411637 to 2.403922, saving the model weights\n",
      "Epoch: 92\tTrain Loss: 2.4281292 \tVal Loss:2.4064343 \tTrain Acc: 18.54412% \tVal Acc: 17.2352946%\n",
      "Epoch: 93\tTrain Loss: 2.4248350 \tVal Loss:2.4208001 \tTrain Acc: 18.94118% \tVal Acc: 16.0000004%\n",
      "Epoch: 94\tTrain Loss: 2.4287839 \tVal Loss:2.4071475 \tTrain Acc: 18.38235% \tVal Acc: 17.1176475%\n",
      "Epoch: 95\tTrain Loss: 2.4246824 \tVal Loss:2.3967438 \tTrain Acc: 18.55882% \tVal Acc: 16.4705886%\n",
      "Validation Loss decreased from 2.403922 to 2.396744, saving the model weights\n",
      "Epoch: 96\tTrain Loss: 2.4191445 \tVal Loss:2.3927444 \tTrain Acc: 18.58824% \tVal Acc: 17.4117651%\n",
      "Validation Loss decreased from 2.396744 to 2.392744, saving the model weights\n",
      "Epoch: 97\tTrain Loss: 2.4137747 \tVal Loss:2.3981764 \tTrain Acc: 19.58824% \tVal Acc: 17.3529416%\n",
      "Epoch: 98\tTrain Loss: 2.4118309 \tVal Loss:2.3983275 \tTrain Acc: 19.04412% \tVal Acc: 17.8823534%\n",
      "Epoch: 99\tTrain Loss: 2.4202712 \tVal Loss:2.3782887 \tTrain Acc: 19.51471% \tVal Acc: 17.8235298%\n",
      "Validation Loss decreased from 2.392744 to 2.378289, saving the model weights\n",
      "Epoch: 100\tTrain Loss: 2.4110123 \tVal Loss:2.3865316 \tTrain Acc: 19.35294% \tVal Acc: 18.1764710%\n",
      "Epoch: 101\tTrain Loss: 2.4100295 \tVal Loss:2.3962054 \tTrain Acc: 19.17647% \tVal Acc: 18.4117651%\n",
      "Epoch: 102\tTrain Loss: 2.3977143 \tVal Loss:2.3842088 \tTrain Acc: 19.45588% \tVal Acc: 18.3529416%\n",
      "Epoch: 103\tTrain Loss: 2.4134559 \tVal Loss:2.3701429 \tTrain Acc: 19.77941% \tVal Acc: 18.2352945%\n",
      "Validation Loss decreased from 2.378289 to 2.370143, saving the model weights\n",
      "Epoch: 104\tTrain Loss: 2.4022256 \tVal Loss:2.3718172 \tTrain Acc: 20.19118% \tVal Acc: 19.9411771%\n",
      "Epoch: 105\tTrain Loss: 2.3944145 \tVal Loss:2.3770499 \tTrain Acc: 19.17647% \tVal Acc: 19.2352945%\n",
      "Epoch: 106\tTrain Loss: 2.3919781 \tVal Loss:2.3762952 \tTrain Acc: 19.80882% \tVal Acc: 19.1764711%\n",
      "Epoch: 107\tTrain Loss: 2.4068899 \tVal Loss:2.3645482 \tTrain Acc: 19.58824% \tVal Acc: 18.4705887%\n",
      "Validation Loss decreased from 2.370143 to 2.364548, saving the model weights\n",
      "Epoch: 108\tTrain Loss: 2.3816037 \tVal Loss:2.3845005 \tTrain Acc: 20.11765% \tVal Acc: 18.3529416%\n",
      "Epoch: 109\tTrain Loss: 2.3817624 \tVal Loss:2.3559373 \tTrain Acc: 19.61765% \tVal Acc: 19.4705886%\n",
      "Validation Loss decreased from 2.364548 to 2.355937, saving the model weights\n",
      "Epoch: 110\tTrain Loss: 2.3898446 \tVal Loss:2.3539023 \tTrain Acc: 20.10294% \tVal Acc: 20.1176476%\n",
      "Validation Loss decreased from 2.355937 to 2.353902, saving the model weights\n",
      "Epoch: 111\tTrain Loss: 2.3833946 \tVal Loss:2.3589694 \tTrain Acc: 19.72059% \tVal Acc: 19.7058827%\n",
      "Epoch: 112\tTrain Loss: 2.3773255 \tVal Loss:2.3590247 \tTrain Acc: 20.52941% \tVal Acc: 20.3529416%\n",
      "Epoch: 113\tTrain Loss: 2.3834852 \tVal Loss:2.3467114 \tTrain Acc: 20.55882% \tVal Acc: 19.6470591%\n",
      "Validation Loss decreased from 2.353902 to 2.346711, saving the model weights\n",
      "Epoch: 114\tTrain Loss: 2.3719463 \tVal Loss:2.3627423 \tTrain Acc: 21.32353% \tVal Acc: 21.4705886%\n",
      "Epoch: 115\tTrain Loss: 2.3944241 \tVal Loss:2.3441589 \tTrain Acc: 20.33824% \tVal Acc: 20.2352947%\n",
      "Validation Loss decreased from 2.346711 to 2.344159, saving the model weights\n",
      "Epoch: 116\tTrain Loss: 2.3698852 \tVal Loss:2.3516477 \tTrain Acc: 20.52941% \tVal Acc: 21.2352945%\n",
      "Epoch: 117\tTrain Loss: 2.3695906 \tVal Loss:2.3361564 \tTrain Acc: 20.32353% \tVal Acc: 20.0000004%\n",
      "Validation Loss decreased from 2.344159 to 2.336156, saving the model weights\n",
      "Epoch: 118\tTrain Loss: 2.3768696 \tVal Loss:2.3316627 \tTrain Acc: 20.66177% \tVal Acc: 21.2352945%\n",
      "Validation Loss decreased from 2.336156 to 2.331663, saving the model weights\n",
      "Epoch: 119\tTrain Loss: 2.3636352 \tVal Loss:2.3257911 \tTrain Acc: 20.92647% \tVal Acc: 21.4117651%\n",
      "Validation Loss decreased from 2.331663 to 2.325791, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120\tTrain Loss: 2.3556506 \tVal Loss:2.3186349 \tTrain Acc: 21.33824% \tVal Acc: 22.1176475%\n",
      "Validation Loss decreased from 2.325791 to 2.318635, saving the model weights\n",
      "Epoch: 121\tTrain Loss: 2.3546812 \tVal Loss:2.3189863 \tTrain Acc: 21.63235% \tVal Acc: 20.9411769%\n",
      "Epoch: 122\tTrain Loss: 2.3534755 \tVal Loss:2.3123602 \tTrain Acc: 21.44118% \tVal Acc: 21.4705887%\n",
      "Validation Loss decreased from 2.318635 to 2.312360, saving the model weights\n",
      "Epoch: 123\tTrain Loss: 2.3429385 \tVal Loss:2.3108058 \tTrain Acc: 21.58824% \tVal Acc: 21.8235299%\n",
      "Validation Loss decreased from 2.312360 to 2.310806, saving the model weights\n",
      "Epoch: 124\tTrain Loss: 2.3343522 \tVal Loss:2.3071954 \tTrain Acc: 21.79412% \tVal Acc: 21.8823534%\n",
      "Validation Loss decreased from 2.310806 to 2.307195, saving the model weights\n",
      "Epoch: 125\tTrain Loss: 2.3414739 \tVal Loss:2.3289087 \tTrain Acc: 21.89706% \tVal Acc: 23.6470592%\n",
      "Epoch: 126\tTrain Loss: 2.3664137 \tVal Loss:2.3085016 \tTrain Acc: 20.51471% \tVal Acc: 22.0588239%\n",
      "Epoch: 127\tTrain Loss: 2.3396926 \tVal Loss:2.3392821 \tTrain Acc: 22.07353% \tVal Acc: 22.8823532%\n",
      "Epoch: 128\tTrain Loss: 2.3488170 \tVal Loss:2.3360636 \tTrain Acc: 21.39706% \tVal Acc: 22.2352944%\n",
      "Epoch: 129\tTrain Loss: 2.3452739 \tVal Loss:2.3091473 \tTrain Acc: 21.97059% \tVal Acc: 21.7647061%\n",
      "Epoch: 130\tTrain Loss: 2.3250586 \tVal Loss:2.3125968 \tTrain Acc: 22.44118% \tVal Acc: 22.2352944%\n",
      "Epoch: 131\tTrain Loss: 2.3314550 \tVal Loss:2.3015678 \tTrain Acc: 21.83824% \tVal Acc: 22.6470591%\n",
      "Validation Loss decreased from 2.307195 to 2.301568, saving the model weights\n",
      "Epoch: 132\tTrain Loss: 2.3282548 \tVal Loss:2.2995363 \tTrain Acc: 22.05882% \tVal Acc: 22.0588237%\n",
      "Validation Loss decreased from 2.301568 to 2.299536, saving the model weights\n",
      "Epoch: 133\tTrain Loss: 2.3178523 \tVal Loss:2.2899253 \tTrain Acc: 22.76471% \tVal Acc: 23.0588238%\n",
      "Validation Loss decreased from 2.299536 to 2.289925, saving the model weights\n",
      "Epoch: 134\tTrain Loss: 2.3154343 \tVal Loss:2.2904587 \tTrain Acc: 22.45588% \tVal Acc: 22.3529413%\n",
      "Epoch: 135\tTrain Loss: 2.3260376 \tVal Loss:2.2963862 \tTrain Acc: 22.05882% \tVal Acc: 22.1764709%\n",
      "Epoch: 136\tTrain Loss: 2.3239791 \tVal Loss:2.2989218 \tTrain Acc: 23.02941% \tVal Acc: 23.0588242%\n",
      "Epoch: 137\tTrain Loss: 2.3261441 \tVal Loss:2.3177472 \tTrain Acc: 22.35294% \tVal Acc: 22.8823532%\n",
      "Epoch: 138\tTrain Loss: 2.3057261 \tVal Loss:2.2814446 \tTrain Acc: 22.66177% \tVal Acc: 21.8235296%\n",
      "Validation Loss decreased from 2.289925 to 2.281445, saving the model weights\n",
      "Epoch: 139\tTrain Loss: 2.3054778 \tVal Loss:2.2738118 \tTrain Acc: 23.72059% \tVal Acc: 23.0000004%\n",
      "Validation Loss decreased from 2.281445 to 2.273812, saving the model weights\n",
      "Epoch: 140\tTrain Loss: 2.2992923 \tVal Loss:2.2722353 \tTrain Acc: 24.08824% \tVal Acc: 22.4117652%\n",
      "Validation Loss decreased from 2.273812 to 2.272235, saving the model weights\n",
      "Epoch: 141\tTrain Loss: 2.2953021 \tVal Loss:2.2711392 \tTrain Acc: 23.89706% \tVal Acc: 22.9411770%\n",
      "Validation Loss decreased from 2.272235 to 2.271139, saving the model weights\n",
      "Epoch: 142\tTrain Loss: 2.2914947 \tVal Loss:2.2645678 \tTrain Acc: 23.95588% \tVal Acc: 21.6470592%\n",
      "Validation Loss decreased from 2.271139 to 2.264568, saving the model weights\n",
      "Epoch: 143\tTrain Loss: 2.2944564 \tVal Loss:2.2658650 \tTrain Acc: 24.29412% \tVal Acc: 23.6470594%\n",
      "Epoch: 144\tTrain Loss: 2.2836711 \tVal Loss:2.2587447 \tTrain Acc: 24.22059% \tVal Acc: 23.8235299%\n",
      "Validation Loss decreased from 2.264568 to 2.258745, saving the model weights\n",
      "Epoch: 145\tTrain Loss: 2.2755501 \tVal Loss:2.2525297 \tTrain Acc: 24.54412% \tVal Acc: 23.6470594%\n",
      "Validation Loss decreased from 2.258745 to 2.252530, saving the model weights\n",
      "Epoch: 146\tTrain Loss: 2.2769418 \tVal Loss:2.2547596 \tTrain Acc: 23.77941% \tVal Acc: 23.8823535%\n",
      "Epoch: 147\tTrain Loss: 2.2750595 \tVal Loss:2.2475548 \tTrain Acc: 24.73529% \tVal Acc: 24.8823538%\n",
      "Validation Loss decreased from 2.252530 to 2.247555, saving the model weights\n",
      "Epoch: 148\tTrain Loss: 2.2746359 \tVal Loss:2.2529249 \tTrain Acc: 24.26471% \tVal Acc: 22.9411767%\n",
      "Epoch: 149\tTrain Loss: 2.2698991 \tVal Loss:2.2471205 \tTrain Acc: 24.27941% \tVal Acc: 25.1176479%\n",
      "Validation Loss decreased from 2.247555 to 2.247120, saving the model weights\n",
      "Epoch: 150\tTrain Loss: 2.2905330 \tVal Loss:2.2708185 \tTrain Acc: 23.82353% \tVal Acc: 22.4705885%\n",
      "Epoch: 151\tTrain Loss: 2.2911081 \tVal Loss:2.2727279 \tTrain Acc: 24.19118% \tVal Acc: 23.5882358%\n",
      "Epoch: 152\tTrain Loss: 2.2820443 \tVal Loss:2.2459641 \tTrain Acc: 23.91177% \tVal Acc: 24.6470596%\n",
      "Validation Loss decreased from 2.247120 to 2.245964, saving the model weights\n",
      "Epoch: 153\tTrain Loss: 2.2717196 \tVal Loss:2.2513167 \tTrain Acc: 25.27941% \tVal Acc: 25.4705888%\n",
      "Epoch: 154\tTrain Loss: 2.2720967 \tVal Loss:2.2575891 \tTrain Acc: 24.73529% \tVal Acc: 23.3529419%\n",
      "Epoch: 155\tTrain Loss: 2.2720707 \tVal Loss:2.2489745 \tTrain Acc: 24.33824% \tVal Acc: 24.3529420%\n",
      "Epoch: 156\tTrain Loss: 2.2603041 \tVal Loss:2.2385802 \tTrain Acc: 25.14706% \tVal Acc: 24.9411774%\n",
      "Validation Loss decreased from 2.245964 to 2.238580, saving the model weights\n",
      "Epoch: 157\tTrain Loss: 2.2521361 \tVal Loss:2.2454561 \tTrain Acc: 24.67647% \tVal Acc: 22.4117650%\n",
      "Epoch: 158\tTrain Loss: 2.2595776 \tVal Loss:2.2990277 \tTrain Acc: 24.45588% \tVal Acc: 20.1176473%\n",
      "Epoch: 159\tTrain Loss: 2.2754270 \tVal Loss:2.2586864 \tTrain Acc: 24.60294% \tVal Acc: 23.1176475%\n",
      "Epoch: 160\tTrain Loss: 2.2611750 \tVal Loss:2.2265946 \tTrain Acc: 25.25% \tVal Acc: 23.6470592%\n",
      "Validation Loss decreased from 2.238580 to 2.226595, saving the model weights\n",
      "Epoch: 161\tTrain Loss: 2.2474164 \tVal Loss:2.2297713 \tTrain Acc: 25.08824% \tVal Acc: 23.5294124%\n",
      "Epoch: 162\tTrain Loss: 2.2426690 \tVal Loss:2.2253245 \tTrain Acc: 24.77941% \tVal Acc: 25.5882363%\n",
      "Validation Loss decreased from 2.226595 to 2.225325, saving the model weights\n",
      "Epoch: 163\tTrain Loss: 2.2390518 \tVal Loss:2.2208532 \tTrain Acc: 26.26471% \tVal Acc: 23.5294126%\n",
      "Validation Loss decreased from 2.225325 to 2.220853, saving the model weights\n",
      "Epoch: 164\tTrain Loss: 2.2404189 \tVal Loss:2.2301890 \tTrain Acc: 25.83824% \tVal Acc: 23.8823536%\n",
      "Epoch: 165\tTrain Loss: 2.2434737 \tVal Loss:2.2190295 \tTrain Acc: 25.85294% \tVal Acc: 23.6470592%\n",
      "Validation Loss decreased from 2.220853 to 2.219030, saving the model weights\n",
      "Epoch: 166\tTrain Loss: 2.2268082 \tVal Loss:2.2111184 \tTrain Acc: 26.36765% \tVal Acc: 24.1764712%\n",
      "Validation Loss decreased from 2.219030 to 2.211118, saving the model weights\n",
      "Epoch: 167\tTrain Loss: 2.2228228 \tVal Loss:2.2209059 \tTrain Acc: 26.55882% \tVal Acc: 22.9411772%\n",
      "Epoch: 168\tTrain Loss: 2.2355589 \tVal Loss:2.2401264 \tTrain Acc: 26.36765% \tVal Acc: 21.8823533%\n",
      "Epoch: 169\tTrain Loss: 2.2356725 \tVal Loss:2.1938181 \tTrain Acc: 26.16177% \tVal Acc: 25.1176481%\n",
      "Validation Loss decreased from 2.211118 to 2.193818, saving the model weights\n",
      "Epoch: 170\tTrain Loss: 2.2254431 \tVal Loss:2.2016118 \tTrain Acc: 26.0% \tVal Acc: 23.8823533%\n",
      "Epoch: 171\tTrain Loss: 2.2268268 \tVal Loss:2.2058183 \tTrain Acc: 26.16177% \tVal Acc: 25.5294125%\n",
      "Epoch: 172\tTrain Loss: 2.2286187 \tVal Loss:2.2104134 \tTrain Acc: 26.51471% \tVal Acc: 24.1176477%\n",
      "Epoch: 173\tTrain Loss: 2.2197985 \tVal Loss:2.2028358 \tTrain Acc: 26.91177% \tVal Acc: 24.8235303%\n",
      "Epoch: 174\tTrain Loss: 2.2248683 \tVal Loss:2.2055058 \tTrain Acc: 26.48529% \tVal Acc: 23.4117652%\n",
      "Epoch: 175\tTrain Loss: 2.2054697 \tVal Loss:2.1865726 \tTrain Acc: 27.61765% \tVal Acc: 24.4117656%\n",
      "Validation Loss decreased from 2.193818 to 2.186573, saving the model weights\n",
      "Epoch: 176\tTrain Loss: 2.2030838 \tVal Loss:2.1784632 \tTrain Acc: 27.77941% \tVal Acc: 25.1176478%\n",
      "Validation Loss decreased from 2.186573 to 2.178463, saving the model weights\n",
      "Epoch: 177\tTrain Loss: 2.2039062 \tVal Loss:2.2041837 \tTrain Acc: 26.79412% \tVal Acc: 23.0588242%\n",
      "Epoch: 178\tTrain Loss: 2.2680206 \tVal Loss:2.1973300 \tTrain Acc: 25.29412% \tVal Acc: 25.4117654%\n",
      "Epoch: 179\tTrain Loss: 2.2199610 \tVal Loss:2.2026862 \tTrain Acc: 25.70588% \tVal Acc: 25.9411773%\n",
      "Epoch: 180\tTrain Loss: 2.2438452 \tVal Loss:2.2252510 \tTrain Acc: 25.77941% \tVal Acc: 23.1764714%\n",
      "Epoch: 181\tTrain Loss: 2.2341819 \tVal Loss:2.1991195 \tTrain Acc: 25.85294% \tVal Acc: 23.4117652%\n",
      "Epoch: 182\tTrain Loss: 2.2291024 \tVal Loss:2.3203092 \tTrain Acc: 26.66177% \tVal Acc: 21.5882359%\n",
      "Epoch: 183\tTrain Loss: 2.2400065 \tVal Loss:2.2394488 \tTrain Acc: 25.98529% \tVal Acc: 22.0000006%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 184\tTrain Loss: 2.2410796 \tVal Loss:2.2361085 \tTrain Acc: 25.58824% \tVal Acc: 23.7647066%\n",
      "Epoch: 185\tTrain Loss: 2.2384618 \tVal Loss:2.2327602 \tTrain Acc: 25.82353% \tVal Acc: 23.6470596%\n",
      "Epoch: 186\tTrain Loss: 2.2335426 \tVal Loss:2.1713254 \tTrain Acc: 27.30882% \tVal Acc: 26.8823539%\n",
      "Validation Loss decreased from 2.178463 to 2.171325, saving the model weights\n",
      "Epoch: 187\tTrain Loss: 2.2043098 \tVal Loss:2.1713372 \tTrain Acc: 27.44118% \tVal Acc: 26.4705889%\n",
      "Epoch: 188\tTrain Loss: 2.1760639 \tVal Loss:2.1694090 \tTrain Acc: 27.61765% \tVal Acc: 26.3529418%\n",
      "Validation Loss decreased from 2.171325 to 2.169409, saving the model weights\n",
      "Epoch: 189\tTrain Loss: 2.1913618 \tVal Loss:2.1860462 \tTrain Acc: 27.64706% \tVal Acc: 25.4117654%\n",
      "Epoch: 190\tTrain Loss: 2.1841600 \tVal Loss:2.1711944 \tTrain Acc: 28.10294% \tVal Acc: 26.1176480%\n",
      "Epoch: 191\tTrain Loss: 2.1876867 \tVal Loss:2.1693578 \tTrain Acc: 27.54412% \tVal Acc: 27.7647066%\n",
      "Validation Loss decreased from 2.169409 to 2.169358, saving the model weights\n",
      "Epoch: 192\tTrain Loss: 2.2055867 \tVal Loss:2.1774981 \tTrain Acc: 27.26471% \tVal Acc: 26.9411772%\n",
      "Epoch: 193\tTrain Loss: 2.1895012 \tVal Loss:2.2167924 \tTrain Acc: 27.30882% \tVal Acc: 24.1176479%\n",
      "Epoch: 194\tTrain Loss: 2.1826732 \tVal Loss:2.1564656 \tTrain Acc: 27.86765% \tVal Acc: 28.4117655%\n",
      "Validation Loss decreased from 2.169358 to 2.156466, saving the model weights\n",
      "Epoch: 195\tTrain Loss: 2.1625087 \tVal Loss:2.1378127 \tTrain Acc: 28.54412% \tVal Acc: 27.9411775%\n",
      "Validation Loss decreased from 2.156466 to 2.137813, saving the model weights\n",
      "Epoch: 196\tTrain Loss: 2.1595744 \tVal Loss:2.1562953 \tTrain Acc: 28.23529% \tVal Acc: 28.2941185%\n",
      "Epoch: 197\tTrain Loss: 2.1473665 \tVal Loss:2.1218936 \tTrain Acc: 28.76471% \tVal Acc: 28.5294126%\n",
      "Validation Loss decreased from 2.137813 to 2.121894, saving the model weights\n",
      "Epoch: 198\tTrain Loss: 2.1416707 \tVal Loss:2.1350739 \tTrain Acc: 29.42647% \tVal Acc: 27.8823538%\n",
      "Epoch: 199\tTrain Loss: 2.1380089 \tVal Loss:2.1459145 \tTrain Acc: 29.57353% \tVal Acc: 26.4117655%\n",
      "Epoch: 200\tTrain Loss: 2.1382221 \tVal Loss:2.2092756 \tTrain Acc: 29.94118% \tVal Acc: 25.2352948%\n",
      "Epoch: 201\tTrain Loss: 2.1477328 \tVal Loss:2.1691862 \tTrain Acc: 29.44118% \tVal Acc: 27.2941184%\n",
      "Epoch: 202\tTrain Loss: 2.1281886 \tVal Loss:2.1657468 \tTrain Acc: 30.20588% \tVal Acc: 26.3529418%\n",
      "Epoch: 203\tTrain Loss: 2.1302993 \tVal Loss:2.1374669 \tTrain Acc: 29.44118% \tVal Acc: 28.4117654%\n",
      "Epoch: 204\tTrain Loss: 2.1362973 \tVal Loss:2.1544113 \tTrain Acc: 29.48529% \tVal Acc: 27.5882362%\n",
      "Epoch: 205\tTrain Loss: 2.1257190 \tVal Loss:2.1474795 \tTrain Acc: 29.64706% \tVal Acc: 28.3529419%\n",
      "Epoch: 206\tTrain Loss: 2.1215629 \tVal Loss:2.1510535 \tTrain Acc: 31.14706% \tVal Acc: 27.2352949%\n",
      "Epoch: 207\tTrain Loss: 2.1260975 \tVal Loss:2.1617556 \tTrain Acc: 29.27941% \tVal Acc: 26.7058831%\n",
      "Epoch: 208\tTrain Loss: 2.1194290 \tVal Loss:2.1742216 \tTrain Acc: 30.08824% \tVal Acc: 26.5882359%\n",
      "Epoch: 209\tTrain Loss: 2.1353255 \tVal Loss:2.1509953 \tTrain Acc: 29.92647% \tVal Acc: 28.2352948%\n",
      "Epoch: 210\tTrain Loss: 2.1460735 \tVal Loss:2.1174859 \tTrain Acc: 28.98529% \tVal Acc: 29.5294128%\n",
      "Validation Loss decreased from 2.121894 to 2.117486, saving the model weights\n",
      "Epoch: 211\tTrain Loss: 2.1295376 \tVal Loss:2.1479904 \tTrain Acc: 29.55882% \tVal Acc: 28.0588242%\n",
      "Epoch: 212\tTrain Loss: 2.1491920 \tVal Loss:2.1625018 \tTrain Acc: 29.13235% \tVal Acc: 27.6470599%\n",
      "Epoch: 213\tTrain Loss: 2.3148820 \tVal Loss:2.1936163 \tTrain Acc: 24.23529% \tVal Acc: 25.5294125%\n",
      "Epoch: 214\tTrain Loss: 2.2578254 \tVal Loss:2.1511584 \tTrain Acc: 24.79412% \tVal Acc: 28.6470596%\n",
      "Epoch: 215\tTrain Loss: 2.1965250 \tVal Loss:2.1319793 \tTrain Acc: 26.98529% \tVal Acc: 29.7647069%\n",
      "Epoch: 216\tTrain Loss: 2.1698006 \tVal Loss:2.1267721 \tTrain Acc: 28.25% \tVal Acc: 30.2941185%\n",
      "Epoch: 217\tTrain Loss: 2.1247728 \tVal Loss:2.1168671 \tTrain Acc: 29.47059% \tVal Acc: 28.9411773%\n",
      "Validation Loss decreased from 2.117486 to 2.116867, saving the model weights\n",
      "Epoch: 218\tTrain Loss: 2.0897592 \tVal Loss:2.1009673 \tTrain Acc: 30.36765% \tVal Acc: 29.8823538%\n",
      "Validation Loss decreased from 2.116867 to 2.100967, saving the model weights\n",
      "Epoch: 219\tTrain Loss: 2.0960662 \tVal Loss:2.0875708 \tTrain Acc: 30.69118% \tVal Acc: 31.5294127%\n",
      "Validation Loss decreased from 2.100967 to 2.087571, saving the model weights\n",
      "Epoch: 220\tTrain Loss: 2.1077134 \tVal Loss:2.0810886 \tTrain Acc: 30.13235% \tVal Acc: 32.1176481%\n",
      "Validation Loss decreased from 2.087571 to 2.081089, saving the model weights\n",
      "Epoch: 221\tTrain Loss: 2.0919337 \tVal Loss:2.0987641 \tTrain Acc: 31.44118% \tVal Acc: 31.7058833%\n",
      "Epoch: 222\tTrain Loss: 2.0845120 \tVal Loss:2.0822321 \tTrain Acc: 31.42647% \tVal Acc: 31.4705893%\n",
      "Epoch: 223\tTrain Loss: 2.0967600 \tVal Loss:2.0635138 \tTrain Acc: 30.55882% \tVal Acc: 32.4705893%\n",
      "Validation Loss decreased from 2.081089 to 2.063514, saving the model weights\n",
      "Epoch: 224\tTrain Loss: 2.0669652 \tVal Loss:2.1034243 \tTrain Acc: 32.33824% \tVal Acc: 29.4705890%\n",
      "Epoch: 225\tTrain Loss: 2.0564351 \tVal Loss:2.0786855 \tTrain Acc: 33.36765% \tVal Acc: 31.5882362%\n",
      "Epoch: 226\tTrain Loss: 2.0461122 \tVal Loss:2.0787479 \tTrain Acc: 32.41177% \tVal Acc: 30.5294129%\n",
      "Epoch: 227\tTrain Loss: 2.0588811 \tVal Loss:2.0873388 \tTrain Acc: 33.07353% \tVal Acc: 30.4705891%\n",
      "Epoch: 228\tTrain Loss: 2.0439101 \tVal Loss:2.0732496 \tTrain Acc: 32.39706% \tVal Acc: 32.1176480%\n",
      "Epoch: 229\tTrain Loss: 2.0491150 \tVal Loss:2.0810335 \tTrain Acc: 32.38235% \tVal Acc: 31.1764716%\n",
      "Epoch: 230\tTrain Loss: 2.0533499 \tVal Loss:2.1019314 \tTrain Acc: 32.33824% \tVal Acc: 29.3529421%\n",
      "Epoch: 231\tTrain Loss: 2.0531196 \tVal Loss:2.0680303 \tTrain Acc: 31.80882% \tVal Acc: 31.9411775%\n",
      "Epoch: 232\tTrain Loss: 2.0409028 \tVal Loss:2.0574321 \tTrain Acc: 33.47059% \tVal Acc: 32.1764717%\n",
      "Validation Loss decreased from 2.063514 to 2.057432, saving the model weights\n",
      "Epoch: 233\tTrain Loss: 2.0200131 \tVal Loss:2.0479805 \tTrain Acc: 33.42647% \tVal Acc: 31.7647068%\n",
      "Validation Loss decreased from 2.057432 to 2.047980, saving the model weights\n",
      "Epoch: 234\tTrain Loss: 2.0226239 \tVal Loss:2.0280564 \tTrain Acc: 34.33824% \tVal Acc: 32.5294128%\n",
      "Validation Loss decreased from 2.047980 to 2.028056, saving the model weights\n",
      "Epoch: 235\tTrain Loss: 2.0204095 \tVal Loss:2.0539427 \tTrain Acc: 34.25% \tVal Acc: 31.4117657%\n",
      "Epoch: 236\tTrain Loss: 2.0357924 \tVal Loss:2.0489241 \tTrain Acc: 32.72059% \tVal Acc: 34.2941186%\n",
      "Epoch: 237\tTrain Loss: 2.0254294 \tVal Loss:2.1041281 \tTrain Acc: 33.05882% \tVal Acc: 29.4117656%\n",
      "Epoch: 238\tTrain Loss: 2.0365237 \tVal Loss:2.0880669 \tTrain Acc: 32.51471% \tVal Acc: 31.9411774%\n",
      "Epoch: 239\tTrain Loss: 2.0150105 \tVal Loss:2.1276363 \tTrain Acc: 34.0% \tVal Acc: 29.5882362%\n",
      "Epoch: 240\tTrain Loss: 2.0220620 \tVal Loss:2.0908295 \tTrain Acc: 34.05882% \tVal Acc: 30.8823539%\n",
      "Epoch: 241\tTrain Loss: 2.0222400 \tVal Loss:2.0957824 \tTrain Acc: 34.0% \tVal Acc: 31.1176479%\n",
      "Epoch: 242\tTrain Loss: 2.0066119 \tVal Loss:2.0984861 \tTrain Acc: 34.33824% \tVal Acc: 30.4705893%\n",
      "Epoch: 243\tTrain Loss: 2.0034058 \tVal Loss:2.0591100 \tTrain Acc: 34.11765% \tVal Acc: 31.5882362%\n",
      "Epoch: 244\tTrain Loss: 2.0034754 \tVal Loss:2.0095430 \tTrain Acc: 34.17647% \tVal Acc: 33.4705892%\n",
      "Validation Loss decreased from 2.028056 to 2.009543, saving the model weights\n",
      "Epoch: 245\tTrain Loss: 2.0309747 \tVal Loss:2.1500848 \tTrain Acc: 33.5% \tVal Acc: 28.4117655%\n",
      "Epoch: 246\tTrain Loss: 2.0253400 \tVal Loss:2.1448339 \tTrain Acc: 33.88235% \tVal Acc: 29.2352952%\n",
      "Epoch: 247\tTrain Loss: 2.0277566 \tVal Loss:2.1068246 \tTrain Acc: 33.17647% \tVal Acc: 30.0000007%\n",
      "Epoch: 248\tTrain Loss: 2.0127371 \tVal Loss:2.1338257 \tTrain Acc: 34.55882% \tVal Acc: 30.8235303%\n",
      "Epoch: 249\tTrain Loss: 1.9933996 \tVal Loss:2.1118725 \tTrain Acc: 35.38235% \tVal Acc: 29.9411772%\n",
      "Epoch: 250\tTrain Loss: 2.0296625 \tVal Loss:2.0593251 \tTrain Acc: 32.88235% \tVal Acc: 32.2941187%\n",
      "Epoch: 251\tTrain Loss: 2.0151228 \tVal Loss:2.1471172 \tTrain Acc: 34.89706% \tVal Acc: 29.2941183%\n",
      "Epoch: 252\tTrain Loss: 2.0282973 \tVal Loss:2.1515986 \tTrain Acc: 33.75% \tVal Acc: 28.2941182%\n",
      "Epoch: 253\tTrain Loss: 2.0227225 \tVal Loss:2.1665267 \tTrain Acc: 33.94118% \tVal Acc: 27.8823535%\n",
      "Epoch: 254\tTrain Loss: 2.0271871 \tVal Loss:2.1657233 \tTrain Acc: 33.54412% \tVal Acc: 27.0000006%\n",
      "Epoch: 255\tTrain Loss: 2.0532357 \tVal Loss:2.2064125 \tTrain Acc: 33.4853% \tVal Acc: 26.4705889%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 256\tTrain Loss: 2.0713815 \tVal Loss:2.2584716 \tTrain Acc: 32.75% \tVal Acc: 25.1176478%\n",
      "Epoch: 257\tTrain Loss: 2.1694637 \tVal Loss:2.4134885 \tTrain Acc: 28.73529% \tVal Acc: 22.0588242%\n",
      "Epoch: 258\tTrain Loss: 2.1694488 \tVal Loss:2.1912389 \tTrain Acc: 28.73529% \tVal Acc: 28.3529419%\n",
      "Epoch: 259\tTrain Loss: 2.1216606 \tVal Loss:2.2268994 \tTrain Acc: 30.73529% \tVal Acc: 27.1176478%\n",
      "Epoch: 260\tTrain Loss: 2.0943689 \tVal Loss:2.0901474 \tTrain Acc: 31.54412% \tVal Acc: 30.8823538%\n",
      "Epoch: 261\tTrain Loss: 2.0549412 \tVal Loss:2.0667582 \tTrain Acc: 32.57353% \tVal Acc: 30.1764715%\n",
      "Epoch: 262\tTrain Loss: 2.0385152 \tVal Loss:2.0937021 \tTrain Acc: 33.39706% \tVal Acc: 29.6470597%\n",
      "Epoch: 263\tTrain Loss: 2.0153606 \tVal Loss:2.0790850 \tTrain Acc: 33.69118% \tVal Acc: 30.8823539%\n",
      "Epoch: 264\tTrain Loss: 2.0222410 \tVal Loss:2.0668973 \tTrain Acc: 33.92647% \tVal Acc: 31.1764717%\n",
      "Epoch: 265\tTrain Loss: 2.0392090 \tVal Loss:2.1804308 \tTrain Acc: 32.82353% \tVal Acc: 25.4705890%\n",
      "Epoch: 266\tTrain Loss: 2.0249967 \tVal Loss:2.1253879 \tTrain Acc: 33.67647% \tVal Acc: 29.1764715%\n",
      "Epoch: 267\tTrain Loss: 2.0336898 \tVal Loss:2.0505768 \tTrain Acc: 33.19118% \tVal Acc: 32.1176481%\n",
      "Epoch: 268\tTrain Loss: 2.0187424 \tVal Loss:2.0309402 \tTrain Acc: 33.92647% \tVal Acc: 32.9411775%\n",
      "Epoch: 269\tTrain Loss: 1.9950206 \tVal Loss:2.0136362 \tTrain Acc: 34.11765% \tVal Acc: 33.4117657%\n",
      "Epoch: 270\tTrain Loss: 1.9903330 \tVal Loss:2.0055461 \tTrain Acc: 35.01471% \tVal Acc: 33.5294127%\n",
      "Validation Loss decreased from 2.009543 to 2.005546, saving the model weights\n",
      "Epoch: 271\tTrain Loss: 2.0013834 \tVal Loss:2.0067094 \tTrain Acc: 35.22059% \tVal Acc: 33.6470598%\n",
      "Epoch: 272\tTrain Loss: 1.9886384 \tVal Loss:1.9948299 \tTrain Acc: 35.41177% \tVal Acc: 35.8235303%\n",
      "Validation Loss decreased from 2.005546 to 1.994830, saving the model weights\n",
      "Epoch: 273\tTrain Loss: 1.9820128 \tVal Loss:2.0031156 \tTrain Acc: 35.66177% \tVal Acc: 33.0000009%\n",
      "Epoch: 274\tTrain Loss: 1.9885499 \tVal Loss:2.0411464 \tTrain Acc: 35.17647% \tVal Acc: 33.0000010%\n",
      "Epoch: 275\tTrain Loss: 1.9757753 \tVal Loss:2.0313301 \tTrain Acc: 36.13235% \tVal Acc: 33.5882363%\n",
      "Epoch: 276\tTrain Loss: 1.9634144 \tVal Loss:2.0495132 \tTrain Acc: 36.20588% \tVal Acc: 33.4705892%\n",
      "Epoch: 277\tTrain Loss: 1.9651846 \tVal Loss:2.0857867 \tTrain Acc: 35.95588% \tVal Acc: 32.2352950%\n",
      "Epoch: 278\tTrain Loss: 1.9586475 \tVal Loss:2.0478718 \tTrain Acc: 36.94118% \tVal Acc: 32.8235303%\n",
      "Epoch: 279\tTrain Loss: 1.9621617 \tVal Loss:2.1434541 \tTrain Acc: 36.29412% \tVal Acc: 30.2352948%\n",
      "Epoch: 280\tTrain Loss: 1.9808687 \tVal Loss:2.0813130 \tTrain Acc: 35.25% \tVal Acc: 32.1176481%\n",
      "Epoch: 281\tTrain Loss: 1.9770678 \tVal Loss:2.0955627 \tTrain Acc: 35.44118% \tVal Acc: 30.8823538%\n",
      "Epoch: 282\tTrain Loss: 1.9954560 \tVal Loss:2.0420452 \tTrain Acc: 35.20588% \tVal Acc: 32.8823540%\n",
      "Epoch: 283\tTrain Loss: 2.0387735 \tVal Loss:2.0074824 \tTrain Acc: 33.07353% \tVal Acc: 35.8235303%\n",
      "Epoch: 284\tTrain Loss: 1.9986296 \tVal Loss:2.1409966 \tTrain Acc: 35.05882% \tVal Acc: 29.2352951%\n",
      "Epoch: 285\tTrain Loss: 1.9868420 \tVal Loss:2.1601492 \tTrain Acc: 35.20588% \tVal Acc: 30.0000007%\n",
      "Epoch: 286\tTrain Loss: 1.9747307 \tVal Loss:2.2412325 \tTrain Acc: 35.83824% \tVal Acc: 26.4117654%\n",
      "Epoch: 287\tTrain Loss: 2.0333527 \tVal Loss:2.1218716 \tTrain Acc: 34.64706% \tVal Acc: 30.0000009%\n",
      "Epoch: 288\tTrain Loss: 1.9859390 \tVal Loss:2.1948796 \tTrain Acc: 35.36765% \tVal Acc: 29.1176479%\n",
      "Epoch: 289\tTrain Loss: 2.0630817 \tVal Loss:2.1251514 \tTrain Acc: 32.47059% \tVal Acc: 31.8823540%\n",
      "Epoch: 290\tTrain Loss: 2.0910136 \tVal Loss:2.1128994 \tTrain Acc: 31.69118% \tVal Acc: 32.0588244%\n",
      "Epoch: 291\tTrain Loss: 2.0555904 \tVal Loss:2.1064815 \tTrain Acc: 33.0% \tVal Acc: 32.2941184%\n",
      "Epoch: 292\tTrain Loss: 2.0320506 \tVal Loss:2.0346492 \tTrain Acc: 34.33824% \tVal Acc: 35.0588243%\n",
      "Epoch: 293\tTrain Loss: 2.0369687 \tVal Loss:2.0773833 \tTrain Acc: 34.11765% \tVal Acc: 33.2352951%\n",
      "Epoch: 294\tTrain Loss: 2.0948104 \tVal Loss:2.0935622 \tTrain Acc: 32.11765% \tVal Acc: 32.7647066%\n",
      "Epoch: 295\tTrain Loss: 2.0377194 \tVal Loss:2.1212973 \tTrain Acc: 33.75% \tVal Acc: 30.0588244%\n",
      "Epoch: 296\tTrain Loss: 2.0118625 \tVal Loss:2.1400934 \tTrain Acc: 34.76471% \tVal Acc: 32.4117658%\n",
      "Epoch: 297\tTrain Loss: 2.0505741 \tVal Loss:2.1049189 \tTrain Acc: 33.51471% \tVal Acc: 33.2352950%\n",
      "Epoch: 298\tTrain Loss: 2.0229988 \tVal Loss:2.0621772 \tTrain Acc: 34.13235% \tVal Acc: 32.6470597%\n",
      "Epoch: 299\tTrain Loss: 2.0246822 \tVal Loss:2.1591599 \tTrain Acc: 33.54412% \tVal Acc: 30.4117653%\n",
      "Epoch: 300\tTrain Loss: 2.0715161 \tVal Loss:2.0424517 \tTrain Acc: 32.76471% \tVal Acc: 34.0000010%\n",
      "Epoch: 301\tTrain Loss: 2.0881854 \tVal Loss:2.2402179 \tTrain Acc: 32.5% \tVal Acc: 26.0000007%\n",
      "Epoch: 302\tTrain Loss: 2.0655706 \tVal Loss:2.0319056 \tTrain Acc: 32.86765% \tVal Acc: 32.1176480%\n",
      "Epoch: 303\tTrain Loss: 2.1704829 \tVal Loss:1.9584903 \tTrain Acc: 29.72059% \tVal Acc: 35.0588244%\n",
      "Validation Loss decreased from 1.994830 to 1.958490, saving the model weights\n",
      "Epoch: 304\tTrain Loss: 2.1162297 \tVal Loss:1.8736344 \tTrain Acc: 30.63235% \tVal Acc: 40.2352947%\n",
      "Validation Loss decreased from 1.958490 to 1.873634, saving the model weights\n",
      "Epoch: 305\tTrain Loss: 2.0546850 \tVal Loss:1.8790624 \tTrain Acc: 33.13235% \tVal Acc: 40.5882359%\n",
      "Epoch: 306\tTrain Loss: 1.9920607 \tVal Loss:1.8902893 \tTrain Acc: 35.77941% \tVal Acc: 39.1176477%\n",
      "Epoch: 307\tTrain Loss: 1.9468302 \tVal Loss:1.8269460 \tTrain Acc: 36.51471% \tVal Acc: 42.6470596%\n",
      "Validation Loss decreased from 1.873634 to 1.826946, saving the model weights\n",
      "Epoch: 308\tTrain Loss: 1.8992369 \tVal Loss:1.7923964 \tTrain Acc: 38.70588% \tVal Acc: 43.2941180%\n",
      "Validation Loss decreased from 1.826946 to 1.792396, saving the model weights\n",
      "Epoch: 309\tTrain Loss: 1.8775923 \tVal Loss:1.7710955 \tTrain Acc: 39.05882% \tVal Acc: 43.7647066%\n",
      "Validation Loss decreased from 1.792396 to 1.771095, saving the model weights\n",
      "Epoch: 310\tTrain Loss: 1.8467166 \tVal Loss:1.7600061 \tTrain Acc: 40.22059% \tVal Acc: 45.8235300%\n",
      "Validation Loss decreased from 1.771095 to 1.760006, saving the model weights\n",
      "Epoch: 311\tTrain Loss: 1.8346561 \tVal Loss:1.7426017 \tTrain Acc: 40.27941% \tVal Acc: 44.4705886%\n",
      "Validation Loss decreased from 1.760006 to 1.742602, saving the model weights\n",
      "Epoch: 312\tTrain Loss: 1.8192419 \tVal Loss:1.7317296 \tTrain Acc: 41.14706% \tVal Acc: 46.5882355%\n",
      "Validation Loss decreased from 1.742602 to 1.731730, saving the model weights\n",
      "Epoch: 313\tTrain Loss: 1.7972269 \tVal Loss:1.7329826 \tTrain Acc: 42.27941% \tVal Acc: 45.3529418%\n",
      "Epoch: 314\tTrain Loss: 1.7963119 \tVal Loss:1.7039183 \tTrain Acc: 42.01471% \tVal Acc: 46.1764711%\n",
      "Validation Loss decreased from 1.731730 to 1.703918, saving the model weights\n",
      "Epoch: 315\tTrain Loss: 1.7853428 \tVal Loss:1.7170893 \tTrain Acc: 43.05882% \tVal Acc: 45.4117653%\n",
      "Epoch: 316\tTrain Loss: 1.7867376 \tVal Loss:1.7159412 \tTrain Acc: 42.26471% \tVal Acc: 45.7647064%\n",
      "Epoch: 317\tTrain Loss: 1.7673936 \tVal Loss:1.7006395 \tTrain Acc: 43.79412% \tVal Acc: 45.8235297%\n",
      "Validation Loss decreased from 1.703918 to 1.700639, saving the model weights\n",
      "Epoch: 318\tTrain Loss: 1.7713146 \tVal Loss:1.6919054 \tTrain Acc: 43.05882% \tVal Acc: 47.8823534%\n",
      "Validation Loss decreased from 1.700639 to 1.691905, saving the model weights\n",
      "Epoch: 319\tTrain Loss: 1.7658259 \tVal Loss:1.6780619 \tTrain Acc: 43.38235% \tVal Acc: 48.0000007%\n",
      "Validation Loss decreased from 1.691905 to 1.678062, saving the model weights\n",
      "Epoch: 320\tTrain Loss: 1.7440920 \tVal Loss:1.7021509 \tTrain Acc: 43.44118% \tVal Acc: 45.8823532%\n",
      "Epoch: 321\tTrain Loss: 1.7374066 \tVal Loss:1.6908382 \tTrain Acc: 44.26471% \tVal Acc: 47.1176478%\n",
      "Epoch: 322\tTrain Loss: 1.7396502 \tVal Loss:1.6636523 \tTrain Acc: 43.77941% \tVal Acc: 48.4705886%\n",
      "Validation Loss decreased from 1.678062 to 1.663652, saving the model weights\n",
      "Epoch: 323\tTrain Loss: 1.7488948 \tVal Loss:1.6794587 \tTrain Acc: 43.98529% \tVal Acc: 47.7058825%\n",
      "Epoch: 324\tTrain Loss: 1.7370846 \tVal Loss:1.6470606 \tTrain Acc: 43.57353% \tVal Acc: 47.9411778%\n",
      "Validation Loss decreased from 1.663652 to 1.647061, saving the model weights\n",
      "Epoch: 325\tTrain Loss: 1.7467500 \tVal Loss:1.6741349 \tTrain Acc: 43.23529% \tVal Acc: 47.7647060%\n",
      "Epoch: 326\tTrain Loss: 1.7433199 \tVal Loss:1.6111597 \tTrain Acc: 43.55882% \tVal Acc: 49.9411768%\n",
      "Validation Loss decreased from 1.647061 to 1.611160, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 327\tTrain Loss: 1.7175130 \tVal Loss:1.6689204 \tTrain Acc: 45.23529% \tVal Acc: 47.0588246%\n",
      "Epoch: 328\tTrain Loss: 1.7319995 \tVal Loss:1.6269247 \tTrain Acc: 43.95588% \tVal Acc: 49.4705889%\n",
      "Epoch: 329\tTrain Loss: 1.7197523 \tVal Loss:1.6735374 \tTrain Acc: 44.38235% \tVal Acc: 46.2352943%\n",
      "Epoch: 330\tTrain Loss: 1.7074203 \tVal Loss:1.6008882 \tTrain Acc: 44.66177% \tVal Acc: 49.4117659%\n",
      "Validation Loss decreased from 1.611160 to 1.600888, saving the model weights\n",
      "Epoch: 331\tTrain Loss: 1.7235635 \tVal Loss:1.6465123 \tTrain Acc: 44.54412% \tVal Acc: 47.3529413%\n",
      "Epoch: 332\tTrain Loss: 1.6991572 \tVal Loss:1.6973015 \tTrain Acc: 44.48529% \tVal Acc: 46.3529417%\n",
      "Epoch: 333\tTrain Loss: 1.7224483 \tVal Loss:1.7452251 \tTrain Acc: 44.2353% \tVal Acc: 44.0588242%\n",
      "Epoch: 334\tTrain Loss: 1.7440221 \tVal Loss:1.6547145 \tTrain Acc: 43.66177% \tVal Acc: 46.2352946%\n",
      "Epoch: 335\tTrain Loss: 1.7095190 \tVal Loss:1.6169124 \tTrain Acc: 44.69118% \tVal Acc: 48.5882363%\n",
      "Epoch: 336\tTrain Loss: 1.6705147 \tVal Loss:1.6297038 \tTrain Acc: 45.83824% \tVal Acc: 47.4117649%\n",
      "Epoch: 337\tTrain Loss: 1.6976010 \tVal Loss:1.6111098 \tTrain Acc: 45.11765% \tVal Acc: 48.3529419%\n",
      "Epoch: 338\tTrain Loss: 1.6676337 \tVal Loss:1.6665546 \tTrain Acc: 46.69118% \tVal Acc: 45.7647061%\n",
      "Epoch: 339\tTrain Loss: 1.6841908 \tVal Loss:1.6023276 \tTrain Acc: 45.35294% \tVal Acc: 48.7058827%\n",
      "Epoch: 340\tTrain Loss: 1.6678928 \tVal Loss:1.5590202 \tTrain Acc: 46.98529% \tVal Acc: 50.8235297%\n",
      "Validation Loss decreased from 1.600888 to 1.559020, saving the model weights\n",
      "Epoch: 341\tTrain Loss: 1.6850739 \tVal Loss:1.5827226 \tTrain Acc: 45.45588% \tVal Acc: 50.8235306%\n",
      "Epoch: 342\tTrain Loss: 1.7005255 \tVal Loss:1.5414261 \tTrain Acc: 45.0% \tVal Acc: 51.7058828%\n",
      "Validation Loss decreased from 1.559020 to 1.541426, saving the model weights\n",
      "Epoch: 343\tTrain Loss: 1.6364402 \tVal Loss:1.5190386 \tTrain Acc: 47.54412% \tVal Acc: 53.1764716%\n",
      "Validation Loss decreased from 1.541426 to 1.519039, saving the model weights\n",
      "Epoch: 344\tTrain Loss: 1.6632841 \tVal Loss:1.5689763 \tTrain Acc: 46.66177% \tVal Acc: 51.6470596%\n",
      "Epoch: 345\tTrain Loss: 1.6493012 \tVal Loss:1.5778937 \tTrain Acc: 46.95588% \tVal Acc: 51.5294129%\n",
      "Epoch: 346\tTrain Loss: 1.6421789 \tVal Loss:1.5006378 \tTrain Acc: 46.95588% \tVal Acc: 54.1764712%\n",
      "Validation Loss decreased from 1.519039 to 1.500638, saving the model weights\n",
      "Epoch: 347\tTrain Loss: 1.6238679 \tVal Loss:1.5127192 \tTrain Acc: 48.01471% \tVal Acc: 51.7058831%\n",
      "Epoch: 348\tTrain Loss: 1.6193969 \tVal Loss:1.5340142 \tTrain Acc: 47.36765% \tVal Acc: 51.6470593%\n",
      "Epoch: 349\tTrain Loss: 1.6258697 \tVal Loss:1.5266362 \tTrain Acc: 47.89706% \tVal Acc: 53.3529413%\n",
      "Epoch: 350\tTrain Loss: 1.5962717 \tVal Loss:1.5319108 \tTrain Acc: 49.16177% \tVal Acc: 53.0000016%\n",
      "Epoch: 351\tTrain Loss: 1.5867359 \tVal Loss:1.5023501 \tTrain Acc: 49.20588% \tVal Acc: 53.5882357%\n",
      "Epoch: 352\tTrain Loss: 1.5959899 \tVal Loss:1.4623010 \tTrain Acc: 48.82353% \tVal Acc: 54.5294121%\n",
      "Validation Loss decreased from 1.500638 to 1.462301, saving the model weights\n",
      "Epoch: 353\tTrain Loss: 1.5763885 \tVal Loss:1.4890038 \tTrain Acc: 49.22059% \tVal Acc: 55.7647064%\n",
      "Epoch: 354\tTrain Loss: 1.5922693 \tVal Loss:1.4767087 \tTrain Acc: 49.04412% \tVal Acc: 54.7647071%\n",
      "Epoch: 355\tTrain Loss: 1.6051271 \tVal Loss:1.4354824 \tTrain Acc: 48.67647% \tVal Acc: 56.7058828%\n",
      "Validation Loss decreased from 1.462301 to 1.435482, saving the model weights\n",
      "Epoch: 356\tTrain Loss: 1.6061868 \tVal Loss:1.4230915 \tTrain Acc: 49.27941% \tVal Acc: 57.0588246%\n",
      "Validation Loss decreased from 1.435482 to 1.423092, saving the model weights\n",
      "Epoch: 357\tTrain Loss: 1.6418705 \tVal Loss:1.4823195 \tTrain Acc: 47.57353% \tVal Acc: 53.2941189%\n",
      "Epoch: 358\tTrain Loss: 1.6380827 \tVal Loss:1.4644868 \tTrain Acc: 47.19118% \tVal Acc: 55.5882362%\n",
      "Epoch: 359\tTrain Loss: 1.5995845 \tVal Loss:1.4490270 \tTrain Acc: 48.41177% \tVal Acc: 56.1764714%\n",
      "Epoch: 360\tTrain Loss: 1.5593959 \tVal Loss:1.4869069 \tTrain Acc: 50.76471% \tVal Acc: 54.0588245%\n",
      "Epoch: 361\tTrain Loss: 1.5496785 \tVal Loss:1.4152562 \tTrain Acc: 49.82353% \tVal Acc: 57.8235295%\n",
      "Validation Loss decreased from 1.423092 to 1.415256, saving the model weights\n",
      "Epoch: 362\tTrain Loss: 1.5399940 \tVal Loss:1.4326295 \tTrain Acc: 50.85294% \tVal Acc: 57.7058825%\n",
      "Epoch: 363\tTrain Loss: 1.5659561 \tVal Loss:1.4052640 \tTrain Acc: 50.08824% \tVal Acc: 58.4705889%\n",
      "Validation Loss decreased from 1.415256 to 1.405264, saving the model weights\n",
      "Epoch: 364\tTrain Loss: 1.5576729 \tVal Loss:1.4347994 \tTrain Acc: 50.05882% \tVal Acc: 56.5882361%\n",
      "Epoch: 365\tTrain Loss: 1.5511187 \tVal Loss:1.4180861 \tTrain Acc: 50.72059% \tVal Acc: 55.9411776%\n",
      "Epoch: 366\tTrain Loss: 1.5561724 \tVal Loss:1.3719895 \tTrain Acc: 49.61765% \tVal Acc: 58.0588245%\n",
      "Validation Loss decreased from 1.405264 to 1.371989, saving the model weights\n",
      "Epoch: 367\tTrain Loss: 1.5941565 \tVal Loss:1.4644822 \tTrain Acc: 49.04412% \tVal Acc: 55.8823538%\n",
      "Epoch: 368\tTrain Loss: 1.5387930 \tVal Loss:1.3771774 \tTrain Acc: 50.76471% \tVal Acc: 58.0000010%\n",
      "Epoch: 369\tTrain Loss: 1.5402592 \tVal Loss:1.3797409 \tTrain Acc: 51.13235% \tVal Acc: 58.8235307%\n",
      "Epoch: 370\tTrain Loss: 1.5382686 \tVal Loss:1.4375004 \tTrain Acc: 51.44118% \tVal Acc: 56.7647070%\n",
      "Epoch: 371\tTrain Loss: 1.5300180 \tVal Loss:1.4032159 \tTrain Acc: 51.76471% \tVal Acc: 57.3529413%\n",
      "Epoch: 372\tTrain Loss: 1.5139330 \tVal Loss:1.4182931 \tTrain Acc: 51.79412% \tVal Acc: 58.0588248%\n",
      "Epoch: 373\tTrain Loss: 1.5192019 \tVal Loss:1.4500030 \tTrain Acc: 52.97059% \tVal Acc: 55.1176476%\n",
      "Epoch: 374\tTrain Loss: 1.5498888 \tVal Loss:1.4555937 \tTrain Acc: 51.02941% \tVal Acc: 54.8235300%\n",
      "Epoch: 375\tTrain Loss: 1.5435219 \tVal Loss:1.4102138 \tTrain Acc: 50.72059% \tVal Acc: 56.6470602%\n",
      "Epoch: 376\tTrain Loss: 1.5229422 \tVal Loss:1.4694651 \tTrain Acc: 50.64706% \tVal Acc: 54.7058842%\n",
      "Epoch: 377\tTrain Loss: 1.5267032 \tVal Loss:1.4323606 \tTrain Acc: 51.61765% \tVal Acc: 55.5882362%\n",
      "Epoch: 378\tTrain Loss: 1.5273926 \tVal Loss:1.4577352 \tTrain Acc: 51.89706% \tVal Acc: 55.8823544%\n",
      "Epoch: 379\tTrain Loss: 1.5474397 \tVal Loss:1.4262478 \tTrain Acc: 50.88235% \tVal Acc: 56.1764714%\n",
      "Epoch: 380\tTrain Loss: 1.6094550 \tVal Loss:1.5235271 \tTrain Acc: 48.55882% \tVal Acc: 54.1176471%\n",
      "Epoch: 381\tTrain Loss: 1.6168609 \tVal Loss:1.4338958 \tTrain Acc: 47.67647% \tVal Acc: 57.1176475%\n",
      "Epoch: 382\tTrain Loss: 1.5649797 \tVal Loss:1.4306406 \tTrain Acc: 50.5% \tVal Acc: 56.2941188%\n",
      "Epoch: 383\tTrain Loss: 1.5208899 \tVal Loss:1.3989319 \tTrain Acc: 51.38235% \tVal Acc: 58.5294127%\n",
      "Epoch: 384\tTrain Loss: 1.5141063 \tVal Loss:1.3360301 \tTrain Acc: 51.86765% \tVal Acc: 59.9411768%\n",
      "Validation Loss decreased from 1.371989 to 1.336030, saving the model weights\n",
      "Epoch: 385\tTrain Loss: 1.4859854 \tVal Loss:1.4970195 \tTrain Acc: 52.88235% \tVal Acc: 52.5294116%\n",
      "Epoch: 386\tTrain Loss: 1.5177508 \tVal Loss:1.4448551 \tTrain Acc: 51.82353% \tVal Acc: 55.8235303%\n",
      "Epoch: 387\tTrain Loss: 1.5194552 \tVal Loss:1.3982288 \tTrain Acc: 51.98529% \tVal Acc: 56.8235308%\n",
      "Epoch: 388\tTrain Loss: 1.5040941 \tVal Loss:1.5171108 \tTrain Acc: 52.35294% \tVal Acc: 53.0000013%\n",
      "Epoch: 389\tTrain Loss: 1.4866725 \tVal Loss:1.4906716 \tTrain Acc: 52.83824% \tVal Acc: 54.4705892%\n",
      "Epoch: 390\tTrain Loss: 1.5323624 \tVal Loss:1.5744715 \tTrain Acc: 50.94118% \tVal Acc: 51.4705890%\n",
      "Epoch: 391\tTrain Loss: 1.5298664 \tVal Loss:1.5641930 \tTrain Acc: 52.08824% \tVal Acc: 50.2941179%\n",
      "Epoch: 392\tTrain Loss: 1.5499856 \tVal Loss:1.4872035 \tTrain Acc: 50.94118% \tVal Acc: 54.4117659%\n",
      "Epoch: 393\tTrain Loss: 1.5434851 \tVal Loss:1.5917902 \tTrain Acc: 51.17647% \tVal Acc: 50.4705882%\n",
      "Epoch: 394\tTrain Loss: 1.5340634 \tVal Loss:1.7975481 \tTrain Acc: 51.10294% \tVal Acc: 43.4117654%\n",
      "Epoch: 395\tTrain Loss: 1.5596825 \tVal Loss:1.7145733 \tTrain Acc: 50.92647% \tVal Acc: 44.0000007%\n",
      "Epoch: 396\tTrain Loss: 1.5589586 \tVal Loss:1.5601446 \tTrain Acc: 51.08824% \tVal Acc: 50.4705888%\n",
      "Epoch: 397\tTrain Loss: 1.6622432 \tVal Loss:1.4224006 \tTrain Acc: 47.23529% \tVal Acc: 54.9411780%\n",
      "Epoch: 398\tTrain Loss: 1.5944532 \tVal Loss:1.3283450 \tTrain Acc: 48.52941% \tVal Acc: 58.8823539%\n",
      "Validation Loss decreased from 1.336030 to 1.328345, saving the model weights\n",
      "Epoch: 399\tTrain Loss: 1.5013781 \tVal Loss:1.3022628 \tTrain Acc: 51.38235% \tVal Acc: 60.8235300%\n",
      "Validation Loss decreased from 1.328345 to 1.302263, saving the model weights\n",
      "Epoch: 400\tTrain Loss: 1.4292706 \tVal Loss:1.2303356 \tTrain Acc: 54.98529% \tVal Acc: 64.2352945%\n",
      "Validation Loss decreased from 1.302263 to 1.230336, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 401\tTrain Loss: 1.3581283 \tVal Loss:1.1612062 \tTrain Acc: 58.33824% \tVal Acc: 66.1764711%\n",
      "Validation Loss decreased from 1.230336 to 1.161206, saving the model weights\n",
      "Epoch: 402\tTrain Loss: 1.3093110 \tVal Loss:1.1238968 \tTrain Acc: 59.47059% \tVal Acc: 67.9411763%\n",
      "Validation Loss decreased from 1.161206 to 1.123897, saving the model weights\n",
      "Epoch: 403\tTrain Loss: 1.2859181 \tVal Loss:1.1105361 \tTrain Acc: 59.67647% \tVal Acc: 67.0588243%\n",
      "Validation Loss decreased from 1.123897 to 1.110536, saving the model weights\n",
      "Epoch: 404\tTrain Loss: 1.2964725 \tVal Loss:1.1004819 \tTrain Acc: 59.54412% \tVal Acc: 67.7647066%\n",
      "Validation Loss decreased from 1.110536 to 1.100482, saving the model weights\n",
      "Epoch: 405\tTrain Loss: 1.2901407 \tVal Loss:1.0674929 \tTrain Acc: 58.95588% \tVal Acc: 70.5882353%\n",
      "Validation Loss decreased from 1.100482 to 1.067493, saving the model weights\n",
      "Epoch: 406\tTrain Loss: 1.2923778 \tVal Loss:1.0863791 \tTrain Acc: 58.55882% \tVal Acc: 69.1176474%\n",
      "Epoch: 407\tTrain Loss: 1.2543086 \tVal Loss:1.0712710 \tTrain Acc: 61.05882% \tVal Acc: 68.0588239%\n",
      "Epoch: 408\tTrain Loss: 1.2681335 \tVal Loss:1.0573764 \tTrain Acc: 60.58824% \tVal Acc: 70.4117644%\n",
      "Validation Loss decreased from 1.067493 to 1.057376, saving the model weights\n",
      "Epoch: 409\tTrain Loss: 1.2798974 \tVal Loss:1.0695917 \tTrain Acc: 59.72059% \tVal Acc: 69.6470594%\n",
      "Epoch: 410\tTrain Loss: 1.2687497 \tVal Loss:1.0279211 \tTrain Acc: 60.11765% \tVal Acc: 70.1176471%\n",
      "Validation Loss decreased from 1.057376 to 1.027921, saving the model weights\n",
      "Epoch: 411\tTrain Loss: 1.2665173 \tVal Loss:1.0764537 \tTrain Acc: 60.01471% \tVal Acc: 67.8823531%\n",
      "Epoch: 412\tTrain Loss: 1.2719343 \tVal Loss:1.0632735 \tTrain Acc: 59.97059% \tVal Acc: 68.4117651%\n",
      "Epoch: 413\tTrain Loss: 1.2877207 \tVal Loss:1.0540783 \tTrain Acc: 59.72059% \tVal Acc: 69.7647059%\n",
      "Epoch: 414\tTrain Loss: 1.2472934 \tVal Loss:1.0434370 \tTrain Acc: 60.22059% \tVal Acc: 69.3529415%\n",
      "Epoch: 415\tTrain Loss: 1.2372262 \tVal Loss:0.9908735 \tTrain Acc: 61.39706% \tVal Acc: 71.8823534%\n",
      "Validation Loss decreased from 1.027921 to 0.990874, saving the model weights\n",
      "Epoch: 416\tTrain Loss: 1.5225069 \tVal Loss:1.8381843 \tTrain Acc: 53.52941% \tVal Acc: 43.1176475%\n",
      "Epoch: 417\tTrain Loss: 1.8484724 \tVal Loss:1.4711656 \tTrain Acc: 41.60294% \tVal Acc: 52.5882354%\n",
      "Epoch: 418\tTrain Loss: 1.4903829 \tVal Loss:1.1828829 \tTrain Acc: 52.19118% \tVal Acc: 64.8823535%\n",
      "Epoch: 419\tTrain Loss: 1.3492805 \tVal Loss:1.1128839 \tTrain Acc: 56.92647% \tVal Acc: 66.5294123%\n",
      "Epoch: 420\tTrain Loss: 1.2942424 \tVal Loss:1.0474840 \tTrain Acc: 58.97059% \tVal Acc: 69.0588236%\n",
      "Epoch: 421\tTrain Loss: 1.2330079 \tVal Loss:1.0107896 \tTrain Acc: 60.70588% \tVal Acc: 70.8823532%\n",
      "Epoch: 422\tTrain Loss: 1.1984765 \tVal Loss:0.9944842 \tTrain Acc: 62.22059% \tVal Acc: 71.0000008%\n",
      "Epoch: 423\tTrain Loss: 1.2033930 \tVal Loss:0.9730115 \tTrain Acc: 62.02941% \tVal Acc: 71.6470599%\n",
      "Validation Loss decreased from 0.990874 to 0.973012, saving the model weights\n",
      "Epoch: 424\tTrain Loss: 1.1903698 \tVal Loss:1.0052419 \tTrain Acc: 62.45588% \tVal Acc: 70.4117656%\n",
      "Epoch: 425\tTrain Loss: 1.2004011 \tVal Loss:1.0157057 \tTrain Acc: 62.07353% \tVal Acc: 69.5882356%\n",
      "Epoch: 426\tTrain Loss: 1.1842993 \tVal Loss:0.9976098 \tTrain Acc: 62.20588% \tVal Acc: 70.2941185%\n",
      "Epoch: 427\tTrain Loss: 1.2307730 \tVal Loss:1.0187864 \tTrain Acc: 60.82353% \tVal Acc: 69.8235309%\n",
      "Epoch: 428\tTrain Loss: 1.1999990 \tVal Loss:1.0333664 \tTrain Acc: 61.63235% \tVal Acc: 68.6470598%\n",
      "Epoch: 429\tTrain Loss: 1.1951493 \tVal Loss:1.0236137 \tTrain Acc: 62.02941% \tVal Acc: 69.2352939%\n",
      "Epoch: 430\tTrain Loss: 1.1793317 \tVal Loss:1.1239361 \tTrain Acc: 62.25% \tVal Acc: 65.6470597%\n",
      "Epoch: 431\tTrain Loss: 1.1875918 \tVal Loss:1.0484129 \tTrain Acc: 62.20588% \tVal Acc: 68.5882354%\n",
      "Epoch: 432\tTrain Loss: 1.1695989 \tVal Loss:1.0237984 \tTrain Acc: 63.27941% \tVal Acc: 69.7647065%\n",
      "Epoch: 433\tTrain Loss: 1.1578880 \tVal Loss:1.0359484 \tTrain Acc: 63.38235% \tVal Acc: 68.9411771%\n",
      "Epoch: 434\tTrain Loss: 1.1536261 \tVal Loss:1.0734226 \tTrain Acc: 63.79412% \tVal Acc: 66.5882361%\n",
      "Epoch: 435\tTrain Loss: 1.1793274 \tVal Loss:1.1093584 \tTrain Acc: 62.17647% \tVal Acc: 66.1764723%\n",
      "Epoch: 436\tTrain Loss: 1.1582696 \tVal Loss:1.0271283 \tTrain Acc: 63.0% \tVal Acc: 70.1764709%\n",
      "Epoch: 437\tTrain Loss: 1.1389066 \tVal Loss:1.0989294 \tTrain Acc: 64.58824% \tVal Acc: 66.1764711%\n",
      "Epoch: 438\tTrain Loss: 1.1390909 \tVal Loss:1.0364140 \tTrain Acc: 63.76471% \tVal Acc: 68.8823533%\n",
      "Epoch: 439\tTrain Loss: 1.1515914 \tVal Loss:0.9884347 \tTrain Acc: 63.33824% \tVal Acc: 70.6470597%\n",
      "Epoch: 440\tTrain Loss: 1.1237021 \tVal Loss:1.0059212 \tTrain Acc: 64.5% \tVal Acc: 70.4705888%\n",
      "Epoch: 441\tTrain Loss: 1.1213371 \tVal Loss:0.9778447 \tTrain Acc: 64.72059% \tVal Acc: 69.8235297%\n",
      "Epoch: 442\tTrain Loss: 1.0967875 \tVal Loss:0.9050091 \tTrain Acc: 65.30882% \tVal Acc: 72.5882357%\n",
      "Validation Loss decreased from 0.973012 to 0.905009, saving the model weights\n",
      "Epoch: 443\tTrain Loss: 1.0808356 \tVal Loss:0.9495811 \tTrain Acc: 66.01471% \tVal Acc: 70.6470585%\n",
      "Epoch: 444\tTrain Loss: 1.0911718 \tVal Loss:0.9366924 \tTrain Acc: 65.33824% \tVal Acc: 71.9411772%\n",
      "Epoch: 445\tTrain Loss: 1.0853644 \tVal Loss:0.9124289 \tTrain Acc: 65.39706% \tVal Acc: 73.1176478%\n",
      "Epoch: 446\tTrain Loss: 1.0727796 \tVal Loss:0.8874969 \tTrain Acc: 66.48529% \tVal Acc: 73.6470586%\n",
      "Validation Loss decreased from 0.905009 to 0.887497, saving the model weights\n",
      "Epoch: 447\tTrain Loss: 1.0570635 \tVal Loss:0.8852413 \tTrain Acc: 66.32353% \tVal Acc: 73.0588239%\n",
      "Validation Loss decreased from 0.887497 to 0.885241, saving the model weights\n",
      "Epoch: 448\tTrain Loss: 1.0506427 \tVal Loss:0.8295327 \tTrain Acc: 66.72059% \tVal Acc: 74.7647059%\n",
      "Validation Loss decreased from 0.885241 to 0.829533, saving the model weights\n",
      "Epoch: 449\tTrain Loss: 1.0381308 \tVal Loss:0.8681074 \tTrain Acc: 67.76471% \tVal Acc: 73.8823521%\n",
      "Epoch: 450\tTrain Loss: 1.0136141 \tVal Loss:0.9382855 \tTrain Acc: 67.92647% \tVal Acc: 71.1176473%\n",
      "Epoch: 451\tTrain Loss: 1.0522694 \tVal Loss:0.9006427 \tTrain Acc: 66.92647% \tVal Acc: 72.5882351%\n",
      "Epoch: 452\tTrain Loss: 1.0413070 \tVal Loss:0.8374862 \tTrain Acc: 67.67647% \tVal Acc: 75.5294114%\n",
      "Epoch: 453\tTrain Loss: 1.0193618 \tVal Loss:0.8175668 \tTrain Acc: 67.75% \tVal Acc: 76.1764711%\n",
      "Validation Loss decreased from 0.829533 to 0.817567, saving the model weights\n",
      "Epoch: 454\tTrain Loss: 1.0289159 \tVal Loss:0.8551649 \tTrain Acc: 68.04412% \tVal Acc: 73.2352948%\n",
      "Epoch: 455\tTrain Loss: 1.0354464 \tVal Loss:0.8049584 \tTrain Acc: 67.60294% \tVal Acc: 75.9411770%\n",
      "Validation Loss decreased from 0.817567 to 0.804958, saving the model weights\n",
      "Epoch: 456\tTrain Loss: 1.0490712 \tVal Loss:0.8885424 \tTrain Acc: 66.67647% \tVal Acc: 73.1764704%\n",
      "Epoch: 457\tTrain Loss: 1.0427187 \tVal Loss:0.8524261 \tTrain Acc: 66.94118% \tVal Acc: 75.1176465%\n",
      "Epoch: 458\tTrain Loss: 1.0258832 \tVal Loss:0.8566630 \tTrain Acc: 67.97059% \tVal Acc: 74.6470582%\n",
      "Epoch: 459\tTrain Loss: 1.0430735 \tVal Loss:0.9462061 \tTrain Acc: 67.52941% \tVal Acc: 70.8823532%\n",
      "Epoch: 460\tTrain Loss: 1.0196556 \tVal Loss:0.9146276 \tTrain Acc: 67.76471% \tVal Acc: 71.0588235%\n",
      "Epoch: 461\tTrain Loss: 1.0372269 \tVal Loss:0.8302442 \tTrain Acc: 67.54412% \tVal Acc: 74.7058821%\n",
      "Epoch: 462\tTrain Loss: 1.0843496 \tVal Loss:0.8611106 \tTrain Acc: 66.23529% \tVal Acc: 73.8823533%\n",
      "Epoch: 463\tTrain Loss: 1.1050626 \tVal Loss:0.8749799 \tTrain Acc: 65.23529% \tVal Acc: 73.2941186%\n",
      "Epoch: 464\tTrain Loss: 1.0448719 \tVal Loss:0.8486575 \tTrain Acc: 66.67647% \tVal Acc: 74.6470588%\n",
      "Epoch: 465\tTrain Loss: 0.9962685 \tVal Loss:0.8915768 \tTrain Acc: 68.11765% \tVal Acc: 72.7058834%\n",
      "Epoch: 466\tTrain Loss: 0.9741704 \tVal Loss:0.9210422 \tTrain Acc: 69.61765% \tVal Acc: 71.5294117%\n",
      "Epoch: 467\tTrain Loss: 0.9716398 \tVal Loss:0.7928287 \tTrain Acc: 69.91177% \tVal Acc: 75.7647061%\n",
      "Validation Loss decreased from 0.804958 to 0.792829, saving the model weights\n",
      "Epoch: 468\tTrain Loss: 0.9597394 \tVal Loss:0.7688043 \tTrain Acc: 70.35294% \tVal Acc: 76.9411761%\n",
      "Validation Loss decreased from 0.792829 to 0.768804, saving the model weights\n",
      "Epoch: 469\tTrain Loss: 0.9232258 \tVal Loss:0.8568586 \tTrain Acc: 71.01471% \tVal Acc: 73.4705895%\n",
      "Epoch: 470\tTrain Loss: 0.9279177 \tVal Loss:0.7688476 \tTrain Acc: 70.97059% \tVal Acc: 76.9411761%\n",
      "Epoch: 471\tTrain Loss: 0.9409235 \tVal Loss:0.7760080 \tTrain Acc: 70.82353% \tVal Acc: 77.3529410%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 472\tTrain Loss: 0.9523204 \tVal Loss:0.7758206 \tTrain Acc: 70.16176% \tVal Acc: 77.8235292%\n",
      "Epoch: 473\tTrain Loss: 0.9524773 \tVal Loss:0.7378816 \tTrain Acc: 69.69118% \tVal Acc: 78.4705883%\n",
      "Validation Loss decreased from 0.768804 to 0.737882, saving the model weights\n",
      "Epoch: 474\tTrain Loss: 0.9763717 \tVal Loss:0.8041264 \tTrain Acc: 69.27941% \tVal Acc: 75.4705876%\n",
      "Epoch: 475\tTrain Loss: 0.9606206 \tVal Loss:0.7999308 \tTrain Acc: 70.23529% \tVal Acc: 76.2352943%\n",
      "Epoch: 476\tTrain Loss: 0.9671556 \tVal Loss:0.8165788 \tTrain Acc: 69.20588% \tVal Acc: 75.3529412%\n",
      "Epoch: 477\tTrain Loss: 0.9453627 \tVal Loss:0.7550468 \tTrain Acc: 70.36765% \tVal Acc: 77.8823525%\n",
      "Epoch: 478\tTrain Loss: 0.9170957 \tVal Loss:0.7164104 \tTrain Acc: 71.41176% \tVal Acc: 78.6470586%\n",
      "Validation Loss decreased from 0.737882 to 0.716410, saving the model weights\n",
      "Epoch: 479\tTrain Loss: 0.9077663 \tVal Loss:0.7715735 \tTrain Acc: 72.23529% \tVal Acc: 77.1176463%\n",
      "Epoch: 480\tTrain Loss: 0.9096650 \tVal Loss:0.8117018 \tTrain Acc: 71.54412% \tVal Acc: 75.7058823%\n",
      "Epoch: 481\tTrain Loss: 0.9303840 \tVal Loss:0.7429410 \tTrain Acc: 70.52941% \tVal Acc: 78.3529407%\n",
      "Epoch: 482\tTrain Loss: 0.9212880 \tVal Loss:0.6768227 \tTrain Acc: 71.17647% \tVal Acc: 80.4705870%\n",
      "Validation Loss decreased from 0.716410 to 0.676823, saving the model weights\n",
      "Epoch: 483\tTrain Loss: 0.9063121 \tVal Loss:0.6599091 \tTrain Acc: 71.38235% \tVal Acc: 80.8823526%\n",
      "Validation Loss decreased from 0.676823 to 0.659909, saving the model weights\n",
      "Epoch: 484\tTrain Loss: 0.8905582 \tVal Loss:0.7186747 \tTrain Acc: 72.22059% \tVal Acc: 78.5294122%\n",
      "Epoch: 485\tTrain Loss: 0.8730873 \tVal Loss:0.6801372 \tTrain Acc: 72.86765% \tVal Acc: 79.8235297%\n",
      "Epoch: 486\tTrain Loss: 0.8582258 \tVal Loss:0.7428197 \tTrain Acc: 73.61765% \tVal Acc: 78.5882348%\n",
      "Epoch: 487\tTrain Loss: 0.8520680 \tVal Loss:0.6993234 \tTrain Acc: 73.04412% \tVal Acc: 80.0000000%\n",
      "Epoch: 488\tTrain Loss: 0.8536906 \tVal Loss:0.7081431 \tTrain Acc: 73.69118% \tVal Acc: 78.8823533%\n",
      "Epoch: 489\tTrain Loss: 0.8699031 \tVal Loss:0.7251583 \tTrain Acc: 72.73529% \tVal Acc: 78.1176478%\n",
      "Epoch: 490\tTrain Loss: 0.8942820 \tVal Loss:0.7083005 \tTrain Acc: 71.79412% \tVal Acc: 78.4117651%\n",
      "Epoch: 491\tTrain Loss: 0.8533534 \tVal Loss:0.6951208 \tTrain Acc: 73.23529% \tVal Acc: 79.5294118%\n",
      "Epoch: 492\tTrain Loss: 0.8532913 \tVal Loss:0.7080474 \tTrain Acc: 73.14706% \tVal Acc: 77.8823537%\n",
      "Epoch: 493\tTrain Loss: 0.8692251 \tVal Loss:0.7527520 \tTrain Acc: 72.58824% \tVal Acc: 77.4705887%\n",
      "Epoch: 494\tTrain Loss: 0.8693758 \tVal Loss:0.8492823 \tTrain Acc: 73.07353% \tVal Acc: 73.8235295%\n",
      "Epoch: 495\tTrain Loss: 0.8985662 \tVal Loss:0.8203969 \tTrain Acc: 71.57353% \tVal Acc: 73.8235289%\n",
      "Epoch: 496\tTrain Loss: 0.9196595 \tVal Loss:0.9765577 \tTrain Acc: 71.08823% \tVal Acc: 68.7058824%\n",
      "Epoch: 497\tTrain Loss: 0.8993019 \tVal Loss:1.0174805 \tTrain Acc: 71.45588% \tVal Acc: 66.9411767%\n",
      "Epoch: 498\tTrain Loss: 0.8861540 \tVal Loss:0.8805958 \tTrain Acc: 71.70588% \tVal Acc: 72.0588243%\n",
      "Epoch: 499\tTrain Loss: 0.8837736 \tVal Loss:0.9751888 \tTrain Acc: 72.73529% \tVal Acc: 70.0588244%\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        '''\n",
    "        Creating new variables for the hidden state, otherwise\n",
    "        we'd backprop through the entire training history\n",
    "        '''\n",
    "        \n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "       \n",
    "        # get the output from the model\n",
    "        output = model.forward(inputs, train_batch_size)\n",
    "        #print('OUTPUT', output)\n",
    "        \n",
    "        \n",
    "        #print('Labels Shape :-', (torch.max(labels, 1)[1]).shape)\n",
    "    \n",
    "        # calculate the loss and perform backprop\n",
    "        #print('Labels Long :-', labels.long())\n",
    "        loss = criterion(output,labels.long())\n",
    "        #print('LOSS IS :-', loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #calculate training accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        #logging.debug(' top probab {} top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        #print(train_loss)\n",
    "              \n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "                \n",
    "        output = model.forward(inputs, val_batch_size)\n",
    "       \n",
    "        loss = criterion(output,labels.long())\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        #calculate validation accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        \n",
    "        #logging.debug(output)\n",
    "        #logging.debug('VALIDATION top probab {} VALIDATION top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        #print('Top Class:- ',top_class)\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        #print('Equals:- ', equals)\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    #Averaging losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    val_accuracy = val_accuracy/len(val_loader)\n",
    "    train_accuracy = train_accuracy/len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}\\tTrain Loss: {:.7f} \\tVal Loss:{:.7f} \\tTrain Acc: {:.7}% \\tVal Acc: {:.7f}%'.format(e, train_loss, val_loss, train_accuracy*100,val_accuracy*100))\n",
    "    \n",
    "    #saving the model if validation loss is decreased\n",
    "    if val_loss <= min_val_loss:\n",
    "        print('Validation Loss decreased from {:6f} to {:6f}, saving the model weights'.format(min_val_loss, val_loss))\n",
    "        torch.save(model.state_dict(), 'lstm_state_256-38-bak_norm.pt')\n",
    "        min_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUSIC GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load weights\n",
    "test_model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "test_model.load_state_dict(torch.load('lstm_state_256-38-our.pt'))\n",
    "test_model.eval()\n",
    "test_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population database\n",
    "#testing_data = np.ones(200)*1\n",
    "testing_data = list(range(50,90))\n",
    "testing_data.extend(testing_data[::-1])\n",
    "testing_data_rev = testing_data[::-1]\n",
    "testing_data_rev.extend(testing_data)\n",
    "testing_data_rev.extend(testing_data_rev)\n",
    "testing_data = testing_data_rev\n",
    "\n",
    "\n",
    "testing_data = np.asarray(testing_data)\n",
    "testing_data = testing_data.reshape(testing_data.shape[0],1)\n",
    "\n",
    "initial_seq = [network_input[0][1:].cpu().numpy().tolist()]\n",
    "\n",
    "testing_data_unnorm = testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "testing_data=testing_data.tolist()\n",
    "for i in range(len(testing_data)):\n",
    "    list1.extend(testing_data[i])\n",
    "\n",
    "#list1\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    list1[i]=(list1[i]-50)/(89-50)\n",
    "#     list1[i]=(list1[i])/(89)\n",
    "\n",
    "list1 = np.asarray(list1)\n",
    "list1 = list1.reshape(list1.shape[0],1)\n",
    "testing_data = list1\n",
    "#list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting\n",
    "def prediction_with_influence(influence,int2note,initial_seq, max_note, min_note,test_batch_size = 1):\n",
    "\n",
    "    predicted_notes = []\n",
    "    initial_seq[0].extend([[0]]*len(testing_data))\n",
    "    test_seq = torch.Tensor(initial_seq).cuda()\n",
    "    \n",
    "    for i in range(len(influence)):\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = float(influence[i])\n",
    "        \n",
    "        test_slice = test_seq[0][i : i + sequence_length]        \n",
    "        test_slice = test_slice.view(1, test_slice.shape[0], test_slice.shape[1])\n",
    "        \n",
    "        test_output = test_model.forward(test_slice, test_batch_size)\n",
    "    \n",
    "        test_output = F.softmax(test_output, dim = 1)\n",
    "        top_p, top_class = test_output.topk(1,dim =1)\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = (int2note[top_class.item()] - min_note)/(max_note - min_note)\n",
    "#         test_seq[0][sequence_length - 1 + i][0] = int2note[top_class.item()]/max_note\n",
    "        \n",
    "        predicted_notes.append(int2note[top_class.item()])\n",
    "        \n",
    "    return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_notes_lst = prediction_with_influence(testing_data,int_to_note,initial_seq, max_midi_number, min_midi_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_notes_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(list(np.ones(200)*89))\n",
    "\n",
    "#plt.plot(list(np.ones(200)*50))\n",
    "#plt.plot(list(np.ones(20)*50))\n",
    "plt.plot(testing_data_unnorm)\n",
    "plt.plot(predicted_notes_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(predicted_notes_lst)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PostProcessing().generate_midi_file('hello.midi', predicted_notes_lst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
