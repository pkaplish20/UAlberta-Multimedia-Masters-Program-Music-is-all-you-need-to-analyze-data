{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.tensorboard as tb\n",
    "from Preprocessing.preprocessing_sorted import PreprocessingTrainingData\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as  plt\n",
    "import os\n",
    "import logging\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static parameters\n",
    "train_batch_size = 30\n",
    "val_batch_size = 30\n",
    "sequence_length=50\n",
    "test_batch_size = 1\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layer = 2\n",
    "output_size = 38\n",
    "clip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from preprocessing.py\n",
    "dataset_path = os.path.join(os.path.abspath('..'),'Dataset\\\\Clementi dataset\\\\Clementi dataset' )\n",
    "network_input,network_output,max_midi_number,min_midi_number,int_to_note = PreprocessingTrainingData().preprocess_notes(dataset_path)\n",
    "network_input, network_output = network_input.cuda(), network_output.cuda()\n",
    "\n",
    "# print(network_input)\n",
    "#print(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(network_output.max())\n",
    "print(network_output.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "89\n",
      "50\n",
      "{0: 50, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61, 11: 62, 12: 63, 13: 64, 14: 65, 15: 66, 16: 67, 17: 68, 18: 69, 19: 70, 20: 71, 21: 72, 22: 73, 23: 74, 24: 75, 25: 76, 26: 77, 27: 78, 28: 79, 29: 80, 30: 81, 31: 82, 32: 83, 33: 84, 34: 85, 35: 86, 36: 88, 37: 89}\n"
     ]
    }
   ],
   "source": [
    "print(network_input.max())\n",
    "print(network_input.min())\n",
    "print(max_midi_number)\n",
    "print(min_midi_number)\n",
    "print(int_to_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# data is highly unbalanced\n",
    "# # '''\n",
    "# sns.distplot(torch.tensor(network_output).cpu())\n",
    "# xx = pd.DataFrame(torch.tensor(network_output).cpu())\n",
    "# xx.groupby(0).size().to_frame(name='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1800, 50, 1])\n",
      "torch.Size([1800])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "to make batch of equal sizes\n",
    "Quick Fix\n",
    "'''\n",
    "network_input = network_input[: -29]\n",
    "network_output = network_output[: -29]\n",
    "\n",
    "print(network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create Stacked LSTM model\n",
    "'''\n",
    "class Stacked_LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size = input_size, hidden_size = hidden_size, batch_first = True)\n",
    "        self.lstm2 = nn.LSTM(input_size = hidden_size, hidden_size = output_size,batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(output_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden1, hidden2,batch_size):\n",
    "        \n",
    "        output, _ = self.lstm1(x)        \n",
    "        #print(output.shape)\n",
    "        \n",
    "        #output = self.dropout(output)\n",
    "        \n",
    "        output, _ = self.lstm2(output)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        output = output.contiguous().view(-1, 38)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        #print('Linear Output :-',output.shape)\n",
    "        \n",
    "        #output = F.softmax(output, dim = 1)\n",
    "        #print('SOFTMAX OUTPUT :--', output)\n",
    "        \n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1)\n",
    "        #print('Reshape to batch size first :-',output.shape)\n",
    "        \n",
    "        output = output[:, -self.output_size:] # get last batch of labels\n",
    "        #print('Final Output :-',output)\n",
    "        #print('RESHAPE SIZE :-', output.shape)\n",
    "        \n",
    "        return output, hidden2\n",
    "    \n",
    "    def hidden_init(self,batch_size):\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden1 = (weight.new(1, batch_size, self.hidden_size).zero_().cuda(),\n",
    "          weight.new(1, batch_size, self.hidden_size).zero_().cuda())\n",
    "        \n",
    "        hidden2 = (weight.new(1, batch_size, 38).zero_().cuda(),\n",
    "          weight.new(1, batch_size, 38).zero_().cuda())\n",
    "        return hidden1,hidden2\n",
    "\n",
    "#initialize the weights of LSTM using Xavier initialization    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Divide the dataset into train/val \n",
    "'''\n",
    "train_size = 0.8\n",
    "indices = list(range(len(network_input)))\n",
    "split = int(np.floor(train_size*len(network_input)))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SequentialSampler(train_idx)\n",
    "val_sampler = SequentialSampler(val_idx)\n",
    "\n",
    "dataset = TensorDataset(network_input,network_output)\n",
    "train_loader = DataLoader(dataset, batch_size= train_batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size= val_batch_size,sampler= val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm1): LSTM(1, 256, batch_first=True)\n",
       "  (lstm2): LSTM(256, 38, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=38, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optimizer\n",
    "\n",
    "model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer.AdamW(model.parameters())\n",
    "#optimizer = optimizer.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "#make sure to transfer model to GPU after initializing optimizer\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden = model.hidden_init(train_batch_size) \n",
    "#hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 3.5989211 \tVal Loss:3.4720872 \tTrain Acc: 4.722222% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from    inf to 3.472087, saving the model weights\n",
      "Epoch: 1\tTrain Loss: 3.4904966 \tVal Loss:3.3996584 \tTrain Acc: 5.138889% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.472087 to 3.399658, saving the model weights\n",
      "Epoch: 2\tTrain Loss: 3.4545486 \tVal Loss:3.2771249 \tTrain Acc: 3.055556% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.399658 to 3.277125, saving the model weights\n",
      "Epoch: 3\tTrain Loss: 3.4033162 \tVal Loss:3.2093401 \tTrain Acc: 4.375% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.277125 to 3.209340, saving the model weights\n",
      "Epoch: 4\tTrain Loss: 3.3676482 \tVal Loss:3.2099965 \tTrain Acc: 4.097222% \tVal Acc: 8.8888892%\n",
      "Epoch: 5\tTrain Loss: 3.3487010 \tVal Loss:3.1837465 \tTrain Acc: 4.444445% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.209340 to 3.183746, saving the model weights\n",
      "Epoch: 6\tTrain Loss: 3.3309414 \tVal Loss:3.1665536 \tTrain Acc: 5.138889% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.183746 to 3.166554, saving the model weights\n",
      "Epoch: 7\tTrain Loss: 3.3270168 \tVal Loss:3.1600460 \tTrain Acc: 4.097222% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.166554 to 3.160046, saving the model weights\n",
      "Epoch: 8\tTrain Loss: 3.3246473 \tVal Loss:3.1599278 \tTrain Acc: 5.138889% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.160046 to 3.159928, saving the model weights\n",
      "Epoch: 9\tTrain Loss: 3.3189334 \tVal Loss:3.1561938 \tTrain Acc: 5.069445% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.159928 to 3.156194, saving the model weights\n",
      "Epoch: 10\tTrain Loss: 3.3129547 \tVal Loss:3.1518404 \tTrain Acc: 5.208334% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.156194 to 3.151840, saving the model weights\n",
      "Epoch: 11\tTrain Loss: 3.3066227 \tVal Loss:3.1445307 \tTrain Acc: 5.208334% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.151840 to 3.144531, saving the model weights\n",
      "Epoch: 12\tTrain Loss: 3.2979053 \tVal Loss:3.1942485 \tTrain Acc: 4.861111% \tVal Acc: 8.8888892%\n",
      "Epoch: 13\tTrain Loss: 3.2994647 \tVal Loss:3.1531558 \tTrain Acc: 5.208334% \tVal Acc: 8.8888892%\n",
      "Epoch: 14\tTrain Loss: 3.3235459 \tVal Loss:3.1319149 \tTrain Acc: 4.444445% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.144531 to 3.131915, saving the model weights\n",
      "Epoch: 15\tTrain Loss: 3.3013310 \tVal Loss:3.1391706 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 16\tTrain Loss: 3.2856074 \tVal Loss:3.1275975 \tTrain Acc: 5.555556% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.131915 to 3.127598, saving the model weights\n",
      "Epoch: 17\tTrain Loss: 3.2657711 \tVal Loss:3.0927862 \tTrain Acc: 6.25% \tVal Acc: 10.0000004%\n",
      "Validation Loss decreased from 3.127598 to 3.092786, saving the model weights\n",
      "Epoch: 18\tTrain Loss: 3.2311949 \tVal Loss:3.0450015 \tTrain Acc: 6.875% \tVal Acc: 12.2222226%\n",
      "Validation Loss decreased from 3.092786 to 3.045001, saving the model weights\n",
      "Epoch: 19\tTrain Loss: 3.1980433 \tVal Loss:3.0972437 \tTrain Acc: 7.847222% \tVal Acc: 11.3888893%\n",
      "Epoch: 20\tTrain Loss: 3.2022341 \tVal Loss:2.9991786 \tTrain Acc: 8.472222% \tVal Acc: 10.8333337%\n",
      "Validation Loss decreased from 3.045001 to 2.999179, saving the model weights\n",
      "Epoch: 21\tTrain Loss: 3.1260085 \tVal Loss:2.9253851 \tTrain Acc: 9.027778% \tVal Acc: 9.7222226%\n",
      "Validation Loss decreased from 2.999179 to 2.925385, saving the model weights\n",
      "Epoch: 22\tTrain Loss: 3.0651524 \tVal Loss:2.8858462 \tTrain Acc: 9.722223% \tVal Acc: 13.0555560%\n",
      "Validation Loss decreased from 2.925385 to 2.885846, saving the model weights\n",
      "Epoch: 23\tTrain Loss: 3.0448112 \tVal Loss:2.8473240 \tTrain Acc: 8.541667% \tVal Acc: 13.0555560%\n",
      "Validation Loss decreased from 2.885846 to 2.847324, saving the model weights\n",
      "Epoch: 24\tTrain Loss: 3.0143395 \tVal Loss:2.8283677 \tTrain Acc: 10.625% \tVal Acc: 12.7777782%\n",
      "Validation Loss decreased from 2.847324 to 2.828368, saving the model weights\n",
      "Epoch: 25\tTrain Loss: 2.9819028 \tVal Loss:2.8281452 \tTrain Acc: 9.791667% \tVal Acc: 13.6111116%\n",
      "Validation Loss decreased from 2.828368 to 2.828145, saving the model weights\n",
      "Epoch: 26\tTrain Loss: 2.9611875 \tVal Loss:2.8189952 \tTrain Acc: 9.305556% \tVal Acc: 14.4444451%\n",
      "Validation Loss decreased from 2.828145 to 2.818995, saving the model weights\n",
      "Epoch: 27\tTrain Loss: 2.9524214 \tVal Loss:2.8335557 \tTrain Acc: 10.13889% \tVal Acc: 13.0555561%\n",
      "Epoch: 28\tTrain Loss: 2.9316248 \tVal Loss:2.8656963 \tTrain Acc: 10.06944% \tVal Acc: 10.8333337%\n",
      "Epoch: 29\tTrain Loss: 2.9227153 \tVal Loss:2.8863892 \tTrain Acc: 10.20833% \tVal Acc: 10.5555559%\n",
      "Epoch: 30\tTrain Loss: 2.9040795 \tVal Loss:2.8575067 \tTrain Acc: 11.52778% \tVal Acc: 10.8333337%\n",
      "Epoch: 31\tTrain Loss: 2.8857870 \tVal Loss:2.8708012 \tTrain Acc: 9.444445% \tVal Acc: 10.5555559%\n",
      "Epoch: 32\tTrain Loss: 2.8846773 \tVal Loss:2.8223166 \tTrain Acc: 10.625% \tVal Acc: 11.1111114%\n",
      "Epoch: 33\tTrain Loss: 2.8556797 \tVal Loss:2.7952371 \tTrain Acc: 11.94444% \tVal Acc: 11.9444448%\n",
      "Validation Loss decreased from 2.818995 to 2.795237, saving the model weights\n",
      "Epoch: 34\tTrain Loss: 2.8402136 \tVal Loss:2.7334026 \tTrain Acc: 10.48611% \tVal Acc: 13.8888893%\n",
      "Validation Loss decreased from 2.795237 to 2.733403, saving the model weights\n",
      "Epoch: 35\tTrain Loss: 2.8125546 \tVal Loss:2.7248006 \tTrain Acc: 12.43056% \tVal Acc: 13.3333337%\n",
      "Validation Loss decreased from 2.733403 to 2.724801, saving the model weights\n",
      "Epoch: 36\tTrain Loss: 2.8067441 \tVal Loss:2.7055686 \tTrain Acc: 11.31944% \tVal Acc: 17.5000004%\n",
      "Validation Loss decreased from 2.724801 to 2.705569, saving the model weights\n",
      "Epoch: 37\tTrain Loss: 2.7964262 \tVal Loss:2.6777250 \tTrain Acc: 13.54167% \tVal Acc: 15.2777783%\n",
      "Validation Loss decreased from 2.705569 to 2.677725, saving the model weights\n",
      "Epoch: 38\tTrain Loss: 2.7688876 \tVal Loss:2.6510030 \tTrain Acc: 13.61111% \tVal Acc: 15.8333339%\n",
      "Validation Loss decreased from 2.677725 to 2.651003, saving the model weights\n",
      "Epoch: 39\tTrain Loss: 2.7677191 \tVal Loss:2.6347706 \tTrain Acc: 13.26389% \tVal Acc: 16.9444450%\n",
      "Validation Loss decreased from 2.651003 to 2.634771, saving the model weights\n",
      "Epoch: 40\tTrain Loss: 2.7392108 \tVal Loss:2.6155732 \tTrain Acc: 14.65278% \tVal Acc: 18.0555562%\n",
      "Validation Loss decreased from 2.634771 to 2.615573, saving the model weights\n",
      "Epoch: 41\tTrain Loss: 2.7266125 \tVal Loss:2.5880953 \tTrain Acc: 14.02778% \tVal Acc: 18.3333340%\n",
      "Validation Loss decreased from 2.615573 to 2.588095, saving the model weights\n",
      "Epoch: 42\tTrain Loss: 2.7219323 \tVal Loss:2.5795685 \tTrain Acc: 12.91667% \tVal Acc: 19.1666673%\n",
      "Validation Loss decreased from 2.588095 to 2.579569, saving the model weights\n",
      "Epoch: 43\tTrain Loss: 2.7027720 \tVal Loss:2.5631117 \tTrain Acc: 14.23611% \tVal Acc: 18.6111117%\n",
      "Validation Loss decreased from 2.579569 to 2.563112, saving the model weights\n",
      "Epoch: 44\tTrain Loss: 2.6664155 \tVal Loss:2.5441196 \tTrain Acc: 15.20833% \tVal Acc: 18.0555562%\n",
      "Validation Loss decreased from 2.563112 to 2.544120, saving the model weights\n",
      "Epoch: 45\tTrain Loss: 2.6654048 \tVal Loss:2.5402612 \tTrain Acc: 15.97222% \tVal Acc: 18.3333339%\n",
      "Validation Loss decreased from 2.544120 to 2.540261, saving the model weights\n",
      "Epoch: 46\tTrain Loss: 2.6842286 \tVal Loss:2.5156333 \tTrain Acc: 16.25% \tVal Acc: 19.4444451%\n",
      "Validation Loss decreased from 2.540261 to 2.515633, saving the model weights\n",
      "Epoch: 47\tTrain Loss: 2.6475158 \tVal Loss:2.4927294 \tTrain Acc: 16.04167% \tVal Acc: 22.2222228%\n",
      "Validation Loss decreased from 2.515633 to 2.492729, saving the model weights\n",
      "Epoch: 48\tTrain Loss: 2.6451482 \tVal Loss:2.4934483 \tTrain Acc: 16.52778% \tVal Acc: 22.5000009%\n",
      "Epoch: 49\tTrain Loss: 2.6218532 \tVal Loss:2.4995499 \tTrain Acc: 17.43056% \tVal Acc: 22.2222229%\n",
      "Epoch: 50\tTrain Loss: 2.6208999 \tVal Loss:2.5163242 \tTrain Acc: 18.33333% \tVal Acc: 22.5000006%\n",
      "Epoch: 51\tTrain Loss: 2.6261987 \tVal Loss:2.4690792 \tTrain Acc: 18.125% \tVal Acc: 22.2222229%\n",
      "Validation Loss decreased from 2.492729 to 2.469079, saving the model weights\n",
      "Epoch: 52\tTrain Loss: 2.6077560 \tVal Loss:2.4576692 \tTrain Acc: 17.15278% \tVal Acc: 23.6111118%\n",
      "Validation Loss decreased from 2.469079 to 2.457669, saving the model weights\n",
      "Epoch: 53\tTrain Loss: 2.5898116 \tVal Loss:2.4335405 \tTrain Acc: 18.26389% \tVal Acc: 24.4444450%\n",
      "Validation Loss decreased from 2.457669 to 2.433540, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54\tTrain Loss: 2.5865567 \tVal Loss:2.4502822 \tTrain Acc: 18.68056% \tVal Acc: 25.8333342%\n",
      "Epoch: 55\tTrain Loss: 2.5743151 \tVal Loss:2.4992113 \tTrain Acc: 18.95833% \tVal Acc: 19.7222228%\n",
      "Epoch: 56\tTrain Loss: 2.5575442 \tVal Loss:2.4358719 \tTrain Acc: 18.68056% \tVal Acc: 26.6666674%\n",
      "Epoch: 57\tTrain Loss: 2.5625008 \tVal Loss:2.4741701 \tTrain Acc: 18.75% \tVal Acc: 20.2777784%\n",
      "Epoch: 58\tTrain Loss: 2.5415765 \tVal Loss:2.4665473 \tTrain Acc: 19.86111% \tVal Acc: 25.0000007%\n",
      "Epoch: 59\tTrain Loss: 2.5369124 \tVal Loss:2.3965934 \tTrain Acc: 19.09722% \tVal Acc: 25.2777784%\n",
      "Validation Loss decreased from 2.433540 to 2.396593, saving the model weights\n",
      "Epoch: 60\tTrain Loss: 2.5360814 \tVal Loss:2.4696671 \tTrain Acc: 18.81945% \tVal Acc: 20.0000007%\n",
      "Epoch: 61\tTrain Loss: 2.5258858 \tVal Loss:2.4827928 \tTrain Acc: 20.55556% \tVal Acc: 22.2222230%\n",
      "Epoch: 62\tTrain Loss: 2.5198193 \tVal Loss:2.4560348 \tTrain Acc: 20.20833% \tVal Acc: 24.7222229%\n",
      "Epoch: 63\tTrain Loss: 2.4945222 \tVal Loss:2.4156387 \tTrain Acc: 20.97222% \tVal Acc: 25.0000007%\n",
      "Epoch: 64\tTrain Loss: 2.4911445 \tVal Loss:2.4233465 \tTrain Acc: 20.48611% \tVal Acc: 22.7777784%\n",
      "Epoch: 65\tTrain Loss: 2.4922368 \tVal Loss:2.4211642 \tTrain Acc: 21.52778% \tVal Acc: 21.3888897%\n",
      "Epoch: 66\tTrain Loss: 2.4354685 \tVal Loss:2.3646450 \tTrain Acc: 23.26389% \tVal Acc: 25.8333340%\n",
      "Validation Loss decreased from 2.396593 to 2.364645, saving the model weights\n",
      "Epoch: 67\tTrain Loss: 2.4582332 \tVal Loss:2.3957912 \tTrain Acc: 21.66667% \tVal Acc: 22.2222230%\n",
      "Epoch: 68\tTrain Loss: 2.4545793 \tVal Loss:2.3639028 \tTrain Acc: 21.66667% \tVal Acc: 25.2777784%\n",
      "Validation Loss decreased from 2.364645 to 2.363903, saving the model weights\n",
      "Epoch: 69\tTrain Loss: 2.4535202 \tVal Loss:2.3797786 \tTrain Acc: 22.29167% \tVal Acc: 22.7777785%\n",
      "Epoch: 70\tTrain Loss: 2.4645938 \tVal Loss:2.3211585 \tTrain Acc: 22.70833% \tVal Acc: 27.2222230%\n",
      "Validation Loss decreased from 2.363903 to 2.321158, saving the model weights\n",
      "Epoch: 71\tTrain Loss: 2.4054244 \tVal Loss:2.3755779 \tTrain Acc: 23.19445% \tVal Acc: 24.4444450%\n",
      "Epoch: 72\tTrain Loss: 2.4259163 \tVal Loss:2.2516543 \tTrain Acc: 22.77778% \tVal Acc: 32.2222229%\n",
      "Validation Loss decreased from 2.321158 to 2.251654, saving the model weights\n",
      "Epoch: 73\tTrain Loss: 2.3993137 \tVal Loss:2.2329068 \tTrain Acc: 23.05556% \tVal Acc: 33.6111118%\n",
      "Validation Loss decreased from 2.251654 to 2.232907, saving the model weights\n",
      "Epoch: 74\tTrain Loss: 2.4134792 \tVal Loss:2.2198791 \tTrain Acc: 23.47222% \tVal Acc: 34.4444451%\n",
      "Validation Loss decreased from 2.232907 to 2.219879, saving the model weights\n",
      "Epoch: 75\tTrain Loss: 2.3994099 \tVal Loss:2.1917518 \tTrain Acc: 24.30556% \tVal Acc: 36.1111118%\n",
      "Validation Loss decreased from 2.219879 to 2.191752, saving the model weights\n",
      "Epoch: 76\tTrain Loss: 2.3811112 \tVal Loss:2.1779731 \tTrain Acc: 23.88889% \tVal Acc: 33.6111118%\n",
      "Validation Loss decreased from 2.191752 to 2.177973, saving the model weights\n",
      "Epoch: 77\tTrain Loss: 2.3758819 \tVal Loss:2.1918641 \tTrain Acc: 24.30556% \tVal Acc: 33.6111117%\n",
      "Epoch: 78\tTrain Loss: 2.3496899 \tVal Loss:2.0767693 \tTrain Acc: 24.86111% \tVal Acc: 36.1111118%\n",
      "Validation Loss decreased from 2.177973 to 2.076769, saving the model weights\n",
      "Epoch: 79\tTrain Loss: 2.3404159 \tVal Loss:2.1113421 \tTrain Acc: 25.06945% \tVal Acc: 36.1111118%\n",
      "Epoch: 80\tTrain Loss: 2.3313389 \tVal Loss:2.0974840 \tTrain Acc: 25.83333% \tVal Acc: 35.5555562%\n",
      "Epoch: 81\tTrain Loss: 2.3030951 \tVal Loss:1.9993729 \tTrain Acc: 25.34722% \tVal Acc: 39.4444448%\n",
      "Validation Loss decreased from 2.076769 to 1.999373, saving the model weights\n",
      "Epoch: 82\tTrain Loss: 2.2623660 \tVal Loss:2.0224097 \tTrain Acc: 28.47222% \tVal Acc: 36.3888895%\n",
      "Epoch: 83\tTrain Loss: 2.2770293 \tVal Loss:2.0526966 \tTrain Acc: 27.63889% \tVal Acc: 36.1111118%\n",
      "Epoch: 84\tTrain Loss: 2.2472990 \tVal Loss:1.9989573 \tTrain Acc: 27.22222% \tVal Acc: 37.5000006%\n",
      "Validation Loss decreased from 1.999373 to 1.998957, saving the model weights\n",
      "Epoch: 85\tTrain Loss: 2.2524462 \tVal Loss:1.9744770 \tTrain Acc: 28.26389% \tVal Acc: 40.0000005%\n",
      "Validation Loss decreased from 1.998957 to 1.974477, saving the model weights\n",
      "Epoch: 86\tTrain Loss: 2.2621896 \tVal Loss:2.1234769 \tTrain Acc: 27.08333% \tVal Acc: 34.4444451%\n",
      "Epoch: 87\tTrain Loss: 2.2714983 \tVal Loss:2.0563748 \tTrain Acc: 28.125% \tVal Acc: 34.1666674%\n",
      "Epoch: 88\tTrain Loss: 2.2381602 \tVal Loss:1.9866384 \tTrain Acc: 28.19445% \tVal Acc: 37.2222230%\n",
      "Epoch: 89\tTrain Loss: 2.1941796 \tVal Loss:1.9957760 \tTrain Acc: 29.16667% \tVal Acc: 35.5555563%\n",
      "Epoch: 90\tTrain Loss: 2.1865598 \tVal Loss:1.8762147 \tTrain Acc: 29.09722% \tVal Acc: 43.0555561%\n",
      "Validation Loss decreased from 1.974477 to 1.876215, saving the model weights\n",
      "Epoch: 91\tTrain Loss: 2.1643439 \tVal Loss:1.9801287 \tTrain Acc: 31.31945% \tVal Acc: 36.9444451%\n",
      "Epoch: 92\tTrain Loss: 2.1668889 \tVal Loss:1.9864543 \tTrain Acc: 31.94445% \tVal Acc: 40.0000007%\n",
      "Epoch: 93\tTrain Loss: 2.1842216 \tVal Loss:1.9631033 \tTrain Acc: 30.13889% \tVal Acc: 36.6666673%\n",
      "Epoch: 94\tTrain Loss: 2.1953888 \tVal Loss:1.8528074 \tTrain Acc: 29.72222% \tVal Acc: 41.3888898%\n",
      "Validation Loss decreased from 1.876215 to 1.852807, saving the model weights\n",
      "Epoch: 95\tTrain Loss: 2.2056314 \tVal Loss:1.8827100 \tTrain Acc: 29.86111% \tVal Acc: 38.8888897%\n",
      "Epoch: 96\tTrain Loss: 2.1447883 \tVal Loss:1.8126810 \tTrain Acc: 30.76389% \tVal Acc: 45.0000003%\n",
      "Validation Loss decreased from 1.852807 to 1.812681, saving the model weights\n",
      "Epoch: 97\tTrain Loss: 2.1396839 \tVal Loss:1.8213858 \tTrain Acc: 31.66667% \tVal Acc: 40.8333341%\n",
      "Epoch: 98\tTrain Loss: 2.1168077 \tVal Loss:1.7079459 \tTrain Acc: 32.70833% \tVal Acc: 46.1111122%\n",
      "Validation Loss decreased from 1.812681 to 1.707946, saving the model weights\n",
      "Epoch: 99\tTrain Loss: 2.0701315 \tVal Loss:1.8053731 \tTrain Acc: 32.91667% \tVal Acc: 39.1666676%\n",
      "Epoch: 100\tTrain Loss: 2.0858089 \tVal Loss:1.9555325 \tTrain Acc: 32.84722% \tVal Acc: 37.2222227%\n",
      "Epoch: 101\tTrain Loss: 2.0605968 \tVal Loss:1.8153494 \tTrain Acc: 34.375% \tVal Acc: 42.7777787%\n",
      "Epoch: 102\tTrain Loss: 2.0154620 \tVal Loss:1.6618989 \tTrain Acc: 36.18056% \tVal Acc: 52.5000010%\n",
      "Validation Loss decreased from 1.707946 to 1.661899, saving the model weights\n",
      "Epoch: 103\tTrain Loss: 2.0304698 \tVal Loss:1.6534877 \tTrain Acc: 37.56944% \tVal Acc: 46.3888900%\n",
      "Validation Loss decreased from 1.661899 to 1.653488, saving the model weights\n",
      "Epoch: 104\tTrain Loss: 2.0263241 \tVal Loss:1.6899207 \tTrain Acc: 34.65278% \tVal Acc: 46.3888897%\n",
      "Epoch: 105\tTrain Loss: 2.0356522 \tVal Loss:1.9252782 \tTrain Acc: 34.65278% \tVal Acc: 36.9444451%\n",
      "Epoch: 106\tTrain Loss: 2.0418574 \tVal Loss:1.8864850 \tTrain Acc: 35.48611% \tVal Acc: 37.5000006%\n",
      "Epoch: 107\tTrain Loss: 2.0293781 \tVal Loss:1.7671526 \tTrain Acc: 34.86111% \tVal Acc: 43.6111121%\n",
      "Epoch: 108\tTrain Loss: 1.9912644 \tVal Loss:1.6087724 \tTrain Acc: 36.59722% \tVal Acc: 53.0555562%\n",
      "Validation Loss decreased from 1.653488 to 1.608772, saving the model weights\n",
      "Epoch: 109\tTrain Loss: 1.9355291 \tVal Loss:1.5218147 \tTrain Acc: 37.91667% \tVal Acc: 51.9444459%\n",
      "Validation Loss decreased from 1.608772 to 1.521815, saving the model weights\n",
      "Epoch: 110\tTrain Loss: 1.8966232 \tVal Loss:1.5687871 \tTrain Acc: 37.98611% \tVal Acc: 53.8888903%\n",
      "Epoch: 111\tTrain Loss: 1.8575567 \tVal Loss:1.6542733 \tTrain Acc: 40.83333% \tVal Acc: 46.9444450%\n",
      "Epoch: 112\tTrain Loss: 1.8388275 \tVal Loss:1.6445466 \tTrain Acc: 42.5% \tVal Acc: 50.5555556%\n",
      "Epoch: 113\tTrain Loss: 1.8454363 \tVal Loss:1.5541149 \tTrain Acc: 39.72222% \tVal Acc: 51.6666677%\n",
      "Epoch: 114\tTrain Loss: 1.8542350 \tVal Loss:1.4978679 \tTrain Acc: 41.875% \tVal Acc: 54.4444452%\n",
      "Validation Loss decreased from 1.521815 to 1.497868, saving the model weights\n",
      "Epoch: 115\tTrain Loss: 1.8022810 \tVal Loss:1.6101416 \tTrain Acc: 42.77778% \tVal Acc: 51.3888894%\n",
      "Epoch: 116\tTrain Loss: 1.7962632 \tVal Loss:1.4967231 \tTrain Acc: 43.125% \tVal Acc: 56.9444448%\n",
      "Validation Loss decreased from 1.497868 to 1.496723, saving the model weights\n",
      "Epoch: 117\tTrain Loss: 1.7565398 \tVal Loss:1.4196720 \tTrain Acc: 42.43056% \tVal Acc: 60.0000005%\n",
      "Validation Loss decreased from 1.496723 to 1.419672, saving the model weights\n",
      "Epoch: 118\tTrain Loss: 1.7365552 \tVal Loss:1.4136279 \tTrain Acc: 45.0% \tVal Acc: 62.2222230%\n",
      "Validation Loss decreased from 1.419672 to 1.413628, saving the model weights\n",
      "Epoch: 119\tTrain Loss: 1.7038714 \tVal Loss:1.5580686 \tTrain Acc: 45.97222% \tVal Acc: 54.4444452%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 120\tTrain Loss: 1.7403619 \tVal Loss:1.6138095 \tTrain Acc: 44.72222% \tVal Acc: 50.8333342%\n",
      "Epoch: 121\tTrain Loss: 1.7200820 \tVal Loss:1.4883748 \tTrain Acc: 46.59722% \tVal Acc: 55.2777781%\n",
      "Epoch: 122\tTrain Loss: 1.6816915 \tVal Loss:1.3080283 \tTrain Acc: 47.91667% \tVal Acc: 61.1111122%\n",
      "Validation Loss decreased from 1.413628 to 1.308028, saving the model weights\n",
      "Epoch: 123\tTrain Loss: 1.6660987 \tVal Loss:1.3418362 \tTrain Acc: 46.31945% \tVal Acc: 60.5555565%\n",
      "Epoch: 124\tTrain Loss: 1.6944879 \tVal Loss:1.2349773 \tTrain Acc: 45.83333% \tVal Acc: 63.6111113%\n",
      "Validation Loss decreased from 1.308028 to 1.234977, saving the model weights\n",
      "Epoch: 125\tTrain Loss: 1.6410875 \tVal Loss:1.4129359 \tTrain Acc: 48.05556% \tVal Acc: 58.6111116%\n",
      "Epoch: 126\tTrain Loss: 1.6200037 \tVal Loss:1.2515551 \tTrain Acc: 47.56945% \tVal Acc: 63.3333338%\n",
      "Epoch: 127\tTrain Loss: 1.5651688 \tVal Loss:1.2259383 \tTrain Acc: 50.13889% \tVal Acc: 67.2222222%\n",
      "Validation Loss decreased from 1.234977 to 1.225938, saving the model weights\n",
      "Epoch: 128\tTrain Loss: 1.5678610 \tVal Loss:1.2636779 \tTrain Acc: 50.76389% \tVal Acc: 66.3888889%\n",
      "Epoch: 129\tTrain Loss: 1.5441161 \tVal Loss:1.2063583 \tTrain Acc: 52.63889% \tVal Acc: 67.2222234%\n",
      "Validation Loss decreased from 1.225938 to 1.206358, saving the model weights\n",
      "Epoch: 130\tTrain Loss: 1.5813399 \tVal Loss:1.2348402 \tTrain Acc: 50.20833% \tVal Acc: 65.5555556%\n",
      "Epoch: 131\tTrain Loss: 1.5609534 \tVal Loss:1.1604528 \tTrain Acc: 50.27778% \tVal Acc: 64.7222228%\n",
      "Validation Loss decreased from 1.206358 to 1.160453, saving the model weights\n",
      "Epoch: 132\tTrain Loss: 1.4709857 \tVal Loss:1.1559938 \tTrain Acc: 54.375% \tVal Acc: 65.8333336%\n",
      "Validation Loss decreased from 1.160453 to 1.155994, saving the model weights\n",
      "Epoch: 133\tTrain Loss: 1.4085384 \tVal Loss:1.0728244 \tTrain Acc: 54.51389% \tVal Acc: 70.8333331%\n",
      "Validation Loss decreased from 1.155994 to 1.072824, saving the model weights\n",
      "Epoch: 134\tTrain Loss: 1.4192129 \tVal Loss:1.1660956 \tTrain Acc: 55.76389% \tVal Acc: 64.7222228%\n",
      "Epoch: 135\tTrain Loss: 1.4908906 \tVal Loss:1.1732369 \tTrain Acc: 53.40278% \tVal Acc: 65.2777779%\n",
      "Epoch: 136\tTrain Loss: 1.4241406 \tVal Loss:1.0108630 \tTrain Acc: 54.72222% \tVal Acc: 74.1666670%\n",
      "Validation Loss decreased from 1.072824 to 1.010863, saving the model weights\n",
      "Epoch: 137\tTrain Loss: 1.3876897 \tVal Loss:1.0169591 \tTrain Acc: 57.84722% \tVal Acc: 72.7777774%\n",
      "Epoch: 138\tTrain Loss: 1.3588616 \tVal Loss:0.9755768 \tTrain Acc: 56.45833% \tVal Acc: 73.3333334%\n",
      "Validation Loss decreased from 1.010863 to 0.975577, saving the model weights\n",
      "Epoch: 139\tTrain Loss: 1.4112656 \tVal Loss:0.9531197 \tTrain Acc: 55.76389% \tVal Acc: 74.9999995%\n",
      "Validation Loss decreased from 0.975577 to 0.953120, saving the model weights\n",
      "Epoch: 140\tTrain Loss: 1.3492884 \tVal Loss:1.0519759 \tTrain Acc: 57.91667% \tVal Acc: 70.8333333%\n",
      "Epoch: 141\tTrain Loss: 1.3097341 \tVal Loss:0.9397688 \tTrain Acc: 59.09722% \tVal Acc: 78.0555554%\n",
      "Validation Loss decreased from 0.953120 to 0.939769, saving the model weights\n",
      "Epoch: 142\tTrain Loss: 1.2357838 \tVal Loss:0.9708974 \tTrain Acc: 63.68056% \tVal Acc: 73.8888887%\n",
      "Epoch: 143\tTrain Loss: 1.1933350 \tVal Loss:0.8595829 \tTrain Acc: 64.58333% \tVal Acc: 78.3333326%\n",
      "Validation Loss decreased from 0.939769 to 0.859583, saving the model weights\n",
      "Epoch: 144\tTrain Loss: 1.1960714 \tVal Loss:0.8836532 \tTrain Acc: 63.33333% \tVal Acc: 76.3888891%\n",
      "Epoch: 145\tTrain Loss: 1.2832051 \tVal Loss:0.9920876 \tTrain Acc: 61.31944% \tVal Acc: 72.5000004%\n",
      "Epoch: 146\tTrain Loss: 1.2978875 \tVal Loss:0.9517829 \tTrain Acc: 59.72222% \tVal Acc: 75.8333330%\n",
      "Epoch: 147\tTrain Loss: 1.2256628 \tVal Loss:0.8743976 \tTrain Acc: 62.70833% \tVal Acc: 77.5000001%\n",
      "Epoch: 148\tTrain Loss: 1.2065466 \tVal Loss:0.8089968 \tTrain Acc: 63.33333% \tVal Acc: 80.8333330%\n",
      "Validation Loss decreased from 0.859583 to 0.808997, saving the model weights\n",
      "Epoch: 149\tTrain Loss: 1.1509243 \tVal Loss:0.7935850 \tTrain Acc: 65.06944% \tVal Acc: 80.0000002%\n",
      "Validation Loss decreased from 0.808997 to 0.793585, saving the model weights\n",
      "Epoch: 150\tTrain Loss: 1.1813840 \tVal Loss:0.8380166 \tTrain Acc: 63.54167% \tVal Acc: 77.7777781%\n",
      "Epoch: 151\tTrain Loss: 1.1530918 \tVal Loss:0.8065041 \tTrain Acc: 64.86111% \tVal Acc: 80.5555552%\n",
      "Epoch: 152\tTrain Loss: 1.0785520 \tVal Loss:0.7796973 \tTrain Acc: 65.48611% \tVal Acc: 82.5000000%\n",
      "Validation Loss decreased from 0.793585 to 0.779697, saving the model weights\n",
      "Epoch: 153\tTrain Loss: 1.0554279 \tVal Loss:0.7644945 \tTrain Acc: 67.77778% \tVal Acc: 80.5555552%\n",
      "Validation Loss decreased from 0.779697 to 0.764495, saving the model weights\n",
      "Epoch: 154\tTrain Loss: 1.0218526 \tVal Loss:0.6877848 \tTrain Acc: 70.48611% \tVal Acc: 83.3333323%\n",
      "Validation Loss decreased from 0.764495 to 0.687785, saving the model weights\n",
      "Epoch: 155\tTrain Loss: 1.0319849 \tVal Loss:0.6784553 \tTrain Acc: 70.34722% \tVal Acc: 85.2777779%\n",
      "Validation Loss decreased from 0.687785 to 0.678455, saving the model weights\n",
      "Epoch: 156\tTrain Loss: 1.0242649 \tVal Loss:0.6724259 \tTrain Acc: 68.81944% \tVal Acc: 85.5555544%\n",
      "Validation Loss decreased from 0.678455 to 0.672426, saving the model weights\n",
      "Epoch: 157\tTrain Loss: 0.9964338 \tVal Loss:0.6574844 \tTrain Acc: 69.65278% \tVal Acc: 86.1111104%\n",
      "Validation Loss decreased from 0.672426 to 0.657484, saving the model weights\n",
      "Epoch: 158\tTrain Loss: 1.0801956 \tVal Loss:0.7579367 \tTrain Acc: 65.0% \tVal Acc: 80.8333327%\n",
      "Epoch: 159\tTrain Loss: 1.0472810 \tVal Loss:0.6499762 \tTrain Acc: 68.19444% \tVal Acc: 84.7222211%\n",
      "Validation Loss decreased from 0.657484 to 0.649976, saving the model weights\n",
      "Epoch: 160\tTrain Loss: 0.9643504 \tVal Loss:0.6087217 \tTrain Acc: 70.48611% \tVal Acc: 85.8333319%\n",
      "Validation Loss decreased from 0.649976 to 0.608722, saving the model weights\n",
      "Epoch: 161\tTrain Loss: 0.9612349 \tVal Loss:0.5934126 \tTrain Acc: 70.13889% \tVal Acc: 85.5555559%\n",
      "Validation Loss decreased from 0.608722 to 0.593413, saving the model weights\n",
      "Epoch: 162\tTrain Loss: 0.9256095 \tVal Loss:0.5705576 \tTrain Acc: 72.56944% \tVal Acc: 88.8888878%\n",
      "Validation Loss decreased from 0.593413 to 0.570558, saving the model weights\n",
      "Epoch: 163\tTrain Loss: 0.8871966 \tVal Loss:0.5500759 \tTrain Acc: 74.09722% \tVal Acc: 89.4444441%\n",
      "Validation Loss decreased from 0.570558 to 0.550076, saving the model weights\n",
      "Epoch: 164\tTrain Loss: 0.8872352 \tVal Loss:0.6317500 \tTrain Acc: 74.79167% \tVal Acc: 83.8888884%\n",
      "Epoch: 165\tTrain Loss: 0.9247714 \tVal Loss:0.6420829 \tTrain Acc: 71.73611% \tVal Acc: 83.0555548%\n",
      "Epoch: 166\tTrain Loss: 0.9532677 \tVal Loss:0.7921490 \tTrain Acc: 69.79167% \tVal Acc: 77.7777779%\n",
      "Epoch: 167\tTrain Loss: 0.9775983 \tVal Loss:0.6153025 \tTrain Acc: 70.27778% \tVal Acc: 85.5555539%\n",
      "Epoch: 168\tTrain Loss: 0.9258190 \tVal Loss:0.5228674 \tTrain Acc: 71.80556% \tVal Acc: 88.6111105%\n",
      "Validation Loss decreased from 0.550076 to 0.522867, saving the model weights\n",
      "Epoch: 169\tTrain Loss: 0.8813785 \tVal Loss:0.5304514 \tTrain Acc: 73.88889% \tVal Acc: 87.4999993%\n",
      "Epoch: 170\tTrain Loss: 0.9077392 \tVal Loss:0.5051191 \tTrain Acc: 72.22222% \tVal Acc: 89.4444441%\n",
      "Validation Loss decreased from 0.522867 to 0.505119, saving the model weights\n",
      "Epoch: 171\tTrain Loss: 0.9757154 \tVal Loss:0.6200379 \tTrain Acc: 70.20833% \tVal Acc: 84.9999984%\n",
      "Epoch: 172\tTrain Loss: 0.9406935 \tVal Loss:0.5084195 \tTrain Acc: 70.625% \tVal Acc: 89.1666651%\n",
      "Epoch: 173\tTrain Loss: 0.8484772 \tVal Loss:0.5129017 \tTrain Acc: 75.06944% \tVal Acc: 86.9444445%\n",
      "Epoch: 174\tTrain Loss: 0.8070419 \tVal Loss:0.4224760 \tTrain Acc: 76.04167% \tVal Acc: 91.3888882%\n",
      "Validation Loss decreased from 0.505119 to 0.422476, saving the model weights\n",
      "Epoch: 175\tTrain Loss: 0.7726252 \tVal Loss:0.4488621 \tTrain Acc: 79.16667% \tVal Acc: 89.9999981%\n",
      "Epoch: 176\tTrain Loss: 0.7747788 \tVal Loss:0.5142484 \tTrain Acc: 76.875% \tVal Acc: 87.7777770%\n",
      "Epoch: 177\tTrain Loss: 0.7789219 \tVal Loss:0.4673647 \tTrain Acc: 76.38889% \tVal Acc: 90.2777771%\n",
      "Epoch: 178\tTrain Loss: 0.7454420 \tVal Loss:0.3837219 \tTrain Acc: 78.40278% \tVal Acc: 91.9444442%\n",
      "Validation Loss decreased from 0.422476 to 0.383722, saving the model weights\n",
      "Epoch: 179\tTrain Loss: 0.7441066 \tVal Loss:0.4297396 \tTrain Acc: 77.98611% \tVal Acc: 91.1111111%\n",
      "Epoch: 180\tTrain Loss: 0.8295599 \tVal Loss:0.7298606 \tTrain Acc: 74.58333% \tVal Acc: 78.3333331%\n",
      "Epoch: 181\tTrain Loss: 0.8894662 \tVal Loss:0.6193780 \tTrain Acc: 71.52778% \tVal Acc: 82.2222218%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 182\tTrain Loss: 0.8662339 \tVal Loss:0.4501192 \tTrain Acc: 74.51389% \tVal Acc: 90.2777771%\n",
      "Epoch: 183\tTrain Loss: 0.7456052 \tVal Loss:0.4205113 \tTrain Acc: 78.47222% \tVal Acc: 89.4444436%\n",
      "Epoch: 184\tTrain Loss: 0.6807855 \tVal Loss:0.3469600 \tTrain Acc: 81.04167% \tVal Acc: 92.2222207%\n",
      "Validation Loss decreased from 0.383722 to 0.346960, saving the model weights\n",
      "Epoch: 185\tTrain Loss: 0.6591369 \tVal Loss:0.3512944 \tTrain Acc: 81.52778% \tVal Acc: 92.7777772%\n",
      "Epoch: 186\tTrain Loss: 0.6361388 \tVal Loss:0.2990073 \tTrain Acc: 81.38889% \tVal Acc: 94.9999998%\n",
      "Validation Loss decreased from 0.346960 to 0.299007, saving the model weights\n",
      "Epoch: 187\tTrain Loss: 0.5948277 \tVal Loss:0.3098814 \tTrain Acc: 82.70833% \tVal Acc: 94.7222223%\n",
      "Epoch: 188\tTrain Loss: 0.5688851 \tVal Loss:0.2744363 \tTrain Acc: 84.02778% \tVal Acc: 94.4444438%\n",
      "Validation Loss decreased from 0.299007 to 0.274436, saving the model weights\n",
      "Epoch: 189\tTrain Loss: 0.5521631 \tVal Loss:0.2803419 \tTrain Acc: 84.51389% \tVal Acc: 94.7222218%\n",
      "Epoch: 190\tTrain Loss: 0.5428826 \tVal Loss:0.3030517 \tTrain Acc: 84.79167% \tVal Acc: 93.3333327%\n",
      "Epoch: 191\tTrain Loss: 0.5657884 \tVal Loss:0.2762369 \tTrain Acc: 84.93055% \tVal Acc: 95.2777768%\n",
      "Epoch: 192\tTrain Loss: 0.5320562 \tVal Loss:0.2900546 \tTrain Acc: 84.93056% \tVal Acc: 93.6111107%\n",
      "Epoch: 193\tTrain Loss: 0.5362506 \tVal Loss:0.2802597 \tTrain Acc: 85.55555% \tVal Acc: 94.4444438%\n",
      "Epoch: 194\tTrain Loss: 0.5529822 \tVal Loss:0.2954745 \tTrain Acc: 84.23611% \tVal Acc: 94.4444438%\n",
      "Epoch: 195\tTrain Loss: 0.6072967 \tVal Loss:0.2975944 \tTrain Acc: 83.125% \tVal Acc: 92.7777772%\n",
      "Epoch: 196\tTrain Loss: 0.5536622 \tVal Loss:0.2684582 \tTrain Acc: 84.58333% \tVal Acc: 93.8888888%\n",
      "Validation Loss decreased from 0.274436 to 0.268458, saving the model weights\n",
      "Epoch: 197\tTrain Loss: 0.5408595 \tVal Loss:0.2852455 \tTrain Acc: 85.0% \tVal Acc: 95.2777768%\n",
      "Epoch: 198\tTrain Loss: 0.5253770 \tVal Loss:0.3021462 \tTrain Acc: 84.93056% \tVal Acc: 93.6111107%\n",
      "Epoch: 199\tTrain Loss: 0.5188512 \tVal Loss:0.2617861 \tTrain Acc: 85.55555% \tVal Acc: 92.7777767%\n",
      "Validation Loss decreased from 0.268458 to 0.261786, saving the model weights\n",
      "Epoch: 200\tTrain Loss: 0.5146024 \tVal Loss:0.2708082 \tTrain Acc: 86.31944% \tVal Acc: 92.4999982%\n",
      "Epoch: 201\tTrain Loss: 0.5037823 \tVal Loss:0.2221007 \tTrain Acc: 85.48611% \tVal Acc: 96.9444444%\n",
      "Validation Loss decreased from 0.261786 to 0.222101, saving the model weights\n",
      "Epoch: 202\tTrain Loss: 0.4963306 \tVal Loss:0.2455874 \tTrain Acc: 85.90278% \tVal Acc: 97.2222219%\n",
      "Epoch: 203\tTrain Loss: 0.5192726 \tVal Loss:0.1914692 \tTrain Acc: 84.65278% \tVal Acc: 96.9444444%\n",
      "Validation Loss decreased from 0.222101 to 0.191469, saving the model weights\n",
      "Epoch: 204\tTrain Loss: 0.5137294 \tVal Loss:0.2134277 \tTrain Acc: 85.34722% \tVal Acc: 94.9999993%\n",
      "Epoch: 205\tTrain Loss: 0.6046728 \tVal Loss:0.3229347 \tTrain Acc: 81.11111% \tVal Acc: 90.8333326%\n",
      "Epoch: 206\tTrain Loss: 0.5715336 \tVal Loss:0.4269204 \tTrain Acc: 84.16667% \tVal Acc: 91.1111102%\n",
      "Epoch: 207\tTrain Loss: 0.7215949 \tVal Loss:0.5384448 \tTrain Acc: 79.375% \tVal Acc: 85.2777764%\n",
      "Epoch: 208\tTrain Loss: 0.6515965 \tVal Loss:0.3781402 \tTrain Acc: 80.34722% \tVal Acc: 92.4999987%\n",
      "Epoch: 209\tTrain Loss: 0.5217715 \tVal Loss:0.2426660 \tTrain Acc: 85.13889% \tVal Acc: 95.2777768%\n",
      "Epoch: 210\tTrain Loss: 0.4757263 \tVal Loss:0.2045592 \tTrain Acc: 86.31944% \tVal Acc: 96.6666664%\n",
      "Epoch: 211\tTrain Loss: 0.4139552 \tVal Loss:0.1682976 \tTrain Acc: 88.61111% \tVal Acc: 98.6111104%\n",
      "Validation Loss decreased from 0.191469 to 0.168298, saving the model weights\n",
      "Epoch: 212\tTrain Loss: 0.4096617 \tVal Loss:0.1968151 \tTrain Acc: 88.68055% \tVal Acc: 96.3888884%\n",
      "Epoch: 213\tTrain Loss: 0.3889645 \tVal Loss:0.1829976 \tTrain Acc: 90.20833% \tVal Acc: 96.9444439%\n",
      "Epoch: 214\tTrain Loss: 0.3556619 \tVal Loss:0.1714042 \tTrain Acc: 90.625% \tVal Acc: 96.1111108%\n",
      "Epoch: 215\tTrain Loss: 0.3694290 \tVal Loss:0.1685981 \tTrain Acc: 89.93055% \tVal Acc: 96.9444439%\n",
      "Epoch: 216\tTrain Loss: 0.4081805 \tVal Loss:0.1865004 \tTrain Acc: 89.02778% \tVal Acc: 96.1111103%\n",
      "Epoch: 217\tTrain Loss: 0.3799310 \tVal Loss:0.1799275 \tTrain Acc: 89.65278% \tVal Acc: 96.3888884%\n",
      "Epoch: 218\tTrain Loss: 0.3974963 \tVal Loss:0.1849155 \tTrain Acc: 89.65278% \tVal Acc: 96.3888884%\n",
      "Epoch: 219\tTrain Loss: 0.4008549 \tVal Loss:0.1787389 \tTrain Acc: 88.47222% \tVal Acc: 96.1111103%\n",
      "Epoch: 220\tTrain Loss: 0.3897540 \tVal Loss:0.1221863 \tTrain Acc: 88.81944% \tVal Acc: 98.0555549%\n",
      "Validation Loss decreased from 0.168298 to 0.122186, saving the model weights\n",
      "Epoch: 221\tTrain Loss: 0.3174742 \tVal Loss:0.1310942 \tTrain Acc: 91.25% \tVal Acc: 98.0555549%\n",
      "Epoch: 222\tTrain Loss: 0.3096229 \tVal Loss:0.1440410 \tTrain Acc: 92.70833% \tVal Acc: 97.4999994%\n",
      "Epoch: 223\tTrain Loss: 0.2976477 \tVal Loss:0.1002715 \tTrain Acc: 92.84722% \tVal Acc: 98.6111109%\n",
      "Validation Loss decreased from 0.122186 to 0.100272, saving the model weights\n",
      "Epoch: 224\tTrain Loss: 0.2965795 \tVal Loss:0.1018166 \tTrain Acc: 92.29167% \tVal Acc: 98.6111104%\n",
      "Epoch: 225\tTrain Loss: 0.3326423 \tVal Loss:0.1153819 \tTrain Acc: 91.31944% \tVal Acc: 97.7777774%\n",
      "Epoch: 226\tTrain Loss: 0.2925060 \tVal Loss:0.1046121 \tTrain Acc: 92.15278% \tVal Acc: 98.3333329%\n",
      "Epoch: 227\tTrain Loss: 0.3127226 \tVal Loss:0.1305471 \tTrain Acc: 91.94444% \tVal Acc: 98.3333329%\n",
      "Epoch: 228\tTrain Loss: 0.3219900 \tVal Loss:0.1200941 \tTrain Acc: 91.52778% \tVal Acc: 97.4999994%\n",
      "Epoch: 229\tTrain Loss: 0.3178489 \tVal Loss:0.0926279 \tTrain Acc: 91.11111% \tVal Acc: 98.6111109%\n",
      "Validation Loss decreased from 0.100272 to 0.092628, saving the model weights\n",
      "Epoch: 230\tTrain Loss: 0.2791228 \tVal Loss:0.1015804 \tTrain Acc: 93.47222% \tVal Acc: 98.6111104%\n",
      "Epoch: 231\tTrain Loss: 0.2908019 \tVal Loss:0.1098572 \tTrain Acc: 91.94444% \tVal Acc: 98.0555549%\n",
      "Epoch: 232\tTrain Loss: 0.3211568 \tVal Loss:0.1586309 \tTrain Acc: 91.11111% \tVal Acc: 96.6666659%\n",
      "Epoch: 233\tTrain Loss: 0.3346456 \tVal Loss:0.1652055 \tTrain Acc: 90.34722% \tVal Acc: 96.3888879%\n",
      "Epoch: 234\tTrain Loss: 0.3266451 \tVal Loss:0.1241073 \tTrain Acc: 90.76389% \tVal Acc: 97.7777769%\n",
      "Epoch: 235\tTrain Loss: 0.5571553 \tVal Loss:0.5416701 \tTrain Acc: 82.91667% \tVal Acc: 84.4444439%\n",
      "Epoch: 236\tTrain Loss: 0.7744531 \tVal Loss:0.9926624 \tTrain Acc: 76.45833% \tVal Acc: 72.2222216%\n",
      "Epoch: 237\tTrain Loss: 0.9168378 \tVal Loss:0.4327232 \tTrain Acc: 73.125% \tVal Acc: 86.1111109%\n",
      "Epoch: 238\tTrain Loss: 0.7601159 \tVal Loss:0.2794563 \tTrain Acc: 74.86111% \tVal Acc: 92.7777767%\n",
      "Epoch: 239\tTrain Loss: 0.5669421 \tVal Loss:0.1844519 \tTrain Acc: 82.22222% \tVal Acc: 96.1111108%\n",
      "Epoch: 240\tTrain Loss: 0.4590226 \tVal Loss:0.1186213 \tTrain Acc: 85.83333% \tVal Acc: 97.4999989%\n",
      "Epoch: 241\tTrain Loss: 0.3226568 \tVal Loss:0.0894154 \tTrain Acc: 90.83333% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.092628 to 0.089415, saving the model weights\n",
      "Epoch: 242\tTrain Loss: 0.2776520 \tVal Loss:0.1155659 \tTrain Acc: 93.05555% \tVal Acc: 97.4999994%\n",
      "Epoch: 243\tTrain Loss: 0.2640157 \tVal Loss:0.0833334 \tTrain Acc: 93.26389% \tVal Acc: 98.3333329%\n",
      "Validation Loss decreased from 0.089415 to 0.083333, saving the model weights\n",
      "Epoch: 244\tTrain Loss: 0.2360300 \tVal Loss:0.0680000 \tTrain Acc: 94.44444% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.083333 to 0.068000, saving the model weights\n",
      "Epoch: 245\tTrain Loss: 0.2205082 \tVal Loss:0.0662243 \tTrain Acc: 94.16667% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.068000 to 0.066224, saving the model weights\n",
      "Epoch: 246\tTrain Loss: 0.1940985 \tVal Loss:0.0614624 \tTrain Acc: 96.11111% \tVal Acc: 98.8888890%\n",
      "Validation Loss decreased from 0.066224 to 0.061462, saving the model weights\n",
      "Epoch: 247\tTrain Loss: 0.1968342 \tVal Loss:0.0604426 \tTrain Acc: 95.41667% \tVal Acc: 98.3333329%\n",
      "Validation Loss decreased from 0.061462 to 0.060443, saving the model weights\n",
      "Epoch: 248\tTrain Loss: 0.1974912 \tVal Loss:0.0595090 \tTrain Acc: 95.55555% \tVal Acc: 98.6111109%\n",
      "Validation Loss decreased from 0.060443 to 0.059509, saving the model weights\n",
      "Epoch: 249\tTrain Loss: 0.1969051 \tVal Loss:0.0629852 \tTrain Acc: 95.55555% \tVal Acc: 98.0555554%\n",
      "Epoch: 250\tTrain Loss: 0.1898575 \tVal Loss:0.0545581 \tTrain Acc: 95.48611% \tVal Acc: 98.6111109%\n",
      "Validation Loss decreased from 0.059509 to 0.054558, saving the model weights\n",
      "Epoch: 251\tTrain Loss: 0.1816178 \tVal Loss:0.0543247 \tTrain Acc: 96.45833% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.054558 to 0.054325, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 252\tTrain Loss: 0.1762737 \tVal Loss:0.0618085 \tTrain Acc: 96.18055% \tVal Acc: 98.6111109%\n",
      "Epoch: 253\tTrain Loss: 0.1922633 \tVal Loss:0.0572983 \tTrain Acc: 95.20833% \tVal Acc: 98.6111104%\n",
      "Epoch: 254\tTrain Loss: 0.1743032 \tVal Loss:0.0559043 \tTrain Acc: 96.45833% \tVal Acc: 97.7777774%\n",
      "Epoch: 255\tTrain Loss: 0.1849541 \tVal Loss:0.0479545 \tTrain Acc: 95.625% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.054325 to 0.047954, saving the model weights\n",
      "Epoch: 256\tTrain Loss: 0.1693922 \tVal Loss:0.0496185 \tTrain Acc: 96.18055% \tVal Acc: 98.0555554%\n",
      "Epoch: 257\tTrain Loss: 0.1635614 \tVal Loss:0.0480752 \tTrain Acc: 95.90278% \tVal Acc: 98.3333334%\n",
      "Epoch: 258\tTrain Loss: 0.1638648 \tVal Loss:0.0446108 \tTrain Acc: 96.38889% \tVal Acc: 98.6111104%\n",
      "Validation Loss decreased from 0.047954 to 0.044611, saving the model weights\n",
      "Epoch: 259\tTrain Loss: 0.1636580 \tVal Loss:0.0531335 \tTrain Acc: 96.04167% \tVal Acc: 98.6111109%\n",
      "Epoch: 260\tTrain Loss: 0.1618775 \tVal Loss:0.0482231 \tTrain Acc: 96.59722% \tVal Acc: 98.8888885%\n",
      "Epoch: 261\tTrain Loss: 0.1582574 \tVal Loss:0.0543875 \tTrain Acc: 96.875% \tVal Acc: 98.0555549%\n",
      "Epoch: 262\tTrain Loss: 0.1479902 \tVal Loss:0.0593391 \tTrain Acc: 97.01389% \tVal Acc: 97.7777774%\n",
      "Epoch: 263\tTrain Loss: 0.3079838 \tVal Loss:0.3114287 \tTrain Acc: 91.25% \tVal Acc: 91.3888887%\n",
      "Epoch: 264\tTrain Loss: 0.8974174 \tVal Loss:0.9503336 \tTrain Acc: 75.13889% \tVal Acc: 69.4444448%\n",
      "Epoch: 265\tTrain Loss: 1.2861713 \tVal Loss:0.8488621 \tTrain Acc: 63.40278% \tVal Acc: 75.2777768%\n",
      "Epoch: 266\tTrain Loss: 0.9567065 \tVal Loss:0.7724544 \tTrain Acc: 71.18056% \tVal Acc: 77.2222221%\n",
      "Epoch: 267\tTrain Loss: 0.6777643 \tVal Loss:0.2993163 \tTrain Acc: 81.18056% \tVal Acc: 91.9444442%\n",
      "Epoch: 268\tTrain Loss: 0.3812274 \tVal Loss:0.1334548 \tTrain Acc: 88.75% \tVal Acc: 96.3888879%\n",
      "Epoch: 269\tTrain Loss: 0.2946790 \tVal Loss:0.1039304 \tTrain Acc: 92.43055% \tVal Acc: 97.7777769%\n",
      "Epoch: 270\tTrain Loss: 0.2358280 \tVal Loss:0.0814017 \tTrain Acc: 94.16667% \tVal Acc: 98.3333329%\n",
      "Epoch: 271\tTrain Loss: 0.2027933 \tVal Loss:0.1324070 \tTrain Acc: 95.625% \tVal Acc: 97.2222219%\n",
      "Epoch: 272\tTrain Loss: 0.2100479 \tVal Loss:0.0562519 \tTrain Acc: 95.34722% \tVal Acc: 98.6111109%\n",
      "Epoch: 273\tTrain Loss: 0.1624439 \tVal Loss:0.0489686 \tTrain Acc: 96.73611% \tVal Acc: 98.6111109%\n",
      "Epoch: 274\tTrain Loss: 0.1383474 \tVal Loss:0.0433080 \tTrain Acc: 98.26389% \tVal Acc: 98.6111109%\n",
      "Validation Loss decreased from 0.044611 to 0.043308, saving the model weights\n",
      "Epoch: 275\tTrain Loss: 0.1538824 \tVal Loss:0.0482802 \tTrain Acc: 96.38889% \tVal Acc: 98.6111104%\n",
      "Epoch: 276\tTrain Loss: 0.1477860 \tVal Loss:0.0427761 \tTrain Acc: 96.45833% \tVal Acc: 98.6111109%\n",
      "Validation Loss decreased from 0.043308 to 0.042776, saving the model weights\n",
      "Epoch: 277\tTrain Loss: 0.1339792 \tVal Loss:0.0397065 \tTrain Acc: 97.36111% \tVal Acc: 98.6111109%\n",
      "Validation Loss decreased from 0.042776 to 0.039706, saving the model weights\n",
      "Epoch: 278\tTrain Loss: 0.1348794 \tVal Loss:0.0400147 \tTrain Acc: 97.01389% \tVal Acc: 98.6111109%\n",
      "Epoch: 279\tTrain Loss: 0.1363611 \tVal Loss:0.0364875 \tTrain Acc: 96.875% \tVal Acc: 98.6111109%\n",
      "Validation Loss decreased from 0.039706 to 0.036488, saving the model weights\n",
      "Epoch: 280\tTrain Loss: 0.1265729 \tVal Loss:0.0379039 \tTrain Acc: 97.77778% \tVal Acc: 98.3333329%\n",
      "Epoch: 281\tTrain Loss: 0.1296640 \tVal Loss:0.0429670 \tTrain Acc: 97.5% \tVal Acc: 98.6111109%\n",
      "Epoch: 282\tTrain Loss: 0.1305859 \tVal Loss:0.0558329 \tTrain Acc: 97.43056% \tVal Acc: 98.3333329%\n",
      "Epoch: 283\tTrain Loss: 0.1226423 \tVal Loss:0.0520466 \tTrain Acc: 97.5% \tVal Acc: 98.0555549%\n",
      "Epoch: 284\tTrain Loss: 0.1185488 \tVal Loss:0.0384803 \tTrain Acc: 97.70833% \tVal Acc: 98.3333329%\n",
      "Epoch: 285\tTrain Loss: 0.1205657 \tVal Loss:0.0347708 \tTrain Acc: 97.56944% \tVal Acc: 98.3333329%\n",
      "Validation Loss decreased from 0.036488 to 0.034771, saving the model weights\n",
      "Epoch: 286\tTrain Loss: 0.1118590 \tVal Loss:0.0395230 \tTrain Acc: 97.56944% \tVal Acc: 98.3333329%\n",
      "Epoch: 287\tTrain Loss: 0.1215472 \tVal Loss:0.0336381 \tTrain Acc: 97.70833% \tVal Acc: 98.8888890%\n",
      "Validation Loss decreased from 0.034771 to 0.033638, saving the model weights\n",
      "Epoch: 288\tTrain Loss: 0.1102084 \tVal Loss:0.0412036 \tTrain Acc: 97.43055% \tVal Acc: 98.6111104%\n",
      "Epoch: 289\tTrain Loss: 0.1498579 \tVal Loss:0.0428409 \tTrain Acc: 96.45833% \tVal Acc: 98.8888885%\n",
      "Epoch: 290\tTrain Loss: 0.1785417 \tVal Loss:0.0602663 \tTrain Acc: 95.69444% \tVal Acc: 98.6111104%\n",
      "Epoch: 291\tTrain Loss: 0.1342029 \tVal Loss:0.0422823 \tTrain Acc: 97.77778% \tVal Acc: 98.8888890%\n",
      "Epoch: 292\tTrain Loss: 0.1580433 \tVal Loss:0.1162194 \tTrain Acc: 95.76389% \tVal Acc: 96.6666659%\n",
      "Epoch: 293\tTrain Loss: 0.3607646 \tVal Loss:0.1389780 \tTrain Acc: 88.95833% \tVal Acc: 95.8333328%\n",
      "Epoch: 294\tTrain Loss: 0.6240571 \tVal Loss:0.4212584 \tTrain Acc: 81.45833% \tVal Acc: 85.5555557%\n",
      "Epoch: 295\tTrain Loss: 0.9652280 \tVal Loss:0.5038735 \tTrain Acc: 72.43056% \tVal Acc: 81.6666668%\n",
      "Epoch: 296\tTrain Loss: 0.5620043 \tVal Loss:0.2462317 \tTrain Acc: 82.5% \tVal Acc: 92.4999987%\n",
      "Epoch: 297\tTrain Loss: 0.3474694 \tVal Loss:0.0966917 \tTrain Acc: 89.44444% \tVal Acc: 98.0555554%\n",
      "Epoch: 298\tTrain Loss: 0.2325886 \tVal Loss:0.0537212 \tTrain Acc: 93.88889% \tVal Acc: 98.3333329%\n",
      "Epoch: 299\tTrain Loss: 0.1629910 \tVal Loss:0.0444519 \tTrain Acc: 96.18055% \tVal Acc: 98.6111109%\n",
      "Epoch: 300\tTrain Loss: 0.1417237 \tVal Loss:0.0450115 \tTrain Acc: 97.08333% \tVal Acc: 98.6111109%\n",
      "Epoch: 301\tTrain Loss: 0.1320705 \tVal Loss:0.0675493 \tTrain Acc: 96.38889% \tVal Acc: 97.2222219%\n",
      "Epoch: 302\tTrain Loss: 0.1341222 \tVal Loss:0.0642456 \tTrain Acc: 96.875% \tVal Acc: 97.4999994%\n",
      "Epoch: 303\tTrain Loss: 0.1307560 \tVal Loss:0.1565522 \tTrain Acc: 97.15278% \tVal Acc: 96.3888884%\n",
      "Epoch: 304\tTrain Loss: 0.1545202 \tVal Loss:0.0413348 \tTrain Acc: 96.73611% \tVal Acc: 98.3333329%\n",
      "Epoch: 305\tTrain Loss: 0.1208353 \tVal Loss:0.0363976 \tTrain Acc: 97.15278% \tVal Acc: 98.6111109%\n",
      "Epoch: 306\tTrain Loss: 0.1041443 \tVal Loss:0.0308681 \tTrain Acc: 97.84722% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.033638 to 0.030868, saving the model weights\n",
      "Epoch: 307\tTrain Loss: 0.1022821 \tVal Loss:0.0310376 \tTrain Acc: 97.63889% \tVal Acc: 98.3333329%\n",
      "Epoch: 308\tTrain Loss: 0.0903958 \tVal Loss:0.0454711 \tTrain Acc: 98.61111% \tVal Acc: 98.3333334%\n",
      "Epoch: 309\tTrain Loss: 0.0922986 \tVal Loss:0.0344366 \tTrain Acc: 98.40278% \tVal Acc: 98.8888885%\n",
      "Epoch: 310\tTrain Loss: 0.0998127 \tVal Loss:0.0293609 \tTrain Acc: 97.77778% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.030868 to 0.029361, saving the model weights\n",
      "Epoch: 311\tTrain Loss: 0.0892395 \tVal Loss:0.0380441 \tTrain Acc: 98.125% \tVal Acc: 98.3333334%\n",
      "Epoch: 312\tTrain Loss: 0.1079941 \tVal Loss:0.0305762 \tTrain Acc: 97.70833% \tVal Acc: 98.8888885%\n",
      "Epoch: 313\tTrain Loss: 0.1011660 \tVal Loss:0.0288704 \tTrain Acc: 97.98611% \tVal Acc: 98.6111109%\n",
      "Validation Loss decreased from 0.029361 to 0.028870, saving the model weights\n",
      "Epoch: 314\tTrain Loss: 0.0878895 \tVal Loss:0.0274146 \tTrain Acc: 98.81944% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.028870 to 0.027415, saving the model weights\n",
      "Epoch: 315\tTrain Loss: 0.0761270 \tVal Loss:0.0271342 \tTrain Acc: 98.88889% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.027415 to 0.027134, saving the model weights\n",
      "Epoch: 316\tTrain Loss: 0.0790032 \tVal Loss:0.0348235 \tTrain Acc: 98.75% \tVal Acc: 98.3333329%\n",
      "Epoch: 317\tTrain Loss: 0.0807148 \tVal Loss:0.0270946 \tTrain Acc: 98.54167% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.027134 to 0.027095, saving the model weights\n",
      "Epoch: 318\tTrain Loss: 0.1011331 \tVal Loss:0.0292047 \tTrain Acc: 97.98611% \tVal Acc: 98.3333329%\n",
      "Epoch: 319\tTrain Loss: 0.0984141 \tVal Loss:0.0310498 \tTrain Acc: 97.56944% \tVal Acc: 98.6111104%\n",
      "Epoch: 320\tTrain Loss: 0.0863385 \tVal Loss:0.0407213 \tTrain Acc: 97.70833% \tVal Acc: 98.3333329%\n",
      "Epoch: 321\tTrain Loss: 0.0952696 \tVal Loss:0.0302992 \tTrain Acc: 97.98611% \tVal Acc: 98.3333329%\n",
      "Epoch: 322\tTrain Loss: 0.0929787 \tVal Loss:0.0292345 \tTrain Acc: 97.63889% \tVal Acc: 98.8888885%\n",
      "Epoch: 323\tTrain Loss: 0.1008897 \tVal Loss:0.0297987 \tTrain Acc: 97.63889% \tVal Acc: 99.1666665%\n",
      "Epoch: 324\tTrain Loss: 0.0857881 \tVal Loss:0.0327103 \tTrain Acc: 97.77778% \tVal Acc: 98.8888885%\n",
      "Epoch: 325\tTrain Loss: 0.0817570 \tVal Loss:0.0306048 \tTrain Acc: 98.68056% \tVal Acc: 98.8888885%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 326\tTrain Loss: 0.0870633 \tVal Loss:0.0281713 \tTrain Acc: 98.05555% \tVal Acc: 98.8888885%\n",
      "Epoch: 327\tTrain Loss: 0.0749794 \tVal Loss:0.0348670 \tTrain Acc: 98.40278% \tVal Acc: 98.3333329%\n",
      "Epoch: 328\tTrain Loss: 0.0771762 \tVal Loss:0.0242469 \tTrain Acc: 98.40278% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.027095 to 0.024247, saving the model weights\n",
      "Epoch: 329\tTrain Loss: 0.0864558 \tVal Loss:0.0256828 \tTrain Acc: 98.05555% \tVal Acc: 98.8888885%\n",
      "Epoch: 330\tTrain Loss: 0.1817615 \tVal Loss:0.0648792 \tTrain Acc: 94.72222% \tVal Acc: 98.0555549%\n",
      "Epoch: 331\tTrain Loss: 0.6602617 \tVal Loss:1.0407497 \tTrain Acc: 80.0% \tVal Acc: 72.4999992%\n",
      "Epoch: 332\tTrain Loss: 1.5149748 \tVal Loss:1.2972596 \tTrain Acc: 58.75% \tVal Acc: 65.0000001%\n",
      "Epoch: 333\tTrain Loss: 1.1909584 \tVal Loss:0.8783404 \tTrain Acc: 65.625% \tVal Acc: 73.8888885%\n",
      "Epoch: 334\tTrain Loss: 0.6401588 \tVal Loss:0.1745527 \tTrain Acc: 79.79167% \tVal Acc: 93.8888888%\n",
      "Epoch: 335\tTrain Loss: 0.3961488 \tVal Loss:0.0585069 \tTrain Acc: 88.19444% \tVal Acc: 98.6111104%\n",
      "Epoch: 336\tTrain Loss: 0.2327414 \tVal Loss:0.0687562 \tTrain Acc: 94.02778% \tVal Acc: 98.3333334%\n",
      "Epoch: 337\tTrain Loss: 0.2017470 \tVal Loss:0.0728764 \tTrain Acc: 94.23611% \tVal Acc: 97.4999994%\n",
      "Epoch: 338\tTrain Loss: 0.2023369 \tVal Loss:0.1401325 \tTrain Acc: 95.48611% \tVal Acc: 95.5555548%\n",
      "Epoch: 339\tTrain Loss: 0.1618988 \tVal Loss:0.0421924 \tTrain Acc: 96.18055% \tVal Acc: 98.3333334%\n",
      "Epoch: 340\tTrain Loss: 0.1215634 \tVal Loss:0.0292068 \tTrain Acc: 97.43055% \tVal Acc: 99.1666665%\n",
      "Epoch: 341\tTrain Loss: 0.1100436 \tVal Loss:0.0272707 \tTrain Acc: 97.70833% \tVal Acc: 98.8888885%\n",
      "Epoch: 342\tTrain Loss: 0.0988103 \tVal Loss:0.0255806 \tTrain Acc: 97.91667% \tVal Acc: 99.4444440%\n",
      "Epoch: 343\tTrain Loss: 0.0835107 \tVal Loss:0.0267407 \tTrain Acc: 98.75% \tVal Acc: 99.1666660%\n",
      "Epoch: 344\tTrain Loss: 0.0920681 \tVal Loss:0.0239456 \tTrain Acc: 98.40278% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.024247 to 0.023946, saving the model weights\n",
      "Epoch: 345\tTrain Loss: 0.0869737 \tVal Loss:0.0237471 \tTrain Acc: 98.125% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.023946 to 0.023747, saving the model weights\n",
      "Epoch: 346\tTrain Loss: 0.0804177 \tVal Loss:0.0224222 \tTrain Acc: 98.75% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.023747 to 0.022422, saving the model weights\n",
      "Epoch: 347\tTrain Loss: 0.0808894 \tVal Loss:0.0232016 \tTrain Acc: 98.33333% \tVal Acc: 99.1666665%\n",
      "Epoch: 348\tTrain Loss: 0.0834193 \tVal Loss:0.0265692 \tTrain Acc: 98.05555% \tVal Acc: 98.8888890%\n",
      "Epoch: 349\tTrain Loss: 0.0834725 \tVal Loss:0.0519231 \tTrain Acc: 98.47222% \tVal Acc: 98.3333334%\n",
      "Epoch: 350\tTrain Loss: 0.0905596 \tVal Loss:0.0287757 \tTrain Acc: 97.84722% \tVal Acc: 98.6111109%\n",
      "Epoch: 351\tTrain Loss: 0.0823459 \tVal Loss:0.0219137 \tTrain Acc: 98.88889% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.022422 to 0.021914, saving the model weights\n",
      "Epoch: 352\tTrain Loss: 0.0750806 \tVal Loss:0.0247456 \tTrain Acc: 98.81944% \tVal Acc: 98.6111109%\n",
      "Epoch: 353\tTrain Loss: 0.0829445 \tVal Loss:0.0215635 \tTrain Acc: 98.19444% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.021914 to 0.021563, saving the model weights\n",
      "Epoch: 354\tTrain Loss: 0.0765003 \tVal Loss:0.0304232 \tTrain Acc: 98.26389% \tVal Acc: 98.6111109%\n",
      "Epoch: 355\tTrain Loss: 0.0776698 \tVal Loss:0.0346553 \tTrain Acc: 98.40278% \tVal Acc: 98.3333329%\n",
      "Epoch: 356\tTrain Loss: 0.0734803 \tVal Loss:0.0312209 \tTrain Acc: 98.47222% \tVal Acc: 98.6111109%\n",
      "Epoch: 357\tTrain Loss: 0.0831232 \tVal Loss:0.0320775 \tTrain Acc: 98.54167% \tVal Acc: 98.3333329%\n",
      "Epoch: 358\tTrain Loss: 0.0692007 \tVal Loss:0.0299343 \tTrain Acc: 98.88889% \tVal Acc: 98.3333329%\n",
      "Epoch: 359\tTrain Loss: 0.0769130 \tVal Loss:0.0304072 \tTrain Acc: 98.61111% \tVal Acc: 98.6111109%\n",
      "Epoch: 360\tTrain Loss: 0.0780758 \tVal Loss:0.0329477 \tTrain Acc: 98.19444% \tVal Acc: 98.3333334%\n",
      "Epoch: 361\tTrain Loss: 0.0980552 \tVal Loss:0.0426973 \tTrain Acc: 97.84722% \tVal Acc: 98.0555549%\n",
      "Epoch: 362\tTrain Loss: 0.1278033 \tVal Loss:0.0567211 \tTrain Acc: 96.59722% \tVal Acc: 97.7777774%\n",
      "Epoch: 363\tTrain Loss: 0.1286965 \tVal Loss:0.0994530 \tTrain Acc: 96.66667% \tVal Acc: 96.3888879%\n",
      "Epoch: 364\tTrain Loss: 0.1291876 \tVal Loss:0.1579277 \tTrain Acc: 96.38889% \tVal Acc: 95.8333328%\n",
      "Epoch: 365\tTrain Loss: 0.1562604 \tVal Loss:0.0836393 \tTrain Acc: 96.25% \tVal Acc: 96.6666659%\n",
      "Epoch: 366\tTrain Loss: 0.1115365 \tVal Loss:0.0978423 \tTrain Acc: 97.5% \tVal Acc: 96.9444444%\n",
      "Epoch: 367\tTrain Loss: 0.1048442 \tVal Loss:0.0476011 \tTrain Acc: 97.08333% \tVal Acc: 98.0555554%\n",
      "Epoch: 368\tTrain Loss: 0.0887132 \tVal Loss:0.0640569 \tTrain Acc: 98.33333% \tVal Acc: 97.4999999%\n",
      "Epoch: 369\tTrain Loss: 0.1921638 \tVal Loss:0.0975888 \tTrain Acc: 94.93055% \tVal Acc: 95.2777763%\n",
      "Epoch: 370\tTrain Loss: 0.2145841 \tVal Loss:0.0657794 \tTrain Acc: 93.68055% \tVal Acc: 98.3333324%\n",
      "Epoch: 371\tTrain Loss: 0.3483148 \tVal Loss:0.1222385 \tTrain Acc: 89.72222% \tVal Acc: 96.1111103%\n",
      "Epoch: 372\tTrain Loss: 0.4523874 \tVal Loss:0.3397910 \tTrain Acc: 86.875% \tVal Acc: 89.1666661%\n",
      "Epoch: 373\tTrain Loss: 0.4678443 \tVal Loss:0.1380581 \tTrain Acc: 87.29167% \tVal Acc: 94.9999993%\n",
      "Epoch: 374\tTrain Loss: 0.4253854 \tVal Loss:0.7312975 \tTrain Acc: 86.66667% \tVal Acc: 78.6111109%\n",
      "Epoch: 375\tTrain Loss: 0.4309753 \tVal Loss:0.3701652 \tTrain Acc: 88.54167% \tVal Acc: 91.3888882%\n",
      "Epoch: 376\tTrain Loss: 0.2892493 \tVal Loss:0.1597405 \tTrain Acc: 91.80555% \tVal Acc: 94.9999993%\n",
      "Epoch: 377\tTrain Loss: 0.2071029 \tVal Loss:0.0893624 \tTrain Acc: 94.23611% \tVal Acc: 97.2222224%\n",
      "Epoch: 378\tTrain Loss: 0.1448019 \tVal Loss:0.0584169 \tTrain Acc: 96.25% \tVal Acc: 96.9444439%\n",
      "Epoch: 379\tTrain Loss: 0.1104638 \tVal Loss:0.0363496 \tTrain Acc: 97.22222% \tVal Acc: 99.4444440%\n",
      "Epoch: 380\tTrain Loss: 0.0916773 \tVal Loss:0.0312090 \tTrain Acc: 98.05555% \tVal Acc: 99.1666665%\n",
      "Epoch: 381\tTrain Loss: 0.0801773 \tVal Loss:0.0293954 \tTrain Acc: 98.33333% \tVal Acc: 98.6111104%\n",
      "Epoch: 382\tTrain Loss: 0.0922244 \tVal Loss:0.0266832 \tTrain Acc: 97.15278% \tVal Acc: 99.4444445%\n",
      "Epoch: 383\tTrain Loss: 0.0852670 \tVal Loss:0.0361001 \tTrain Acc: 98.05555% \tVal Acc: 98.3333329%\n",
      "Epoch: 384\tTrain Loss: 0.0763318 \tVal Loss:0.0398172 \tTrain Acc: 98.33333% \tVal Acc: 98.3333329%\n",
      "Epoch: 385\tTrain Loss: 0.0904955 \tVal Loss:0.0278513 \tTrain Acc: 97.98611% \tVal Acc: 98.0555554%\n",
      "Epoch: 386\tTrain Loss: 0.0711247 \tVal Loss:0.0265321 \tTrain Acc: 98.75% \tVal Acc: 98.6111109%\n",
      "Epoch: 387\tTrain Loss: 0.0695919 \tVal Loss:0.0232111 \tTrain Acc: 98.95833% \tVal Acc: 98.8888885%\n",
      "Epoch: 388\tTrain Loss: 0.0641931 \tVal Loss:0.0341358 \tTrain Acc: 98.75% \tVal Acc: 98.3333329%\n",
      "Epoch: 389\tTrain Loss: 0.0546417 \tVal Loss:0.0380357 \tTrain Acc: 98.88889% \tVal Acc: 98.0555554%\n",
      "Epoch: 390\tTrain Loss: 0.0587034 \tVal Loss:0.0316740 \tTrain Acc: 98.88889% \tVal Acc: 98.6111109%\n",
      "Epoch: 391\tTrain Loss: 0.0681014 \tVal Loss:0.0285248 \tTrain Acc: 98.19444% \tVal Acc: 98.6111109%\n",
      "Epoch: 392\tTrain Loss: 0.0727610 \tVal Loss:0.0569754 \tTrain Acc: 98.33333% \tVal Acc: 98.3333334%\n",
      "Epoch: 393\tTrain Loss: 0.1158362 \tVal Loss:0.1690107 \tTrain Acc: 96.45833% \tVal Acc: 95.2777768%\n",
      "Epoch: 394\tTrain Loss: 0.1711604 \tVal Loss:0.0977950 \tTrain Acc: 95.625% \tVal Acc: 98.0555554%\n",
      "Epoch: 395\tTrain Loss: 0.1915415 \tVal Loss:0.0551349 \tTrain Acc: 94.65278% \tVal Acc: 97.7777774%\n",
      "Epoch: 396\tTrain Loss: 0.1723753 \tVal Loss:0.0836112 \tTrain Acc: 94.72222% \tVal Acc: 97.2222224%\n",
      "Epoch: 397\tTrain Loss: 0.2207174 \tVal Loss:0.0685214 \tTrain Acc: 93.47222% \tVal Acc: 97.7777774%\n",
      "Epoch: 398\tTrain Loss: 0.2017578 \tVal Loss:0.0726027 \tTrain Acc: 93.95833% \tVal Acc: 97.2222219%\n",
      "Epoch: 399\tTrain Loss: 0.1470051 \tVal Loss:0.0328531 \tTrain Acc: 95.27778% \tVal Acc: 98.6111104%\n",
      "Epoch: 400\tTrain Loss: 0.0933587 \tVal Loss:0.0310346 \tTrain Acc: 97.63889% \tVal Acc: 98.3333329%\n",
      "Epoch: 401\tTrain Loss: 0.1179014 \tVal Loss:0.0340554 \tTrain Acc: 97.5% \tVal Acc: 98.6111109%\n",
      "Epoch: 402\tTrain Loss: 0.0966936 \tVal Loss:0.0429824 \tTrain Acc: 97.56944% \tVal Acc: 98.6111109%\n",
      "Epoch: 403\tTrain Loss: 0.0817551 \tVal Loss:0.0491782 \tTrain Acc: 98.125% \tVal Acc: 97.7777774%\n",
      "Epoch: 404\tTrain Loss: 0.0796063 \tVal Loss:0.0314609 \tTrain Acc: 98.125% \tVal Acc: 98.6111109%\n",
      "Epoch: 405\tTrain Loss: 0.0639223 \tVal Loss:0.0329416 \tTrain Acc: 98.88889% \tVal Acc: 98.6111109%\n",
      "Epoch: 406\tTrain Loss: 0.0673815 \tVal Loss:0.0208436 \tTrain Acc: 98.26389% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.021563 to 0.020844, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 407\tTrain Loss: 0.0674272 \tVal Loss:0.0241188 \tTrain Acc: 98.47222% \tVal Acc: 98.6111109%\n",
      "Epoch: 408\tTrain Loss: 0.0604829 \tVal Loss:0.0180699 \tTrain Acc: 98.88889% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.020844 to 0.018070, saving the model weights\n",
      "Epoch: 409\tTrain Loss: 0.0638660 \tVal Loss:0.0169956 \tTrain Acc: 98.81944% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.018070 to 0.016996, saving the model weights\n",
      "Epoch: 410\tTrain Loss: 0.0531034 \tVal Loss:0.0184666 \tTrain Acc: 98.61111% \tVal Acc: 98.8888885%\n",
      "Epoch: 411\tTrain Loss: 0.0611424 \tVal Loss:0.0351328 \tTrain Acc: 98.47222% \tVal Acc: 98.3333329%\n",
      "Epoch: 412\tTrain Loss: 0.0743730 \tVal Loss:0.0268996 \tTrain Acc: 98.19444% \tVal Acc: 98.3333334%\n",
      "Epoch: 413\tTrain Loss: 0.0599533 \tVal Loss:0.0180864 \tTrain Acc: 98.54167% \tVal Acc: 99.4444445%\n",
      "Epoch: 414\tTrain Loss: 0.0544116 \tVal Loss:0.0172728 \tTrain Acc: 98.95833% \tVal Acc: 99.4444445%\n",
      "Epoch: 415\tTrain Loss: 0.0675660 \tVal Loss:0.0265692 \tTrain Acc: 98.81944% \tVal Acc: 98.6111109%\n",
      "Epoch: 416\tTrain Loss: 0.0591136 \tVal Loss:0.0299807 \tTrain Acc: 98.40278% \tVal Acc: 98.6111104%\n",
      "Epoch: 417\tTrain Loss: 0.0568547 \tVal Loss:0.0288000 \tTrain Acc: 98.75% \tVal Acc: 98.6111109%\n",
      "Epoch: 418\tTrain Loss: 0.0647146 \tVal Loss:0.0437276 \tTrain Acc: 98.33333% \tVal Acc: 98.6111109%\n",
      "Epoch: 419\tTrain Loss: 0.0920329 \tVal Loss:0.1062380 \tTrain Acc: 97.98611% \tVal Acc: 96.9444444%\n",
      "Epoch: 420\tTrain Loss: 0.0968134 \tVal Loss:0.0406616 \tTrain Acc: 97.29167% \tVal Acc: 98.6111104%\n",
      "Epoch: 421\tTrain Loss: 0.1142735 \tVal Loss:0.0980393 \tTrain Acc: 96.80555% \tVal Acc: 97.4999994%\n",
      "Epoch: 422\tTrain Loss: 0.3207865 \tVal Loss:0.6374175 \tTrain Acc: 91.11111% \tVal Acc: 83.3333333%\n",
      "Epoch: 423\tTrain Loss: 1.1284765 \tVal Loss:1.1747636 \tTrain Acc: 68.54167% \tVal Acc: 67.7777777%\n",
      "Epoch: 424\tTrain Loss: 1.6245552 \tVal Loss:1.1940924 \tTrain Acc: 56.11111% \tVal Acc: 65.0000003%\n",
      "Epoch: 425\tTrain Loss: 1.0382061 \tVal Loss:0.7054873 \tTrain Acc: 70.34722% \tVal Acc: 79.1666662%\n",
      "Epoch: 426\tTrain Loss: 0.5578969 \tVal Loss:0.2690135 \tTrain Acc: 82.08333% \tVal Acc: 92.2222217%\n",
      "Epoch: 427\tTrain Loss: 0.2689781 \tVal Loss:0.0960502 \tTrain Acc: 92.22222% \tVal Acc: 96.9444439%\n",
      "Epoch: 428\tTrain Loss: 0.1674269 \tVal Loss:0.0513582 \tTrain Acc: 95.55555% \tVal Acc: 98.3333329%\n",
      "Epoch: 429\tTrain Loss: 0.1208043 \tVal Loss:0.0358782 \tTrain Acc: 97.15278% \tVal Acc: 98.6111109%\n",
      "Epoch: 430\tTrain Loss: 0.0919612 \tVal Loss:0.0255119 \tTrain Acc: 98.19444% \tVal Acc: 99.1666665%\n",
      "Epoch: 431\tTrain Loss: 0.0890406 \tVal Loss:0.0286628 \tTrain Acc: 98.26389% \tVal Acc: 98.3333334%\n",
      "Epoch: 432\tTrain Loss: 0.0818214 \tVal Loss:0.0245218 \tTrain Acc: 98.40278% \tVal Acc: 99.1666665%\n",
      "Epoch: 433\tTrain Loss: 0.0715197 \tVal Loss:0.0214573 \tTrain Acc: 98.54167% \tVal Acc: 99.4444440%\n",
      "Epoch: 434\tTrain Loss: 0.0769198 \tVal Loss:0.0206040 \tTrain Acc: 98.47222% \tVal Acc: 99.4444440%\n",
      "Epoch: 435\tTrain Loss: 0.0697309 \tVal Loss:0.0217191 \tTrain Acc: 98.61111% \tVal Acc: 99.1666660%\n",
      "Epoch: 436\tTrain Loss: 0.0608125 \tVal Loss:0.0212281 \tTrain Acc: 98.95833% \tVal Acc: 99.1666665%\n",
      "Epoch: 437\tTrain Loss: 0.0625822 \tVal Loss:0.0187107 \tTrain Acc: 98.61111% \tVal Acc: 99.4444440%\n",
      "Epoch: 438\tTrain Loss: 0.0676661 \tVal Loss:0.0214114 \tTrain Acc: 98.68056% \tVal Acc: 98.8888885%\n",
      "Epoch: 439\tTrain Loss: 0.0613804 \tVal Loss:0.0202426 \tTrain Acc: 98.88889% \tVal Acc: 99.1666660%\n",
      "Epoch: 440\tTrain Loss: 0.2918976 \tVal Loss:0.1265557 \tTrain Acc: 91.45833% \tVal Acc: 95.2777773%\n",
      "Epoch: 441\tTrain Loss: 0.2358510 \tVal Loss:0.0312288 \tTrain Acc: 93.26389% \tVal Acc: 98.8888885%\n",
      "Epoch: 442\tTrain Loss: 0.1950575 \tVal Loss:0.1130647 \tTrain Acc: 94.58333% \tVal Acc: 97.4999994%\n",
      "Epoch: 443\tTrain Loss: 0.1565710 \tVal Loss:0.0808801 \tTrain Acc: 95.90278% \tVal Acc: 97.7777779%\n",
      "Epoch: 444\tTrain Loss: 0.1037262 \tVal Loss:0.0561277 \tTrain Acc: 97.22222% \tVal Acc: 97.2222219%\n",
      "Epoch: 445\tTrain Loss: 0.0921241 \tVal Loss:0.0592510 \tTrain Acc: 97.63889% \tVal Acc: 97.4999994%\n",
      "Epoch: 446\tTrain Loss: 0.0815577 \tVal Loss:0.0302269 \tTrain Acc: 97.98611% \tVal Acc: 98.6111109%\n",
      "Epoch: 447\tTrain Loss: 0.0625127 \tVal Loss:0.0271892 \tTrain Acc: 98.81944% \tVal Acc: 98.8888890%\n",
      "Epoch: 448\tTrain Loss: 0.0632628 \tVal Loss:0.0272272 \tTrain Acc: 98.81944% \tVal Acc: 98.3333334%\n",
      "Epoch: 449\tTrain Loss: 0.0591231 \tVal Loss:0.0336085 \tTrain Acc: 98.61111% \tVal Acc: 98.3333334%\n",
      "Epoch: 450\tTrain Loss: 0.0517161 \tVal Loss:0.0272715 \tTrain Acc: 99.02778% \tVal Acc: 98.3333329%\n",
      "Epoch: 451\tTrain Loss: 0.0565199 \tVal Loss:0.0196718 \tTrain Acc: 98.81944% \tVal Acc: 99.1666665%\n",
      "Epoch: 452\tTrain Loss: 0.0503824 \tVal Loss:0.0223961 \tTrain Acc: 98.95833% \tVal Acc: 98.6111109%\n",
      "Epoch: 453\tTrain Loss: 0.0477697 \tVal Loss:0.0201299 \tTrain Acc: 99.16667% \tVal Acc: 98.8888885%\n",
      "Epoch: 454\tTrain Loss: 0.0514408 \tVal Loss:0.0356253 \tTrain Acc: 98.68056% \tVal Acc: 98.3333329%\n",
      "Epoch: 455\tTrain Loss: 0.0644972 \tVal Loss:0.0186621 \tTrain Acc: 98.75% \tVal Acc: 99.4444445%\n",
      "Epoch: 456\tTrain Loss: 0.0591876 \tVal Loss:0.0183982 \tTrain Acc: 98.33333% \tVal Acc: 99.4444445%\n",
      "Epoch: 457\tTrain Loss: 0.0558831 \tVal Loss:0.0174635 \tTrain Acc: 98.40278% \tVal Acc: 99.4444445%\n",
      "Epoch: 458\tTrain Loss: 0.0536073 \tVal Loss:0.0173975 \tTrain Acc: 98.68055% \tVal Acc: 99.4444440%\n",
      "Epoch: 459\tTrain Loss: 0.0548351 \tVal Loss:0.0356972 \tTrain Acc: 98.61111% \tVal Acc: 98.3333329%\n",
      "Epoch: 460\tTrain Loss: 0.0484046 \tVal Loss:0.0175428 \tTrain Acc: 98.68055% \tVal Acc: 98.8888885%\n",
      "Epoch: 461\tTrain Loss: 0.0480968 \tVal Loss:0.0173186 \tTrain Acc: 98.75% \tVal Acc: 99.1666665%\n",
      "Epoch: 462\tTrain Loss: 0.0445601 \tVal Loss:0.0161353 \tTrain Acc: 98.95833% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.016996 to 0.016135, saving the model weights\n",
      "Epoch: 463\tTrain Loss: 0.0463007 \tVal Loss:0.0210116 \tTrain Acc: 99.16667% \tVal Acc: 98.3333334%\n",
      "Epoch: 464\tTrain Loss: 0.0439327 \tVal Loss:0.0190203 \tTrain Acc: 98.88889% \tVal Acc: 99.1666665%\n",
      "Epoch: 465\tTrain Loss: 0.0430535 \tVal Loss:0.0417732 \tTrain Acc: 99.16667% \tVal Acc: 98.3333329%\n",
      "Epoch: 466\tTrain Loss: 0.0476326 \tVal Loss:0.0184372 \tTrain Acc: 98.95833% \tVal Acc: 98.8888885%\n",
      "Epoch: 467\tTrain Loss: 0.0400805 \tVal Loss:0.0147759 \tTrain Acc: 99.02778% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.016135 to 0.014776, saving the model weights\n",
      "Epoch: 468\tTrain Loss: 0.0414826 \tVal Loss:0.0148556 \tTrain Acc: 98.95833% \tVal Acc: 99.1666660%\n",
      "Epoch: 469\tTrain Loss: 0.0363069 \tVal Loss:0.0173703 \tTrain Acc: 99.58333% \tVal Acc: 99.1666665%\n",
      "Epoch: 470\tTrain Loss: 0.0366873 \tVal Loss:0.0179644 \tTrain Acc: 99.44444% \tVal Acc: 99.1666665%\n",
      "Epoch: 471\tTrain Loss: 0.0383032 \tVal Loss:0.0526072 \tTrain Acc: 98.95833% \tVal Acc: 98.0555554%\n",
      "Epoch: 472\tTrain Loss: 0.0487826 \tVal Loss:0.0189734 \tTrain Acc: 98.81944% \tVal Acc: 99.1666665%\n",
      "Epoch: 473\tTrain Loss: 0.0417959 \tVal Loss:0.0155292 \tTrain Acc: 99.02778% \tVal Acc: 99.4444440%\n",
      "Epoch: 474\tTrain Loss: 0.0390530 \tVal Loss:0.0151394 \tTrain Acc: 99.09722% \tVal Acc: 99.4444440%\n",
      "Epoch: 475\tTrain Loss: 0.0395415 \tVal Loss:0.0231464 \tTrain Acc: 99.16667% \tVal Acc: 98.6111104%\n",
      "Epoch: 476\tTrain Loss: 0.0383799 \tVal Loss:0.0828817 \tTrain Acc: 99.02778% \tVal Acc: 97.4999999%\n",
      "Epoch: 477\tTrain Loss: 0.4404315 \tVal Loss:0.2321560 \tTrain Acc: 87.98611% \tVal Acc: 91.1111111%\n",
      "Epoch: 478\tTrain Loss: 1.1353369 \tVal Loss:0.7797789 \tTrain Acc: 68.40278% \tVal Acc: 79.1666657%\n",
      "Epoch: 479\tTrain Loss: 1.0267424 \tVal Loss:0.3945161 \tTrain Acc: 70.76389% \tVal Acc: 84.7222209%\n",
      "Epoch: 480\tTrain Loss: 0.5476914 \tVal Loss:0.3581391 \tTrain Acc: 83.68056% \tVal Acc: 86.9444445%\n",
      "Epoch: 481\tTrain Loss: 0.4472938 \tVal Loss:1.1601262 \tTrain Acc: 88.33333% \tVal Acc: 76.6666671%\n",
      "Epoch: 482\tTrain Loss: 0.5627225 \tVal Loss:0.6721532 \tTrain Acc: 86.45833% \tVal Acc: 80.8333332%\n",
      "Epoch: 483\tTrain Loss: 0.2633782 \tVal Loss:0.4445691 \tTrain Acc: 93.47222% \tVal Acc: 86.9444450%\n",
      "Epoch: 484\tTrain Loss: 0.1808061 \tVal Loss:0.1025687 \tTrain Acc: 95.55555% \tVal Acc: 96.1111103%\n",
      "Epoch: 485\tTrain Loss: 0.1139750 \tVal Loss:0.0367319 \tTrain Acc: 96.80555% \tVal Acc: 99.1666665%\n",
      "Epoch: 486\tTrain Loss: 0.0817677 \tVal Loss:0.0289298 \tTrain Acc: 98.05555% \tVal Acc: 98.8888885%\n",
      "Epoch: 487\tTrain Loss: 0.0712906 \tVal Loss:0.0233684 \tTrain Acc: 98.47222% \tVal Acc: 99.4444440%\n",
      "Epoch: 488\tTrain Loss: 0.0596708 \tVal Loss:0.0212677 \tTrain Acc: 99.09722% \tVal Acc: 99.4444440%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 489\tTrain Loss: 0.0591425 \tVal Loss:0.0192112 \tTrain Acc: 99.16667% \tVal Acc: 99.4444440%\n",
      "Epoch: 490\tTrain Loss: 0.0459099 \tVal Loss:0.0179412 \tTrain Acc: 99.30556% \tVal Acc: 99.4444445%\n",
      "Epoch: 491\tTrain Loss: 0.0536598 \tVal Loss:0.0189473 \tTrain Acc: 98.33333% \tVal Acc: 99.1666665%\n",
      "Epoch: 492\tTrain Loss: 0.0450519 \tVal Loss:0.0235898 \tTrain Acc: 98.95833% \tVal Acc: 98.3333334%\n",
      "Epoch: 493\tTrain Loss: 0.0519966 \tVal Loss:0.0408796 \tTrain Acc: 98.88889% \tVal Acc: 98.3333334%\n",
      "Epoch: 494\tTrain Loss: 0.0610679 \tVal Loss:0.0216820 \tTrain Acc: 98.61111% \tVal Acc: 98.6111109%\n",
      "Epoch: 495\tTrain Loss: 0.0511079 \tVal Loss:0.0166952 \tTrain Acc: 98.88889% \tVal Acc: 99.4444440%\n",
      "Epoch: 496\tTrain Loss: 0.0514259 \tVal Loss:0.0158129 \tTrain Acc: 98.95833% \tVal Acc: 99.4444445%\n",
      "Epoch: 497\tTrain Loss: 0.0477088 \tVal Loss:0.0166282 \tTrain Acc: 99.23611% \tVal Acc: 99.1666665%\n",
      "Epoch: 498\tTrain Loss: 0.0480982 \tVal Loss:0.0156859 \tTrain Acc: 99.16667% \tVal Acc: 99.4444440%\n",
      "Epoch: 499\tTrain Loss: 0.0388577 \tVal Loss:0.0151986 \tTrain Acc: 99.44444% \tVal Acc: 99.4444440%\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    \n",
    "    hidden1, hidden2 = model.hidden_init(train_batch_size)    \n",
    "    #print('hidden[0].shape:- ',hidden[0].shape)\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        '''\n",
    "        Creating new variables for the hidden state, otherwise\n",
    "        we'd backprop through the entire training history\n",
    "        '''\n",
    "        #h = tuple([each.data for each in hidden])\n",
    "        \n",
    "        h1 = tuple([each.data for each in hidden1])\n",
    "        h2 = tuple([each.data for each in hidden2])\n",
    "        \n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "       \n",
    "        # get the output from the model\n",
    "        output, _ = model.forward(inputs, h1, h2, train_batch_size)\n",
    "        #print('OUTPUT', output)\n",
    "        \n",
    "        \n",
    "        #print('Labels Shape :-', (torch.max(labels, 1)[1]).shape)\n",
    "    \n",
    "        # calculate the loss and perform backprop\n",
    "        #print('Labels Long :-', labels.long())\n",
    "        loss = criterion(output,labels.long())\n",
    "        #print('LOSS IS :-', loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #calculate training accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        logging.debug(' top probab {} top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        #print(train_loss)\n",
    "              \n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "                \n",
    "        val_h1 = tuple([each.data for each in hidden1])\n",
    "        val_h2 = tuple([each.data for each in hidden2])\n",
    "        \n",
    "        output, _ = model.forward(inputs, val_h1, val_h2,val_batch_size)\n",
    "       \n",
    "        loss = criterion(output,labels.long())\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        #calculate validation accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        \n",
    "        #logging.debug(output)\n",
    "        #logging.debug('VALIDATION top probab {} VALIDATION top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        #print('Top Class:- ',top_class)\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        #print('Equals:- ', equals)\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    #Averaging losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    val_accuracy = val_accuracy/len(val_loader)\n",
    "    train_accuracy = train_accuracy/len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}\\tTrain Loss: {:.7f} \\tVal Loss:{:.7f} \\tTrain Acc: {:.7}% \\tVal Acc: {:.7f}%'.format(e, train_loss, val_loss, train_accuracy*100,val_accuracy*100))\n",
    "    \n",
    "    #saving the model if validation loss is decreased\n",
    "    if val_loss <= min_val_loss:\n",
    "        print('Validation Loss decreased from {:6f} to {:6f}, saving the model weights'.format(min_val_loss, val_loss))\n",
    "        torch.save(model.state_dict(), 'lstm_state_256-38-removed_Ht_Ct_sorted.pt')\n",
    "        min_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUSIC GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm1): LSTM(1, 256, batch_first=True)\n",
       "  (lstm2): LSTM(256, 38, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=38, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights\n",
    "test_model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "test_model.load_state_dict(torch.load('lstm_state_256-38-removed_Ht_Ct_sorted.pt'))\n",
    "test_model.eval()\n",
    "test_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population database\n",
    "#testing_data = np.ones(200)*1\n",
    "testing_data = list(range(50,90))\n",
    "testing_data.extend(testing_data[::-1])\n",
    "testing_data_rev = testing_data[::-1]\n",
    "testing_data_rev.extend(testing_data)\n",
    "testing_data = testing_data_rev\n",
    "\n",
    "\n",
    "testing_data = np.asarray(testing_data)\n",
    "testing_data = testing_data.reshape(testing_data.shape[0],1)\n",
    "\n",
    "initial_seq = [network_input[0][1:].cpu().numpy().tolist()]\n",
    "\n",
    "testing_data_unnorm = testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "testing_data=testing_data.tolist()\n",
    "for i in range(len(testing_data)):\n",
    "    list1.extend(testing_data[i])\n",
    "\n",
    "#list1\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    list1[i]=(list1[i]-50)/(89-50)\n",
    "\n",
    "list1 = np.asarray(list1)\n",
    "list1 = list1.reshape(list1.shape[0],1)\n",
    "testing_data = list1\n",
    "#list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "testing_data=testing_data.tolist()\n",
    "for i in range(len(testing_data)):\n",
    "    list1.extend(testing_data[i])\n",
    "\n",
    "#list1\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    list1[i]=(list1[i]-50)/(89-50)\n",
    "\n",
    "list1 = np.asarray(list1)\n",
    "list1 = list1.reshape(list1.shape[0],1)\n",
    "testing_data = list1\n",
    "#list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting\n",
    "def prediction_with_influence(influence,int2note,initial_seq, max_note, test_batch_size = 1):\n",
    "\n",
    "    predicted_notes = []\n",
    "    initial_seq[0].extend([[0]]*len(testing_data))\n",
    "    test_seq = torch.Tensor(initial_seq).cuda()\n",
    "    \n",
    "    h1, h2 = test_model.hidden_init(test_batch_size)\n",
    "\n",
    "    \n",
    "    for i in range(len(influence)):\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = float(influence[i])\n",
    "        \n",
    "        test_slice = test_seq[0][i : i + sequence_length]        \n",
    "        test_slice = test_slice.view(1, test_slice.shape[0], test_slice.shape[1])\n",
    "                \n",
    "        test_hidden1 = tuple([each.data for each in h1])\n",
    "        test_hidden2 = tuple([each.data for each in h2])\n",
    "        \n",
    "        test_output,_ = test_model.forward(test_slice, test_hidden1, test_hidden2, test_batch_size)\n",
    "    \n",
    "        test_output = F.softmax(test_output, dim = 1)\n",
    "        top_p, top_class = test_output.topk(1,dim =1)\n",
    "        test_seq[0][sequence_length - 1 + i][0] = int2note[top_class.item()]/max_note\n",
    "        \n",
    "        predicted_notes.append(int2note[top_class.item()])\n",
    "        \n",
    "    return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_notes_lst = prediction_with_influence(testing_data,int_to_note,initial_seq, max_midi_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_notes_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26c16554e10>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 369
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(list(np.ones(200)*89))\n",
    "\n",
    "#plt.plot(list(np.ones(200)*50))\n",
    "#plt.plot(list(np.ones(20)*50))\n",
    "plt.plot(testing_data_unnorm)\n",
    "plt.plot(predicted_notes_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(predicted_notes_lst)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "network_input=(network_input)\n",
    "for b in range(len(network_input)):\n",
    "    for k in range(len(network_input[b])):\n",
    "        a.extend(network_input[b][k])\n",
    "#a.extend(x for x in network_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import Counter\n",
    "c = Counter(a)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(a*(89-50)+50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=[(i*(89-50)+50) for i in a ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "a=[math.floor(i) for i in a]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(a)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_output=network_output.cpu().numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter(network_output)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
