{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "#import torch.utils.tensorboard as tb\n",
    "from Preprocessing.preprocessing_sorted import PreprocessingTrainingData\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as  plt\n",
    "import os\n",
    "import logging\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static parameters\n",
    "train_batch_size = 30\n",
    "val_batch_size = 30\n",
    "sequence_length=50\n",
    "test_batch_size = 1\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layer = 2\n",
    "output_size = 38\n",
    "clip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from preprocessing.py\n",
    "dataset_path = os.path.join(os.path.abspath('..'),'Dataset\\\\Clementi dataset\\\\Clementi dataset' )\n",
    "network_input,network_output,max_midi_number,min_midi_number,int_to_note = PreprocessingTrainingData().preprocess_notes(dataset_path)\n",
    "network_input, network_output = network_input.cuda(), network_output.cuda()\n",
    "\n",
    "# print(network_input)\n",
    "#print(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bakchodi Normalization\n",
    "network_input=network_input.cpu().numpy().tolist()\n",
    "for i in range(len(network_input)):\n",
    "    for j in range(len(network_input[i])):\n",
    "        network_input[i][j][0]=((network_input[i][j][0])*(max_midi_number-min_midi_number)+min_midi_number)/max_midi_number\n",
    "network_input=torch.Tensor(network_input).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(network_output.max())\n",
    "print(network_output.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1829])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0')\n",
      "tensor(0.5618, device='cuda:0')\n",
      "89\n",
      "50\n",
      "{0: 50, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61, 11: 62, 12: 63, 13: 64, 14: 65, 15: 66, 16: 67, 17: 68, 18: 69, 19: 70, 20: 71, 21: 72, 22: 73, 23: 74, 24: 75, 25: 76, 26: 77, 27: 78, 28: 79, 29: 80, 30: 81, 31: 82, 32: 83, 33: 84, 34: 85, 35: 86, 36: 88, 37: 89}\n"
     ]
    }
   ],
   "source": [
    "print(network_input.max())\n",
    "print(network_input.min())\n",
    "print(max_midi_number)\n",
    "print(min_midi_number)\n",
    "print(int_to_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1829, 50, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata is highly unbalanced\\n# '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data is highly unbalanced\n",
    "# '''\n",
    "# sns.distplot(torch.tensor(network_output).cpu())\n",
    "# xx = pd.DataFrame(torch.tensor(network_output).cpu())\n",
    "# xx.groupby(0).size().to_frame(name='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1800, 50, 1])\n",
      "torch.Size([1800])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "to make batch of equal sizes\n",
    "Quick Fix\n",
    "'''\n",
    "network_input = network_input[: -29]\n",
    "network_output = network_output[: -29]\n",
    "\n",
    "print(network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create Stacked LSTM model\n",
    "'''\n",
    "class Stacked_LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True, dropout = 0.5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden,batch_size):\n",
    "        \n",
    "        output, hidden = self.lstm(x, hidden)        \n",
    "        #print(output.shape)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        output = output.contiguous().view(-1, self.hidden_size)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        #print('Linear Output :-',output.shape)\n",
    "        \n",
    "        #output = F.softmax(output, dim = 1)\n",
    "        #print('SOFTMAX OUTPUT :--', output)\n",
    "        \n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1)\n",
    "        #print('Reshape to batch size first :-',output.shape)\n",
    "        \n",
    "        output = output[:, -self.output_size:] # get last batch of labels\n",
    "        #print('Final Output :-',output)\n",
    "        #print('RESHAPE SIZE :-', output.shape)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def hidden_init(self,batch_size):\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
    "          weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "#initialize the weights of LSTM using Xavier initialization    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Divide the dataset into train/val \n",
    "'''\n",
    "train_size = 0.8\n",
    "indices = list(range(len(network_input)))\n",
    "split = int(np.floor(train_size*len(network_input)))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SequentialSampler(train_idx)\n",
    "val_sampler = SequentialSampler(val_idx)\n",
    "\n",
    "dataset = TensorDataset(network_input,network_output)\n",
    "train_loader = DataLoader(dataset, batch_size= train_batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size= val_batch_size,sampler= val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=256, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optimizer\n",
    "\n",
    "model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer.AdamW(model.parameters())\n",
    "#optimizer = optimizer.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "#make sure to transfer model to GPU after initializing optimizer\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 3.2930876 \tVal Loss:3.0980298 \tTrain Acc: 5.277778% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from    inf to 3.098030, saving the model weights\n",
      "Epoch: 1\tTrain Loss: 3.2798521 \tVal Loss:3.1138991 \tTrain Acc: 6.736111% \tVal Acc: 8.8888892%\n",
      "Epoch: 2\tTrain Loss: 3.2821612 \tVal Loss:3.1210028 \tTrain Acc: 4.513889% \tVal Acc: 8.8888892%\n",
      "Epoch: 3\tTrain Loss: 3.2826431 \tVal Loss:3.1258677 \tTrain Acc: 5.833334% \tVal Acc: 8.8888892%\n",
      "Epoch: 4\tTrain Loss: 3.2856224 \tVal Loss:3.1293614 \tTrain Acc: 6.597222% \tVal Acc: 8.8888892%\n",
      "Epoch: 5\tTrain Loss: 3.2802803 \tVal Loss:3.1325828 \tTrain Acc: 6.041667% \tVal Acc: 8.8888892%\n",
      "Epoch: 6\tTrain Loss: 3.2804060 \tVal Loss:3.1328339 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 7\tTrain Loss: 3.2828621 \tVal Loss:3.1350965 \tTrain Acc: 6.458334% \tVal Acc: 8.8888892%\n",
      "Epoch: 8\tTrain Loss: 3.2827496 \tVal Loss:3.1354113 \tTrain Acc: 6.25% \tVal Acc: 8.8888892%\n",
      "Epoch: 9\tTrain Loss: 3.2867774 \tVal Loss:3.1342693 \tTrain Acc: 5.625% \tVal Acc: 8.8888892%\n",
      "Epoch: 10\tTrain Loss: 3.2804321 \tVal Loss:3.1345645 \tTrain Acc: 6.319445% \tVal Acc: 8.8888892%\n",
      "Epoch: 11\tTrain Loss: 3.2804238 \tVal Loss:3.1352910 \tTrain Acc: 6.458334% \tVal Acc: 8.8888892%\n",
      "Epoch: 12\tTrain Loss: 3.2834965 \tVal Loss:3.1348908 \tTrain Acc: 6.319445% \tVal Acc: 8.8888892%\n",
      "Epoch: 13\tTrain Loss: 3.2870159 \tVal Loss:3.1364722 \tTrain Acc: 5.763889% \tVal Acc: 8.8888892%\n",
      "Epoch: 14\tTrain Loss: 3.2816652 \tVal Loss:3.1376343 \tTrain Acc: 6.458334% \tVal Acc: 8.8888892%\n",
      "Epoch: 15\tTrain Loss: 3.2845441 \tVal Loss:3.1384767 \tTrain Acc: 6.458334% \tVal Acc: 8.8888892%\n",
      "Epoch: 16\tTrain Loss: 3.2830462 \tVal Loss:3.1384491 \tTrain Acc: 6.388889% \tVal Acc: 8.8888892%\n",
      "Epoch: 17\tTrain Loss: 3.2775080 \tVal Loss:3.1364095 \tTrain Acc: 6.25% \tVal Acc: 8.8888892%\n",
      "Epoch: 18\tTrain Loss: 3.2832282 \tVal Loss:3.1352421 \tTrain Acc: 5.694445% \tVal Acc: 8.8888892%\n",
      "Epoch: 19\tTrain Loss: 3.2786828 \tVal Loss:3.1337247 \tTrain Acc: 6.666667% \tVal Acc: 8.8888892%\n",
      "Epoch: 20\tTrain Loss: 3.2822557 \tVal Loss:3.1336061 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 21\tTrain Loss: 3.2824284 \tVal Loss:3.1358561 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 22\tTrain Loss: 3.2776891 \tVal Loss:3.1349867 \tTrain Acc: 6.458334% \tVal Acc: 8.8888892%\n",
      "Epoch: 23\tTrain Loss: 3.2808487 \tVal Loss:3.1352110 \tTrain Acc: 6.388889% \tVal Acc: 8.8888892%\n",
      "Epoch: 24\tTrain Loss: 3.2827650 \tVal Loss:3.1568345 \tTrain Acc: 6.388889% \tVal Acc: 8.0555558%\n",
      "Epoch: 25\tTrain Loss: 3.2801814 \tVal Loss:3.1619602 \tTrain Acc: 6.458334% \tVal Acc: 6.1111113%\n",
      "Epoch: 26\tTrain Loss: 3.2777589 \tVal Loss:3.2200788 \tTrain Acc: 6.458334% \tVal Acc: 8.0555558%\n",
      "Epoch: 27\tTrain Loss: 3.2784010 \tVal Loss:3.1348730 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 28\tTrain Loss: 3.2788720 \tVal Loss:3.1469781 \tTrain Acc: 6.319445% \tVal Acc: 8.0555558%\n",
      "Epoch: 29\tTrain Loss: 3.2795051 \tVal Loss:3.1369531 \tTrain Acc: 6.388889% \tVal Acc: 8.8888892%\n",
      "Epoch: 30\tTrain Loss: 3.2839290 \tVal Loss:3.1386411 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 31\tTrain Loss: 3.2808627 \tVal Loss:3.1373911 \tTrain Acc: 6.388889% \tVal Acc: 8.8888892%\n",
      "Epoch: 32\tTrain Loss: 3.2770314 \tVal Loss:3.1380820 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 33\tTrain Loss: 3.2822000 \tVal Loss:3.2780855 \tTrain Acc: 6.597222% \tVal Acc: 6.6666669%\n",
      "Epoch: 34\tTrain Loss: 3.2784405 \tVal Loss:3.1911337 \tTrain Acc: 6.25% \tVal Acc: 8.3333336%\n",
      "Epoch: 35\tTrain Loss: 3.2802652 \tVal Loss:3.1695634 \tTrain Acc: 6.736111% \tVal Acc: 8.3333336%\n",
      "Epoch: 36\tTrain Loss: 3.2754170 \tVal Loss:3.2011177 \tTrain Acc: 6.458334% \tVal Acc: 6.1111113%\n",
      "Epoch: 37\tTrain Loss: 3.2762964 \tVal Loss:3.1841725 \tTrain Acc: 6.388889% \tVal Acc: 8.6111113%\n",
      "Epoch: 38\tTrain Loss: 3.2788223 \tVal Loss:3.1774870 \tTrain Acc: 6.180556% \tVal Acc: 8.6111114%\n",
      "Epoch: 39\tTrain Loss: 3.2798173 \tVal Loss:3.1398104 \tTrain Acc: 6.597222% \tVal Acc: 8.0555558%\n",
      "Epoch: 40\tTrain Loss: 3.2780699 \tVal Loss:3.1340267 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 41\tTrain Loss: 3.2825395 \tVal Loss:3.1361386 \tTrain Acc: 6.458334% \tVal Acc: 8.8888892%\n",
      "Epoch: 42\tTrain Loss: 3.2780024 \tVal Loss:3.2313716 \tTrain Acc: 6.458334% \tVal Acc: 7.2222225%\n",
      "Epoch: 43\tTrain Loss: 3.2758996 \tVal Loss:3.2348588 \tTrain Acc: 6.25% \tVal Acc: 7.7777781%\n",
      "Epoch: 44\tTrain Loss: 3.2781793 \tVal Loss:3.3066469 \tTrain Acc: 6.388889% \tVal Acc: 5.5555558%\n",
      "Epoch: 45\tTrain Loss: 3.2751223 \tVal Loss:3.1356636 \tTrain Acc: 6.458334% \tVal Acc: 8.8888892%\n",
      "Epoch: 46\tTrain Loss: 3.2775853 \tVal Loss:3.1829958 \tTrain Acc: 6.319445% \tVal Acc: 8.6111113%\n",
      "Epoch: 47\tTrain Loss: 3.2785848 \tVal Loss:3.1351848 \tTrain Acc: 6.597222% \tVal Acc: 8.8888892%\n",
      "Epoch: 48\tTrain Loss: 3.2776231 \tVal Loss:3.2377273 \tTrain Acc: 6.458334% \tVal Acc: 7.7777780%\n",
      "Epoch: 49\tTrain Loss: 3.2769842 \tVal Loss:3.1728919 \tTrain Acc: 6.319445% \tVal Acc: 7.5000002%\n",
      "Epoch: 50\tTrain Loss: 3.2761725 \tVal Loss:3.1847738 \tTrain Acc: 6.736111% \tVal Acc: 7.7777780%\n",
      "Epoch: 51\tTrain Loss: 3.2757571 \tVal Loss:3.1671047 \tTrain Acc: 6.319445% \tVal Acc: 7.5000002%\n",
      "Epoch: 52\tTrain Loss: 3.2768643 \tVal Loss:3.1341788 \tTrain Acc: 6.319445% \tVal Acc: 8.8888892%\n",
      "Epoch: 53\tTrain Loss: 3.2801482 \tVal Loss:3.2460408 \tTrain Acc: 6.527778% \tVal Acc: 7.5000002%\n",
      "Epoch: 54\tTrain Loss: 3.2765852 \tVal Loss:3.2124964 \tTrain Acc: 6.458334% \tVal Acc: 8.0555558%\n",
      "Epoch: 55\tTrain Loss: 3.2771508 \tVal Loss:3.1528515 \tTrain Acc: 6.319445% \tVal Acc: 8.0555558%\n",
      "Epoch: 56\tTrain Loss: 3.2727096 \tVal Loss:3.1488996 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 57\tTrain Loss: 3.2784558 \tVal Loss:3.2323247 \tTrain Acc: 6.527778% \tVal Acc: 7.2222225%\n",
      "Epoch: 58\tTrain Loss: 3.2769659 \tVal Loss:3.2298818 \tTrain Acc: 6.597222% \tVal Acc: 6.6666669%\n",
      "Epoch: 59\tTrain Loss: 3.2764998 \tVal Loss:3.2745785 \tTrain Acc: 6.458334% \tVal Acc: 7.5000002%\n",
      "Epoch: 60\tTrain Loss: 3.2741731 \tVal Loss:3.1755664 \tTrain Acc: 6.597222% \tVal Acc: 7.7777780%\n",
      "Epoch: 61\tTrain Loss: 3.2781696 \tVal Loss:3.2062140 \tTrain Acc: 6.458334% \tVal Acc: 8.0555558%\n",
      "Epoch: 62\tTrain Loss: 3.2745679 \tVal Loss:3.2296029 \tTrain Acc: 6.527778% \tVal Acc: 7.7777780%\n",
      "Epoch: 63\tTrain Loss: 3.2755342 \tVal Loss:3.1470936 \tTrain Acc: 6.458334% \tVal Acc: 8.0555558%\n",
      "Epoch: 64\tTrain Loss: 3.2768525 \tVal Loss:3.2346210 \tTrain Acc: 6.388889% \tVal Acc: 5.8333336%\n",
      "Epoch: 65\tTrain Loss: 3.2770620 \tVal Loss:3.1443734 \tTrain Acc: 6.319445% \tVal Acc: 9.1666670%\n",
      "Epoch: 66\tTrain Loss: 3.2741385 \tVal Loss:3.1347595 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 67\tTrain Loss: 3.2743810 \tVal Loss:3.1831474 \tTrain Acc: 6.597222% \tVal Acc: 8.3333336%\n",
      "Epoch: 68\tTrain Loss: 3.2763633 \tVal Loss:3.1331657 \tTrain Acc: 6.458334% \tVal Acc: 8.8888892%\n",
      "Epoch: 69\tTrain Loss: 3.2778704 \tVal Loss:3.1691863 \tTrain Acc: 6.527778% \tVal Acc: 8.6111114%\n",
      "Epoch: 70\tTrain Loss: 3.2762460 \tVal Loss:3.1323934 \tTrain Acc: 6.25% \tVal Acc: 8.8888892%\n",
      "Epoch: 71\tTrain Loss: 3.2775089 \tVal Loss:3.1405761 \tTrain Acc: 6.388889% \tVal Acc: 8.8888892%\n",
      "Epoch: 72\tTrain Loss: 3.2738855 \tVal Loss:3.1949676 \tTrain Acc: 6.666667% \tVal Acc: 8.3333336%\n",
      "Epoch: 73\tTrain Loss: 3.2726989 \tVal Loss:3.1319277 \tTrain Acc: 6.458334% \tVal Acc: 8.8888892%\n",
      "Epoch: 74\tTrain Loss: 3.2742591 \tVal Loss:3.1691002 \tTrain Acc: 6.597222% \tVal Acc: 8.3333336%\n",
      "Epoch: 75\tTrain Loss: 3.2716885 \tVal Loss:3.1889598 \tTrain Acc: 6.527778% \tVal Acc: 8.0555558%\n",
      "Epoch: 76\tTrain Loss: 3.2771553 \tVal Loss:3.2181967 \tTrain Acc: 6.25% \tVal Acc: 6.9444447%\n",
      "Epoch: 77\tTrain Loss: 3.2756352 \tVal Loss:3.2276853 \tTrain Acc: 6.319445% \tVal Acc: 6.9444447%\n",
      "Epoch: 78\tTrain Loss: 3.2822786 \tVal Loss:3.1335621 \tTrain Acc: 6.388889% \tVal Acc: 8.8888892%\n",
      "Epoch: 79\tTrain Loss: 3.2717459 \tVal Loss:3.1344683 \tTrain Acc: 6.597222% \tVal Acc: 8.8888892%\n",
      "Epoch: 80\tTrain Loss: 3.2756781 \tVal Loss:3.1331433 \tTrain Acc: 6.388889% \tVal Acc: 8.8888892%\n",
      "Epoch: 81\tTrain Loss: 3.2737543 \tVal Loss:3.2124760 \tTrain Acc: 6.736111% \tVal Acc: 7.5000002%\n",
      "Epoch: 82\tTrain Loss: 3.2715585 \tVal Loss:3.2025672 \tTrain Acc: 6.180556% \tVal Acc: 7.7777780%\n",
      "Epoch: 83\tTrain Loss: 3.2731854 \tVal Loss:3.2299573 \tTrain Acc: 6.458334% \tVal Acc: 7.5000002%\n",
      "Epoch: 84\tTrain Loss: 3.2733072 \tVal Loss:3.1380428 \tTrain Acc: 6.666667% \tVal Acc: 8.6111114%\n",
      "Epoch: 85\tTrain Loss: 3.2723451 \tVal Loss:3.1675367 \tTrain Acc: 6.319445% \tVal Acc: 7.2222225%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86\tTrain Loss: 3.2767020 \tVal Loss:3.1345695 \tTrain Acc: 6.597222% \tVal Acc: 8.6111114%\n",
      "Epoch: 87\tTrain Loss: 3.2685553 \tVal Loss:3.1290601 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 88\tTrain Loss: 3.2705478 \tVal Loss:3.1293744 \tTrain Acc: 6.458334% \tVal Acc: 8.8888892%\n",
      "Epoch: 89\tTrain Loss: 3.2723439 \tVal Loss:3.1324963 \tTrain Acc: 6.388889% \tVal Acc: 8.8888892%\n",
      "Epoch: 90\tTrain Loss: 3.2732308 \tVal Loss:3.1290101 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 91\tTrain Loss: 3.2702609 \tVal Loss:3.1279799 \tTrain Acc: 6.388889% \tVal Acc: 8.8888892%\n",
      "Epoch: 92\tTrain Loss: 3.2714054 \tVal Loss:3.1676039 \tTrain Acc: 6.527778% \tVal Acc: 8.0555558%\n",
      "Epoch: 93\tTrain Loss: 3.2729525 \tVal Loss:3.1414162 \tTrain Acc: 6.527778% \tVal Acc: 8.3333336%\n",
      "Epoch: 94\tTrain Loss: 3.2756876 \tVal Loss:3.1345781 \tTrain Acc: 6.319445% \tVal Acc: 8.8888892%\n",
      "Epoch: 95\tTrain Loss: 3.2758311 \tVal Loss:3.1322343 \tTrain Acc: 6.319445% \tVal Acc: 8.8888892%\n",
      "Epoch: 96\tTrain Loss: 3.2725111 \tVal Loss:3.1300187 \tTrain Acc: 6.388889% \tVal Acc: 8.8888892%\n",
      "Epoch: 97\tTrain Loss: 3.2708257 \tVal Loss:3.1292728 \tTrain Acc: 6.319445% \tVal Acc: 8.8888892%\n",
      "Epoch: 98\tTrain Loss: 3.2683587 \tVal Loss:3.1260527 \tTrain Acc: 6.319445% \tVal Acc: 8.8888892%\n",
      "Epoch: 99\tTrain Loss: 3.2704554 \tVal Loss:3.1603965 \tTrain Acc: 6.736111% \tVal Acc: 9.7222225%\n",
      "Epoch: 100\tTrain Loss: 3.2731489 \tVal Loss:3.1979852 \tTrain Acc: 6.666667% \tVal Acc: 6.9444447%\n",
      "Epoch: 101\tTrain Loss: 3.2943239 \tVal Loss:3.1366961 \tTrain Acc: 6.041667% \tVal Acc: 8.8888892%\n",
      "Epoch: 102\tTrain Loss: 3.2756851 \tVal Loss:3.1364741 \tTrain Acc: 6.666667% \tVal Acc: 8.8888892%\n",
      "Epoch: 103\tTrain Loss: 3.2755252 \tVal Loss:3.1344508 \tTrain Acc: 6.25% \tVal Acc: 8.8888892%\n",
      "Epoch: 104\tTrain Loss: 3.2747633 \tVal Loss:3.1321010 \tTrain Acc: 6.319445% \tVal Acc: 8.8888892%\n",
      "Epoch: 105\tTrain Loss: 3.2738926 \tVal Loss:3.1307574 \tTrain Acc: 6.458334% \tVal Acc: 8.8888892%\n",
      "Epoch: 106\tTrain Loss: 3.2715733 \tVal Loss:3.1308115 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 107\tTrain Loss: 3.2692329 \tVal Loss:3.1269161 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 108\tTrain Loss: 3.2656963 \tVal Loss:3.1179484 \tTrain Acc: 6.388889% \tVal Acc: 8.8888892%\n",
      "Epoch: 109\tTrain Loss: 3.2783960 \tVal Loss:3.1286416 \tTrain Acc: 5.069445% \tVal Acc: 8.8888892%\n",
      "Epoch: 110\tTrain Loss: 3.2681400 \tVal Loss:3.1272709 \tTrain Acc: 6.180556% \tVal Acc: 8.8888892%\n",
      "Epoch: 111\tTrain Loss: 3.2664560 \tVal Loss:3.1242591 \tTrain Acc: 6.388889% \tVal Acc: 8.8888892%\n",
      "Epoch: 112\tTrain Loss: 3.2731266 \tVal Loss:3.1551458 \tTrain Acc: 5.972222% \tVal Acc: 8.8888892%\n",
      "Epoch: 113\tTrain Loss: 3.2729477 \tVal Loss:3.1282691 \tTrain Acc: 6.319445% \tVal Acc: 8.8888892%\n",
      "Epoch: 114\tTrain Loss: 3.2658393 \tVal Loss:3.1208569 \tTrain Acc: 6.388889% \tVal Acc: 8.8888892%\n",
      "Epoch: 115\tTrain Loss: 3.2557012 \tVal Loss:3.0949565 \tTrain Acc: 6.319445% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.098030 to 3.094957, saving the model weights\n",
      "Epoch: 116\tTrain Loss: 3.2485952 \tVal Loss:3.0789217 \tTrain Acc: 6.319445% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.094957 to 3.078922, saving the model weights\n",
      "Epoch: 117\tTrain Loss: 3.2470504 \tVal Loss:3.0878266 \tTrain Acc: 4.861111% \tVal Acc: 5.5555558%\n",
      "Epoch: 118\tTrain Loss: 3.2604923 \tVal Loss:3.0966369 \tTrain Acc: 4.930556% \tVal Acc: 8.8888892%\n",
      "Epoch: 119\tTrain Loss: 3.2749673 \tVal Loss:3.1505610 \tTrain Acc: 5.138889% \tVal Acc: 8.8888892%\n",
      "Epoch: 120\tTrain Loss: 3.2674117 \tVal Loss:3.1311538 \tTrain Acc: 6.319445% \tVal Acc: 8.8888892%\n",
      "Epoch: 121\tTrain Loss: 3.2521547 \tVal Loss:3.1074260 \tTrain Acc: 6.527778% \tVal Acc: 8.8888892%\n",
      "Epoch: 122\tTrain Loss: 3.2234027 \tVal Loss:3.1034766 \tTrain Acc: 7.777778% \tVal Acc: 13.3333338%\n",
      "Epoch: 123\tTrain Loss: 3.2255916 \tVal Loss:3.1855385 \tTrain Acc: 7.986111% \tVal Acc: 6.9444447%\n",
      "Epoch: 124\tTrain Loss: 3.2336946 \tVal Loss:3.1161470 \tTrain Acc: 6.944445% \tVal Acc: 6.9444447%\n",
      "Epoch: 125\tTrain Loss: 3.2427927 \tVal Loss:3.1274556 \tTrain Acc: 3.958334% \tVal Acc: 8.8888892%\n",
      "Epoch: 126\tTrain Loss: 3.2012558 \tVal Loss:3.2196576 \tTrain Acc: 6.388889% \tVal Acc: 1.1111112%\n",
      "Epoch: 127\tTrain Loss: 3.2715904 \tVal Loss:3.1037399 \tTrain Acc: 4.444445% \tVal Acc: 9.4444448%\n",
      "Epoch: 128\tTrain Loss: 3.2155684 \tVal Loss:3.0109288 \tTrain Acc: 5.902778% \tVal Acc: 14.7222226%\n",
      "Validation Loss decreased from 3.078922 to 3.010929, saving the model weights\n",
      "Epoch: 129\tTrain Loss: 3.1287505 \tVal Loss:2.9082496 \tTrain Acc: 7.152778% \tVal Acc: 15.2777783%\n",
      "Validation Loss decreased from 3.010929 to 2.908250, saving the model weights\n",
      "Epoch: 130\tTrain Loss: 3.0617240 \tVal Loss:2.8519930 \tTrain Acc: 7.083334% \tVal Acc: 18.6111116%\n",
      "Validation Loss decreased from 2.908250 to 2.851993, saving the model weights\n",
      "Epoch: 131\tTrain Loss: 3.0063668 \tVal Loss:2.7904362 \tTrain Acc: 8.888889% \tVal Acc: 16.9444450%\n",
      "Validation Loss decreased from 2.851993 to 2.790436, saving the model weights\n",
      "Epoch: 132\tTrain Loss: 2.9857931 \tVal Loss:2.8005190 \tTrain Acc: 8.958334% \tVal Acc: 16.6666672%\n",
      "Epoch: 133\tTrain Loss: 2.9464746 \tVal Loss:2.7720953 \tTrain Acc: 9.861111% \tVal Acc: 14.7222227%\n",
      "Validation Loss decreased from 2.790436 to 2.772095, saving the model weights\n",
      "Epoch: 134\tTrain Loss: 2.9234215 \tVal Loss:2.7176120 \tTrain Acc: 9.513889% \tVal Acc: 18.8888894%\n",
      "Validation Loss decreased from 2.772095 to 2.717612, saving the model weights\n",
      "Epoch: 135\tTrain Loss: 2.9035847 \tVal Loss:2.7157144 \tTrain Acc: 9.861111% \tVal Acc: 14.7222227%\n",
      "Validation Loss decreased from 2.717612 to 2.715714, saving the model weights\n",
      "Epoch: 136\tTrain Loss: 2.8539848 \tVal Loss:2.6802282 \tTrain Acc: 10.34722% \tVal Acc: 12.5000005%\n",
      "Validation Loss decreased from 2.715714 to 2.680228, saving the model weights\n",
      "Epoch: 137\tTrain Loss: 2.8464548 \tVal Loss:2.6620409 \tTrain Acc: 10.90278% \tVal Acc: 13.3333338%\n",
      "Validation Loss decreased from 2.680228 to 2.662041, saving the model weights\n",
      "Epoch: 138\tTrain Loss: 2.8225428 \tVal Loss:2.6309437 \tTrain Acc: 11.94444% \tVal Acc: 12.7777783%\n",
      "Validation Loss decreased from 2.662041 to 2.630944, saving the model weights\n",
      "Epoch: 139\tTrain Loss: 2.7936936 \tVal Loss:2.6252476 \tTrain Acc: 11.52778% \tVal Acc: 13.6111116%\n",
      "Validation Loss decreased from 2.630944 to 2.625248, saving the model weights\n",
      "Epoch: 140\tTrain Loss: 2.8071142 \tVal Loss:2.6171271 \tTrain Acc: 10.0% \tVal Acc: 11.1111115%\n",
      "Validation Loss decreased from 2.625248 to 2.617127, saving the model weights\n",
      "Epoch: 141\tTrain Loss: 2.7858656 \tVal Loss:2.6472600 \tTrain Acc: 12.70833% \tVal Acc: 11.9444448%\n",
      "Epoch: 142\tTrain Loss: 2.7630325 \tVal Loss:2.6675250 \tTrain Acc: 12.98611% \tVal Acc: 11.9444447%\n",
      "Epoch: 143\tTrain Loss: 2.7481333 \tVal Loss:2.9083341 \tTrain Acc: 14.09722% \tVal Acc: 9.7222226%\n",
      "Epoch: 144\tTrain Loss: 2.7833011 \tVal Loss:2.7767607 \tTrain Acc: 12.56944% \tVal Acc: 10.8333337%\n",
      "Epoch: 145\tTrain Loss: 2.7740778 \tVal Loss:2.7125354 \tTrain Acc: 13.05556% \tVal Acc: 8.6111114%\n",
      "Epoch: 146\tTrain Loss: 2.7497108 \tVal Loss:2.6367717 \tTrain Acc: 13.95833% \tVal Acc: 11.9444448%\n",
      "Epoch: 147\tTrain Loss: 2.7118306 \tVal Loss:2.6064716 \tTrain Acc: 15.625% \tVal Acc: 11.9444447%\n",
      "Validation Loss decreased from 2.617127 to 2.606472, saving the model weights\n",
      "Epoch: 148\tTrain Loss: 2.6877723 \tVal Loss:2.7386919 \tTrain Acc: 16.31944% \tVal Acc: 11.3888891%\n",
      "Epoch: 149\tTrain Loss: 2.6885054 \tVal Loss:2.7079468 \tTrain Acc: 16.38889% \tVal Acc: 12.5000003%\n",
      "Epoch: 150\tTrain Loss: 2.6766863 \tVal Loss:2.6566355 \tTrain Acc: 15.90278% \tVal Acc: 14.1666671%\n",
      "Epoch: 151\tTrain Loss: 2.6720872 \tVal Loss:2.4663747 \tTrain Acc: 14.23611% \tVal Acc: 17.5000005%\n",
      "Validation Loss decreased from 2.606472 to 2.466375, saving the model weights\n",
      "Epoch: 152\tTrain Loss: 2.6318957 \tVal Loss:2.4987238 \tTrain Acc: 16.80556% \tVal Acc: 19.4444449%\n",
      "Epoch: 153\tTrain Loss: 2.6267025 \tVal Loss:2.4232737 \tTrain Acc: 17.63889% \tVal Acc: 21.1111117%\n",
      "Validation Loss decreased from 2.466375 to 2.423274, saving the model weights\n",
      "Epoch: 154\tTrain Loss: 2.5890430 \tVal Loss:2.6337116 \tTrain Acc: 18.88889% \tVal Acc: 13.3333337%\n",
      "Epoch: 155\tTrain Loss: 2.5978992 \tVal Loss:2.4345871 \tTrain Acc: 18.81944% \tVal Acc: 18.8888894%\n",
      "Epoch: 156\tTrain Loss: 2.6107532 \tVal Loss:2.3893060 \tTrain Acc: 19.23611% \tVal Acc: 20.2777783%\n",
      "Validation Loss decreased from 2.423274 to 2.389306, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 157\tTrain Loss: 2.6098593 \tVal Loss:2.4418636 \tTrain Acc: 17.98611% \tVal Acc: 18.6111115%\n",
      "Epoch: 158\tTrain Loss: 2.6176571 \tVal Loss:2.6908617 \tTrain Acc: 19.09722% \tVal Acc: 18.3333339%\n",
      "Epoch: 159\tTrain Loss: 2.6837147 \tVal Loss:2.6226920 \tTrain Acc: 16.52778% \tVal Acc: 15.0000005%\n",
      "Epoch: 160\tTrain Loss: 2.6019621 \tVal Loss:2.5154458 \tTrain Acc: 19.30556% \tVal Acc: 19.7222231%\n",
      "Epoch: 161\tTrain Loss: 2.5914048 \tVal Loss:2.4032666 \tTrain Acc: 18.88889% \tVal Acc: 25.0000009%\n",
      "Epoch: 162\tTrain Loss: 2.5829833 \tVal Loss:2.3809976 \tTrain Acc: 21.94445% \tVal Acc: 28.8888895%\n",
      "Validation Loss decreased from 2.389306 to 2.380998, saving the model weights\n",
      "Epoch: 163\tTrain Loss: 2.5069104 \tVal Loss:2.5237178 \tTrain Acc: 23.54167% \tVal Acc: 23.3333337%\n",
      "Epoch: 164\tTrain Loss: 2.5640986 \tVal Loss:2.2572824 \tTrain Acc: 21.59722% \tVal Acc: 32.5000008%\n",
      "Validation Loss decreased from 2.380998 to 2.257282, saving the model weights\n",
      "Epoch: 165\tTrain Loss: 2.3988116 \tVal Loss:2.1820276 \tTrain Acc: 26.73611% \tVal Acc: 33.6111117%\n",
      "Validation Loss decreased from 2.257282 to 2.182028, saving the model weights\n",
      "Epoch: 166\tTrain Loss: 2.3985193 \tVal Loss:2.0718866 \tTrain Acc: 26.59722% \tVal Acc: 39.4444453%\n",
      "Validation Loss decreased from 2.182028 to 2.071887, saving the model weights\n",
      "Epoch: 167\tTrain Loss: 2.3263771 \tVal Loss:2.0551069 \tTrain Acc: 28.19445% \tVal Acc: 36.6666672%\n",
      "Validation Loss decreased from 2.071887 to 2.055107, saving the model weights\n",
      "Epoch: 168\tTrain Loss: 2.3060837 \tVal Loss:2.3323264 \tTrain Acc: 27.91667% \tVal Acc: 25.2777784%\n",
      "Epoch: 169\tTrain Loss: 2.3887899 \tVal Loss:2.2495075 \tTrain Acc: 25.76389% \tVal Acc: 30.2777786%\n",
      "Epoch: 170\tTrain Loss: 2.3386705 \tVal Loss:1.9814535 \tTrain Acc: 28.33333% \tVal Acc: 37.5000005%\n",
      "Validation Loss decreased from 2.055107 to 1.981454, saving the model weights\n",
      "Epoch: 171\tTrain Loss: 2.2263633 \tVal Loss:1.9774734 \tTrain Acc: 29.58333% \tVal Acc: 38.8888896%\n",
      "Validation Loss decreased from 1.981454 to 1.977473, saving the model weights\n",
      "Epoch: 172\tTrain Loss: 2.2966602 \tVal Loss:2.1124911 \tTrain Acc: 27.15278% \tVal Acc: 33.3333342%\n",
      "Epoch: 173\tTrain Loss: 2.2124319 \tVal Loss:1.9520903 \tTrain Acc: 31.66667% \tVal Acc: 38.3333342%\n",
      "Validation Loss decreased from 1.977473 to 1.952090, saving the model weights\n",
      "Epoch: 174\tTrain Loss: 2.1999819 \tVal Loss:2.0191917 \tTrain Acc: 30.76389% \tVal Acc: 37.7777784%\n",
      "Epoch: 175\tTrain Loss: 2.2170085 \tVal Loss:1.8955229 \tTrain Acc: 30.20833% \tVal Acc: 40.8333340%\n",
      "Validation Loss decreased from 1.952090 to 1.895523, saving the model weights\n",
      "Epoch: 176\tTrain Loss: 2.2881018 \tVal Loss:2.0641490 \tTrain Acc: 27.98611% \tVal Acc: 30.8333338%\n",
      "Epoch: 177\tTrain Loss: 2.1479790 \tVal Loss:2.0423836 \tTrain Acc: 30.83333% \tVal Acc: 35.0000009%\n",
      "Epoch: 178\tTrain Loss: 2.1041638 \tVal Loss:1.8653212 \tTrain Acc: 32.70833% \tVal Acc: 44.7222234%\n",
      "Validation Loss decreased from 1.895523 to 1.865321, saving the model weights\n",
      "Epoch: 179\tTrain Loss: 1.9865315 \tVal Loss:1.8380495 \tTrain Acc: 37.63889% \tVal Acc: 40.8333339%\n",
      "Validation Loss decreased from 1.865321 to 1.838050, saving the model weights\n",
      "Epoch: 180\tTrain Loss: 1.9225373 \tVal Loss:2.1430758 \tTrain Acc: 39.72222% \tVal Acc: 28.6111115%\n",
      "Epoch: 181\tTrain Loss: 1.8965618 \tVal Loss:1.9379091 \tTrain Acc: 39.09722% \tVal Acc: 36.1111115%\n",
      "Epoch: 182\tTrain Loss: 1.9636920 \tVal Loss:2.0885746 \tTrain Acc: 37.84722% \tVal Acc: 33.3333338%\n",
      "Epoch: 183\tTrain Loss: 1.9094422 \tVal Loss:1.7893359 \tTrain Acc: 40.83333% \tVal Acc: 44.1666676%\n",
      "Validation Loss decreased from 1.838050 to 1.789336, saving the model weights\n",
      "Epoch: 184\tTrain Loss: 1.8878015 \tVal Loss:2.3986122 \tTrain Acc: 41.04167% \tVal Acc: 29.7222232%\n",
      "Epoch: 185\tTrain Loss: 2.0401171 \tVal Loss:1.7965096 \tTrain Acc: 36.18056% \tVal Acc: 44.7222224%\n",
      "Epoch: 186\tTrain Loss: 1.9384811 \tVal Loss:1.9695662 \tTrain Acc: 38.26389% \tVal Acc: 34.1666675%\n",
      "Epoch: 187\tTrain Loss: 1.7943900 \tVal Loss:1.6095166 \tTrain Acc: 44.44445% \tVal Acc: 51.3888898%\n",
      "Validation Loss decreased from 1.789336 to 1.609517, saving the model weights\n",
      "Epoch: 188\tTrain Loss: 1.7518751 \tVal Loss:1.6493694 \tTrain Acc: 42.63889% \tVal Acc: 45.5555566%\n",
      "Epoch: 189\tTrain Loss: 1.8250737 \tVal Loss:1.4729932 \tTrain Acc: 42.84722% \tVal Acc: 55.8333342%\n",
      "Validation Loss decreased from 1.609517 to 1.472993, saving the model weights\n",
      "Epoch: 190\tTrain Loss: 1.8538049 \tVal Loss:1.2351166 \tTrain Acc: 43.95833% \tVal Acc: 62.7777783%\n",
      "Validation Loss decreased from 1.472993 to 1.235117, saving the model weights\n",
      "Epoch: 191\tTrain Loss: 1.7784072 \tVal Loss:1.1544697 \tTrain Acc: 44.79167% \tVal Acc: 62.4999999%\n",
      "Validation Loss decreased from 1.235117 to 1.154470, saving the model weights\n",
      "Epoch: 192\tTrain Loss: 1.6757125 \tVal Loss:1.1270177 \tTrain Acc: 48.19445% \tVal Acc: 69.7222225%\n",
      "Validation Loss decreased from 1.154470 to 1.127018, saving the model weights\n",
      "Epoch: 193\tTrain Loss: 1.6854359 \tVal Loss:1.0540810 \tTrain Acc: 48.19445% \tVal Acc: 69.4444440%\n",
      "Validation Loss decreased from 1.127018 to 1.054081, saving the model weights\n",
      "Epoch: 194\tTrain Loss: 1.6496544 \tVal Loss:1.0334043 \tTrain Acc: 48.47222% \tVal Acc: 72.2222224%\n",
      "Validation Loss decreased from 1.054081 to 1.033404, saving the model weights\n",
      "Epoch: 195\tTrain Loss: 1.5387883 \tVal Loss:0.9835887 \tTrain Acc: 52.36111% \tVal Acc: 72.7777777%\n",
      "Validation Loss decreased from 1.033404 to 0.983589, saving the model weights\n",
      "Epoch: 196\tTrain Loss: 1.5196919 \tVal Loss:1.0102400 \tTrain Acc: 50.83333% \tVal Acc: 68.6111105%\n",
      "Epoch: 197\tTrain Loss: 1.5142832 \tVal Loss:1.0520907 \tTrain Acc: 51.45833% \tVal Acc: 68.6111115%\n",
      "Epoch: 198\tTrain Loss: 1.5165528 \tVal Loss:1.1685109 \tTrain Acc: 53.88889% \tVal Acc: 66.1111116%\n",
      "Epoch: 199\tTrain Loss: 1.3864551 \tVal Loss:1.0966567 \tTrain Acc: 55.90278% \tVal Acc: 70.5555556%\n",
      "Epoch: 200\tTrain Loss: 1.2971076 \tVal Loss:1.1809560 \tTrain Acc: 58.68056% \tVal Acc: 65.5555554%\n",
      "Epoch: 201\tTrain Loss: 1.2742880 \tVal Loss:1.3281634 \tTrain Acc: 59.23611% \tVal Acc: 62.2222225%\n",
      "Epoch: 202\tTrain Loss: 1.1911973 \tVal Loss:0.9567239 \tTrain Acc: 64.09722% \tVal Acc: 75.0000002%\n",
      "Validation Loss decreased from 0.983589 to 0.956724, saving the model weights\n",
      "Epoch: 203\tTrain Loss: 1.1964690 \tVal Loss:0.8412073 \tTrain Acc: 60.41667% \tVal Acc: 78.3333326%\n",
      "Validation Loss decreased from 0.956724 to 0.841207, saving the model weights\n",
      "Epoch: 204\tTrain Loss: 1.2285018 \tVal Loss:1.0913758 \tTrain Acc: 62.29167% \tVal Acc: 68.8888893%\n",
      "Epoch: 205\tTrain Loss: 1.2555340 \tVal Loss:0.6765027 \tTrain Acc: 60.76389% \tVal Acc: 83.0555553%\n",
      "Validation Loss decreased from 0.841207 to 0.676503, saving the model weights\n",
      "Epoch: 206\tTrain Loss: 1.4520636 \tVal Loss:0.8410371 \tTrain Acc: 56.31944% \tVal Acc: 76.3888886%\n",
      "Epoch: 207\tTrain Loss: 1.1978919 \tVal Loss:0.7599714 \tTrain Acc: 61.66667% \tVal Acc: 79.1666664%\n",
      "Epoch: 208\tTrain Loss: 1.1040626 \tVal Loss:0.6719552 \tTrain Acc: 65.13889% \tVal Acc: 83.0555553%\n",
      "Validation Loss decreased from 0.676503 to 0.671955, saving the model weights\n",
      "Epoch: 209\tTrain Loss: 1.1424057 \tVal Loss:1.5189139 \tTrain Acc: 63.33333% \tVal Acc: 58.8888900%\n",
      "Epoch: 210\tTrain Loss: 1.3697395 \tVal Loss:1.0294659 \tTrain Acc: 58.88889% \tVal Acc: 71.9444441%\n",
      "Epoch: 211\tTrain Loss: 1.2307409 \tVal Loss:0.9032497 \tTrain Acc: 62.56944% \tVal Acc: 77.2222216%\n",
      "Epoch: 212\tTrain Loss: 1.0773066 \tVal Loss:1.0226288 \tTrain Acc: 67.36111% \tVal Acc: 71.6666664%\n",
      "Epoch: 213\tTrain Loss: 1.0385403 \tVal Loss:1.5975079 \tTrain Acc: 68.33333% \tVal Acc: 57.4999999%\n",
      "Epoch: 214\tTrain Loss: 2.2259420 \tVal Loss:1.3944810 \tTrain Acc: 42.01389% \tVal Acc: 60.8333341%\n",
      "Epoch: 215\tTrain Loss: 1.2742673 \tVal Loss:1.2917885 \tTrain Acc: 61.38889% \tVal Acc: 64.4444444%\n",
      "Epoch: 216\tTrain Loss: 1.0848229 \tVal Loss:0.8293875 \tTrain Acc: 66.18056% \tVal Acc: 76.3888888%\n",
      "Epoch: 217\tTrain Loss: 1.0084290 \tVal Loss:0.8737178 \tTrain Acc: 68.88889% \tVal Acc: 76.9444443%\n",
      "Epoch: 218\tTrain Loss: 1.0741777 \tVal Loss:0.4843331 \tTrain Acc: 66.73611% \tVal Acc: 86.9444445%\n",
      "Validation Loss decreased from 0.671955 to 0.484333, saving the model weights\n",
      "Epoch: 219\tTrain Loss: 0.9356471 \tVal Loss:0.4708752 \tTrain Acc: 70.55556% \tVal Acc: 86.1111114%\n",
      "Validation Loss decreased from 0.484333 to 0.470875, saving the model weights\n",
      "Epoch: 220\tTrain Loss: 0.9578168 \tVal Loss:0.5034406 \tTrain Acc: 70.625% \tVal Acc: 86.1111104%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 221\tTrain Loss: 1.1187564 \tVal Loss:0.6016716 \tTrain Acc: 65.34722% \tVal Acc: 82.7777781%\n",
      "Epoch: 222\tTrain Loss: 1.1452157 \tVal Loss:0.4882752 \tTrain Acc: 65.83333% \tVal Acc: 85.5555544%\n",
      "Epoch: 223\tTrain Loss: 2.4979372 \tVal Loss:1.5162168 \tTrain Acc: 36.04167% \tVal Acc: 55.0000003%\n",
      "Epoch: 224\tTrain Loss: 1.6543364 \tVal Loss:0.9142478 \tTrain Acc: 50.625% \tVal Acc: 69.1666678%\n",
      "Epoch: 225\tTrain Loss: 1.2134792 \tVal Loss:0.5000617 \tTrain Acc: 62.56944% \tVal Acc: 88.6111105%\n",
      "Epoch: 226\tTrain Loss: 1.1784262 \tVal Loss:0.4310503 \tTrain Acc: 65.13889% \tVal Acc: 89.7222216%\n",
      "Validation Loss decreased from 0.470875 to 0.431050, saving the model weights\n",
      "Epoch: 227\tTrain Loss: 1.0487335 \tVal Loss:0.4862075 \tTrain Acc: 68.125% \tVal Acc: 84.7222224%\n",
      "Epoch: 228\tTrain Loss: 1.0246075 \tVal Loss:0.5083781 \tTrain Acc: 67.91667% \tVal Acc: 84.9999999%\n",
      "Epoch: 229\tTrain Loss: 1.1086785 \tVal Loss:0.4652332 \tTrain Acc: 65.83333% \tVal Acc: 86.9444445%\n",
      "Epoch: 230\tTrain Loss: 1.1432588 \tVal Loss:0.7771252 \tTrain Acc: 64.93056% \tVal Acc: 78.3333334%\n",
      "Epoch: 231\tTrain Loss: 1.1449357 \tVal Loss:0.7622978 \tTrain Acc: 64.51389% \tVal Acc: 76.9444451%\n",
      "Epoch: 232\tTrain Loss: 1.0838862 \tVal Loss:0.9062712 \tTrain Acc: 67.22222% \tVal Acc: 72.2222229%\n",
      "Epoch: 233\tTrain Loss: 1.2301252 \tVal Loss:1.0372098 \tTrain Acc: 61.31944% \tVal Acc: 67.2222222%\n",
      "Epoch: 234\tTrain Loss: 1.1510596 \tVal Loss:0.8665472 \tTrain Acc: 63.75% \tVal Acc: 75.5555553%\n",
      "Epoch: 235\tTrain Loss: 1.0597117 \tVal Loss:1.4040615 \tTrain Acc: 66.80556% \tVal Acc: 60.0000005%\n",
      "Epoch: 236\tTrain Loss: 1.0635699 \tVal Loss:1.0803657 \tTrain Acc: 66.38889% \tVal Acc: 64.1666668%\n",
      "Epoch: 237\tTrain Loss: 0.9463259 \tVal Loss:0.7392290 \tTrain Acc: 71.59722% \tVal Acc: 76.6666658%\n",
      "Epoch: 238\tTrain Loss: 0.8553465 \tVal Loss:0.5862849 \tTrain Acc: 73.75% \tVal Acc: 81.3888878%\n",
      "Epoch: 239\tTrain Loss: 0.8301260 \tVal Loss:0.3506857 \tTrain Acc: 74.51389% \tVal Acc: 88.6111105%\n",
      "Validation Loss decreased from 0.431050 to 0.350686, saving the model weights\n",
      "Epoch: 240\tTrain Loss: 0.7168168 \tVal Loss:0.3315279 \tTrain Acc: 78.26389% \tVal Acc: 90.5555556%\n",
      "Validation Loss decreased from 0.350686 to 0.331528, saving the model weights\n",
      "Epoch: 241\tTrain Loss: 0.6563723 \tVal Loss:0.2792751 \tTrain Acc: 79.30556% \tVal Acc: 93.0555557%\n",
      "Validation Loss decreased from 0.331528 to 0.279275, saving the model weights\n",
      "Epoch: 242\tTrain Loss: 0.5915733 \tVal Loss:0.2527885 \tTrain Acc: 82.91667% \tVal Acc: 94.4444438%\n",
      "Validation Loss decreased from 0.279275 to 0.252788, saving the model weights\n",
      "Epoch: 243\tTrain Loss: 0.5623212 \tVal Loss:0.2428547 \tTrain Acc: 83.68055% \tVal Acc: 94.4444433%\n",
      "Validation Loss decreased from 0.252788 to 0.242855, saving the model weights\n",
      "Epoch: 244\tTrain Loss: 0.5498184 \tVal Loss:0.2482941 \tTrain Acc: 84.02778% \tVal Acc: 93.6111103%\n",
      "Epoch: 245\tTrain Loss: 0.5255064 \tVal Loss:0.2618054 \tTrain Acc: 83.81944% \tVal Acc: 93.6111103%\n",
      "Epoch: 246\tTrain Loss: 0.5209565 \tVal Loss:0.2160800 \tTrain Acc: 84.44444% \tVal Acc: 94.1666663%\n",
      "Validation Loss decreased from 0.242855 to 0.216080, saving the model weights\n",
      "Epoch: 247\tTrain Loss: 0.4809341 \tVal Loss:0.2038329 \tTrain Acc: 84.93056% \tVal Acc: 94.1666658%\n",
      "Validation Loss decreased from 0.216080 to 0.203833, saving the model weights\n",
      "Epoch: 248\tTrain Loss: 0.4638749 \tVal Loss:0.2153624 \tTrain Acc: 86.45833% \tVal Acc: 94.1666663%\n",
      "Epoch: 249\tTrain Loss: 0.4613778 \tVal Loss:0.1985512 \tTrain Acc: 85.76389% \tVal Acc: 94.1666658%\n",
      "Validation Loss decreased from 0.203833 to 0.198551, saving the model weights\n",
      "Epoch: 250\tTrain Loss: 0.4013996 \tVal Loss:0.2113976 \tTrain Acc: 87.91667% \tVal Acc: 94.1666658%\n",
      "Epoch: 251\tTrain Loss: 0.3994351 \tVal Loss:0.1781718 \tTrain Acc: 88.81944% \tVal Acc: 96.3888884%\n",
      "Validation Loss decreased from 0.198551 to 0.178172, saving the model weights\n",
      "Epoch: 252\tTrain Loss: 0.4024647 \tVal Loss:0.1606017 \tTrain Acc: 87.70833% \tVal Acc: 95.5555543%\n",
      "Validation Loss decreased from 0.178172 to 0.160602, saving the model weights\n",
      "Epoch: 253\tTrain Loss: 0.3885470 \tVal Loss:0.1665519 \tTrain Acc: 88.95833% \tVal Acc: 95.2777768%\n",
      "Epoch: 254\tTrain Loss: 0.3907671 \tVal Loss:0.1503512 \tTrain Acc: 89.09722% \tVal Acc: 95.2777768%\n",
      "Validation Loss decreased from 0.160602 to 0.150351, saving the model weights\n",
      "Epoch: 255\tTrain Loss: 0.3602838 \tVal Loss:0.1480287 \tTrain Acc: 89.79167% \tVal Acc: 96.3888879%\n",
      "Validation Loss decreased from 0.150351 to 0.148029, saving the model weights\n",
      "Epoch: 256\tTrain Loss: 0.3476674 \tVal Loss:0.1566405 \tTrain Acc: 90.90278% \tVal Acc: 96.1111103%\n",
      "Epoch: 257\tTrain Loss: 0.3152361 \tVal Loss:0.1534670 \tTrain Acc: 92.29167% \tVal Acc: 94.9999993%\n",
      "Epoch: 258\tTrain Loss: 0.3045827 \tVal Loss:0.1349069 \tTrain Acc: 92.43055% \tVal Acc: 96.6666664%\n",
      "Validation Loss decreased from 0.148029 to 0.134907, saving the model weights\n",
      "Epoch: 259\tTrain Loss: 0.3044832 \tVal Loss:0.1300958 \tTrain Acc: 91.25% \tVal Acc: 95.8333323%\n",
      "Validation Loss decreased from 0.134907 to 0.130096, saving the model weights\n",
      "Epoch: 260\tTrain Loss: 0.3475196 \tVal Loss:0.1265831 \tTrain Acc: 89.44444% \tVal Acc: 97.7777774%\n",
      "Validation Loss decreased from 0.130096 to 0.126583, saving the model weights\n",
      "Epoch: 261\tTrain Loss: 0.3665766 \tVal Loss:0.1296627 \tTrain Acc: 88.54167% \tVal Acc: 96.9444434%\n",
      "Epoch: 262\tTrain Loss: 0.3255295 \tVal Loss:0.1350574 \tTrain Acc: 90.13889% \tVal Acc: 96.3888879%\n",
      "Epoch: 263\tTrain Loss: 0.3285455 \tVal Loss:0.1127829 \tTrain Acc: 90.90278% \tVal Acc: 97.4999994%\n",
      "Validation Loss decreased from 0.126583 to 0.112783, saving the model weights\n",
      "Epoch: 264\tTrain Loss: 0.3363634 \tVal Loss:0.2516551 \tTrain Acc: 90.34722% \tVal Acc: 91.1111106%\n",
      "Epoch: 265\tTrain Loss: 0.3125994 \tVal Loss:0.2010958 \tTrain Acc: 91.31944% \tVal Acc: 93.8888883%\n",
      "Epoch: 266\tTrain Loss: 0.2955571 \tVal Loss:0.1517609 \tTrain Acc: 92.08333% \tVal Acc: 95.8333323%\n",
      "Epoch: 267\tTrain Loss: 0.2934265 \tVal Loss:0.1078089 \tTrain Acc: 91.875% \tVal Acc: 96.6666664%\n",
      "Validation Loss decreased from 0.112783 to 0.107809, saving the model weights\n",
      "Epoch: 268\tTrain Loss: 0.2699214 \tVal Loss:0.1043515 \tTrain Acc: 92.91667% \tVal Acc: 96.3888884%\n",
      "Validation Loss decreased from 0.107809 to 0.104351, saving the model weights\n",
      "Epoch: 269\tTrain Loss: 0.2451938 \tVal Loss:0.1214772 \tTrain Acc: 93.40278% \tVal Acc: 96.1111099%\n",
      "Epoch: 270\tTrain Loss: 0.2395704 \tVal Loss:0.0880198 \tTrain Acc: 93.26389% \tVal Acc: 97.2222219%\n",
      "Validation Loss decreased from 0.104351 to 0.088020, saving the model weights\n",
      "Epoch: 271\tTrain Loss: 0.2187064 \tVal Loss:0.1166144 \tTrain Acc: 94.16667% \tVal Acc: 95.8333328%\n",
      "Epoch: 272\tTrain Loss: 0.2200935 \tVal Loss:0.0970487 \tTrain Acc: 93.68055% \tVal Acc: 96.6666659%\n",
      "Epoch: 273\tTrain Loss: 0.2213674 \tVal Loss:0.0825741 \tTrain Acc: 93.61111% \tVal Acc: 97.2222219%\n",
      "Validation Loss decreased from 0.088020 to 0.082574, saving the model weights\n",
      "Epoch: 274\tTrain Loss: 0.1965295 \tVal Loss:0.0962461 \tTrain Acc: 94.79167% \tVal Acc: 96.6666659%\n",
      "Epoch: 275\tTrain Loss: 0.2234903 \tVal Loss:0.1428371 \tTrain Acc: 94.30555% \tVal Acc: 95.2777773%\n",
      "Epoch: 276\tTrain Loss: 0.2102482 \tVal Loss:0.0746909 \tTrain Acc: 94.02778% \tVal Acc: 97.4999994%\n",
      "Validation Loss decreased from 0.082574 to 0.074691, saving the model weights\n",
      "Epoch: 277\tTrain Loss: 0.1755285 \tVal Loss:0.0678762 \tTrain Acc: 95.20833% \tVal Acc: 98.6111109%\n",
      "Validation Loss decreased from 0.074691 to 0.067876, saving the model weights\n",
      "Epoch: 278\tTrain Loss: 0.1668962 \tVal Loss:0.0606414 \tTrain Acc: 96.25% \tVal Acc: 98.3333324%\n",
      "Validation Loss decreased from 0.067876 to 0.060641, saving the model weights\n",
      "Epoch: 279\tTrain Loss: 0.1730333 \tVal Loss:0.0594274 \tTrain Acc: 95.41667% \tVal Acc: 98.6111104%\n",
      "Validation Loss decreased from 0.060641 to 0.059427, saving the model weights\n",
      "Epoch: 280\tTrain Loss: 0.1822608 \tVal Loss:0.0617279 \tTrain Acc: 94.72222% \tVal Acc: 98.0555549%\n",
      "Epoch: 281\tTrain Loss: 0.2037773 \tVal Loss:0.0702727 \tTrain Acc: 94.65278% \tVal Acc: 97.4999989%\n",
      "Epoch: 282\tTrain Loss: 0.1948134 \tVal Loss:0.0789155 \tTrain Acc: 94.86111% \tVal Acc: 96.9444429%\n",
      "Epoch: 283\tTrain Loss: 0.2051602 \tVal Loss:0.0801416 \tTrain Acc: 94.23611% \tVal Acc: 97.4999994%\n",
      "Epoch: 284\tTrain Loss: 0.3114422 \tVal Loss:0.0857637 \tTrain Acc: 90.55555% \tVal Acc: 97.7777769%\n",
      "Epoch: 285\tTrain Loss: 0.3243107 \tVal Loss:0.0766783 \tTrain Acc: 88.95833% \tVal Acc: 97.7777774%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 286\tTrain Loss: 0.3066932 \tVal Loss:0.1561709 \tTrain Acc: 90.76389% \tVal Acc: 94.1666648%\n",
      "Epoch: 287\tTrain Loss: 0.2688618 \tVal Loss:0.1392710 \tTrain Acc: 92.08333% \tVal Acc: 96.1111103%\n",
      "Epoch: 288\tTrain Loss: 0.2766439 \tVal Loss:0.1805360 \tTrain Acc: 91.31944% \tVal Acc: 96.1111108%\n",
      "Epoch: 289\tTrain Loss: 0.2149386 \tVal Loss:0.0969431 \tTrain Acc: 94.09722% \tVal Acc: 96.6666664%\n",
      "Epoch: 290\tTrain Loss: 0.1762203 \tVal Loss:0.0873026 \tTrain Acc: 95.27778% \tVal Acc: 97.4999989%\n",
      "Epoch: 291\tTrain Loss: 0.1739762 \tVal Loss:0.0695700 \tTrain Acc: 95.13889% \tVal Acc: 96.6666659%\n",
      "Epoch: 292\tTrain Loss: 0.1498915 \tVal Loss:0.0471485 \tTrain Acc: 96.25% \tVal Acc: 98.6111100%\n",
      "Validation Loss decreased from 0.059427 to 0.047148, saving the model weights\n",
      "Epoch: 293\tTrain Loss: 0.1354194 \tVal Loss:0.0527716 \tTrain Acc: 96.11111% \tVal Acc: 98.0555554%\n",
      "Epoch: 294\tTrain Loss: 0.1480377 \tVal Loss:0.0756130 \tTrain Acc: 96.11111% \tVal Acc: 97.2222214%\n",
      "Epoch: 295\tTrain Loss: 0.1458389 \tVal Loss:0.1124228 \tTrain Acc: 96.45833% \tVal Acc: 95.8333328%\n",
      "Epoch: 296\tTrain Loss: 0.2032972 \tVal Loss:0.0860649 \tTrain Acc: 94.44444% \tVal Acc: 96.6666659%\n",
      "Epoch: 297\tTrain Loss: 0.2247140 \tVal Loss:0.1311615 \tTrain Acc: 93.05555% \tVal Acc: 96.9444434%\n",
      "Epoch: 298\tTrain Loss: 0.2024285 \tVal Loss:0.1098525 \tTrain Acc: 94.65278% \tVal Acc: 96.6666659%\n",
      "Epoch: 299\tTrain Loss: 0.1715307 \tVal Loss:0.1379580 \tTrain Acc: 95.34722% \tVal Acc: 94.9999988%\n",
      "Epoch: 300\tTrain Loss: 0.1478139 \tVal Loss:0.1093882 \tTrain Acc: 95.90278% \tVal Acc: 94.7222213%\n",
      "Epoch: 301\tTrain Loss: 0.1378314 \tVal Loss:0.0727436 \tTrain Acc: 96.80555% \tVal Acc: 97.7777769%\n",
      "Epoch: 302\tTrain Loss: 0.1285278 \tVal Loss:0.0880173 \tTrain Acc: 96.59722% \tVal Acc: 97.4999994%\n",
      "Epoch: 303\tTrain Loss: 0.1167803 \tVal Loss:0.1115652 \tTrain Acc: 97.43055% \tVal Acc: 95.8333328%\n",
      "Epoch: 304\tTrain Loss: 0.1365767 \tVal Loss:0.1007728 \tTrain Acc: 96.25% \tVal Acc: 95.5555553%\n",
      "Epoch: 305\tTrain Loss: 0.1159182 \tVal Loss:0.0553996 \tTrain Acc: 97.08333% \tVal Acc: 98.0555549%\n",
      "Epoch: 306\tTrain Loss: 0.0998602 \tVal Loss:0.0508757 \tTrain Acc: 97.63889% \tVal Acc: 97.7777769%\n",
      "Epoch: 307\tTrain Loss: 0.1090044 \tVal Loss:0.0384158 \tTrain Acc: 97.22222% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.047148 to 0.038416, saving the model weights\n",
      "Epoch: 308\tTrain Loss: 0.1148264 \tVal Loss:0.0422138 \tTrain Acc: 96.52778% \tVal Acc: 98.6111109%\n",
      "Epoch: 309\tTrain Loss: 0.1060136 \tVal Loss:0.0381576 \tTrain Acc: 97.15278% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.038416 to 0.038158, saving the model weights\n",
      "Epoch: 310\tTrain Loss: 0.1039997 \tVal Loss:0.0485421 \tTrain Acc: 97.22222% \tVal Acc: 97.4999999%\n",
      "Epoch: 311\tTrain Loss: 0.1272893 \tVal Loss:0.0394184 \tTrain Acc: 96.66667% \tVal Acc: 98.6111104%\n",
      "Epoch: 312\tTrain Loss: 0.1435914 \tVal Loss:0.0537680 \tTrain Acc: 95.76389% \tVal Acc: 98.0555549%\n",
      "Epoch: 313\tTrain Loss: 0.3519622 \tVal Loss:0.0889834 \tTrain Acc: 89.86111% \tVal Acc: 97.4999994%\n",
      "Epoch: 314\tTrain Loss: 0.2473109 \tVal Loss:0.1848167 \tTrain Acc: 92.43055% \tVal Acc: 93.6111098%\n",
      "Epoch: 315\tTrain Loss: 0.2203773 \tVal Loss:0.2158135 \tTrain Acc: 93.125% \tVal Acc: 91.6666657%\n",
      "Epoch: 316\tTrain Loss: 0.3754602 \tVal Loss:0.2432517 \tTrain Acc: 89.51389% \tVal Acc: 91.6666657%\n",
      "Epoch: 317\tTrain Loss: 0.3866195 \tVal Loss:0.0870339 \tTrain Acc: 88.88889% \tVal Acc: 98.3333324%\n",
      "Epoch: 318\tTrain Loss: 0.2634884 \tVal Loss:0.0534720 \tTrain Acc: 92.84722% \tVal Acc: 98.8888890%\n",
      "Epoch: 319\tTrain Loss: 0.2179546 \tVal Loss:0.0439332 \tTrain Acc: 93.81944% \tVal Acc: 98.6111109%\n",
      "Epoch: 320\tTrain Loss: 0.1895234 \tVal Loss:0.0737256 \tTrain Acc: 94.72222% \tVal Acc: 96.9444439%\n",
      "Epoch: 321\tTrain Loss: 0.1631465 \tVal Loss:0.0687800 \tTrain Acc: 95.41667% \tVal Acc: 97.2222214%\n",
      "Epoch: 322\tTrain Loss: 0.1188545 \tVal Loss:0.1384007 \tTrain Acc: 97.15278% \tVal Acc: 96.6666659%\n",
      "Epoch: 323\tTrain Loss: 0.1225282 \tVal Loss:0.1169321 \tTrain Acc: 97.29167% \tVal Acc: 96.6666659%\n",
      "Epoch: 324\tTrain Loss: 0.1269507 \tVal Loss:0.0619647 \tTrain Acc: 97.01389% \tVal Acc: 97.2222214%\n",
      "Epoch: 325\tTrain Loss: 0.1105630 \tVal Loss:0.0385368 \tTrain Acc: 96.875% \tVal Acc: 98.0555554%\n",
      "Epoch: 326\tTrain Loss: 0.0889828 \tVal Loss:0.0302653 \tTrain Acc: 97.91667% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.038158 to 0.030265, saving the model weights\n",
      "Epoch: 327\tTrain Loss: 0.0860788 \tVal Loss:0.0335812 \tTrain Acc: 97.63889% \tVal Acc: 99.1666660%\n",
      "Epoch: 328\tTrain Loss: 0.0813127 \tVal Loss:0.0303113 \tTrain Acc: 98.26389% \tVal Acc: 98.8888885%\n",
      "Epoch: 329\tTrain Loss: 0.0845986 \tVal Loss:0.0235813 \tTrain Acc: 97.84722% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.030265 to 0.023581, saving the model weights\n",
      "Epoch: 330\tTrain Loss: 0.0755086 \tVal Loss:0.0309468 \tTrain Acc: 97.98611% \tVal Acc: 98.3333329%\n",
      "Epoch: 331\tTrain Loss: 0.0842066 \tVal Loss:0.0424513 \tTrain Acc: 98.05555% \tVal Acc: 98.0555549%\n",
      "Epoch: 332\tTrain Loss: 0.0786524 \tVal Loss:0.0233430 \tTrain Acc: 97.77778% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.023581 to 0.023343, saving the model weights\n",
      "Epoch: 333\tTrain Loss: 0.0694175 \tVal Loss:0.0213464 \tTrain Acc: 98.19444% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.023343 to 0.021346, saving the model weights\n",
      "Epoch: 334\tTrain Loss: 0.0613234 \tVal Loss:0.0306562 \tTrain Acc: 98.61111% \tVal Acc: 99.1666665%\n",
      "Epoch: 335\tTrain Loss: 0.1094578 \tVal Loss:0.1076875 \tTrain Acc: 96.52778% \tVal Acc: 96.3888884%\n",
      "Epoch: 336\tTrain Loss: 0.2879340 \tVal Loss:0.2512251 \tTrain Acc: 91.59722% \tVal Acc: 92.2222222%\n",
      "Epoch: 337\tTrain Loss: 0.2883359 \tVal Loss:0.1910027 \tTrain Acc: 92.22222% \tVal Acc: 94.9999998%\n",
      "Epoch: 338\tTrain Loss: 0.2259998 \tVal Loss:0.1034559 \tTrain Acc: 93.19444% \tVal Acc: 96.1111108%\n",
      "Epoch: 339\tTrain Loss: 0.2178977 \tVal Loss:0.0536317 \tTrain Acc: 94.30555% \tVal Acc: 97.4999994%\n",
      "Epoch: 340\tTrain Loss: 0.1468010 \tVal Loss:0.1278696 \tTrain Acc: 95.55555% \tVal Acc: 96.9444444%\n",
      "Epoch: 341\tTrain Loss: 0.1308622 \tVal Loss:0.0730086 \tTrain Acc: 96.66667% \tVal Acc: 97.4999989%\n",
      "Epoch: 342\tTrain Loss: 0.0985589 \tVal Loss:0.0832118 \tTrain Acc: 97.5% \tVal Acc: 96.1111103%\n",
      "Epoch: 343\tTrain Loss: 0.1064445 \tVal Loss:0.0499536 \tTrain Acc: 97.5% \tVal Acc: 98.3333329%\n",
      "Epoch: 344\tTrain Loss: 0.0915955 \tVal Loss:0.0622507 \tTrain Acc: 97.91667% \tVal Acc: 97.4999994%\n",
      "Epoch: 345\tTrain Loss: 0.1143978 \tVal Loss:0.0911306 \tTrain Acc: 97.29167% \tVal Acc: 96.6666659%\n",
      "Epoch: 346\tTrain Loss: 0.1082810 \tVal Loss:0.0456941 \tTrain Acc: 96.875% \tVal Acc: 97.4999999%\n",
      "Epoch: 347\tTrain Loss: 0.1093719 \tVal Loss:0.0676807 \tTrain Acc: 96.94444% \tVal Acc: 97.4999994%\n",
      "Epoch: 348\tTrain Loss: 0.0932679 \tVal Loss:0.0436717 \tTrain Acc: 97.22222% \tVal Acc: 98.6111109%\n",
      "Epoch: 349\tTrain Loss: 0.0841652 \tVal Loss:0.0272658 \tTrain Acc: 97.70833% \tVal Acc: 98.8888880%\n",
      "Epoch: 350\tTrain Loss: 0.0740364 \tVal Loss:0.0881498 \tTrain Acc: 98.05555% \tVal Acc: 96.6666659%\n",
      "Epoch: 351\tTrain Loss: 0.0753560 \tVal Loss:0.0293369 \tTrain Acc: 97.91667% \tVal Acc: 99.1666660%\n",
      "Epoch: 352\tTrain Loss: 0.0561539 \tVal Loss:0.0410471 \tTrain Acc: 98.95833% \tVal Acc: 98.3333329%\n",
      "Epoch: 353\tTrain Loss: 0.0657092 \tVal Loss:0.0269928 \tTrain Acc: 98.54167% \tVal Acc: 98.6111104%\n",
      "Epoch: 354\tTrain Loss: 0.0650166 \tVal Loss:0.0213619 \tTrain Acc: 98.40278% \tVal Acc: 99.1666665%\n",
      "Epoch: 355\tTrain Loss: 0.0493130 \tVal Loss:0.0685490 \tTrain Acc: 99.16667% \tVal Acc: 97.4999994%\n",
      "Epoch: 356\tTrain Loss: 0.0668086 \tVal Loss:0.0256056 \tTrain Acc: 97.91667% \tVal Acc: 98.8888880%\n",
      "Epoch: 357\tTrain Loss: 0.0713407 \tVal Loss:0.0336104 \tTrain Acc: 97.98611% \tVal Acc: 98.3333329%\n",
      "Epoch: 358\tTrain Loss: 0.0459023 \tVal Loss:0.0227354 \tTrain Acc: 99.09722% \tVal Acc: 98.8888885%\n",
      "Epoch: 359\tTrain Loss: 0.0519980 \tVal Loss:0.0261955 \tTrain Acc: 98.54167% \tVal Acc: 98.8888885%\n",
      "Epoch: 360\tTrain Loss: 0.0555782 \tVal Loss:0.0199439 \tTrain Acc: 98.81944% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.021346 to 0.019944, saving the model weights\n",
      "Epoch: 361\tTrain Loss: 0.0613738 \tVal Loss:0.0399892 \tTrain Acc: 98.40278% \tVal Acc: 97.7777774%\n",
      "Epoch: 362\tTrain Loss: 0.0800007 \tVal Loss:0.0646854 \tTrain Acc: 97.70833% \tVal Acc: 96.6666659%\n",
      "Epoch: 363\tTrain Loss: 0.0942937 \tVal Loss:0.1470846 \tTrain Acc: 97.22222% \tVal Acc: 95.8333318%\n",
      "Epoch: 364\tTrain Loss: 0.0936913 \tVal Loss:0.0383113 \tTrain Acc: 97.56944% \tVal Acc: 98.3333329%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 365\tTrain Loss: 0.0776648 \tVal Loss:0.0765766 \tTrain Acc: 97.77778% \tVal Acc: 97.4999994%\n",
      "Epoch: 366\tTrain Loss: 0.0645778 \tVal Loss:0.0220597 \tTrain Acc: 98.05556% \tVal Acc: 99.7222220%\n",
      "Epoch: 367\tTrain Loss: 0.0483639 \tVal Loss:0.0209942 \tTrain Acc: 99.02778% \tVal Acc: 99.1666665%\n",
      "Epoch: 368\tTrain Loss: 0.0522044 \tVal Loss:0.0247668 \tTrain Acc: 98.54167% \tVal Acc: 98.6111109%\n",
      "Epoch: 369\tTrain Loss: 0.0558711 \tVal Loss:0.0162071 \tTrain Acc: 98.33333% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.019944 to 0.016207, saving the model weights\n",
      "Epoch: 370\tTrain Loss: 0.0390347 \tVal Loss:0.0166174 \tTrain Acc: 98.81944% \tVal Acc: 99.4444445%\n",
      "Epoch: 371\tTrain Loss: 0.0543187 \tVal Loss:0.0650820 \tTrain Acc: 98.61111% \tVal Acc: 98.0555554%\n",
      "Epoch: 372\tTrain Loss: 0.0847899 \tVal Loss:0.0363553 \tTrain Acc: 98.125% \tVal Acc: 98.3333334%\n",
      "Epoch: 373\tTrain Loss: 0.0683894 \tVal Loss:0.0439660 \tTrain Acc: 98.26389% \tVal Acc: 98.3333329%\n",
      "Epoch: 374\tTrain Loss: 0.0568337 \tVal Loss:0.1373004 \tTrain Acc: 98.40278% \tVal Acc: 94.9999993%\n",
      "Epoch: 375\tTrain Loss: 0.0578965 \tVal Loss:0.0324874 \tTrain Acc: 98.19444% \tVal Acc: 98.6111109%\n",
      "Epoch: 376\tTrain Loss: 0.0488644 \tVal Loss:0.0333618 \tTrain Acc: 98.68056% \tVal Acc: 98.3333329%\n",
      "Epoch: 377\tTrain Loss: 0.0451287 \tVal Loss:0.0201743 \tTrain Acc: 98.88889% \tVal Acc: 99.4444445%\n",
      "Epoch: 378\tTrain Loss: 0.0486948 \tVal Loss:0.0187934 \tTrain Acc: 98.81944% \tVal Acc: 98.8888885%\n",
      "Epoch: 379\tTrain Loss: 0.0389713 \tVal Loss:0.0430660 \tTrain Acc: 99.02778% \tVal Acc: 98.0555554%\n",
      "Epoch: 380\tTrain Loss: 0.0588599 \tVal Loss:0.0363756 \tTrain Acc: 98.54167% \tVal Acc: 98.6111100%\n",
      "Epoch: 381\tTrain Loss: 0.0582717 \tVal Loss:0.0290551 \tTrain Acc: 98.81944% \tVal Acc: 98.8888885%\n",
      "Epoch: 382\tTrain Loss: 0.0429974 \tVal Loss:0.0146848 \tTrain Acc: 98.68056% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.016207 to 0.014685, saving the model weights\n",
      "Epoch: 383\tTrain Loss: 0.0442046 \tVal Loss:0.0143845 \tTrain Acc: 98.88889% \tVal Acc: 99.7222220%\n",
      "Validation Loss decreased from 0.014685 to 0.014385, saving the model weights\n",
      "Epoch: 384\tTrain Loss: 0.0387532 \tVal Loss:0.0197909 \tTrain Acc: 99.16667% \tVal Acc: 98.8888885%\n",
      "Epoch: 385\tTrain Loss: 0.0455388 \tVal Loss:0.1207797 \tTrain Acc: 98.68056% \tVal Acc: 96.3888884%\n",
      "Epoch: 386\tTrain Loss: 0.0673758 \tVal Loss:0.1018764 \tTrain Acc: 98.05555% \tVal Acc: 96.9444439%\n",
      "Epoch: 387\tTrain Loss: 0.0607865 \tVal Loss:0.0825278 \tTrain Acc: 98.26389% \tVal Acc: 96.6666659%\n",
      "Epoch: 388\tTrain Loss: 0.0591138 \tVal Loss:0.0233837 \tTrain Acc: 98.75% \tVal Acc: 98.6111104%\n",
      "Epoch: 389\tTrain Loss: 0.0470830 \tVal Loss:0.0326794 \tTrain Acc: 98.75% \tVal Acc: 98.6111109%\n",
      "Epoch: 390\tTrain Loss: 0.0358217 \tVal Loss:0.0306716 \tTrain Acc: 99.16667% \tVal Acc: 98.3333334%\n",
      "Epoch: 391\tTrain Loss: 0.0604170 \tVal Loss:0.0248411 \tTrain Acc: 98.33333% \tVal Acc: 98.8888885%\n",
      "Epoch: 392\tTrain Loss: 1.0567656 \tVal Loss:2.6861484 \tTrain Acc: 73.26389% \tVal Acc: 41.1111115%\n",
      "Epoch: 393\tTrain Loss: 1.6987474 \tVal Loss:0.7353371 \tTrain Acc: 55.0% \tVal Acc: 79.9999995%\n",
      "Epoch: 394\tTrain Loss: 0.8805702 \tVal Loss:0.1517873 \tTrain Acc: 73.95833% \tVal Acc: 96.3888879%\n",
      "Epoch: 395\tTrain Loss: 0.3331652 \tVal Loss:0.1679889 \tTrain Acc: 89.30555% \tVal Acc: 95.2777773%\n",
      "Epoch: 396\tTrain Loss: 0.2376817 \tVal Loss:0.0461782 \tTrain Acc: 93.19444% \tVal Acc: 98.6111109%\n",
      "Epoch: 397\tTrain Loss: 0.1475335 \tVal Loss:0.0599161 \tTrain Acc: 96.11111% \tVal Acc: 98.8888890%\n",
      "Epoch: 398\tTrain Loss: 0.1101903 \tVal Loss:0.0913022 \tTrain Acc: 97.5% \tVal Acc: 96.1111108%\n",
      "Epoch: 399\tTrain Loss: 0.0899974 \tVal Loss:0.2432375 \tTrain Acc: 97.98611% \tVal Acc: 90.5555554%\n",
      "Epoch: 400\tTrain Loss: 0.1103916 \tVal Loss:0.0947482 \tTrain Acc: 97.01389% \tVal Acc: 95.8333323%\n",
      "Epoch: 401\tTrain Loss: 0.0837919 \tVal Loss:0.0810641 \tTrain Acc: 98.26389% \tVal Acc: 96.1111108%\n",
      "Epoch: 402\tTrain Loss: 0.0896976 \tVal Loss:0.0354633 \tTrain Acc: 97.22222% \tVal Acc: 98.6111109%\n",
      "Epoch: 403\tTrain Loss: 0.0755280 \tVal Loss:0.0205805 \tTrain Acc: 98.05555% \tVal Acc: 99.4444445%\n",
      "Epoch: 404\tTrain Loss: 0.0627054 \tVal Loss:0.0299867 \tTrain Acc: 98.40278% \tVal Acc: 98.8888890%\n",
      "Epoch: 405\tTrain Loss: 0.0583323 \tVal Loss:0.0264227 \tTrain Acc: 98.75% \tVal Acc: 99.4444445%\n",
      "Epoch: 406\tTrain Loss: 0.0566729 \tVal Loss:0.0180234 \tTrain Acc: 98.75% \tVal Acc: 99.4444445%\n",
      "Epoch: 407\tTrain Loss: 0.0583244 \tVal Loss:0.0163795 \tTrain Acc: 98.26389% \tVal Acc: 99.4444440%\n",
      "Epoch: 408\tTrain Loss: 0.0464298 \tVal Loss:0.0159364 \tTrain Acc: 99.02778% \tVal Acc: 99.4444440%\n",
      "Epoch: 409\tTrain Loss: 0.0412760 \tVal Loss:0.0166157 \tTrain Acc: 99.16667% \tVal Acc: 99.4444445%\n",
      "Epoch: 410\tTrain Loss: 0.0478735 \tVal Loss:0.0171164 \tTrain Acc: 98.81944% \tVal Acc: 99.4444445%\n",
      "Epoch: 411\tTrain Loss: 0.0453709 \tVal Loss:0.0178955 \tTrain Acc: 99.02778% \tVal Acc: 99.4444445%\n",
      "Epoch: 412\tTrain Loss: 0.0478681 \tVal Loss:0.0165014 \tTrain Acc: 98.95833% \tVal Acc: 99.1666665%\n",
      "Epoch: 413\tTrain Loss: 0.0438535 \tVal Loss:0.0163529 \tTrain Acc: 98.88889% \tVal Acc: 99.4444440%\n",
      "Epoch: 414\tTrain Loss: 0.0438214 \tVal Loss:0.0478049 \tTrain Acc: 98.81944% \tVal Acc: 98.3333324%\n",
      "Epoch: 415\tTrain Loss: 0.0497681 \tVal Loss:0.0182719 \tTrain Acc: 99.02778% \tVal Acc: 99.1666665%\n",
      "Epoch: 416\tTrain Loss: 0.0419283 \tVal Loss:0.0168477 \tTrain Acc: 99.23611% \tVal Acc: 99.1666665%\n",
      "Epoch: 417\tTrain Loss: 0.0529859 \tVal Loss:0.0153434 \tTrain Acc: 98.47222% \tVal Acc: 99.4444440%\n",
      "Epoch: 418\tTrain Loss: 0.0446241 \tVal Loss:0.0149227 \tTrain Acc: 99.16667% \tVal Acc: 99.4444445%\n",
      "Epoch: 419\tTrain Loss: 0.0441614 \tVal Loss:0.0144415 \tTrain Acc: 99.02778% \tVal Acc: 99.4444445%\n",
      "Epoch: 420\tTrain Loss: 0.0437159 \tVal Loss:0.0165266 \tTrain Acc: 98.95833% \tVal Acc: 99.1666665%\n",
      "Epoch: 421\tTrain Loss: 0.0484197 \tVal Loss:0.0631623 \tTrain Acc: 98.61111% \tVal Acc: 97.4999999%\n",
      "Epoch: 422\tTrain Loss: 0.5517430 \tVal Loss:0.3411379 \tTrain Acc: 85.83333% \tVal Acc: 85.5555554%\n",
      "Epoch: 423\tTrain Loss: 0.4945223 \tVal Loss:0.0450853 \tTrain Acc: 85.48611% \tVal Acc: 98.8888885%\n",
      "Epoch: 424\tTrain Loss: 0.1823253 \tVal Loss:0.0208414 \tTrain Acc: 95.20833% \tVal Acc: 99.4444440%\n",
      "Epoch: 425\tTrain Loss: 0.1065257 \tVal Loss:0.0183011 \tTrain Acc: 96.875% \tVal Acc: 99.4444440%\n",
      "Epoch: 426\tTrain Loss: 0.0674611 \tVal Loss:0.0188880 \tTrain Acc: 98.26389% \tVal Acc: 99.4444445%\n",
      "Epoch: 427\tTrain Loss: 0.0694090 \tVal Loss:0.0157049 \tTrain Acc: 98.19444% \tVal Acc: 99.4444445%\n",
      "Epoch: 428\tTrain Loss: 0.0500213 \tVal Loss:0.0151354 \tTrain Acc: 98.95833% \tVal Acc: 99.4444445%\n",
      "Epoch: 429\tTrain Loss: 0.0531573 \tVal Loss:0.0238774 \tTrain Acc: 98.68056% \tVal Acc: 98.6111104%\n",
      "Epoch: 430\tTrain Loss: 0.0528046 \tVal Loss:0.0185868 \tTrain Acc: 98.88889% \tVal Acc: 99.4444445%\n",
      "Epoch: 431\tTrain Loss: 0.0556449 \tVal Loss:0.0802562 \tTrain Acc: 98.68055% \tVal Acc: 97.2222224%\n",
      "Epoch: 432\tTrain Loss: 0.0802706 \tVal Loss:0.0934396 \tTrain Acc: 97.98611% \tVal Acc: 98.3333334%\n",
      "Epoch: 433\tTrain Loss: 0.0673784 \tVal Loss:0.0674253 \tTrain Acc: 98.68056% \tVal Acc: 98.6111109%\n",
      "Epoch: 434\tTrain Loss: 0.0575633 \tVal Loss:0.0461213 \tTrain Acc: 98.75% \tVal Acc: 98.6111109%\n",
      "Epoch: 435\tTrain Loss: 0.0602614 \tVal Loss:0.0332659 \tTrain Acc: 98.61111% \tVal Acc: 98.6111109%\n",
      "Epoch: 436\tTrain Loss: 0.0570932 \tVal Loss:0.0233456 \tTrain Acc: 98.47222% \tVal Acc: 98.6111109%\n",
      "Epoch: 437\tTrain Loss: 0.0427047 \tVal Loss:0.0214757 \tTrain Acc: 99.02778% \tVal Acc: 99.1666665%\n",
      "Epoch: 438\tTrain Loss: 0.0434770 \tVal Loss:0.0215978 \tTrain Acc: 98.75% \tVal Acc: 98.6111109%\n",
      "Epoch: 439\tTrain Loss: 0.0494215 \tVal Loss:0.0161201 \tTrain Acc: 98.75% \tVal Acc: 99.4444445%\n",
      "Epoch: 440\tTrain Loss: 0.0384796 \tVal Loss:0.0147750 \tTrain Acc: 99.09722% \tVal Acc: 99.1666665%\n",
      "Epoch: 441\tTrain Loss: 0.0439032 \tVal Loss:0.0194682 \tTrain Acc: 98.81944% \tVal Acc: 99.1666665%\n",
      "Epoch: 442\tTrain Loss: 0.0466316 \tVal Loss:0.0206247 \tTrain Acc: 99.16667% \tVal Acc: 99.1666665%\n",
      "Epoch: 443\tTrain Loss: 0.0561633 \tVal Loss:0.0260938 \tTrain Acc: 98.47222% \tVal Acc: 98.3333329%\n",
      "Epoch: 444\tTrain Loss: 0.0454838 \tVal Loss:0.0143537 \tTrain Acc: 98.88889% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.014385 to 0.014354, saving the model weights\n",
      "Epoch: 445\tTrain Loss: 0.0324640 \tVal Loss:0.0138628 \tTrain Acc: 98.88889% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.014354 to 0.013863, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 446\tTrain Loss: 0.0389363 \tVal Loss:0.0140887 \tTrain Acc: 99.23611% \tVal Acc: 99.4444440%\n",
      "Epoch: 447\tTrain Loss: 0.0275320 \tVal Loss:0.0144806 \tTrain Acc: 99.375% \tVal Acc: 99.4444440%\n",
      "Epoch: 448\tTrain Loss: 0.0345939 \tVal Loss:0.0149435 \tTrain Acc: 99.16667% \tVal Acc: 99.4444440%\n",
      "Epoch: 449\tTrain Loss: 0.0292394 \tVal Loss:0.0210241 \tTrain Acc: 99.375% \tVal Acc: 99.1666665%\n",
      "Epoch: 450\tTrain Loss: 0.0430808 \tVal Loss:0.0167051 \tTrain Acc: 98.95833% \tVal Acc: 99.4444445%\n",
      "Epoch: 451\tTrain Loss: 0.0368755 \tVal Loss:0.0193786 \tTrain Acc: 98.95833% \tVal Acc: 99.1666665%\n",
      "Epoch: 452\tTrain Loss: 0.0581001 \tVal Loss:0.0144232 \tTrain Acc: 98.19444% \tVal Acc: 99.4444440%\n",
      "Epoch: 453\tTrain Loss: 0.0374315 \tVal Loss:0.0294444 \tTrain Acc: 98.95833% \tVal Acc: 98.6111109%\n",
      "Epoch: 454\tTrain Loss: 0.0732547 \tVal Loss:0.0550085 \tTrain Acc: 97.98611% \tVal Acc: 97.7777779%\n",
      "Epoch: 455\tTrain Loss: 0.0645306 \tVal Loss:0.0621489 \tTrain Acc: 98.19444% \tVal Acc: 97.7777779%\n",
      "Epoch: 456\tTrain Loss: 0.0480595 \tVal Loss:0.0333425 \tTrain Acc: 98.81944% \tVal Acc: 97.7777779%\n",
      "Epoch: 457\tTrain Loss: 0.0460713 \tVal Loss:0.0173118 \tTrain Acc: 98.68056% \tVal Acc: 99.4444445%\n",
      "Epoch: 458\tTrain Loss: 0.0334721 \tVal Loss:0.0211419 \tTrain Acc: 99.16667% \tVal Acc: 99.1666665%\n",
      "Epoch: 459\tTrain Loss: 0.0428530 \tVal Loss:0.0155110 \tTrain Acc: 98.68056% \tVal Acc: 99.4444445%\n",
      "Epoch: 460\tTrain Loss: 0.0351549 \tVal Loss:0.0143506 \tTrain Acc: 99.09722% \tVal Acc: 99.4444440%\n",
      "Epoch: 461\tTrain Loss: 0.0332973 \tVal Loss:0.0122955 \tTrain Acc: 99.23611% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.013863 to 0.012296, saving the model weights\n",
      "Epoch: 462\tTrain Loss: 0.0289001 \tVal Loss:0.0123216 \tTrain Acc: 99.375% \tVal Acc: 99.4444445%\n",
      "Epoch: 463\tTrain Loss: 0.0256563 \tVal Loss:0.0138005 \tTrain Acc: 99.375% \tVal Acc: 99.4444440%\n",
      "Epoch: 464\tTrain Loss: 0.0286308 \tVal Loss:0.0155433 \tTrain Acc: 99.16667% \tVal Acc: 99.1666665%\n",
      "Epoch: 465\tTrain Loss: 0.0320594 \tVal Loss:0.0117300 \tTrain Acc: 99.02778% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.012296 to 0.011730, saving the model weights\n",
      "Epoch: 466\tTrain Loss: 0.0306422 \tVal Loss:0.0195513 \tTrain Acc: 99.23611% \tVal Acc: 99.1666665%\n",
      "Epoch: 467\tTrain Loss: 0.0465950 \tVal Loss:0.0408218 \tTrain Acc: 98.88889% \tVal Acc: 98.3333334%\n",
      "Epoch: 468\tTrain Loss: 0.0532045 \tVal Loss:0.0945149 \tTrain Acc: 98.40278% \tVal Acc: 96.6666664%\n",
      "Epoch: 469\tTrain Loss: 0.0793286 \tVal Loss:0.0225517 \tTrain Acc: 97.91667% \tVal Acc: 98.6111104%\n",
      "Epoch: 470\tTrain Loss: 0.0737830 \tVal Loss:0.0158778 \tTrain Acc: 97.63889% \tVal Acc: 99.4444445%\n",
      "Epoch: 471\tTrain Loss: 0.0748568 \tVal Loss:0.0488565 \tTrain Acc: 97.70833% \tVal Acc: 98.3333329%\n",
      "Epoch: 472\tTrain Loss: 0.2248569 \tVal Loss:0.2711218 \tTrain Acc: 93.75% \tVal Acc: 92.4999992%\n",
      "Epoch: 473\tTrain Loss: 0.4555417 \tVal Loss:0.2435398 \tTrain Acc: 87.01389% \tVal Acc: 91.6666652%\n",
      "Epoch: 474\tTrain Loss: 0.9187513 \tVal Loss:0.6811971 \tTrain Acc: 76.59722% \tVal Acc: 80.0000002%\n",
      "Epoch: 475\tTrain Loss: 0.7833959 \tVal Loss:0.2105942 \tTrain Acc: 76.25% \tVal Acc: 92.2222217%\n",
      "Epoch: 476\tTrain Loss: 0.3741759 \tVal Loss:0.1350062 \tTrain Acc: 88.75% \tVal Acc: 93.8888878%\n",
      "Epoch: 477\tTrain Loss: 0.1943418 \tVal Loss:0.0290945 \tTrain Acc: 93.88889% \tVal Acc: 99.1666665%\n",
      "Epoch: 478\tTrain Loss: 0.1071750 \tVal Loss:0.0219313 \tTrain Acc: 97.56944% \tVal Acc: 99.1666665%\n",
      "Epoch: 479\tTrain Loss: 0.0819816 \tVal Loss:0.0195372 \tTrain Acc: 97.77778% \tVal Acc: 99.1666665%\n",
      "Epoch: 480\tTrain Loss: 0.0723373 \tVal Loss:0.0184273 \tTrain Acc: 98.19444% \tVal Acc: 99.1666660%\n",
      "Epoch: 481\tTrain Loss: 0.0697987 \tVal Loss:0.0210710 \tTrain Acc: 98.19444% \tVal Acc: 99.1666665%\n",
      "Epoch: 482\tTrain Loss: 0.0675502 \tVal Loss:0.0166066 \tTrain Acc: 98.40278% \tVal Acc: 99.1666660%\n",
      "Epoch: 483\tTrain Loss: 0.0585950 \tVal Loss:0.0177129 \tTrain Acc: 98.61111% \tVal Acc: 99.1666660%\n",
      "Epoch: 484\tTrain Loss: 0.0523837 \tVal Loss:0.0154959 \tTrain Acc: 98.68055% \tVal Acc: 99.4444440%\n",
      "Epoch: 485\tTrain Loss: 0.0436755 \tVal Loss:0.0152101 \tTrain Acc: 99.02778% \tVal Acc: 99.4444440%\n",
      "Epoch: 486\tTrain Loss: 0.0445109 \tVal Loss:0.0150371 \tTrain Acc: 98.95833% \tVal Acc: 99.4444445%\n",
      "Epoch: 487\tTrain Loss: 0.0445680 \tVal Loss:0.0136329 \tTrain Acc: 99.09722% \tVal Acc: 99.4444445%\n",
      "Epoch: 488\tTrain Loss: 0.0395864 \tVal Loss:0.0137607 \tTrain Acc: 99.16667% \tVal Acc: 99.4444445%\n",
      "Epoch: 489\tTrain Loss: 0.0435498 \tVal Loss:0.0135687 \tTrain Acc: 98.95833% \tVal Acc: 99.4444440%\n",
      "Epoch: 490\tTrain Loss: 0.0403363 \tVal Loss:0.0134836 \tTrain Acc: 98.95833% \tVal Acc: 99.4444445%\n",
      "Epoch: 491\tTrain Loss: 0.0403896 \tVal Loss:0.0138418 \tTrain Acc: 98.88889% \tVal Acc: 99.4444440%\n",
      "Epoch: 492\tTrain Loss: 0.0363690 \tVal Loss:0.0122958 \tTrain Acc: 99.16667% \tVal Acc: 99.4444440%\n",
      "Epoch: 493\tTrain Loss: 0.0353275 \tVal Loss:0.0136840 \tTrain Acc: 99.16667% \tVal Acc: 99.4444440%\n",
      "Epoch: 494\tTrain Loss: 0.0383554 \tVal Loss:0.0135196 \tTrain Acc: 99.09722% \tVal Acc: 99.4444445%\n",
      "Epoch: 495\tTrain Loss: 0.0361893 \tVal Loss:0.0124179 \tTrain Acc: 99.02778% \tVal Acc: 99.4444440%\n",
      "Epoch: 496\tTrain Loss: 0.0381788 \tVal Loss:0.0127225 \tTrain Acc: 98.81944% \tVal Acc: 99.4444440%\n",
      "Epoch: 497\tTrain Loss: 0.0314939 \tVal Loss:0.0124090 \tTrain Acc: 99.30556% \tVal Acc: 99.4444445%\n",
      "Epoch: 498\tTrain Loss: 0.0352437 \tVal Loss:0.0127632 \tTrain Acc: 98.68055% \tVal Acc: 99.4444440%\n",
      "Epoch: 499\tTrain Loss: 0.0364750 \tVal Loss:0.0120743 \tTrain Acc: 98.88889% \tVal Acc: 99.4444445%\n",
      "Epoch: 500\tTrain Loss: 0.0266045 \tVal Loss:0.0121689 \tTrain Acc: 99.30556% \tVal Acc: 99.4444440%\n",
      "Epoch: 501\tTrain Loss: 0.0290375 \tVal Loss:0.0117552 \tTrain Acc: 99.30556% \tVal Acc: 99.1666660%\n",
      "Epoch: 502\tTrain Loss: 0.0307964 \tVal Loss:0.0121160 \tTrain Acc: 99.09722% \tVal Acc: 99.1666660%\n",
      "Epoch: 503\tTrain Loss: 0.0305664 \tVal Loss:0.0197466 \tTrain Acc: 99.09722% \tVal Acc: 98.8888885%\n",
      "Epoch: 504\tTrain Loss: 0.0294406 \tVal Loss:0.0210914 \tTrain Acc: 99.30556% \tVal Acc: 99.1666665%\n",
      "Epoch: 505\tTrain Loss: 0.0322862 \tVal Loss:0.0117086 \tTrain Acc: 99.09722% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.011730 to 0.011709, saving the model weights\n",
      "Epoch: 506\tTrain Loss: 0.0274220 \tVal Loss:0.0117140 \tTrain Acc: 99.16667% \tVal Acc: 99.4444445%\n",
      "Epoch: 507\tTrain Loss: 0.0242013 \tVal Loss:0.0121147 \tTrain Acc: 99.375% \tVal Acc: 99.4444445%\n",
      "Epoch: 508\tTrain Loss: 0.0207147 \tVal Loss:0.0127257 \tTrain Acc: 99.44444% \tVal Acc: 99.4444445%\n",
      "Epoch: 509\tTrain Loss: 0.0303350 \tVal Loss:0.0158433 \tTrain Acc: 99.09722% \tVal Acc: 99.1666665%\n",
      "Epoch: 510\tTrain Loss: 0.0325752 \tVal Loss:0.0417597 \tTrain Acc: 98.95833% \tVal Acc: 98.6111109%\n",
      "Epoch: 511\tTrain Loss: 0.0660362 \tVal Loss:0.0417344 \tTrain Acc: 98.05556% \tVal Acc: 97.7777774%\n",
      "Epoch: 512\tTrain Loss: 0.6854580 \tVal Loss:0.7460252 \tTrain Acc: 83.26389% \tVal Acc: 76.3888893%\n",
      "Epoch: 513\tTrain Loss: 0.8818108 \tVal Loss:0.6145438 \tTrain Acc: 76.25% \tVal Acc: 84.4444439%\n",
      "Epoch: 514\tTrain Loss: 0.4466830 \tVal Loss:0.0456248 \tTrain Acc: 87.08333% \tVal Acc: 98.6111104%\n",
      "Epoch: 515\tTrain Loss: 0.1609922 \tVal Loss:0.0203648 \tTrain Acc: 95.13889% \tVal Acc: 99.1666660%\n",
      "Epoch: 516\tTrain Loss: 0.1221936 \tVal Loss:0.0316162 \tTrain Acc: 96.59722% \tVal Acc: 98.6111109%\n",
      "Epoch: 517\tTrain Loss: 0.0930597 \tVal Loss:0.0193101 \tTrain Acc: 97.08333% \tVal Acc: 99.4444445%\n",
      "Epoch: 518\tTrain Loss: 0.0669259 \tVal Loss:0.0177862 \tTrain Acc: 98.26389% \tVal Acc: 99.4444445%\n",
      "Epoch: 519\tTrain Loss: 0.0719775 \tVal Loss:0.0181038 \tTrain Acc: 98.26389% \tVal Acc: 99.1666660%\n",
      "Epoch: 520\tTrain Loss: 0.0666064 \tVal Loss:0.1394794 \tTrain Acc: 98.61111% \tVal Acc: 95.5555553%\n",
      "Epoch: 521\tTrain Loss: 0.0666697 \tVal Loss:0.0483553 \tTrain Acc: 98.61111% \tVal Acc: 97.4999999%\n",
      "Epoch: 522\tTrain Loss: 0.0642080 \tVal Loss:0.0153141 \tTrain Acc: 97.98611% \tVal Acc: 99.4444445%\n",
      "Epoch: 523\tTrain Loss: 0.1456223 \tVal Loss:0.0247605 \tTrain Acc: 95.76389% \tVal Acc: 98.8888890%\n",
      "Epoch: 524\tTrain Loss: 0.0610429 \tVal Loss:0.0154473 \tTrain Acc: 98.54167% \tVal Acc: 99.4444445%\n",
      "Epoch: 525\tTrain Loss: 0.0507419 \tVal Loss:0.0131331 \tTrain Acc: 98.33333% \tVal Acc: 99.4444445%\n",
      "Epoch: 526\tTrain Loss: 0.0470977 \tVal Loss:0.0135621 \tTrain Acc: 99.09722% \tVal Acc: 99.4444440%\n",
      "Epoch: 527\tTrain Loss: 0.0400537 \tVal Loss:0.0136411 \tTrain Acc: 98.75% \tVal Acc: 99.4444440%\n",
      "Epoch: 528\tTrain Loss: 0.0403764 \tVal Loss:0.0127784 \tTrain Acc: 99.16667% \tVal Acc: 99.4444440%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 529\tTrain Loss: 0.0327546 \tVal Loss:0.0136557 \tTrain Acc: 99.23611% \tVal Acc: 99.1666665%\n",
      "Epoch: 530\tTrain Loss: 0.0317369 \tVal Loss:0.0122903 \tTrain Acc: 99.375% \tVal Acc: 99.4444440%\n",
      "Epoch: 531\tTrain Loss: 0.0327754 \tVal Loss:0.0124582 \tTrain Acc: 99.16667% \tVal Acc: 99.1666665%\n",
      "Epoch: 532\tTrain Loss: 0.0294759 \tVal Loss:0.0135640 \tTrain Acc: 99.44444% \tVal Acc: 99.1666665%\n",
      "Epoch: 533\tTrain Loss: 0.0284829 \tVal Loss:0.0119519 \tTrain Acc: 99.30556% \tVal Acc: 99.4444440%\n",
      "Epoch: 534\tTrain Loss: 0.0350507 \tVal Loss:0.0282782 \tTrain Acc: 98.88889% \tVal Acc: 98.8888885%\n",
      "Epoch: 535\tTrain Loss: 0.0468049 \tVal Loss:0.0252706 \tTrain Acc: 98.54167% \tVal Acc: 98.3333329%\n",
      "Epoch: 536\tTrain Loss: 0.0770894 \tVal Loss:0.0365427 \tTrain Acc: 97.84722% \tVal Acc: 98.8888890%\n",
      "Epoch: 537\tTrain Loss: 0.0496450 \tVal Loss:0.0174771 \tTrain Acc: 98.47222% \tVal Acc: 99.1666660%\n",
      "Epoch: 538\tTrain Loss: 0.0412019 \tVal Loss:0.0129036 \tTrain Acc: 98.81944% \tVal Acc: 99.4444440%\n",
      "Epoch: 539\tTrain Loss: 0.0355603 \tVal Loss:0.0121965 \tTrain Acc: 99.09722% \tVal Acc: 99.7222220%\n",
      "Epoch: 540\tTrain Loss: 0.0289286 \tVal Loss:0.0125450 \tTrain Acc: 99.30556% \tVal Acc: 99.4444440%\n",
      "Epoch: 541\tTrain Loss: 0.0265019 \tVal Loss:0.0126768 \tTrain Acc: 99.23611% \tVal Acc: 99.1666665%\n",
      "Epoch: 542\tTrain Loss: 0.0287990 \tVal Loss:0.0148322 \tTrain Acc: 99.23611% \tVal Acc: 99.1666665%\n",
      "Epoch: 543\tTrain Loss: 0.0304715 \tVal Loss:0.0123567 \tTrain Acc: 99.09722% \tVal Acc: 99.1666665%\n",
      "Epoch: 544\tTrain Loss: 0.0301927 \tVal Loss:0.0124524 \tTrain Acc: 99.23611% \tVal Acc: 99.1666665%\n",
      "Epoch: 545\tTrain Loss: 0.0331152 \tVal Loss:0.0125933 \tTrain Acc: 99.02778% \tVal Acc: 99.1666665%\n",
      "Epoch: 546\tTrain Loss: 0.0259563 \tVal Loss:0.0116432 \tTrain Acc: 99.375% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.011709 to 0.011643, saving the model weights\n",
      "Epoch: 547\tTrain Loss: 0.0222922 \tVal Loss:0.0122008 \tTrain Acc: 99.375% \tVal Acc: 99.4444440%\n",
      "Epoch: 548\tTrain Loss: 0.0298257 \tVal Loss:0.0135878 \tTrain Acc: 98.88889% \tVal Acc: 99.4444445%\n",
      "Epoch: 549\tTrain Loss: 0.0294539 \tVal Loss:0.0123531 \tTrain Acc: 99.375% \tVal Acc: 99.1666665%\n",
      "Epoch: 550\tTrain Loss: 0.0236331 \tVal Loss:0.0127131 \tTrain Acc: 99.09722% \tVal Acc: 99.4444440%\n",
      "Epoch: 551\tTrain Loss: 0.0229793 \tVal Loss:0.0119751 \tTrain Acc: 99.51389% \tVal Acc: 99.4444440%\n",
      "Epoch: 552\tTrain Loss: 0.0242130 \tVal Loss:0.0131185 \tTrain Acc: 99.09722% \tVal Acc: 99.1666665%\n",
      "Epoch: 553\tTrain Loss: 0.0224340 \tVal Loss:0.0118371 \tTrain Acc: 99.44444% \tVal Acc: 99.4444440%\n",
      "Epoch: 554\tTrain Loss: 0.0239824 \tVal Loss:0.0140391 \tTrain Acc: 99.30556% \tVal Acc: 99.1666660%\n",
      "Epoch: 555\tTrain Loss: 0.0313965 \tVal Loss:0.1046085 \tTrain Acc: 98.95833% \tVal Acc: 97.4999999%\n",
      "Epoch: 556\tTrain Loss: 0.0780212 \tVal Loss:0.0279954 \tTrain Acc: 97.98611% \tVal Acc: 98.6111109%\n",
      "Epoch: 557\tTrain Loss: 0.0381749 \tVal Loss:0.0268464 \tTrain Acc: 98.81944% \tVal Acc: 98.6111109%\n",
      "Epoch: 558\tTrain Loss: 0.0448866 \tVal Loss:0.0365409 \tTrain Acc: 98.61111% \tVal Acc: 98.3333329%\n",
      "Epoch: 559\tTrain Loss: 0.0412036 \tVal Loss:0.0311584 \tTrain Acc: 98.88889% \tVal Acc: 98.3333334%\n",
      "Epoch: 560\tTrain Loss: 0.1191135 \tVal Loss:0.1447999 \tTrain Acc: 96.875% \tVal Acc: 95.0000003%\n",
      "Epoch: 561\tTrain Loss: 0.1072244 \tVal Loss:0.0315420 \tTrain Acc: 96.11111% \tVal Acc: 98.3333329%\n",
      "Epoch: 562\tTrain Loss: 0.1587610 \tVal Loss:0.0496611 \tTrain Acc: 95.55555% \tVal Acc: 98.0555554%\n",
      "Epoch: 563\tTrain Loss: 0.1748734 \tVal Loss:0.0721390 \tTrain Acc: 95.0% \tVal Acc: 97.7777774%\n",
      "Epoch: 564\tTrain Loss: 0.0862649 \tVal Loss:0.0582159 \tTrain Acc: 97.70833% \tVal Acc: 97.7777779%\n",
      "Epoch: 565\tTrain Loss: 0.0840679 \tVal Loss:1.0777905 \tTrain Acc: 97.98611% \tVal Acc: 78.8888887%\n",
      "Epoch: 566\tTrain Loss: 0.1015848 \tVal Loss:0.1582898 \tTrain Acc: 97.08333% \tVal Acc: 96.1111103%\n",
      "Epoch: 567\tTrain Loss: 0.0663873 \tVal Loss:0.0398117 \tTrain Acc: 97.98611% \tVal Acc: 98.6111104%\n",
      "Epoch: 568\tTrain Loss: 0.1424189 \tVal Loss:0.7960423 \tTrain Acc: 96.59722% \tVal Acc: 80.2777775%\n",
      "Epoch: 569\tTrain Loss: 0.5225973 \tVal Loss:0.2682816 \tTrain Acc: 86.25% \tVal Acc: 91.9444437%\n",
      "Epoch: 570\tTrain Loss: 0.3580930 \tVal Loss:0.0244889 \tTrain Acc: 89.23611% \tVal Acc: 98.8888885%\n",
      "Epoch: 571\tTrain Loss: 0.0927238 \tVal Loss:0.0169220 \tTrain Acc: 97.15278% \tVal Acc: 99.1666665%\n",
      "Epoch: 572\tTrain Loss: 0.0573279 \tVal Loss:0.0155922 \tTrain Acc: 98.125% \tVal Acc: 99.4444440%\n",
      "Epoch: 573\tTrain Loss: 0.0567095 \tVal Loss:0.0148417 \tTrain Acc: 98.68056% \tVal Acc: 99.4444440%\n",
      "Epoch: 574\tTrain Loss: 0.0403585 \tVal Loss:0.0149114 \tTrain Acc: 99.16667% \tVal Acc: 99.4444445%\n",
      "Epoch: 575\tTrain Loss: 0.0457999 \tVal Loss:0.0127648 \tTrain Acc: 98.75% \tVal Acc: 99.4444445%\n",
      "Epoch: 576\tTrain Loss: 0.0452896 \tVal Loss:0.0127205 \tTrain Acc: 98.68056% \tVal Acc: 99.4444445%\n",
      "Epoch: 577\tTrain Loss: 0.0393010 \tVal Loss:0.0122359 \tTrain Acc: 98.95833% \tVal Acc: 99.4444445%\n",
      "Epoch: 578\tTrain Loss: 0.0356495 \tVal Loss:0.0126414 \tTrain Acc: 98.88889% \tVal Acc: 99.1666665%\n",
      "Epoch: 579\tTrain Loss: 0.0332186 \tVal Loss:0.0119975 \tTrain Acc: 99.09722% \tVal Acc: 99.1666665%\n",
      "Epoch: 580\tTrain Loss: 0.0312598 \tVal Loss:0.0124343 \tTrain Acc: 99.09722% \tVal Acc: 99.4444445%\n",
      "Epoch: 581\tTrain Loss: 0.0336765 \tVal Loss:0.0123692 \tTrain Acc: 98.95833% \tVal Acc: 99.4444445%\n",
      "Epoch: 582\tTrain Loss: 0.0284739 \tVal Loss:0.0115298 \tTrain Acc: 99.23611% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.011643 to 0.011530, saving the model weights\n",
      "Epoch: 583\tTrain Loss: 0.0296423 \tVal Loss:0.0112722 \tTrain Acc: 99.02778% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.011530 to 0.011272, saving the model weights\n",
      "Epoch: 584\tTrain Loss: 0.0285737 \tVal Loss:0.0113665 \tTrain Acc: 99.44444% \tVal Acc: 99.4444440%\n",
      "Epoch: 585\tTrain Loss: 0.0206473 \tVal Loss:0.0112050 \tTrain Acc: 99.375% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.011272 to 0.011205, saving the model weights\n",
      "Epoch: 586\tTrain Loss: 0.0209668 \tVal Loss:0.0115169 \tTrain Acc: 99.375% \tVal Acc: 99.1666665%\n",
      "Epoch: 587\tTrain Loss: 0.0241798 \tVal Loss:0.0110861 \tTrain Acc: 99.375% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.011205 to 0.011086, saving the model weights\n",
      "Epoch: 588\tTrain Loss: 0.0276965 \tVal Loss:0.0110745 \tTrain Acc: 99.16667% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.011086 to 0.011074, saving the model weights\n",
      "Epoch: 589\tTrain Loss: 0.0225629 \tVal Loss:0.0117984 \tTrain Acc: 99.51389% \tVal Acc: 99.4444445%\n",
      "Epoch: 590\tTrain Loss: 0.0275008 \tVal Loss:0.0118567 \tTrain Acc: 99.09722% \tVal Acc: 99.1666665%\n",
      "Epoch: 591\tTrain Loss: 0.0259026 \tVal Loss:0.0121145 \tTrain Acc: 99.16667% \tVal Acc: 99.1666665%\n",
      "Epoch: 592\tTrain Loss: 0.0254375 \tVal Loss:0.0110488 \tTrain Acc: 99.09722% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.011074 to 0.011049, saving the model weights\n",
      "Epoch: 593\tTrain Loss: 0.0266283 \tVal Loss:0.0110142 \tTrain Acc: 99.30556% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.011049 to 0.011014, saving the model weights\n",
      "Epoch: 594\tTrain Loss: 0.0259727 \tVal Loss:0.0392832 \tTrain Acc: 99.02778% \tVal Acc: 98.6111109%\n",
      "Epoch: 595\tTrain Loss: 0.0403515 \tVal Loss:0.0131996 \tTrain Acc: 98.95833% \tVal Acc: 99.4444445%\n",
      "Epoch: 596\tTrain Loss: 0.0233755 \tVal Loss:0.0127565 \tTrain Acc: 99.44444% \tVal Acc: 99.4444440%\n",
      "Epoch: 597\tTrain Loss: 0.0275625 \tVal Loss:0.0116637 \tTrain Acc: 98.88889% \tVal Acc: 99.4444440%\n",
      "Epoch: 598\tTrain Loss: 0.0595575 \tVal Loss:0.0112244 \tTrain Acc: 98.33333% \tVal Acc: 99.4444440%\n",
      "Epoch: 599\tTrain Loss: 0.0455212 \tVal Loss:0.0114675 \tTrain Acc: 99.02778% \tVal Acc: 99.4444445%\n",
      "Epoch: 600\tTrain Loss: 0.0296175 \tVal Loss:0.0113269 \tTrain Acc: 99.02778% \tVal Acc: 99.4444445%\n",
      "Epoch: 601\tTrain Loss: 0.0169505 \tVal Loss:0.0113058 \tTrain Acc: 99.65278% \tVal Acc: 99.4444445%\n",
      "Epoch: 602\tTrain Loss: 0.0252537 \tVal Loss:0.0112684 \tTrain Acc: 99.30556% \tVal Acc: 99.4444445%\n",
      "Epoch: 603\tTrain Loss: 0.0276823 \tVal Loss:0.0616579 \tTrain Acc: 99.23611% \tVal Acc: 97.7777779%\n",
      "Epoch: 604\tTrain Loss: 0.0482606 \tVal Loss:0.0126980 \tTrain Acc: 98.81944% \tVal Acc: 99.4444445%\n",
      "Epoch: 605\tTrain Loss: 0.0240742 \tVal Loss:0.0139835 \tTrain Acc: 99.23611% \tVal Acc: 99.7222220%\n",
      "Epoch: 606\tTrain Loss: 0.0255003 \tVal Loss:0.0112206 \tTrain Acc: 99.16667% \tVal Acc: 99.4444440%\n",
      "Epoch: 607\tTrain Loss: 0.0190507 \tVal Loss:0.0111404 \tTrain Acc: 99.51389% \tVal Acc: 99.4444440%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 608\tTrain Loss: 0.0196691 \tVal Loss:0.0128409 \tTrain Acc: 99.375% \tVal Acc: 99.1666665%\n",
      "Epoch: 609\tTrain Loss: 0.0196393 \tVal Loss:0.0126868 \tTrain Acc: 99.30556% \tVal Acc: 99.1666665%\n",
      "Epoch: 610\tTrain Loss: 0.0225215 \tVal Loss:0.0110868 \tTrain Acc: 99.30556% \tVal Acc: 99.4444440%\n",
      "Epoch: 611\tTrain Loss: 0.0216522 \tVal Loss:0.0113922 \tTrain Acc: 99.23611% \tVal Acc: 99.4444440%\n",
      "Epoch: 612\tTrain Loss: 0.0195597 \tVal Loss:0.0123586 \tTrain Acc: 99.44444% \tVal Acc: 99.1666665%\n",
      "Epoch: 613\tTrain Loss: 0.0186024 \tVal Loss:0.0124807 \tTrain Acc: 99.375% \tVal Acc: 99.1666665%\n",
      "Epoch: 614\tTrain Loss: 0.0279369 \tVal Loss:0.0474658 \tTrain Acc: 99.02778% \tVal Acc: 98.3333329%\n",
      "Epoch: 615\tTrain Loss: 0.0221653 \tVal Loss:0.0119070 \tTrain Acc: 99.16667% \tVal Acc: 99.1666660%\n",
      "Epoch: 616\tTrain Loss: 0.0212494 \tVal Loss:0.0111247 \tTrain Acc: 99.23611% \tVal Acc: 99.4444440%\n",
      "Epoch: 617\tTrain Loss: 0.0263464 \tVal Loss:0.0335954 \tTrain Acc: 99.09722% \tVal Acc: 98.3333329%\n",
      "Epoch: 618\tTrain Loss: 0.0403540 \tVal Loss:0.0887051 \tTrain Acc: 98.75% \tVal Acc: 98.3333329%\n",
      "Epoch: 619\tTrain Loss: 0.8858026 \tVal Loss:0.5210296 \tTrain Acc: 82.56944% \tVal Acc: 85.2777772%\n",
      "Epoch: 620\tTrain Loss: 1.4520761 \tVal Loss:0.8317319 \tTrain Acc: 64.58333% \tVal Acc: 82.7777778%\n",
      "Epoch: 621\tTrain Loss: 0.9889847 \tVal Loss:0.1592111 \tTrain Acc: 72.5% \tVal Acc: 95.2777768%\n",
      "Epoch: 622\tTrain Loss: 0.2752935 \tVal Loss:0.0443670 \tTrain Acc: 91.52778% \tVal Acc: 98.3333329%\n",
      "Epoch: 623\tTrain Loss: 0.1496968 \tVal Loss:0.0310700 \tTrain Acc: 95.76389% \tVal Acc: 99.1666665%\n",
      "Epoch: 624\tTrain Loss: 0.1100974 \tVal Loss:0.0220923 \tTrain Acc: 96.94444% \tVal Acc: 99.4444440%\n",
      "Epoch: 625\tTrain Loss: 0.0983390 \tVal Loss:0.0268437 \tTrain Acc: 97.36111% \tVal Acc: 99.4444440%\n",
      "Epoch: 626\tTrain Loss: 0.0774146 \tVal Loss:0.0167239 \tTrain Acc: 97.84722% \tVal Acc: 99.4444445%\n",
      "Epoch: 627\tTrain Loss: 0.0542838 \tVal Loss:0.0178574 \tTrain Acc: 98.88889% \tVal Acc: 99.1666665%\n",
      "Epoch: 628\tTrain Loss: 0.0544753 \tVal Loss:0.0151540 \tTrain Acc: 98.61111% \tVal Acc: 99.1666665%\n",
      "Epoch: 629\tTrain Loss: 0.0445908 \tVal Loss:0.0143888 \tTrain Acc: 98.68055% \tVal Acc: 99.1666665%\n",
      "Epoch: 630\tTrain Loss: 0.0494427 \tVal Loss:0.0146199 \tTrain Acc: 98.75% \tVal Acc: 99.1666665%\n",
      "Epoch: 631\tTrain Loss: 0.0437339 \tVal Loss:0.0129686 \tTrain Acc: 98.81944% \tVal Acc: 99.4444440%\n",
      "Epoch: 632\tTrain Loss: 0.0379012 \tVal Loss:0.0128855 \tTrain Acc: 99.02778% \tVal Acc: 99.4444440%\n",
      "Epoch: 633\tTrain Loss: 0.0355864 \tVal Loss:0.0125709 \tTrain Acc: 99.16667% \tVal Acc: 99.4444445%\n",
      "Epoch: 634\tTrain Loss: 0.0378524 \tVal Loss:0.0121840 \tTrain Acc: 99.16667% \tVal Acc: 99.4444440%\n",
      "Epoch: 635\tTrain Loss: 0.0392696 \tVal Loss:0.0122358 \tTrain Acc: 98.88889% \tVal Acc: 99.4444440%\n",
      "Epoch: 636\tTrain Loss: 0.0325274 \tVal Loss:0.0121290 \tTrain Acc: 99.16667% \tVal Acc: 99.4444440%\n",
      "Epoch: 637\tTrain Loss: 0.0358677 \tVal Loss:0.0122874 \tTrain Acc: 99.09722% \tVal Acc: 99.1666665%\n",
      "Epoch: 638\tTrain Loss: 0.0279095 \tVal Loss:0.0132119 \tTrain Acc: 99.30556% \tVal Acc: 99.1666665%\n",
      "Epoch: 639\tTrain Loss: 0.0362788 \tVal Loss:0.0122988 \tTrain Acc: 98.81944% \tVal Acc: 99.1666665%\n",
      "Epoch: 640\tTrain Loss: 0.0230934 \tVal Loss:0.0117938 \tTrain Acc: 99.58333% \tVal Acc: 99.4444440%\n",
      "Epoch: 641\tTrain Loss: 0.0288716 \tVal Loss:0.0116585 \tTrain Acc: 99.23611% \tVal Acc: 99.4444440%\n",
      "Epoch: 642\tTrain Loss: 0.0260022 \tVal Loss:0.0114392 \tTrain Acc: 99.44444% \tVal Acc: 99.7222220%\n",
      "Epoch: 643\tTrain Loss: 0.0310368 \tVal Loss:0.0113328 \tTrain Acc: 99.375% \tVal Acc: 99.4444440%\n",
      "Epoch: 644\tTrain Loss: 0.0379052 \tVal Loss:0.0114212 \tTrain Acc: 98.81944% \tVal Acc: 99.4444440%\n",
      "Epoch: 645\tTrain Loss: 0.0277790 \tVal Loss:0.0113174 \tTrain Acc: 99.02778% \tVal Acc: 99.7222220%\n",
      "Epoch: 646\tTrain Loss: 0.0267422 \tVal Loss:0.0118103 \tTrain Acc: 99.30556% \tVal Acc: 99.4444445%\n",
      "Epoch: 647\tTrain Loss: 0.0258491 \tVal Loss:0.0114007 \tTrain Acc: 99.23611% \tVal Acc: 99.1666660%\n",
      "Epoch: 648\tTrain Loss: 0.0264772 \tVal Loss:0.0113760 \tTrain Acc: 99.23611% \tVal Acc: 99.4444440%\n",
      "Epoch: 649\tTrain Loss: 0.0228503 \tVal Loss:0.0120858 \tTrain Acc: 99.30556% \tVal Acc: 99.4444440%\n",
      "Epoch: 650\tTrain Loss: 0.0230602 \tVal Loss:0.0116189 \tTrain Acc: 99.51389% \tVal Acc: 99.4444440%\n",
      "Epoch: 651\tTrain Loss: 0.0301647 \tVal Loss:0.0134580 \tTrain Acc: 99.23611% \tVal Acc: 99.1666665%\n",
      "Epoch: 652\tTrain Loss: 0.0330054 \tVal Loss:0.0115567 \tTrain Acc: 99.02778% \tVal Acc: 99.4444445%\n",
      "Epoch: 653\tTrain Loss: 0.0243548 \tVal Loss:0.0112069 \tTrain Acc: 99.30556% \tVal Acc: 99.4444445%\n",
      "Epoch: 654\tTrain Loss: 0.0288764 \tVal Loss:0.0114761 \tTrain Acc: 98.95833% \tVal Acc: 99.4444445%\n",
      "Epoch: 655\tTrain Loss: 0.0185636 \tVal Loss:0.0114678 \tTrain Acc: 99.58333% \tVal Acc: 99.4444440%\n",
      "Epoch: 656\tTrain Loss: 0.0212843 \tVal Loss:0.0112830 \tTrain Acc: 99.375% \tVal Acc: 99.4444440%\n",
      "Epoch: 657\tTrain Loss: 0.0224633 \tVal Loss:0.0109331 \tTrain Acc: 99.375% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.011014 to 0.010933, saving the model weights\n",
      "Epoch: 658\tTrain Loss: 0.0187026 \tVal Loss:0.0110399 \tTrain Acc: 99.51389% \tVal Acc: 99.4444440%\n",
      "Epoch: 659\tTrain Loss: 0.0227176 \tVal Loss:0.0108999 \tTrain Acc: 99.44444% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.010933 to 0.010900, saving the model weights\n",
      "Epoch: 660\tTrain Loss: 0.0259054 \tVal Loss:0.0121469 \tTrain Acc: 99.23611% \tVal Acc: 99.4444440%\n",
      "Epoch: 661\tTrain Loss: 0.0231911 \tVal Loss:0.0108723 \tTrain Acc: 99.51389% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.010900 to 0.010872, saving the model weights\n",
      "Epoch: 662\tTrain Loss: 0.0268290 \tVal Loss:0.0113611 \tTrain Acc: 99.23611% \tVal Acc: 99.1666665%\n",
      "Epoch: 663\tTrain Loss: 0.0239724 \tVal Loss:0.0137834 \tTrain Acc: 99.375% \tVal Acc: 98.8888885%\n",
      "Epoch: 664\tTrain Loss: 0.0286666 \tVal Loss:0.0108690 \tTrain Acc: 98.88889% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.010872 to 0.010869, saving the model weights\n",
      "Epoch: 665\tTrain Loss: 0.0331006 \tVal Loss:0.0124000 \tTrain Acc: 98.81944% \tVal Acc: 99.1666665%\n",
      "Epoch: 666\tTrain Loss: 0.0245004 \tVal Loss:0.0626423 \tTrain Acc: 99.23611% \tVal Acc: 98.3333334%\n",
      "Epoch: 667\tTrain Loss: 0.0415239 \tVal Loss:0.0153587 \tTrain Acc: 98.54167% \tVal Acc: 99.1666665%\n",
      "Epoch: 668\tTrain Loss: 0.0213837 \tVal Loss:0.0135781 \tTrain Acc: 99.375% \tVal Acc: 99.1666665%\n",
      "Epoch: 669\tTrain Loss: 0.0239554 \tVal Loss:0.0384058 \tTrain Acc: 99.44444% \tVal Acc: 98.6111109%\n",
      "Epoch: 670\tTrain Loss: 0.0342409 \tVal Loss:0.0936968 \tTrain Acc: 98.88889% \tVal Acc: 97.4999999%\n",
      "Epoch: 671\tTrain Loss: 0.0429158 \tVal Loss:0.0393775 \tTrain Acc: 98.61111% \tVal Acc: 97.7777779%\n",
      "Epoch: 672\tTrain Loss: 0.0255850 \tVal Loss:0.0196916 \tTrain Acc: 99.30556% \tVal Acc: 98.8888885%\n",
      "Epoch: 673\tTrain Loss: 0.0238251 \tVal Loss:0.0148879 \tTrain Acc: 99.09722% \tVal Acc: 99.4444445%\n",
      "Epoch: 674\tTrain Loss: 0.0369048 \tVal Loss:0.0148244 \tTrain Acc: 98.68055% \tVal Acc: 99.1666660%\n",
      "Epoch: 675\tTrain Loss: 0.0769407 \tVal Loss:0.0132429 \tTrain Acc: 98.125% \tVal Acc: 99.4444440%\n",
      "Epoch: 676\tTrain Loss: 0.0517994 \tVal Loss:0.0138797 \tTrain Acc: 98.68055% \tVal Acc: 99.1666660%\n",
      "Epoch: 677\tTrain Loss: 0.0619119 \tVal Loss:0.0119090 \tTrain Acc: 97.91667% \tVal Acc: 99.4444445%\n",
      "Epoch: 678\tTrain Loss: 0.1143244 \tVal Loss:0.0315438 \tTrain Acc: 96.66667% \tVal Acc: 98.8888890%\n",
      "Epoch: 679\tTrain Loss: 0.1428481 \tVal Loss:0.1375151 \tTrain Acc: 95.41667% \tVal Acc: 94.4444443%\n",
      "Epoch: 680\tTrain Loss: 0.1231281 \tVal Loss:0.0122961 \tTrain Acc: 96.25% \tVal Acc: 99.4444440%\n",
      "Epoch: 681\tTrain Loss: 0.0691072 \tVal Loss:0.0175776 \tTrain Acc: 97.70833% \tVal Acc: 99.1666660%\n",
      "Epoch: 682\tTrain Loss: 0.0405382 \tVal Loss:0.0115270 \tTrain Acc: 98.88889% \tVal Acc: 99.1666665%\n",
      "Epoch: 683\tTrain Loss: 0.0287116 \tVal Loss:0.0149384 \tTrain Acc: 99.23611% \tVal Acc: 99.4444440%\n",
      "Epoch: 684\tTrain Loss: 0.0296700 \tVal Loss:0.0113803 \tTrain Acc: 99.16667% \tVal Acc: 99.4444440%\n",
      "Epoch: 685\tTrain Loss: 0.0262851 \tVal Loss:0.0107295 \tTrain Acc: 99.44444% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.010869 to 0.010730, saving the model weights\n",
      "Epoch: 686\tTrain Loss: 0.0270206 \tVal Loss:0.0117742 \tTrain Acc: 99.23611% \tVal Acc: 99.4444445%\n",
      "Epoch: 687\tTrain Loss: 0.0304361 \tVal Loss:0.0129886 \tTrain Acc: 98.95833% \tVal Acc: 99.1666665%\n",
      "Epoch: 688\tTrain Loss: 0.0254044 \tVal Loss:0.0106231 \tTrain Acc: 99.16667% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.010730 to 0.010623, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 689\tTrain Loss: 0.0258618 \tVal Loss:0.0105646 \tTrain Acc: 99.30556% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.010623 to 0.010565, saving the model weights\n",
      "Epoch: 690\tTrain Loss: 0.0252426 \tVal Loss:0.0105898 \tTrain Acc: 99.44444% \tVal Acc: 99.4444445%\n",
      "Epoch: 691\tTrain Loss: 0.0201018 \tVal Loss:0.0110136 \tTrain Acc: 99.30556% \tVal Acc: 99.4444440%\n",
      "Epoch: 692\tTrain Loss: 0.0245884 \tVal Loss:0.0122487 \tTrain Acc: 99.44444% \tVal Acc: 99.1666665%\n",
      "Epoch: 693\tTrain Loss: 0.0210516 \tVal Loss:0.0108394 \tTrain Acc: 99.30556% \tVal Acc: 99.1666665%\n",
      "Epoch: 694\tTrain Loss: 0.0216377 \tVal Loss:0.0104543 \tTrain Acc: 99.44444% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.010565 to 0.010454, saving the model weights\n",
      "Epoch: 695\tTrain Loss: 0.0312073 \tVal Loss:0.0108102 \tTrain Acc: 99.09722% \tVal Acc: 99.4444445%\n",
      "Epoch: 696\tTrain Loss: 0.0220826 \tVal Loss:0.0103492 \tTrain Acc: 99.30556% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.010454 to 0.010349, saving the model weights\n",
      "Epoch: 697\tTrain Loss: 0.0203864 \tVal Loss:0.0103592 \tTrain Acc: 99.44444% \tVal Acc: 99.4444445%\n",
      "Epoch: 698\tTrain Loss: 0.0147207 \tVal Loss:0.0101194 \tTrain Acc: 99.65278% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.010349 to 0.010119, saving the model weights\n",
      "Epoch: 699\tTrain Loss: 0.0257265 \tVal Loss:0.0102955 \tTrain Acc: 99.375% \tVal Acc: 99.4444445%\n"
     ]
    }
   ],
   "source": [
    "epochs = 700\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    \n",
    "    hidden = model.hidden_init(train_batch_size)    \n",
    "    #print('hidden[0].shape:- ',hidden[0].shape)\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        '''\n",
    "        Creating new variables for the hidden state, otherwise\n",
    "        we'd backprop through the entire training history\n",
    "        '''\n",
    "        h = tuple([each.data for each in hidden])\n",
    "        \n",
    "\n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "       \n",
    "        # get the output from the model\n",
    "        output, h = model.forward(inputs, h,train_batch_size)\n",
    "        #print('OUTPUT', output)\n",
    "        \n",
    "        \n",
    "        #print('Labels Shape :-', (torch.max(labels, 1)[1]).shape)\n",
    "    \n",
    "        # calculate the loss and perform backprop\n",
    "        #print('Labels Long :-', labels.long())\n",
    "        loss = criterion(output,labels.long())\n",
    "        #print('LOSS IS :-', loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #calculate training accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        #logging.debug(' top probab {} top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        #print(train_loss)\n",
    "              \n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "                \n",
    "        val_h = tuple([each.data for each in hidden])\n",
    "        \n",
    "        output, hidden = model.forward(inputs, val_h,val_batch_size)\n",
    "       \n",
    "        loss = criterion(output,labels.long())\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        #calculate validation accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        \n",
    "        #logging.debug(output)\n",
    "        #logging.debug('VALIDATION top probab {} VALIDATION top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        #print('Top Class:- ',top_class)\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        #print('Equals:- ', equals)\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    #Averaging losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    val_accuracy = val_accuracy/len(val_loader)\n",
    "    train_accuracy = train_accuracy/len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}\\tTrain Loss: {:.7f} \\tVal Loss:{:.7f} \\tTrain Acc: {:.7}% \\tVal Acc: {:.7f}%'.format(e, train_loss, val_loss, train_accuracy*100,val_accuracy*100))\n",
    "    \n",
    "    #saving the model if validation loss is decreased\n",
    "    if val_loss <= min_val_loss:\n",
    "        print('Validation Loss decreased from {:6f} to {:6f}, saving the model weights'.format(min_val_loss, val_loss))\n",
    "        torch.save(model.state_dict(), 'lstm_state_256-256-backchodi_norm_backchodi.pt')\n",
    "        min_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Music Genaration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=256, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights\n",
    "test_model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "test_model.load_state_dict(torch.load('lstm_state_256-256-backchodi_norm_backchodi.pt'))\n",
    "test_model.eval()\n",
    "test_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population database\n",
    "testing_data = np.ones(200)*0.57\n",
    "# testing_data = list(range(50,90))\n",
    "# testing_data.extend(testing_data[::-1])\n",
    "# testing_data_rev = testing_data[::-1]\n",
    "# testing_data_rev.extend(testing_data)\n",
    "# testing_data_rev.extend(testing_data_rev)\n",
    "# testing_data = testing_data_rev\n",
    "\n",
    "\n",
    "testing_data = np.asarray(testing_data)\n",
    "testing_data = testing_data.reshape(testing_data.shape[0],1)\n",
    "\n",
    "initial_seq = [network_input[0][1:].cpu().numpy().tolist()]\n",
    "\n",
    "testing_data_unnorm = testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "testing_data=testing_data.tolist()\n",
    "for i in range(len(testing_data)):\n",
    "    list1.extend(testing_data[i])\n",
    "\n",
    "#list1\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    #list1[i]=(list1[i]-50)/(89-50)\n",
    "    list1[i]=(list1[i])/(89)\n",
    "\n",
    "\n",
    "list1 = np.asarray(list1)\n",
    "list1 = list1.reshape(list1.shape[0],1)\n",
    "testing_data = list1\n",
    "#list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting\n",
    "def prediction_with_influence(influence,int2note,initial_seq, max_note, test_batch_size = 1):\n",
    "\n",
    "    predicted_notes = []\n",
    "    initial_seq[0].extend([[0]]*len(testing_data))\n",
    "    test_seq = torch.Tensor(initial_seq).cuda()\n",
    "    \n",
    "    test_hidden = test_model.hidden_init(test_batch_size)\n",
    "\n",
    "    \n",
    "    for i in range(len(influence)):\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = float(influence[i])\n",
    "        \n",
    "        test_slice = test_seq[0][i : i + sequence_length]        \n",
    "        test_slice = test_slice.view(1, test_slice.shape[0], test_slice.shape[1])\n",
    "        \n",
    "        h = tuple([each.data for each in test_hidden])\n",
    "        \n",
    "        test_output,_ = test_model.forward(test_slice, h, test_batch_size)\n",
    "        test_output = F.softmax(test_output, dim = 1)\n",
    "    \n",
    "        top_p, top_class = test_output.topk(1,dim =1)\n",
    "        test_seq[0][sequence_length - 1 + i][0] = int2note[top_class.item()]/max_note\n",
    "        \n",
    "        predicted_notes.append(int2note[top_class.item()])\n",
    "        \n",
    "    return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_notes_lst = prediction_with_influence(testing_data,int_to_note,initial_seq, max_midi_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_notes_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15a13cbb358>]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 369
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(list(np.ones(200)*89))\n",
    "\n",
    "plt.plot(list(np.ones(200)*50))\n",
    "#plt.plot(list(np.ones(20)*50))\n",
    "#plt.plot(testing_data_unnorm)\n",
    "plt.plot(predicted_notes_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({72: 13,\n",
       "         74: 10,\n",
       "         76: 7,\n",
       "         78: 4,\n",
       "         79: 5,\n",
       "         81: 10,\n",
       "         83: 20,\n",
       "         71: 7,\n",
       "         69: 9,\n",
       "         73: 5,\n",
       "         84: 12,\n",
       "         86: 1,\n",
       "         77: 5,\n",
       "         67: 10,\n",
       "         62: 12,\n",
       "         66: 12,\n",
       "         75: 6,\n",
       "         70: 7,\n",
       "         60: 31,\n",
       "         65: 15,\n",
       "         64: 8,\n",
       "         57: 1,\n",
       "         55: 13,\n",
       "         59: 8,\n",
       "         52: 2,\n",
       "         68: 7})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(predicted_notes_lst)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
