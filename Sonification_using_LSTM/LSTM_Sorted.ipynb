{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "#import torch.utils.tensorboard as tb\n",
    "from Preprocessing.preprocessing_sorted import PreprocessingTrainingData\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as  plt\n",
    "import os\n",
    "import logging\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static parameters\n",
    "train_batch_size = 78\n",
    "val_batch_size = 78\n",
    "sequence_length=50\n",
    "test_batch_size = 1\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layer = 2\n",
    "output_size = 81\n",
    "clip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from preprocessing.py\n",
    "# dataset_path = os.path.join(os.path.abspath('..'),'CombinedData' )\n",
    "# network_input,network_output,max_midi_number,min_midi_number,int_to_note = PreprocessingTrainingData().preprocess_notes(dataset_path)\n",
    "# network_input, network_output = network_input.cuda(), network_output.cuda()\n",
    "\n",
    "# # print(network_input)\n",
    "# #print(network_output)\n",
    "\n",
    "max_midi_number=105\n",
    "min_midi_number=24\n",
    "int_to_note={0: 24, 1: 26, 2: 27, 3: 28, 4: 29, 5: 30, 6: 31, 7: 32, 8: 33, 9: 34, 10: 35, 11: 36, 12: 37, 13: 38, 14: 39, 15: 40, 16: 41, 17: 42, 18: 43, 19: 44, 20: 45, 21: 46, 22: 47, 23: 48, 24: 49, 25: 50, 26: 51, 27: 52, 28: 53, 29: 54, 30: 55, 31: 56, 32: 57, 33: 58, 34: 59, 35: 60, 36: 61, 37: 62, 38: 63, 39: 64, 40: 65, 41: 66, 42: 67, 43: 68, 44: 69, 45: 70, 46: 71, 47: 72, 48: 73, 49: 74, 50: 75, 51: 76, 52: 77, 53: 78, 54: 79, 55: 80, 56: 81, 57: 82, 58: 83, 59: 84, 60: 85, 61: 86, 62: 87, 63: 88, 64: 89, 65: 90, 66: 91, 67: 92, 68: 93, 69: 94, 70: 95, 71: 96, 72: 97, 73: 98, 74: 99, 75: 100, 76: 101, 77: 102, 78: 103, 79: 104, 80: 105}\n",
    "\n",
    "network_input=pd.read_csv('D:\\\\Prem\\\\Sem1\\\\MM in AI\\\\Project\\\\Project\\\\Sonification-using-Deep-Learning\\\\Sonification_using_LSTM\\\\Preprocessing\\\\network_input.csv') \n",
    "network_input=(network_input.values.reshape(network_input.shape[0],network_input.shape[1],1)).tolist()\n",
    "for i in range(len(network_input)):\n",
    "    for j in range(len(network_input[i])):\n",
    "        network_input[i][j][0]=(network_input[i][j][0]-min_midi_number)/(max_midi_number-min_midi_number)\n",
    "network_input=torch.Tensor(network_input).cuda()\n",
    "\n",
    "\n",
    "network_output=pd.read_csv(\"D:\\\\Prem\\\\Sem1\\\\MM in AI\\\\Project\\\\Project\\\\Sonification-using-Deep-Learning\\\\Sonification_using_LSTM\\\\Preprocessing\\\\network_output.csv\")\n",
    "network_output=network_output.values.reshape(network_output.shape[0])\n",
    "network_output=torch.Tensor(network_output).cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(80., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(network_output.max())\n",
    "print(network_output.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7806])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0')\n",
      "tensor(0.0247, device='cuda:0')\n",
      "105\n",
      "24\n",
      "{0: 24, 1: 26, 2: 27, 3: 28, 4: 29, 5: 30, 6: 31, 7: 32, 8: 33, 9: 34, 10: 35, 11: 36, 12: 37, 13: 38, 14: 39, 15: 40, 16: 41, 17: 42, 18: 43, 19: 44, 20: 45, 21: 46, 22: 47, 23: 48, 24: 49, 25: 50, 26: 51, 27: 52, 28: 53, 29: 54, 30: 55, 31: 56, 32: 57, 33: 58, 34: 59, 35: 60, 36: 61, 37: 62, 38: 63, 39: 64, 40: 65, 41: 66, 42: 67, 43: 68, 44: 69, 45: 70, 46: 71, 47: 72, 48: 73, 49: 74, 50: 75, 51: 76, 52: 77, 53: 78, 54: 79, 55: 80, 56: 81, 57: 82, 58: 83, 59: 84, 60: 85, 61: 86, 62: 87, 63: 88, 64: 89, 65: 90, 66: 91, 67: 92, 68: 93, 69: 94, 70: 95, 71: 96, 72: 97, 73: 98, 74: 99, 75: 100, 76: 101, 77: 102, 78: 103, 79: 104, 80: 105}\n"
     ]
    }
   ],
   "source": [
    "print(network_input.max())\n",
    "print(network_input.min())\n",
    "print(max_midi_number)\n",
    "print(min_midi_number)\n",
    "print(int_to_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7806, 50, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata is highly unbalanced\\n# '"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data is highly unbalanced\n",
    "# '''\n",
    "\n",
    "# sns.distplot(torch.tensor(network_output).cpu())\n",
    "# xx = pd.DataFrame(torch.tensor(network_output).cpu())\n",
    "# xx.groupby(0).size().to_frame(name='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7800, 50, 1])\n",
      "torch.Size([7800])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "to make batch of equal sizes\n",
    "Quick Fix\n",
    "'''\n",
    "network_input = network_input[: -6]\n",
    "network_output = network_output[: -6]\n",
    "\n",
    "print(network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create Stacked LSTM model\n",
    "'''\n",
    "class Stacked_LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True, dropout = 0.5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden,batch_size):\n",
    "        \n",
    "        output, hidden = self.lstm(x, hidden)        \n",
    "        #print(output.shape)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        output = output.contiguous().view(-1, self.hidden_size)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        #print('Linear Output :-',output.shape)\n",
    "        \n",
    "        #output = F.softmax(output, dim = 1)\n",
    "        #print('SOFTMAX OUTPUT :--', output)\n",
    "        \n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1)\n",
    "        #print('Reshape to batch size first :-',output.shape)\n",
    "        \n",
    "        output = output[:, -self.output_size:] # get last batch of labels\n",
    "        #print('Final Output :-',output)\n",
    "        #print('RESHAPE SIZE :-', output.shape)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def hidden_init(self,batch_size):\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
    "          weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "#initialize the weights of LSTM using Xavier initialization    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Divide the dataset into train/val \n",
    "'''\n",
    "train_size = 0.8\n",
    "indices = list(range(len(network_input)))\n",
    "split = int(np.floor(train_size*len(network_input)))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SequentialSampler(train_idx)\n",
    "val_sampler = SequentialSampler(val_idx)\n",
    "\n",
    "dataset = TensorDataset(network_input,network_output)\n",
    "train_loader = DataLoader(dataset, batch_size= train_batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size= val_batch_size,sampler= val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=256, out_features=81, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optimizer\n",
    "\n",
    "model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer.AdamW(model.parameters())\n",
    "#optimizer = optimizer.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "#make sure to transfer model to GPU after initializing optimizer\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 4.2916786 \tVal Loss:4.8100552 \tTrain Acc: 2.387821% \tVal Acc: 0.3205128%\n",
      "Validation Loss decreased from    inf to 4.810055, saving the model weights\n",
      "Epoch: 1\tTrain Loss: 4.2206886 \tVal Loss:4.5340360 \tTrain Acc: 1.826923% \tVal Acc: 1.8589744%\n",
      "Validation Loss decreased from 4.810055 to 4.534036, saving the model weights\n",
      "Epoch: 2\tTrain Loss: 4.1278344 \tVal Loss:4.2902694 \tTrain Acc: 2.339744% \tVal Acc: 1.6666667%\n",
      "Validation Loss decreased from 4.534036 to 4.290269, saving the model weights\n",
      "Epoch: 3\tTrain Loss: 4.1758423 \tVal Loss:4.2349818 \tTrain Acc: 2.596154% \tVal Acc: 0.6410257%\n",
      "Validation Loss decreased from 4.290269 to 4.234982, saving the model weights\n",
      "Epoch: 4\tTrain Loss: 4.0320829 \tVal Loss:4.2532448 \tTrain Acc: 3.413462% \tVal Acc: 0.3846154%\n",
      "Epoch: 5\tTrain Loss: 4.1167543 \tVal Loss:4.1602781 \tTrain Acc: 3.541667% \tVal Acc: 1.7307693%\n",
      "Validation Loss decreased from 4.234982 to 4.160278, saving the model weights\n",
      "Epoch: 6\tTrain Loss: 3.9159861 \tVal Loss:4.2272098 \tTrain Acc: 3.269231% \tVal Acc: 1.5384616%\n",
      "Epoch: 7\tTrain Loss: 3.9295558 \tVal Loss:4.2184711 \tTrain Acc: 4.326923% \tVal Acc: 1.3461539%\n",
      "Epoch: 8\tTrain Loss: 3.9428716 \tVal Loss:4.2376332 \tTrain Acc: 4.647436% \tVal Acc: 2.2435898%\n",
      "Epoch: 9\tTrain Loss: 4.2019261 \tVal Loss:4.8919650 \tTrain Acc: 1.378205% \tVal Acc: 0.0641026%\n",
      "Epoch: 10\tTrain Loss: 4.0734164 \tVal Loss:4.1587012 \tTrain Acc: 3.733974% \tVal Acc: 0.7051282%\n",
      "Validation Loss decreased from 4.160278 to 4.158701, saving the model weights\n",
      "Epoch: 11\tTrain Loss: 3.8756196 \tVal Loss:4.4223519 \tTrain Acc: 4.230769% \tVal Acc: 0.8974359%\n",
      "Epoch: 12\tTrain Loss: 3.8265392 \tVal Loss:3.8077123 \tTrain Acc: 4.326923% \tVal Acc: 3.3974360%\n",
      "Validation Loss decreased from 4.158701 to 3.807712, saving the model weights\n",
      "Epoch: 13\tTrain Loss: 3.8771940 \tVal Loss:4.6965597 \tTrain Acc: 4.198718% \tVal Acc: 0.0641026%\n",
      "Epoch: 14\tTrain Loss: 3.7897156 \tVal Loss:3.9034476 \tTrain Acc: 5.384616% \tVal Acc: 3.9102565%\n",
      "Epoch: 15\tTrain Loss: 3.7307905 \tVal Loss:4.2986725 \tTrain Acc: 5.496795% \tVal Acc: 1.1538462%\n",
      "Epoch: 16\tTrain Loss: 3.9311601 \tVal Loss:4.1428762 \tTrain Acc: 3.221154% \tVal Acc: 0.8333334%\n",
      "Epoch: 17\tTrain Loss: 3.8527848 \tVal Loss:4.0981470 \tTrain Acc: 3.782051% \tVal Acc: 0.9615385%\n",
      "Epoch: 18\tTrain Loss: 3.7823581 \tVal Loss:4.3058041 \tTrain Acc: 3.23718% \tVal Acc: 0.1923077%\n",
      "Epoch: 19\tTrain Loss: 3.7583938 \tVal Loss:4.1470175 \tTrain Acc: 3.894231% \tVal Acc: 0.1282051%\n",
      "Epoch: 20\tTrain Loss: 3.6891708 \tVal Loss:4.2016147 \tTrain Acc: 4.086539% \tVal Acc: 0.2564103%\n",
      "Epoch: 21\tTrain Loss: 3.6513291 \tVal Loss:4.4002715 \tTrain Acc: 4.567308% \tVal Acc: 0.2564103%\n",
      "Epoch: 22\tTrain Loss: 3.6474288 \tVal Loss:4.3391133 \tTrain Acc: 5.160257% \tVal Acc: 0.1923077%\n",
      "Epoch: 23\tTrain Loss: 3.6238294 \tVal Loss:4.2540496 \tTrain Acc: 5.560898% \tVal Acc: 0.1923077%\n",
      "Epoch: 24\tTrain Loss: 3.5874579 \tVal Loss:4.2952049 \tTrain Acc: 5.384616% \tVal Acc: 0.3205128%\n",
      "Epoch: 25\tTrain Loss: 3.5898188 \tVal Loss:4.2764858 \tTrain Acc: 5.560898% \tVal Acc: 0.6410257%\n",
      "Epoch: 26\tTrain Loss: 3.5517326 \tVal Loss:4.2084388 \tTrain Acc: 5.913462% \tVal Acc: 0.5769231%\n",
      "Epoch: 27\tTrain Loss: 3.5168832 \tVal Loss:4.2131721 \tTrain Acc: 5.657051% \tVal Acc: 0.7692308%\n",
      "Epoch: 28\tTrain Loss: 3.5064922 \tVal Loss:4.2045857 \tTrain Acc: 6.201923% \tVal Acc: 0.8974359%\n",
      "Epoch: 29\tTrain Loss: 3.4764321 \tVal Loss:4.1357882 \tTrain Acc: 6.971154% \tVal Acc: 1.0897436%\n",
      "Epoch: 30\tTrain Loss: 3.4737067 \tVal Loss:4.2557786 \tTrain Acc: 7.083334% \tVal Acc: 1.0256410%\n",
      "Epoch: 31\tTrain Loss: 3.4726210 \tVal Loss:4.2618714 \tTrain Acc: 6.858975% \tVal Acc: 1.2179487%\n",
      "Epoch: 32\tTrain Loss: 3.4540804 \tVal Loss:4.0641925 \tTrain Acc: 6.698718% \tVal Acc: 1.5384616%\n",
      "Epoch: 33\tTrain Loss: 3.4451434 \tVal Loss:4.1108619 \tTrain Acc: 7.179487% \tVal Acc: 1.2179487%\n",
      "Epoch: 34\tTrain Loss: 3.4280205 \tVal Loss:3.9684031 \tTrain Acc: 7.532052% \tVal Acc: 1.4743590%\n",
      "Epoch: 35\tTrain Loss: 3.4147050 \tVal Loss:4.1023815 \tTrain Acc: 7.163462% \tVal Acc: 1.4743590%\n",
      "Epoch: 36\tTrain Loss: 3.3986279 \tVal Loss:4.1211776 \tTrain Acc: 7.900641% \tVal Acc: 1.2820513%\n",
      "Epoch: 37\tTrain Loss: 3.4141084 \tVal Loss:4.0607994 \tTrain Acc: 7.339744% \tVal Acc: 0.9615385%\n",
      "Epoch: 38\tTrain Loss: 3.4233422 \tVal Loss:3.8141476 \tTrain Acc: 7.24359% \tVal Acc: 2.3076924%\n",
      "Epoch: 39\tTrain Loss: 3.4009534 \tVal Loss:3.7133745 \tTrain Acc: 7.676282% \tVal Acc: 2.2435898%\n",
      "Validation Loss decreased from 3.807712 to 3.713374, saving the model weights\n",
      "Epoch: 40\tTrain Loss: 3.3616629 \tVal Loss:3.7716051 \tTrain Acc: 8.301282% \tVal Acc: 1.2179487%\n",
      "Epoch: 41\tTrain Loss: 3.3340389 \tVal Loss:3.6618219 \tTrain Acc: 8.301282% \tVal Acc: 1.9230770%\n",
      "Validation Loss decreased from 3.713374 to 3.661822, saving the model weights\n",
      "Epoch: 42\tTrain Loss: 3.3142389 \tVal Loss:3.6818876 \tTrain Acc: 8.477564% \tVal Acc: 1.8589744%\n",
      "Epoch: 43\tTrain Loss: 3.2897486 \tVal Loss:3.6110825 \tTrain Acc: 8.894231% \tVal Acc: 3.2051283%\n",
      "Validation Loss decreased from 3.661822 to 3.611082, saving the model weights\n",
      "Epoch: 44\tTrain Loss: 3.2754654 \tVal Loss:3.7786903 \tTrain Acc: 8.926282% \tVal Acc: 1.7948718%\n",
      "Epoch: 45\tTrain Loss: 3.2558927 \tVal Loss:3.6647820 \tTrain Acc: 9.278846% \tVal Acc: 3.3974360%\n",
      "Epoch: 46\tTrain Loss: 3.2499819 \tVal Loss:3.8396548 \tTrain Acc: 9.48718% \tVal Acc: 2.3717949%\n",
      "Epoch: 47\tTrain Loss: 3.2517055 \tVal Loss:3.8505223 \tTrain Acc: 9.342949% \tVal Acc: 2.4358975%\n",
      "Epoch: 48\tTrain Loss: 3.2387699 \tVal Loss:3.9128656 \tTrain Acc: 9.807693% \tVal Acc: 2.6923077%\n",
      "Epoch: 49\tTrain Loss: 3.2253933 \tVal Loss:3.8812944 \tTrain Acc: 10.5609% \tVal Acc: 3.3333334%\n",
      "Epoch: 50\tTrain Loss: 3.2027475 \tVal Loss:3.8631143 \tTrain Acc: 10.51282% \tVal Acc: 3.6538462%\n",
      "Epoch: 51\tTrain Loss: 3.2054753 \tVal Loss:3.8613722 \tTrain Acc: 10.96154% \tVal Acc: 3.7820513%\n",
      "Epoch: 52\tTrain Loss: 3.2177242 \tVal Loss:3.8031770 \tTrain Acc: 11.58654% \tVal Acc: 3.4615385%\n",
      "Epoch: 53\tTrain Loss: 3.1749638 \tVal Loss:3.6890680 \tTrain Acc: 11.34615% \tVal Acc: 4.2307693%\n",
      "Epoch: 54\tTrain Loss: 3.1372330 \tVal Loss:3.6145191 \tTrain Acc: 12.75641% \tVal Acc: 5.5769232%\n",
      "Epoch: 55\tTrain Loss: 3.1844880 \tVal Loss:3.7713492 \tTrain Acc: 11.04167% \tVal Acc: 3.7820514%\n",
      "Epoch: 56\tTrain Loss: 3.1699892 \tVal Loss:3.7062917 \tTrain Acc: 12.82051% \tVal Acc: 4.1666668%\n",
      "Epoch: 57\tTrain Loss: 3.1610149 \tVal Loss:3.7198173 \tTrain Acc: 11.90705% \tVal Acc: 3.7820514%\n",
      "Epoch: 58\tTrain Loss: 3.1770195 \tVal Loss:3.6589473 \tTrain Acc: 12.61218% \tVal Acc: 4.2307694%\n",
      "Epoch: 59\tTrain Loss: 3.1931418 \tVal Loss:3.5489904 \tTrain Acc: 12.29167% \tVal Acc: 4.4871797%\n",
      "Validation Loss decreased from 3.611082 to 3.548990, saving the model weights\n",
      "Epoch: 60\tTrain Loss: 3.1645160 \tVal Loss:3.3811693 \tTrain Acc: 13.09295% \tVal Acc: 8.0128208%\n",
      "Validation Loss decreased from 3.548990 to 3.381169, saving the model weights\n",
      "Epoch: 61\tTrain Loss: 3.1029879 \tVal Loss:3.3735075 \tTrain Acc: 14.4391% \tVal Acc: 8.5897439%\n",
      "Validation Loss decreased from 3.381169 to 3.373507, saving the model weights\n",
      "Epoch: 62\tTrain Loss: 3.0844815 \tVal Loss:3.2981108 \tTrain Acc: 14.21474% \tVal Acc: 10.9615388%\n",
      "Validation Loss decreased from 3.373507 to 3.298111, saving the model weights\n",
      "Epoch: 63\tTrain Loss: 3.0560917 \tVal Loss:3.2716828 \tTrain Acc: 14.74359% \tVal Acc: 10.3205131%\n",
      "Validation Loss decreased from 3.298111 to 3.271683, saving the model weights\n",
      "Epoch: 64\tTrain Loss: 3.0013291 \tVal Loss:3.2850105 \tTrain Acc: 15.59295% \tVal Acc: 9.8717952%\n",
      "Epoch: 65\tTrain Loss: 2.9733961 \tVal Loss:3.1682246 \tTrain Acc: 16.05769% \tVal Acc: 14.0384618%\n",
      "Validation Loss decreased from 3.271683 to 3.168225, saving the model weights\n",
      "Epoch: 66\tTrain Loss: 2.9631363 \tVal Loss:3.1655678 \tTrain Acc: 16.3141% \tVal Acc: 14.0384619%\n",
      "Validation Loss decreased from 3.168225 to 3.165568, saving the model weights\n",
      "Epoch: 67\tTrain Loss: 2.9845707 \tVal Loss:3.1292966 \tTrain Acc: 15.11218% \tVal Acc: 14.3589748%\n",
      "Validation Loss decreased from 3.165568 to 3.129297, saving the model weights\n",
      "Epoch: 68\tTrain Loss: 2.9605416 \tVal Loss:3.1820882 \tTrain Acc: 16.61859% \tVal Acc: 13.7179491%\n",
      "Epoch: 69\tTrain Loss: 2.9184980 \tVal Loss:3.1292976 \tTrain Acc: 17.78846% \tVal Acc: 15.8974363%\n",
      "Epoch: 70\tTrain Loss: 2.8721682 \tVal Loss:3.0400700 \tTrain Acc: 18.87821% \tVal Acc: 18.0128208%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased from 3.129297 to 3.040070, saving the model weights\n",
      "Epoch: 71\tTrain Loss: 2.8416220 \tVal Loss:3.2905922 \tTrain Acc: 18.83013% \tVal Acc: 13.3333337%\n",
      "Epoch: 72\tTrain Loss: 2.8186959 \tVal Loss:3.1083117 \tTrain Acc: 20.36859% \tVal Acc: 18.2692314%\n",
      "Epoch: 73\tTrain Loss: 2.8006609 \tVal Loss:3.0943577 \tTrain Acc: 20.40064% \tVal Acc: 16.3461542%\n",
      "Epoch: 74\tTrain Loss: 2.8013651 \tVal Loss:3.0320003 \tTrain Acc: 19.98397% \tVal Acc: 18.1410262%\n",
      "Validation Loss decreased from 3.040070 to 3.032000, saving the model weights\n",
      "Epoch: 75\tTrain Loss: 2.7664860 \tVal Loss:3.1008786 \tTrain Acc: 21.57051% \tVal Acc: 17.7564106%\n",
      "Epoch: 76\tTrain Loss: 2.7855790 \tVal Loss:3.4237950 \tTrain Acc: 21.28205% \tVal Acc: 12.8205132%\n",
      "Epoch: 77\tTrain Loss: 2.7592402 \tVal Loss:3.3401946 \tTrain Acc: 21.69872% \tVal Acc: 12.6282055%\n",
      "Epoch: 78\tTrain Loss: 2.7273800 \tVal Loss:3.5540149 \tTrain Acc: 21.52244% \tVal Acc: 11.0897439%\n",
      "Epoch: 79\tTrain Loss: 2.7341997 \tVal Loss:3.2866090 \tTrain Acc: 23.26923% \tVal Acc: 12.6923079%\n",
      "Epoch: 80\tTrain Loss: 2.7696694 \tVal Loss:2.9841928 \tTrain Acc: 21.61859% \tVal Acc: 18.4615390%\n",
      "Validation Loss decreased from 3.032000 to 2.984193, saving the model weights\n",
      "Epoch: 81\tTrain Loss: 2.7173103 \tVal Loss:3.2364273 \tTrain Acc: 22.5641% \tVal Acc: 16.2820517%\n",
      "Epoch: 82\tTrain Loss: 2.7109964 \tVal Loss:3.4702964 \tTrain Acc: 23.41346% \tVal Acc: 12.3717952%\n",
      "Epoch: 83\tTrain Loss: 2.7011517 \tVal Loss:2.9612669 \tTrain Acc: 23.15705% \tVal Acc: 19.2307698%\n",
      "Validation Loss decreased from 2.984193 to 2.961267, saving the model weights\n",
      "Epoch: 84\tTrain Loss: 2.7677560 \tVal Loss:2.8488512 \tTrain Acc: 22.61218% \tVal Acc: 22.0512825%\n",
      "Validation Loss decreased from 2.961267 to 2.848851, saving the model weights\n",
      "Epoch: 85\tTrain Loss: 2.7679594 \tVal Loss:2.6799011 \tTrain Acc: 21.8109% \tVal Acc: 23.5897442%\n",
      "Validation Loss decreased from 2.848851 to 2.679901, saving the model weights\n",
      "Epoch: 86\tTrain Loss: 2.7010804 \tVal Loss:2.6991036 \tTrain Acc: 23.95833% \tVal Acc: 25.0641030%\n",
      "Epoch: 87\tTrain Loss: 2.6434603 \tVal Loss:2.8437847 \tTrain Acc: 25.16026% \tVal Acc: 22.5641029%\n",
      "Epoch: 88\tTrain Loss: 2.7035059 \tVal Loss:2.8829926 \tTrain Acc: 24.29487% \tVal Acc: 21.9230775%\n",
      "Epoch: 89\tTrain Loss: 2.7016596 \tVal Loss:2.8449478 \tTrain Acc: 23.58974% \tVal Acc: 21.0256416%\n",
      "Epoch: 90\tTrain Loss: 2.7631058 \tVal Loss:2.8468042 \tTrain Acc: 23.6218% \tVal Acc: 21.2820517%\n",
      "Epoch: 91\tTrain Loss: 2.7311462 \tVal Loss:2.8867580 \tTrain Acc: 24.07051% \tVal Acc: 20.0000005%\n",
      "Epoch: 92\tTrain Loss: 2.6432189 \tVal Loss:2.8316360 \tTrain Acc: 26.00962% \tVal Acc: 21.2179492%\n",
      "Epoch: 93\tTrain Loss: 2.6482796 \tVal Loss:2.8734140 \tTrain Acc: 26.10577% \tVal Acc: 20.4487185%\n",
      "Epoch: 94\tTrain Loss: 2.6137014 \tVal Loss:3.0197653 \tTrain Acc: 26.84295% \tVal Acc: 16.4102568%\n",
      "Epoch: 95\tTrain Loss: 2.6311267 \tVal Loss:3.0604691 \tTrain Acc: 26.55449% \tVal Acc: 15.6410260%\n",
      "Epoch: 96\tTrain Loss: 2.6072210 \tVal Loss:2.6810122 \tTrain Acc: 27.14744% \tVal Acc: 25.2564110%\n",
      "Epoch: 97\tTrain Loss: 2.5643058 \tVal Loss:2.5007053 \tTrain Acc: 27.93269% \tVal Acc: 28.8461548%\n",
      "Validation Loss decreased from 2.679901 to 2.500705, saving the model weights\n",
      "Epoch: 98\tTrain Loss: 2.5001125 \tVal Loss:2.4120771 \tTrain Acc: 29.19872% \tVal Acc: 28.9743600%\n",
      "Validation Loss decreased from 2.500705 to 2.412077, saving the model weights\n",
      "Epoch: 99\tTrain Loss: 2.4449643 \tVal Loss:2.3199541 \tTrain Acc: 29.80769% \tVal Acc: 33.3333340%\n",
      "Validation Loss decreased from 2.412077 to 2.319954, saving the model weights\n",
      "Epoch: 100\tTrain Loss: 2.4006358 \tVal Loss:2.2986957 \tTrain Acc: 31.53846% \tVal Acc: 32.8205134%\n",
      "Validation Loss decreased from 2.319954 to 2.298696, saving the model weights\n",
      "Epoch: 101\tTrain Loss: 2.3333751 \tVal Loss:2.2538369 \tTrain Acc: 33.25321% \tVal Acc: 32.3717953%\n",
      "Validation Loss decreased from 2.298696 to 2.253837, saving the model weights\n",
      "Epoch: 102\tTrain Loss: 2.3038618 \tVal Loss:2.4003968 \tTrain Acc: 33.39744% \tVal Acc: 31.1538466%\n",
      "Epoch: 103\tTrain Loss: 2.2808339 \tVal Loss:2.2694598 \tTrain Acc: 34.05449% \tVal Acc: 33.4615388%\n",
      "Epoch: 104\tTrain Loss: 2.2491267 \tVal Loss:2.2698263 \tTrain Acc: 34.75962% \tVal Acc: 33.5256417%\n",
      "Epoch: 105\tTrain Loss: 2.1892614 \tVal Loss:2.2734627 \tTrain Acc: 35.96154% \tVal Acc: 34.2307700%\n",
      "Epoch: 106\tTrain Loss: 2.1282016 \tVal Loss:2.3859871 \tTrain Acc: 37.78846% \tVal Acc: 29.2948723%\n",
      "Epoch: 107\tTrain Loss: 2.1115314 \tVal Loss:2.3677283 \tTrain Acc: 38.42949% \tVal Acc: 31.2820518%\n",
      "Epoch: 108\tTrain Loss: 2.0723204 \tVal Loss:2.2041783 \tTrain Acc: 39.23077% \tVal Acc: 34.2307697%\n",
      "Validation Loss decreased from 2.253837 to 2.204178, saving the model weights\n",
      "Epoch: 109\tTrain Loss: 2.0196400 \tVal Loss:2.1692899 \tTrain Acc: 40.01603% \tVal Acc: 35.3205134%\n",
      "Validation Loss decreased from 2.204178 to 2.169290, saving the model weights\n",
      "Epoch: 110\tTrain Loss: 2.0153119 \tVal Loss:2.0524453 \tTrain Acc: 40.625% \tVal Acc: 38.5897447%\n",
      "Validation Loss decreased from 2.169290 to 2.052445, saving the model weights\n",
      "Epoch: 111\tTrain Loss: 2.0050044 \tVal Loss:2.1553814 \tTrain Acc: 40.0641% \tVal Acc: 37.1153852%\n",
      "Epoch: 112\tTrain Loss: 1.9522134 \tVal Loss:2.1519071 \tTrain Acc: 42.29167% \tVal Acc: 37.8205135%\n",
      "Epoch: 113\tTrain Loss: 1.8826964 \tVal Loss:2.1470277 \tTrain Acc: 43.95833% \tVal Acc: 39.6153851%\n",
      "Epoch: 114\tTrain Loss: 1.8835366 \tVal Loss:1.9997751 \tTrain Acc: 42.94872% \tVal Acc: 40.6410264%\n",
      "Validation Loss decreased from 2.052445 to 1.999775, saving the model weights\n",
      "Epoch: 115\tTrain Loss: 1.8662104 \tVal Loss:2.0864053 \tTrain Acc: 43.91026% \tVal Acc: 39.8076929%\n",
      "Epoch: 116\tTrain Loss: 1.8056652 \tVal Loss:2.0765149 \tTrain Acc: 45.6891% \tVal Acc: 40.0641032%\n",
      "Epoch: 117\tTrain Loss: 1.7775694 \tVal Loss:2.0748302 \tTrain Acc: 45.96154% \tVal Acc: 39.6794879%\n",
      "Epoch: 118\tTrain Loss: 1.7799243 \tVal Loss:1.9963168 \tTrain Acc: 45.96154% \tVal Acc: 40.7051288%\n",
      "Validation Loss decreased from 1.999775 to 1.996317, saving the model weights\n",
      "Epoch: 119\tTrain Loss: 1.7366474 \tVal Loss:1.9300408 \tTrain Acc: 47.40385% \tVal Acc: 43.2051289%\n",
      "Validation Loss decreased from 1.996317 to 1.930041, saving the model weights\n",
      "Epoch: 120\tTrain Loss: 1.6947294 \tVal Loss:1.8871648 \tTrain Acc: 48.0609% \tVal Acc: 45.0641035%\n",
      "Validation Loss decreased from 1.930041 to 1.887165, saving the model weights\n",
      "Epoch: 121\tTrain Loss: 1.6639548 \tVal Loss:1.8840354 \tTrain Acc: 49.19872% \tVal Acc: 43.8461546%\n",
      "Validation Loss decreased from 1.887165 to 1.884035, saving the model weights\n",
      "Epoch: 122\tTrain Loss: 1.6297500 \tVal Loss:1.9125538 \tTrain Acc: 49.88782% \tVal Acc: 43.0769233%\n",
      "Epoch: 123\tTrain Loss: 1.6212840 \tVal Loss:1.9669654 \tTrain Acc: 49.59936% \tVal Acc: 42.3076935%\n",
      "Epoch: 124\tTrain Loss: 1.6158225 \tVal Loss:2.0139586 \tTrain Acc: 49.79167% \tVal Acc: 40.8974369%\n",
      "Epoch: 125\tTrain Loss: 1.6183405 \tVal Loss:1.8990302 \tTrain Acc: 50.04808% \tVal Acc: 44.0384625%\n",
      "Epoch: 126\tTrain Loss: 1.5737969 \tVal Loss:1.8190154 \tTrain Acc: 51.21795% \tVal Acc: 44.8717956%\n",
      "Validation Loss decreased from 1.884035 to 1.819015, saving the model weights\n",
      "Epoch: 127\tTrain Loss: 1.6233515 \tVal Loss:1.7362052 \tTrain Acc: 50.51282% \tVal Acc: 47.4358983%\n",
      "Validation Loss decreased from 1.819015 to 1.736205, saving the model weights\n",
      "Epoch: 128\tTrain Loss: 1.6105081 \tVal Loss:1.7309780 \tTrain Acc: 50.92949% \tVal Acc: 46.7307699%\n",
      "Validation Loss decreased from 1.736205 to 1.730978, saving the model weights\n",
      "Epoch: 129\tTrain Loss: 1.6321155 \tVal Loss:1.9212255 \tTrain Acc: 49.15064% \tVal Acc: 43.9102572%\n",
      "Epoch: 130\tTrain Loss: 1.6501190 \tVal Loss:1.9895231 \tTrain Acc: 50.96154% \tVal Acc: 43.2692315%\n",
      "Epoch: 131\tTrain Loss: 1.6992113 \tVal Loss:2.3282987 \tTrain Acc: 48.28526% \tVal Acc: 36.1538466%\n",
      "Epoch: 132\tTrain Loss: 1.6700471 \tVal Loss:2.3053382 \tTrain Acc: 48.49359% \tVal Acc: 34.8717956%\n",
      "Epoch: 133\tTrain Loss: 1.7103572 \tVal Loss:2.1003376 \tTrain Acc: 47.29167% \tVal Acc: 40.3205138%\n",
      "Epoch: 134\tTrain Loss: 1.5929602 \tVal Loss:1.8108695 \tTrain Acc: 50.12821% \tVal Acc: 48.7179491%\n",
      "Epoch: 135\tTrain Loss: 1.5732093 \tVal Loss:2.4259390 \tTrain Acc: 51.1859% \tVal Acc: 34.5512828%\n",
      "Epoch: 136\tTrain Loss: 1.5890321 \tVal Loss:2.4838713 \tTrain Acc: 50.78526% \tVal Acc: 31.7948727%\n",
      "Epoch: 137\tTrain Loss: 1.6000620 \tVal Loss:2.4958545 \tTrain Acc: 50.75321% \tVal Acc: 31.1538470%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 138\tTrain Loss: 1.5715097 \tVal Loss:2.3152611 \tTrain Acc: 51.89103% \tVal Acc: 36.4743600%\n",
      "Epoch: 139\tTrain Loss: 1.5354777 \tVal Loss:2.6484503 \tTrain Acc: 52.45192% \tVal Acc: 29.7435904%\n",
      "Epoch: 140\tTrain Loss: 1.4991568 \tVal Loss:2.8445349 \tTrain Acc: 53.47756% \tVal Acc: 25.3205135%\n",
      "Epoch: 141\tTrain Loss: 1.5114363 \tVal Loss:2.4227759 \tTrain Acc: 53.25321% \tVal Acc: 34.1666678%\n",
      "Epoch: 142\tTrain Loss: 1.4591032 \tVal Loss:2.3677768 \tTrain Acc: 54.15064% \tVal Acc: 34.1025649%\n",
      "Epoch: 143\tTrain Loss: 1.4304958 \tVal Loss:2.0455125 \tTrain Acc: 54.95192% \tVal Acc: 41.4102574%\n",
      "Epoch: 144\tTrain Loss: 1.4159636 \tVal Loss:2.1170943 \tTrain Acc: 54.96795% \tVal Acc: 40.5769236%\n",
      "Epoch: 145\tTrain Loss: 1.4607813 \tVal Loss:2.0329858 \tTrain Acc: 54.39103% \tVal Acc: 43.1410263%\n",
      "Epoch: 146\tTrain Loss: 1.4129068 \tVal Loss:1.8968272 \tTrain Acc: 55.33654% \tVal Acc: 45.9615391%\n",
      "Epoch: 147\tTrain Loss: 1.4125138 \tVal Loss:1.7689046 \tTrain Acc: 55.12821% \tVal Acc: 50.8333340%\n",
      "Epoch: 148\tTrain Loss: 1.4172092 \tVal Loss:1.8167240 \tTrain Acc: 55.08013% \tVal Acc: 47.5641031%\n",
      "Epoch: 149\tTrain Loss: 1.5160954 \tVal Loss:1.7674328 \tTrain Acc: 54.00641% \tVal Acc: 48.8461545%\n",
      "Epoch: 150\tTrain Loss: 1.4909231 \tVal Loss:1.8046168 \tTrain Acc: 53.99039% \tVal Acc: 50.0000010%\n",
      "Epoch: 151\tTrain Loss: 1.5038497 \tVal Loss:1.7582983 \tTrain Acc: 53.17308% \tVal Acc: 50.8333336%\n",
      "Epoch: 152\tTrain Loss: 1.4400507 \tVal Loss:1.5874373 \tTrain Acc: 54.72756% \tVal Acc: 53.4615390%\n",
      "Validation Loss decreased from 1.730978 to 1.587437, saving the model weights\n",
      "Epoch: 153\tTrain Loss: 1.4394087 \tVal Loss:1.5202687 \tTrain Acc: 55.09615% \tVal Acc: 56.7307703%\n",
      "Validation Loss decreased from 1.587437 to 1.520269, saving the model weights\n",
      "Epoch: 154\tTrain Loss: 1.3954717 \tVal Loss:1.5859090 \tTrain Acc: 56.26603% \tVal Acc: 55.5128205%\n",
      "Epoch: 155\tTrain Loss: 1.3846299 \tVal Loss:1.6200837 \tTrain Acc: 56.82692% \tVal Acc: 53.7820517%\n",
      "Epoch: 156\tTrain Loss: 1.5141475 \tVal Loss:1.6189876 \tTrain Acc: 53.28526% \tVal Acc: 52.6282057%\n",
      "Epoch: 157\tTrain Loss: 1.4601379 \tVal Loss:1.6197672 \tTrain Acc: 54.53526% \tVal Acc: 52.2435901%\n",
      "Epoch: 158\tTrain Loss: 1.4030243 \tVal Loss:1.6413013 \tTrain Acc: 55.28846% \tVal Acc: 52.6282056%\n",
      "Epoch: 159\tTrain Loss: 1.3345849 \tVal Loss:1.5317763 \tTrain Acc: 57.30769% \tVal Acc: 55.5128209%\n",
      "Epoch: 160\tTrain Loss: 1.3121147 \tVal Loss:1.4929703 \tTrain Acc: 58.17308% \tVal Acc: 57.4358979%\n",
      "Validation Loss decreased from 1.520269 to 1.492970, saving the model weights\n",
      "Epoch: 161\tTrain Loss: 1.2818949 \tVal Loss:1.5251718 \tTrain Acc: 59.51923% \tVal Acc: 57.3076929%\n",
      "Epoch: 162\tTrain Loss: 1.2630425 \tVal Loss:1.3657336 \tTrain Acc: 60.09615% \tVal Acc: 59.4871801%\n",
      "Validation Loss decreased from 1.492970 to 1.365734, saving the model weights\n",
      "Epoch: 163\tTrain Loss: 1.1962406 \tVal Loss:1.5405934 \tTrain Acc: 61.55449% \tVal Acc: 56.2820522%\n",
      "Epoch: 164\tTrain Loss: 1.2064422 \tVal Loss:1.4540754 \tTrain Acc: 61.29808% \tVal Acc: 57.7564108%\n",
      "Epoch: 165\tTrain Loss: 1.1555151 \tVal Loss:1.3987622 \tTrain Acc: 62.66026% \tVal Acc: 59.6153846%\n",
      "Epoch: 166\tTrain Loss: 1.1295564 \tVal Loss:1.4964237 \tTrain Acc: 62.90064% \tVal Acc: 56.2179485%\n",
      "Epoch: 167\tTrain Loss: 1.1709249 \tVal Loss:1.4568061 \tTrain Acc: 63.10897% \tVal Acc: 57.8205134%\n",
      "Epoch: 168\tTrain Loss: 1.1420558 \tVal Loss:1.4494823 \tTrain Acc: 63.6218% \tVal Acc: 58.0128209%\n",
      "Epoch: 169\tTrain Loss: 1.0958072 \tVal Loss:1.2412417 \tTrain Acc: 64.8718% \tVal Acc: 62.9487185%\n",
      "Validation Loss decreased from 1.365734 to 1.241242, saving the model weights\n",
      "Epoch: 170\tTrain Loss: 1.0504890 \tVal Loss:1.2476715 \tTrain Acc: 66.16987% \tVal Acc: 63.0769235%\n",
      "Epoch: 171\tTrain Loss: 1.0450367 \tVal Loss:1.2174983 \tTrain Acc: 66.16987% \tVal Acc: 64.3589748%\n",
      "Validation Loss decreased from 1.241242 to 1.217498, saving the model weights\n",
      "Epoch: 172\tTrain Loss: 1.0303445 \tVal Loss:1.3447742 \tTrain Acc: 67.16346% \tVal Acc: 61.6025648%\n",
      "Epoch: 173\tTrain Loss: 0.9782900 \tVal Loss:1.4534283 \tTrain Acc: 68.95833% \tVal Acc: 56.3461539%\n",
      "Epoch: 174\tTrain Loss: 0.9667124 \tVal Loss:1.5147572 \tTrain Acc: 69.69551% \tVal Acc: 57.1794882%\n",
      "Epoch: 175\tTrain Loss: 0.9415357 \tVal Loss:1.5925073 \tTrain Acc: 69.80769% \tVal Acc: 53.0769239%\n",
      "Epoch: 176\tTrain Loss: 0.9382379 \tVal Loss:1.6875134 \tTrain Acc: 68.95833% \tVal Acc: 52.5000004%\n",
      "Epoch: 177\tTrain Loss: 0.8954936 \tVal Loss:1.5423587 \tTrain Acc: 70.76923% \tVal Acc: 57.5641035%\n",
      "Epoch: 178\tTrain Loss: 0.9109068 \tVal Loss:1.8640824 \tTrain Acc: 70.48077% \tVal Acc: 48.6538465%\n",
      "Epoch: 179\tTrain Loss: 0.8920712 \tVal Loss:1.3827723 \tTrain Acc: 70.72115% \tVal Acc: 59.8076931%\n",
      "Epoch: 180\tTrain Loss: 0.9084503 \tVal Loss:1.4958410 \tTrain Acc: 71.08974% \tVal Acc: 58.1410263%\n",
      "Epoch: 181\tTrain Loss: 0.9747067 \tVal Loss:1.3360040 \tTrain Acc: 69.88782% \tVal Acc: 61.9871803%\n",
      "Epoch: 182\tTrain Loss: 0.9910838 \tVal Loss:1.4114185 \tTrain Acc: 68.54167% \tVal Acc: 62.1153851%\n",
      "Epoch: 183\tTrain Loss: 0.9196867 \tVal Loss:1.0865078 \tTrain Acc: 70.17628% \tVal Acc: 68.7820521%\n",
      "Validation Loss decreased from 1.217498 to 1.086508, saving the model weights\n",
      "Epoch: 184\tTrain Loss: 0.8682415 \tVal Loss:1.2279470 \tTrain Acc: 71.41026% \tVal Acc: 66.2179491%\n",
      "Epoch: 185\tTrain Loss: 0.8509000 \tVal Loss:1.1086634 \tTrain Acc: 72.4359% \tVal Acc: 69.4871798%\n",
      "Epoch: 186\tTrain Loss: 0.8502058 \tVal Loss:1.2733634 \tTrain Acc: 72.5641% \tVal Acc: 64.1025643%\n",
      "Epoch: 187\tTrain Loss: 0.8442094 \tVal Loss:1.1415746 \tTrain Acc: 72.72436% \tVal Acc: 68.5897441%\n",
      "Epoch: 188\tTrain Loss: 0.8047452 \tVal Loss:1.1342845 \tTrain Acc: 73.70192% \tVal Acc: 67.6923075%\n",
      "Epoch: 189\tTrain Loss: 0.8274195 \tVal Loss:1.4283576 \tTrain Acc: 73.42949% \tVal Acc: 60.6410263%\n",
      "Epoch: 190\tTrain Loss: 0.8699661 \tVal Loss:1.7252953 \tTrain Acc: 72.08333% \tVal Acc: 54.1666676%\n",
      "Epoch: 191\tTrain Loss: 0.8426039 \tVal Loss:1.4748537 \tTrain Acc: 72.27564% \tVal Acc: 58.0769232%\n",
      "Epoch: 192\tTrain Loss: 0.7783870 \tVal Loss:1.5016990 \tTrain Acc: 74.66346% \tVal Acc: 59.6794873%\n",
      "Epoch: 193\tTrain Loss: 0.7603148 \tVal Loss:1.6319521 \tTrain Acc: 75.5609% \tVal Acc: 55.7692312%\n",
      "Epoch: 194\tTrain Loss: 0.7502908 \tVal Loss:1.4438352 \tTrain Acc: 75.38462% \tVal Acc: 60.5769233%\n",
      "Epoch: 195\tTrain Loss: 0.7359009 \tVal Loss:1.3734992 \tTrain Acc: 75.625% \tVal Acc: 61.4102568%\n",
      "Epoch: 196\tTrain Loss: 0.7087394 \tVal Loss:1.4105603 \tTrain Acc: 76.66667% \tVal Acc: 61.0897438%\n",
      "Epoch: 197\tTrain Loss: 0.7231089 \tVal Loss:1.2619626 \tTrain Acc: 76.21795% \tVal Acc: 65.4487182%\n",
      "Epoch: 198\tTrain Loss: 0.7724391 \tVal Loss:1.1778086 \tTrain Acc: 75.01603% \tVal Acc: 68.1410262%\n",
      "Epoch: 199\tTrain Loss: 0.7789446 \tVal Loss:1.8446840 \tTrain Acc: 75.04808% \tVal Acc: 50.8333343%\n",
      "Epoch: 200\tTrain Loss: 0.7769216 \tVal Loss:1.5447249 \tTrain Acc: 75.25641% \tVal Acc: 58.3333341%\n",
      "Epoch: 201\tTrain Loss: 0.7501195 \tVal Loss:1.5083942 \tTrain Acc: 75.51282% \tVal Acc: 58.2692321%\n",
      "Epoch: 202\tTrain Loss: 0.6876207 \tVal Loss:1.7963423 \tTrain Acc: 77.67628% \tVal Acc: 53.6538472%\n",
      "Epoch: 203\tTrain Loss: 0.6648368 \tVal Loss:1.7223177 \tTrain Acc: 78.34936% \tVal Acc: 53.8461541%\n",
      "Epoch: 204\tTrain Loss: 0.6548657 \tVal Loss:1.3946040 \tTrain Acc: 78.6859% \tVal Acc: 63.0128208%\n",
      "Epoch: 205\tTrain Loss: 0.6556784 \tVal Loss:1.3049576 \tTrain Acc: 78.65385% \tVal Acc: 65.5769235%\n",
      "Epoch: 206\tTrain Loss: 0.6548465 \tVal Loss:1.2639344 \tTrain Acc: 78.91026% \tVal Acc: 66.7948718%\n",
      "Epoch: 207\tTrain Loss: 0.7203116 \tVal Loss:1.4364716 \tTrain Acc: 76.8109% \tVal Acc: 63.5256413%\n",
      "Epoch: 208\tTrain Loss: 0.6943161 \tVal Loss:1.4451827 \tTrain Acc: 77.05128% \tVal Acc: 62.3717955%\n",
      "Epoch: 209\tTrain Loss: 0.6214577 \tVal Loss:1.5378743 \tTrain Acc: 80.04808% \tVal Acc: 60.4487187%\n",
      "Epoch: 210\tTrain Loss: 0.6145580 \tVal Loss:1.6356979 \tTrain Acc: 79.74359% \tVal Acc: 57.6282060%\n",
      "Epoch: 211\tTrain Loss: 0.6582335 \tVal Loss:1.1696779 \tTrain Acc: 79.375% \tVal Acc: 68.5897440%\n",
      "Epoch: 212\tTrain Loss: 0.6965748 \tVal Loss:1.0950915 \tTrain Acc: 77.27564% \tVal Acc: 70.5128202%\n",
      "Epoch: 213\tTrain Loss: 0.6730880 \tVal Loss:0.9971565 \tTrain Acc: 77.99679% \tVal Acc: 73.2051277%\n",
      "Validation Loss decreased from 1.086508 to 0.997157, saving the model weights\n",
      "Epoch: 214\tTrain Loss: 0.6182459 \tVal Loss:1.1346011 \tTrain Acc: 80.0% \tVal Acc: 70.4487182%\n",
      "Epoch: 215\tTrain Loss: 0.6254388 \tVal Loss:1.3165223 \tTrain Acc: 79.39103% \tVal Acc: 65.7051283%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 216\tTrain Loss: 0.6590251 \tVal Loss:1.2413307 \tTrain Acc: 78.60577% \tVal Acc: 68.4615384%\n",
      "Epoch: 217\tTrain Loss: 0.6480121 \tVal Loss:1.3425993 \tTrain Acc: 79.02244% \tVal Acc: 64.1666669%\n",
      "Epoch: 218\tTrain Loss: 0.6310691 \tVal Loss:1.2248428 \tTrain Acc: 79.66346% \tVal Acc: 69.4230765%\n",
      "Epoch: 219\tTrain Loss: 0.6662827 \tVal Loss:1.3386179 \tTrain Acc: 79.05449% \tVal Acc: 64.8717947%\n",
      "Epoch: 220\tTrain Loss: 0.6239694 \tVal Loss:1.4894544 \tTrain Acc: 80.17628% \tVal Acc: 59.1666670%\n",
      "Epoch: 221\tTrain Loss: 0.6067632 \tVal Loss:1.5585535 \tTrain Acc: 80.52885% \tVal Acc: 57.6923086%\n",
      "Epoch: 222\tTrain Loss: 0.5588423 \tVal Loss:1.3588773 \tTrain Acc: 81.89103% \tVal Acc: 62.8205128%\n",
      "Epoch: 223\tTrain Loss: 0.5432906 \tVal Loss:1.4244759 \tTrain Acc: 81.79487% \tVal Acc: 63.8461542%\n",
      "Epoch: 224\tTrain Loss: 0.5399066 \tVal Loss:1.3932349 \tTrain Acc: 82.74038% \tVal Acc: 63.9102562%\n",
      "Epoch: 225\tTrain Loss: 0.5179497 \tVal Loss:1.4717496 \tTrain Acc: 82.96474% \tVal Acc: 61.9230773%\n",
      "Epoch: 226\tTrain Loss: 0.5312333 \tVal Loss:1.1749558 \tTrain Acc: 82.48397% \tVal Acc: 68.5256408%\n",
      "Epoch: 227\tTrain Loss: 0.5093444 \tVal Loss:1.2268749 \tTrain Acc: 83.2532% \tVal Acc: 67.3717950%\n",
      "Epoch: 228\tTrain Loss: 0.5248871 \tVal Loss:1.4155443 \tTrain Acc: 82.51603% \tVal Acc: 62.0512824%\n",
      "Epoch: 229\tTrain Loss: 0.5087948 \tVal Loss:1.3492441 \tTrain Acc: 83.66987% \tVal Acc: 63.4615390%\n",
      "Epoch: 230\tTrain Loss: 0.5372637 \tVal Loss:1.2429081 \tTrain Acc: 82.98077% \tVal Acc: 66.0897437%\n",
      "Epoch: 231\tTrain Loss: 0.5309431 \tVal Loss:0.9739691 \tTrain Acc: 82.83654% \tVal Acc: 74.6153843%\n",
      "Validation Loss decreased from 0.997157 to 0.973969, saving the model weights\n",
      "Epoch: 232\tTrain Loss: 0.5524913 \tVal Loss:1.4251741 \tTrain Acc: 82.05128% \tVal Acc: 61.7948724%\n",
      "Epoch: 233\tTrain Loss: 0.5275031 \tVal Loss:1.3625504 \tTrain Acc: 82.85256% \tVal Acc: 65.5128211%\n",
      "Epoch: 234\tTrain Loss: 0.5245103 \tVal Loss:1.3491858 \tTrain Acc: 83.50961% \tVal Acc: 64.8717955%\n",
      "Epoch: 235\tTrain Loss: 0.5853348 \tVal Loss:1.3668944 \tTrain Acc: 81.57051% \tVal Acc: 63.2692316%\n",
      "Epoch: 236\tTrain Loss: 0.6172076 \tVal Loss:1.7244367 \tTrain Acc: 80.91346% \tVal Acc: 56.8589747%\n",
      "Epoch: 237\tTrain Loss: 0.5185720 \tVal Loss:1.2780072 \tTrain Acc: 83.04487% \tVal Acc: 66.9871795%\n",
      "Epoch: 238\tTrain Loss: 0.4710966 \tVal Loss:1.2799967 \tTrain Acc: 85.11218% \tVal Acc: 66.9230768%\n",
      "Epoch: 239\tTrain Loss: 0.4669300 \tVal Loss:1.5145213 \tTrain Acc: 85.36859% \tVal Acc: 62.5641026%\n",
      "Epoch: 240\tTrain Loss: 0.4631671 \tVal Loss:1.8312998 \tTrain Acc: 85.35256% \tVal Acc: 54.1025646%\n",
      "Epoch: 241\tTrain Loss: 0.4850156 \tVal Loss:1.9304957 \tTrain Acc: 84.05449% \tVal Acc: 51.8589749%\n",
      "Epoch: 242\tTrain Loss: 0.4798013 \tVal Loss:1.9508789 \tTrain Acc: 84.21474% \tVal Acc: 50.3846158%\n",
      "Epoch: 243\tTrain Loss: 0.4626410 \tVal Loss:1.6601942 \tTrain Acc: 85.6891% \tVal Acc: 58.0769235%\n",
      "Epoch: 244\tTrain Loss: 0.6087313 \tVal Loss:1.9674836 \tTrain Acc: 81.02564% \tVal Acc: 49.1666672%\n",
      "Epoch: 245\tTrain Loss: 0.5730585 \tVal Loss:1.7100100 \tTrain Acc: 81.49038% \tVal Acc: 56.1538461%\n",
      "Epoch: 246\tTrain Loss: 0.4974594 \tVal Loss:1.4953444 \tTrain Acc: 83.65385% \tVal Acc: 61.7307697%\n",
      "Epoch: 247\tTrain Loss: 0.4899820 \tVal Loss:1.5321720 \tTrain Acc: 83.97436% \tVal Acc: 63.1410256%\n",
      "Epoch: 248\tTrain Loss: 0.5017499 \tVal Loss:1.2343287 \tTrain Acc: 84.11859% \tVal Acc: 69.2307697%\n",
      "Epoch: 249\tTrain Loss: 0.4872030 \tVal Loss:1.2830004 \tTrain Acc: 84.26282% \tVal Acc: 68.2692309%\n",
      "Epoch: 250\tTrain Loss: 0.4561456 \tVal Loss:1.4654490 \tTrain Acc: 85.54487% \tVal Acc: 63.3974364%\n",
      "Epoch: 251\tTrain Loss: 0.4396591 \tVal Loss:1.8373829 \tTrain Acc: 85.97756% \tVal Acc: 54.1025645%\n",
      "Epoch: 252\tTrain Loss: 0.4249809 \tVal Loss:1.6656151 \tTrain Acc: 86.42628% \tVal Acc: 58.8461539%\n",
      "Epoch: 253\tTrain Loss: 0.4171718 \tVal Loss:1.7790698 \tTrain Acc: 86.42628% \tVal Acc: 55.0000003%\n",
      "Epoch: 254\tTrain Loss: 0.4444373 \tVal Loss:1.9809909 \tTrain Acc: 85.73718% \tVal Acc: 51.0897441%\n",
      "Epoch: 255\tTrain Loss: 0.4470229 \tVal Loss:1.9929049 \tTrain Acc: 85.83333% \tVal Acc: 50.8974364%\n",
      "Epoch: 256\tTrain Loss: 0.4278400 \tVal Loss:2.0288150 \tTrain Acc: 86.26603% \tVal Acc: 51.4743595%\n",
      "Epoch: 257\tTrain Loss: 0.4181472 \tVal Loss:2.0501861 \tTrain Acc: 86.08974% \tVal Acc: 51.7307696%\n",
      "Epoch: 258\tTrain Loss: 0.4237702 \tVal Loss:1.1860764 \tTrain Acc: 86.16987% \tVal Acc: 70.0641024%\n",
      "Epoch: 259\tTrain Loss: 0.4177325 \tVal Loss:1.6932159 \tTrain Acc: 86.12179% \tVal Acc: 58.2051288%\n",
      "Epoch: 260\tTrain Loss: 0.4175780 \tVal Loss:1.7770997 \tTrain Acc: 86.26603% \tVal Acc: 56.9230773%\n",
      "Epoch: 261\tTrain Loss: 0.4263167 \tVal Loss:2.1775253 \tTrain Acc: 85.99359% \tVal Acc: 47.3717952%\n",
      "Epoch: 262\tTrain Loss: 0.4028890 \tVal Loss:1.8546034 \tTrain Acc: 86.69872% \tVal Acc: 54.2948721%\n",
      "Epoch: 263\tTrain Loss: 0.3969766 \tVal Loss:2.0430117 \tTrain Acc: 87.5% \tVal Acc: 50.6410258%\n",
      "Epoch: 264\tTrain Loss: 0.3984689 \tVal Loss:1.9930137 \tTrain Acc: 87.17949% \tVal Acc: 52.0512825%\n",
      "Epoch: 265\tTrain Loss: 0.4212531 \tVal Loss:2.2411248 \tTrain Acc: 86.66667% \tVal Acc: 47.1153853%\n",
      "Epoch: 266\tTrain Loss: 0.3867685 \tVal Loss:2.4499885 \tTrain Acc: 87.48397% \tVal Acc: 42.5000008%\n",
      "Epoch: 267\tTrain Loss: 0.4106803 \tVal Loss:2.5485472 \tTrain Acc: 86.61859% \tVal Acc: 42.7564104%\n",
      "Epoch: 268\tTrain Loss: 0.4518643 \tVal Loss:2.0541553 \tTrain Acc: 84.98397% \tVal Acc: 49.2307692%\n",
      "Epoch: 269\tTrain Loss: 0.4195159 \tVal Loss:2.4378154 \tTrain Acc: 86.71474% \tVal Acc: 45.2564107%\n",
      "Epoch: 270\tTrain Loss: 0.4078670 \tVal Loss:2.3946058 \tTrain Acc: 86.74679% \tVal Acc: 45.6410259%\n",
      "Epoch: 271\tTrain Loss: 0.3940726 \tVal Loss:2.2982655 \tTrain Acc: 86.95513% \tVal Acc: 48.7820517%\n",
      "Epoch: 272\tTrain Loss: 0.4313325 \tVal Loss:1.8193812 \tTrain Acc: 85.7532% \tVal Acc: 55.5128213%\n",
      "Epoch: 273\tTrain Loss: 0.4391972 \tVal Loss:2.0446967 \tTrain Acc: 85.48077% \tVal Acc: 52.5641032%\n",
      "Epoch: 274\tTrain Loss: 0.4041817 \tVal Loss:2.4417263 \tTrain Acc: 86.84295% \tVal Acc: 43.3333337%\n",
      "Epoch: 275\tTrain Loss: 0.4326210 \tVal Loss:1.8176906 \tTrain Acc: 86.3141% \tVal Acc: 56.9230778%\n",
      "Epoch: 276\tTrain Loss: 0.4136097 \tVal Loss:2.1284044 \tTrain Acc: 86.66667% \tVal Acc: 51.4743596%\n",
      "Epoch: 277\tTrain Loss: 0.4029022 \tVal Loss:2.2714773 \tTrain Acc: 86.875% \tVal Acc: 48.2051285%\n",
      "Epoch: 278\tTrain Loss: 0.3937437 \tVal Loss:2.6470738 \tTrain Acc: 87.01923% \tVal Acc: 41.1538467%\n",
      "Epoch: 279\tTrain Loss: 0.3567502 \tVal Loss:2.6817320 \tTrain Acc: 88.30128% \tVal Acc: 38.4615389%\n",
      "Epoch: 280\tTrain Loss: 0.3723492 \tVal Loss:2.7937734 \tTrain Acc: 88.30128% \tVal Acc: 37.7564108%\n",
      "Epoch: 281\tTrain Loss: 0.3648817 \tVal Loss:2.4790131 \tTrain Acc: 87.90064% \tVal Acc: 45.9615387%\n",
      "Epoch: 282\tTrain Loss: 0.4251458 \tVal Loss:2.1231467 \tTrain Acc: 87.0032% \tVal Acc: 52.3076925%\n",
      "Epoch: 283\tTrain Loss: 0.5267765 \tVal Loss:1.7720956 \tTrain Acc: 83.15705% \tVal Acc: 57.3717955%\n",
      "Epoch: 284\tTrain Loss: 0.4541463 \tVal Loss:2.4902295 \tTrain Acc: 85.49679% \tVal Acc: 41.1538465%\n",
      "Epoch: 285\tTrain Loss: 0.3988459 \tVal Loss:2.3810468 \tTrain Acc: 87.06731% \tVal Acc: 46.0897443%\n",
      "Epoch: 286\tTrain Loss: 0.4047007 \tVal Loss:2.4695428 \tTrain Acc: 87.38782% \tVal Acc: 46.0897440%\n",
      "Epoch: 287\tTrain Loss: 0.3773644 \tVal Loss:2.5274951 \tTrain Acc: 87.70833% \tVal Acc: 44.4871801%\n",
      "Epoch: 288\tTrain Loss: 0.3488694 \tVal Loss:2.4801178 \tTrain Acc: 89.00641% \tVal Acc: 45.6410261%\n",
      "Epoch: 289\tTrain Loss: 0.3375309 \tVal Loss:3.0102006 \tTrain Acc: 89.34295% \tVal Acc: 35.8333342%\n",
      "Epoch: 290\tTrain Loss: 0.3601273 \tVal Loss:3.3303327 \tTrain Acc: 88.99038% \tVal Acc: 30.8974363%\n",
      "Epoch: 291\tTrain Loss: 0.3442226 \tVal Loss:3.0828565 \tTrain Acc: 89.24679% \tVal Acc: 36.4743594%\n",
      "Epoch: 292\tTrain Loss: 0.3487177 \tVal Loss:2.7777188 \tTrain Acc: 88.97436% \tVal Acc: 40.5769235%\n",
      "Epoch: 293\tTrain Loss: 0.4408316 \tVal Loss:2.1429460 \tTrain Acc: 86.13782% \tVal Acc: 49.6794877%\n",
      "Epoch: 294\tTrain Loss: 0.4035279 \tVal Loss:2.3043857 \tTrain Acc: 87.21154% \tVal Acc: 48.2692310%\n",
      "Epoch: 295\tTrain Loss: 0.4242147 \tVal Loss:2.1652773 \tTrain Acc: 85.94551% \tVal Acc: 50.0641030%\n",
      "Epoch: 296\tTrain Loss: 0.3870698 \tVal Loss:2.6278609 \tTrain Acc: 87.35577% \tVal Acc: 42.7564107%\n",
      "Epoch: 297\tTrain Loss: 0.3265412 \tVal Loss:2.3390417 \tTrain Acc: 90.0% \tVal Acc: 48.1410264%\n",
      "Epoch: 298\tTrain Loss: 0.4153503 \tVal Loss:2.1608324 \tTrain Acc: 86.58654% \tVal Acc: 51.6025650%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 299\tTrain Loss: 0.3755772 \tVal Loss:2.3596328 \tTrain Acc: 87.67628% \tVal Acc: 46.9871796%\n",
      "Epoch: 300\tTrain Loss: 0.3837030 \tVal Loss:2.0577630 \tTrain Acc: 88.07692% \tVal Acc: 53.7820520%\n",
      "Epoch: 301\tTrain Loss: 0.3510749 \tVal Loss:2.4902395 \tTrain Acc: 88.84615% \tVal Acc: 48.3974359%\n",
      "Epoch: 302\tTrain Loss: 0.3325149 \tVal Loss:2.2644660 \tTrain Acc: 89.47115% \tVal Acc: 52.2435904%\n",
      "Epoch: 303\tTrain Loss: 0.3503357 \tVal Loss:2.4403075 \tTrain Acc: 89.00641% \tVal Acc: 47.8205130%\n",
      "Epoch: 304\tTrain Loss: 0.3616029 \tVal Loss:2.1150783 \tTrain Acc: 88.49359% \tVal Acc: 53.5256413%\n",
      "Epoch: 305\tTrain Loss: 0.4068109 \tVal Loss:1.8665646 \tTrain Acc: 87.24359% \tVal Acc: 57.5000004%\n",
      "Epoch: 306\tTrain Loss: 0.3572069 \tVal Loss:1.9810452 \tTrain Acc: 88.86218% \tVal Acc: 56.9230770%\n",
      "Epoch: 307\tTrain Loss: 0.3173805 \tVal Loss:1.9206533 \tTrain Acc: 90.38461% \tVal Acc: 56.9230775%\n",
      "Epoch: 308\tTrain Loss: 0.3080797 \tVal Loss:1.6451141 \tTrain Acc: 90.0641% \tVal Acc: 63.1410259%\n",
      "Epoch: 309\tTrain Loss: 0.3019556 \tVal Loss:1.9836940 \tTrain Acc: 90.38461% \tVal Acc: 55.9615387%\n",
      "Epoch: 310\tTrain Loss: 0.3233953 \tVal Loss:1.6282770 \tTrain Acc: 89.77564% \tVal Acc: 64.1666670%\n",
      "Epoch: 311\tTrain Loss: 0.4433952 \tVal Loss:1.1939462 \tTrain Acc: 87.35577% \tVal Acc: 71.2179489%\n",
      "Epoch: 312\tTrain Loss: 0.4828062 \tVal Loss:1.3331026 \tTrain Acc: 84.87179% \tVal Acc: 68.9102566%\n",
      "Epoch: 313\tTrain Loss: 0.4268862 \tVal Loss:1.1612091 \tTrain Acc: 85.76923% \tVal Acc: 70.5128206%\n",
      "Epoch: 314\tTrain Loss: 0.3840387 \tVal Loss:1.5444037 \tTrain Acc: 87.70833% \tVal Acc: 65.0000006%\n",
      "Epoch: 315\tTrain Loss: 0.3581625 \tVal Loss:1.3559550 \tTrain Acc: 88.49359% \tVal Acc: 68.6538461%\n",
      "Epoch: 316\tTrain Loss: 0.3230003 \tVal Loss:1.3541185 \tTrain Acc: 89.47115% \tVal Acc: 69.6794873%\n",
      "Epoch: 317\tTrain Loss: 0.3357946 \tVal Loss:1.2530087 \tTrain Acc: 89.32692% \tVal Acc: 70.7692310%\n",
      "Epoch: 318\tTrain Loss: 0.2926178 \tVal Loss:1.2242049 \tTrain Acc: 90.6891% \tVal Acc: 71.9871798%\n",
      "Epoch: 319\tTrain Loss: 0.2811601 \tVal Loss:1.4577469 \tTrain Acc: 91.3782% \tVal Acc: 68.0128202%\n",
      "Epoch: 320\tTrain Loss: 0.2835800 \tVal Loss:1.3548244 \tTrain Acc: 91.1859% \tVal Acc: 70.8974361%\n",
      "Epoch: 321\tTrain Loss: 0.2951573 \tVal Loss:1.6067400 \tTrain Acc: 90.60897% \tVal Acc: 64.8717952%\n",
      "Epoch: 322\tTrain Loss: 0.3249371 \tVal Loss:1.1575624 \tTrain Acc: 89.90385% \tVal Acc: 72.5641026%\n",
      "Epoch: 323\tTrain Loss: 0.3094250 \tVal Loss:0.9221480 \tTrain Acc: 90.04808% \tVal Acc: 77.4358970%\n",
      "Validation Loss decreased from 0.973969 to 0.922148, saving the model weights\n",
      "Epoch: 324\tTrain Loss: 0.3232306 \tVal Loss:1.1167718 \tTrain Acc: 89.56731% \tVal Acc: 73.9743590%\n",
      "Epoch: 325\tTrain Loss: 0.3029188 \tVal Loss:1.1174300 \tTrain Acc: 90.57692% \tVal Acc: 74.4871795%\n",
      "Epoch: 326\tTrain Loss: 0.2907168 \tVal Loss:0.9931315 \tTrain Acc: 90.81731% \tVal Acc: 75.8974361%\n",
      "Epoch: 327\tTrain Loss: 0.3000953 \tVal Loss:0.9402868 \tTrain Acc: 90.36859% \tVal Acc: 78.1410253%\n",
      "Epoch: 328\tTrain Loss: 0.2936853 \tVal Loss:1.0761543 \tTrain Acc: 90.03205% \tVal Acc: 74.8076929%\n",
      "Epoch: 329\tTrain Loss: 0.2867496 \tVal Loss:1.2889318 \tTrain Acc: 91.05769% \tVal Acc: 69.6153852%\n",
      "Epoch: 330\tTrain Loss: 0.3170505 \tVal Loss:1.2593944 \tTrain Acc: 90.08013% \tVal Acc: 72.7564105%\n",
      "Epoch: 331\tTrain Loss: 0.3541013 \tVal Loss:1.0318635 \tTrain Acc: 88.58974% \tVal Acc: 75.9615386%\n",
      "Epoch: 332\tTrain Loss: 0.3479973 \tVal Loss:1.0455757 \tTrain Acc: 88.91026% \tVal Acc: 75.7051286%\n",
      "Epoch: 333\tTrain Loss: 0.3359895 \tVal Loss:1.2629746 \tTrain Acc: 89.53526% \tVal Acc: 71.2179491%\n",
      "Epoch: 334\tTrain Loss: 0.3464629 \tVal Loss:0.8246683 \tTrain Acc: 89.19872% \tVal Acc: 80.5128205%\n",
      "Validation Loss decreased from 0.922148 to 0.824668, saving the model weights\n",
      "Epoch: 335\tTrain Loss: 0.5242192 \tVal Loss:1.4544265 \tTrain Acc: 83.66987% \tVal Acc: 64.6153848%\n",
      "Epoch: 336\tTrain Loss: 0.4366548 \tVal Loss:1.9498834 \tTrain Acc: 86.29808% \tVal Acc: 53.9743599%\n",
      "Epoch: 337\tTrain Loss: 0.3820260 \tVal Loss:2.0352814 \tTrain Acc: 87.67628% \tVal Acc: 55.3846155%\n",
      "Epoch: 338\tTrain Loss: 0.3353915 \tVal Loss:1.2399640 \tTrain Acc: 89.88782% \tVal Acc: 69.7435896%\n",
      "Epoch: 339\tTrain Loss: 0.3143897 \tVal Loss:1.4718880 \tTrain Acc: 89.67949% \tVal Acc: 65.3205133%\n",
      "Epoch: 340\tTrain Loss: 0.2920845 \tVal Loss:1.4509388 \tTrain Acc: 91.21795% \tVal Acc: 66.4102563%\n",
      "Epoch: 341\tTrain Loss: 0.2773267 \tVal Loss:1.8619022 \tTrain Acc: 91.36218% \tVal Acc: 59.6794874%\n",
      "Epoch: 342\tTrain Loss: 0.2996897 \tVal Loss:1.7981147 \tTrain Acc: 90.97756% \tVal Acc: 56.4102563%\n",
      "Epoch: 343\tTrain Loss: 0.3275780 \tVal Loss:1.8489861 \tTrain Acc: 89.72756% \tVal Acc: 58.1410260%\n",
      "Epoch: 344\tTrain Loss: 0.3422207 \tVal Loss:1.8777452 \tTrain Acc: 89.53526% \tVal Acc: 57.5641034%\n",
      "Epoch: 345\tTrain Loss: 0.3402862 \tVal Loss:1.5092193 \tTrain Acc: 89.29487% \tVal Acc: 64.8717949%\n",
      "Epoch: 346\tTrain Loss: 0.3389733 \tVal Loss:1.4535232 \tTrain Acc: 89.71154% \tVal Acc: 64.4871798%\n",
      "Epoch: 347\tTrain Loss: 0.3271350 \tVal Loss:2.0933351 \tTrain Acc: 89.88782% \tVal Acc: 53.7820523%\n",
      "Epoch: 348\tTrain Loss: 0.3031195 \tVal Loss:2.6739945 \tTrain Acc: 90.33654% \tVal Acc: 40.0641035%\n",
      "Epoch: 349\tTrain Loss: 0.2920677 \tVal Loss:1.9731290 \tTrain Acc: 90.94551% \tVal Acc: 53.8461538%\n",
      "Epoch: 350\tTrain Loss: 0.3357553 \tVal Loss:1.7354482 \tTrain Acc: 89.45513% \tVal Acc: 58.9102566%\n",
      "Epoch: 351\tTrain Loss: 0.3029749 \tVal Loss:1.8672429 \tTrain Acc: 90.5609% \tVal Acc: 58.0128207%\n",
      "Epoch: 352\tTrain Loss: 0.3006108 \tVal Loss:2.0476567 \tTrain Acc: 90.70513% \tVal Acc: 55.1923083%\n",
      "Epoch: 353\tTrain Loss: 0.3091803 \tVal Loss:1.6745342 \tTrain Acc: 90.25641% \tVal Acc: 61.7948721%\n",
      "Epoch: 354\tTrain Loss: 0.3157530 \tVal Loss:1.8857292 \tTrain Acc: 89.98397% \tVal Acc: 58.3333340%\n",
      "Epoch: 355\tTrain Loss: 0.3123481 \tVal Loss:1.8058166 \tTrain Acc: 90.19231% \tVal Acc: 60.1923079%\n",
      "Epoch: 356\tTrain Loss: 0.3026926 \tVal Loss:2.1282071 \tTrain Acc: 90.41667% \tVal Acc: 53.0769232%\n",
      "Epoch: 357\tTrain Loss: 0.3009332 \tVal Loss:1.6409610 \tTrain Acc: 90.43269% \tVal Acc: 61.0897438%\n",
      "Epoch: 358\tTrain Loss: 0.4558098 \tVal Loss:1.4788293 \tTrain Acc: 86.60256% \tVal Acc: 66.8589747%\n",
      "Epoch: 359\tTrain Loss: 0.4126256 \tVal Loss:1.3504992 \tTrain Acc: 86.82692% \tVal Acc: 68.5256413%\n",
      "Epoch: 360\tTrain Loss: 0.3442076 \tVal Loss:1.9129151 \tTrain Acc: 89.03846% \tVal Acc: 58.5897440%\n",
      "Epoch: 361\tTrain Loss: 0.2913312 \tVal Loss:1.5800861 \tTrain Acc: 91.04167% \tVal Acc: 65.1282050%\n",
      "Epoch: 362\tTrain Loss: 0.2945467 \tVal Loss:1.4515310 \tTrain Acc: 90.70513% \tVal Acc: 67.8205125%\n",
      "Epoch: 363\tTrain Loss: 0.2627480 \tVal Loss:1.4495477 \tTrain Acc: 92.01923% \tVal Acc: 66.6666676%\n",
      "Epoch: 364\tTrain Loss: 0.2565894 \tVal Loss:1.3892292 \tTrain Acc: 91.89102% \tVal Acc: 68.8461544%\n",
      "Epoch: 365\tTrain Loss: 0.2590368 \tVal Loss:1.3458255 \tTrain Acc: 92.09936% \tVal Acc: 69.4871798%\n",
      "Epoch: 366\tTrain Loss: 0.2750688 \tVal Loss:1.4494871 \tTrain Acc: 91.02564% \tVal Acc: 67.1153850%\n",
      "Epoch: 367\tTrain Loss: 0.2704840 \tVal Loss:1.1638520 \tTrain Acc: 91.23397% \tVal Acc: 73.7179492%\n",
      "Epoch: 368\tTrain Loss: 0.2788257 \tVal Loss:1.2530496 \tTrain Acc: 90.94551% \tVal Acc: 73.3333336%\n",
      "Epoch: 369\tTrain Loss: 0.2916383 \tVal Loss:1.3098983 \tTrain Acc: 90.96154% \tVal Acc: 71.6666667%\n",
      "Epoch: 370\tTrain Loss: 0.3132502 \tVal Loss:1.0753963 \tTrain Acc: 90.41667% \tVal Acc: 76.0897437%\n",
      "Epoch: 371\tTrain Loss: 0.2908585 \tVal Loss:0.9772184 \tTrain Acc: 90.97756% \tVal Acc: 77.8846154%\n",
      "Epoch: 372\tTrain Loss: 0.2861650 \tVal Loss:1.0377487 \tTrain Acc: 90.96154% \tVal Acc: 78.1410259%\n",
      "Epoch: 373\tTrain Loss: 0.2735335 \tVal Loss:1.2657973 \tTrain Acc: 91.50641% \tVal Acc: 73.3974364%\n",
      "Epoch: 374\tTrain Loss: 0.2785571 \tVal Loss:1.1821100 \tTrain Acc: 91.13782% \tVal Acc: 75.7051285%\n",
      "Epoch: 375\tTrain Loss: 0.2894930 \tVal Loss:1.0244369 \tTrain Acc: 91.1859% \tVal Acc: 77.9487181%\n",
      "Epoch: 376\tTrain Loss: 0.3003567 \tVal Loss:0.9072465 \tTrain Acc: 90.81731% \tVal Acc: 79.0384609%\n",
      "Epoch: 377\tTrain Loss: 0.2831146 \tVal Loss:0.9925588 \tTrain Acc: 90.99359% \tVal Acc: 78.0769226%\n",
      "Epoch: 378\tTrain Loss: 0.2673263 \tVal Loss:0.9492859 \tTrain Acc: 91.77885% \tVal Acc: 78.3974355%\n",
      "Epoch: 379\tTrain Loss: 0.2729843 \tVal Loss:1.1900522 \tTrain Acc: 91.13782% \tVal Acc: 74.8076925%\n",
      "Epoch: 380\tTrain Loss: 0.2427780 \tVal Loss:1.2920498 \tTrain Acc: 92.21154% \tVal Acc: 72.5641020%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 381\tTrain Loss: 0.2357230 \tVal Loss:1.2586033 \tTrain Acc: 92.6282% \tVal Acc: 73.1410259%\n",
      "Epoch: 382\tTrain Loss: 0.2365403 \tVal Loss:1.0999460 \tTrain Acc: 92.33974% \tVal Acc: 75.8974354%\n",
      "Epoch: 383\tTrain Loss: 0.2573722 \tVal Loss:1.2754140 \tTrain Acc: 91.79487% \tVal Acc: 74.0384611%\n",
      "Epoch: 384\tTrain Loss: 0.2712335 \tVal Loss:1.2569415 \tTrain Acc: 91.39423% \tVal Acc: 73.0769232%\n",
      "Epoch: 385\tTrain Loss: 0.2913628 \tVal Loss:1.2417356 \tTrain Acc: 90.89743% \tVal Acc: 72.4358970%\n",
      "Epoch: 386\tTrain Loss: 0.2821514 \tVal Loss:1.4777608 \tTrain Acc: 90.7532% \tVal Acc: 69.5512815%\n",
      "Epoch: 387\tTrain Loss: 0.2620131 \tVal Loss:1.1836226 \tTrain Acc: 91.68269% \tVal Acc: 73.2051292%\n",
      "Epoch: 388\tTrain Loss: 0.2681861 \tVal Loss:1.3687267 \tTrain Acc: 91.45833% \tVal Acc: 69.3589747%\n",
      "Epoch: 389\tTrain Loss: 0.2587979 \tVal Loss:1.2748404 \tTrain Acc: 91.76282% \tVal Acc: 71.3461539%\n",
      "Epoch: 390\tTrain Loss: 0.2626491 \tVal Loss:1.0759960 \tTrain Acc: 91.90705% \tVal Acc: 75.6410260%\n",
      "Epoch: 391\tTrain Loss: 0.2764709 \tVal Loss:1.0977463 \tTrain Acc: 91.45833% \tVal Acc: 76.7948714%\n",
      "Epoch: 392\tTrain Loss: 0.2667503 \tVal Loss:1.1794821 \tTrain Acc: 91.77885% \tVal Acc: 75.3846151%\n",
      "Epoch: 393\tTrain Loss: 0.2472521 \tVal Loss:1.1391509 \tTrain Acc: 92.17949% \tVal Acc: 76.2179486%\n",
      "Epoch: 394\tTrain Loss: 0.2197129 \tVal Loss:1.2274518 \tTrain Acc: 93.0609% \tVal Acc: 72.6282051%\n",
      "Epoch: 395\tTrain Loss: 0.2115612 \tVal Loss:1.0065988 \tTrain Acc: 93.50961% \tVal Acc: 79.4230764%\n",
      "Epoch: 396\tTrain Loss: 0.2291170 \tVal Loss:1.1706915 \tTrain Acc: 92.85256% \tVal Acc: 74.1025642%\n",
      "Epoch: 397\tTrain Loss: 0.2194117 \tVal Loss:1.0696595 \tTrain Acc: 93.01282% \tVal Acc: 74.6153845%\n",
      "Epoch: 398\tTrain Loss: 0.2191361 \tVal Loss:1.1481144 \tTrain Acc: 93.17308% \tVal Acc: 75.6410253%\n",
      "Epoch: 399\tTrain Loss: 0.2306257 \tVal Loss:1.0254594 \tTrain Acc: 92.72436% \tVal Acc: 78.2692310%\n",
      "Epoch: 400\tTrain Loss: 0.2601111 \tVal Loss:0.8329856 \tTrain Acc: 91.90705% \tVal Acc: 81.5384611%\n",
      "Epoch: 401\tTrain Loss: 0.2506228 \tVal Loss:0.8159563 \tTrain Acc: 92.14743% \tVal Acc: 82.1794870%\n",
      "Validation Loss decreased from 0.824668 to 0.815956, saving the model weights\n",
      "Epoch: 402\tTrain Loss: 0.2455865 \tVal Loss:1.0389247 \tTrain Acc: 92.32372% \tVal Acc: 77.6923078%\n",
      "Epoch: 403\tTrain Loss: 0.2362414 \tVal Loss:1.1360502 \tTrain Acc: 92.78846% \tVal Acc: 75.3205131%\n",
      "Epoch: 404\tTrain Loss: 0.2151813 \tVal Loss:0.8998877 \tTrain Acc: 93.34936% \tVal Acc: 80.3846154%\n",
      "Epoch: 405\tTrain Loss: 0.2226921 \tVal Loss:1.0564367 \tTrain Acc: 92.93269% \tVal Acc: 77.2435890%\n",
      "Epoch: 406\tTrain Loss: 0.2450230 \tVal Loss:0.8922558 \tTrain Acc: 93.04487% \tVal Acc: 81.2820509%\n",
      "Epoch: 407\tTrain Loss: 0.2189324 \tVal Loss:0.7045942 \tTrain Acc: 92.90064% \tVal Acc: 84.6153840%\n",
      "Validation Loss decreased from 0.815956 to 0.704594, saving the model weights\n",
      "Epoch: 408\tTrain Loss: 0.2149423 \tVal Loss:0.7542101 \tTrain Acc: 93.23718% \tVal Acc: 84.2307690%\n",
      "Epoch: 409\tTrain Loss: 0.2139716 \tVal Loss:0.9410512 \tTrain Acc: 93.2532% \tVal Acc: 80.3205127%\n",
      "Epoch: 410\tTrain Loss: 0.2297133 \tVal Loss:1.0162485 \tTrain Acc: 92.96474% \tVal Acc: 79.5512819%\n",
      "Epoch: 411\tTrain Loss: 0.1951438 \tVal Loss:0.7342139 \tTrain Acc: 93.75% \tVal Acc: 83.6538458%\n",
      "Epoch: 412\tTrain Loss: 0.2148922 \tVal Loss:0.9014093 \tTrain Acc: 93.62179% \tVal Acc: 80.1282045%\n",
      "Epoch: 413\tTrain Loss: 0.2426504 \tVal Loss:0.8087898 \tTrain Acc: 92.48397% \tVal Acc: 82.6282042%\n",
      "Epoch: 414\tTrain Loss: 0.2144844 \tVal Loss:0.8843614 \tTrain Acc: 93.34936% \tVal Acc: 82.1153845%\n",
      "Epoch: 415\tTrain Loss: 0.2128573 \tVal Loss:0.8319015 \tTrain Acc: 93.44551% \tVal Acc: 81.2820509%\n",
      "Epoch: 416\tTrain Loss: 0.2065174 \tVal Loss:0.8032866 \tTrain Acc: 93.6859% \tVal Acc: 83.0128199%\n",
      "Epoch: 417\tTrain Loss: 0.1889162 \tVal Loss:0.8342459 \tTrain Acc: 94.27885% \tVal Acc: 82.4999994%\n",
      "Epoch: 418\tTrain Loss: 0.2022318 \tVal Loss:0.7866675 \tTrain Acc: 93.71795% \tVal Acc: 81.9230759%\n",
      "Epoch: 419\tTrain Loss: 0.2856692 \tVal Loss:0.8874301 \tTrain Acc: 92.11538% \tVal Acc: 80.5128202%\n",
      "Epoch: 420\tTrain Loss: 0.3871301 \tVal Loss:0.8813010 \tTrain Acc: 88.60577% \tVal Acc: 79.7435901%\n",
      "Epoch: 421\tTrain Loss: 0.3124032 \tVal Loss:0.6871410 \tTrain Acc: 90.36859% \tVal Acc: 85.4487172%\n",
      "Validation Loss decreased from 0.704594 to 0.687141, saving the model weights\n",
      "Epoch: 422\tTrain Loss: 0.2717242 \tVal Loss:0.8553254 \tTrain Acc: 91.55449% \tVal Acc: 81.9230765%\n",
      "Epoch: 423\tTrain Loss: 0.2823576 \tVal Loss:0.8923489 \tTrain Acc: 91.26602% \tVal Acc: 80.7051276%\n",
      "Epoch: 424\tTrain Loss: 0.3241433 \tVal Loss:0.6643660 \tTrain Acc: 90.11218% \tVal Acc: 84.9999991%\n",
      "Validation Loss decreased from 0.687141 to 0.664366, saving the model weights\n",
      "Epoch: 425\tTrain Loss: 0.3370674 \tVal Loss:0.7894938 \tTrain Acc: 89.34295% \tVal Acc: 82.4358973%\n",
      "Epoch: 426\tTrain Loss: 0.2779041 \tVal Loss:0.9718366 \tTrain Acc: 90.80128% \tVal Acc: 80.3205124%\n",
      "Epoch: 427\tTrain Loss: 0.2310158 \tVal Loss:0.8123375 \tTrain Acc: 92.5641% \tVal Acc: 83.2051280%\n",
      "Epoch: 428\tTrain Loss: 0.2007290 \tVal Loss:0.7839805 \tTrain Acc: 93.8782% \tVal Acc: 82.8846154%\n",
      "Epoch: 429\tTrain Loss: 0.2217589 \tVal Loss:0.6048846 \tTrain Acc: 93.46154% \tVal Acc: 86.1538458%\n",
      "Validation Loss decreased from 0.664366 to 0.604885, saving the model weights\n",
      "Epoch: 430\tTrain Loss: 0.3266095 \tVal Loss:0.8241210 \tTrain Acc: 89.91987% \tVal Acc: 82.6923072%\n",
      "Epoch: 431\tTrain Loss: 0.2720649 \tVal Loss:0.8212205 \tTrain Acc: 91.45833% \tVal Acc: 82.5641018%\n",
      "Epoch: 432\tTrain Loss: 0.2091557 \tVal Loss:1.0237237 \tTrain Acc: 93.36538% \tVal Acc: 78.6538464%\n",
      "Epoch: 433\tTrain Loss: 0.2069260 \tVal Loss:0.8118358 \tTrain Acc: 93.6859% \tVal Acc: 82.5641021%\n",
      "Epoch: 434\tTrain Loss: 0.1751810 \tVal Loss:0.9564728 \tTrain Acc: 94.56731% \tVal Acc: 79.8717943%\n",
      "Epoch: 435\tTrain Loss: 0.2122045 \tVal Loss:0.7221012 \tTrain Acc: 93.99038% \tVal Acc: 83.9743584%\n",
      "Epoch: 436\tTrain Loss: 0.2246311 \tVal Loss:0.7927854 \tTrain Acc: 93.02885% \tVal Acc: 83.5256410%\n",
      "Epoch: 437\tTrain Loss: 0.2149466 \tVal Loss:0.7530983 \tTrain Acc: 93.41346% \tVal Acc: 84.5512816%\n",
      "Epoch: 438\tTrain Loss: 0.1901926 \tVal Loss:0.9480908 \tTrain Acc: 94.02243% \tVal Acc: 81.4743587%\n",
      "Epoch: 439\tTrain Loss: 0.1838400 \tVal Loss:0.9851526 \tTrain Acc: 94.35897% \tVal Acc: 81.0256404%\n",
      "Epoch: 440\tTrain Loss: 0.1753292 \tVal Loss:0.9467027 \tTrain Acc: 94.47115% \tVal Acc: 80.5769229%\n",
      "Epoch: 441\tTrain Loss: 0.1674872 \tVal Loss:0.6599273 \tTrain Acc: 94.79167% \tVal Acc: 85.8974358%\n",
      "Epoch: 442\tTrain Loss: 0.1693109 \tVal Loss:0.7557220 \tTrain Acc: 94.90385% \tVal Acc: 83.7820509%\n",
      "Epoch: 443\tTrain Loss: 0.1612191 \tVal Loss:0.8465627 \tTrain Acc: 95.36859% \tVal Acc: 82.4999994%\n",
      "Epoch: 444\tTrain Loss: 0.1483139 \tVal Loss:0.8199433 \tTrain Acc: 95.51282% \tVal Acc: 81.7948717%\n",
      "Epoch: 445\tTrain Loss: 0.1612413 \tVal Loss:0.5988066 \tTrain Acc: 94.80769% \tVal Acc: 86.9230762%\n",
      "Validation Loss decreased from 0.604885 to 0.598807, saving the model weights\n",
      "Epoch: 446\tTrain Loss: 0.1742100 \tVal Loss:0.7593474 \tTrain Acc: 94.61538% \tVal Acc: 83.5256407%\n",
      "Epoch: 447\tTrain Loss: 0.1791526 \tVal Loss:0.7548170 \tTrain Acc: 94.15064% \tVal Acc: 84.8717940%\n",
      "Epoch: 448\tTrain Loss: 0.2088974 \tVal Loss:0.8333371 \tTrain Acc: 93.8141% \tVal Acc: 82.6282048%\n",
      "Epoch: 449\tTrain Loss: 0.1993635 \tVal Loss:0.5866412 \tTrain Acc: 94.27885% \tVal Acc: 87.0512813%\n",
      "Validation Loss decreased from 0.598807 to 0.586641, saving the model weights\n",
      "Epoch: 450\tTrain Loss: 0.1922404 \tVal Loss:0.8294611 \tTrain Acc: 93.86218% \tVal Acc: 82.8205122%\n",
      "Epoch: 451\tTrain Loss: 0.2432031 \tVal Loss:0.6815274 \tTrain Acc: 92.38782% \tVal Acc: 85.3846145%\n",
      "Epoch: 452\tTrain Loss: 0.2037361 \tVal Loss:0.7043728 \tTrain Acc: 93.60577% \tVal Acc: 84.4230762%\n",
      "Epoch: 453\tTrain Loss: 0.1971499 \tVal Loss:0.5838617 \tTrain Acc: 93.92628% \tVal Acc: 86.6025636%\n",
      "Validation Loss decreased from 0.586641 to 0.583862, saving the model weights\n",
      "Epoch: 454\tTrain Loss: 0.1894755 \tVal Loss:0.7017376 \tTrain Acc: 94.19872% \tVal Acc: 85.1282045%\n",
      "Epoch: 455\tTrain Loss: 0.1973341 \tVal Loss:0.5911250 \tTrain Acc: 93.92628% \tVal Acc: 84.6794868%\n",
      "Epoch: 456\tTrain Loss: 0.2715882 \tVal Loss:0.7961953 \tTrain Acc: 92.19551% \tVal Acc: 83.7179473%\n",
      "Epoch: 457\tTrain Loss: 0.2153080 \tVal Loss:0.8732513 \tTrain Acc: 93.23718% \tVal Acc: 81.9230764%\n",
      "Epoch: 458\tTrain Loss: 0.1942381 \tVal Loss:0.7639923 \tTrain Acc: 94.11859% \tVal Acc: 83.9743581%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 459\tTrain Loss: 0.1695422 \tVal Loss:0.7004671 \tTrain Acc: 94.77564% \tVal Acc: 84.1025636%\n",
      "Epoch: 460\tTrain Loss: 0.1613816 \tVal Loss:0.6708915 \tTrain Acc: 95.11218% \tVal Acc: 85.5128193%\n",
      "Epoch: 461\tTrain Loss: 0.1479784 \tVal Loss:0.8518525 \tTrain Acc: 95.57692% \tVal Acc: 81.7307696%\n",
      "Epoch: 462\tTrain Loss: 0.1398433 \tVal Loss:0.7929719 \tTrain Acc: 95.81731% \tVal Acc: 83.3333334%\n",
      "Epoch: 463\tTrain Loss: 0.1773651 \tVal Loss:0.7638326 \tTrain Acc: 94.61538% \tVal Acc: 83.7820509%\n",
      "Epoch: 464\tTrain Loss: 0.2171463 \tVal Loss:0.6950475 \tTrain Acc: 92.94872% \tVal Acc: 84.2307693%\n",
      "Epoch: 465\tTrain Loss: 0.1897174 \tVal Loss:0.7471316 \tTrain Acc: 94.05449% \tVal Acc: 83.6538458%\n",
      "Epoch: 466\tTrain Loss: 0.2003653 \tVal Loss:0.6394550 \tTrain Acc: 93.83013% \tVal Acc: 85.5769223%\n",
      "Epoch: 467\tTrain Loss: 0.2511172 \tVal Loss:0.6127604 \tTrain Acc: 92.48397% \tVal Acc: 86.0897434%\n",
      "Epoch: 468\tTrain Loss: 0.2402385 \tVal Loss:0.7698177 \tTrain Acc: 92.66026% \tVal Acc: 82.9487178%\n",
      "Epoch: 469\tTrain Loss: 0.2262929 \tVal Loss:0.6700519 \tTrain Acc: 93.44551% \tVal Acc: 85.3846148%\n",
      "Epoch: 470\tTrain Loss: 0.2319391 \tVal Loss:0.6610264 \tTrain Acc: 92.82051% \tVal Acc: 85.9615377%\n",
      "Epoch: 471\tTrain Loss: 0.2119585 \tVal Loss:0.6208452 \tTrain Acc: 93.49359% \tVal Acc: 86.0897434%\n",
      "Epoch: 472\tTrain Loss: 0.2151547 \tVal Loss:0.7530083 \tTrain Acc: 93.57372% \tVal Acc: 84.8717949%\n",
      "Epoch: 473\tTrain Loss: 0.1906465 \tVal Loss:0.6328195 \tTrain Acc: 94.21474% \tVal Acc: 87.4999997%\n",
      "Epoch: 474\tTrain Loss: 0.1694546 \tVal Loss:0.5875790 \tTrain Acc: 94.88782% \tVal Acc: 87.7564096%\n",
      "Epoch: 475\tTrain Loss: 0.1663232 \tVal Loss:0.7206166 \tTrain Acc: 94.90384% \tVal Acc: 85.0641021%\n",
      "Epoch: 476\tTrain Loss: 0.1482028 \tVal Loss:0.6447683 \tTrain Acc: 95.44872% \tVal Acc: 86.3461527%\n",
      "Epoch: 477\tTrain Loss: 0.1577640 \tVal Loss:0.5735948 \tTrain Acc: 94.88782% \tVal Acc: 88.0128202%\n",
      "Validation Loss decreased from 0.583862 to 0.573595, saving the model weights\n",
      "Epoch: 478\tTrain Loss: 0.1501811 \tVal Loss:0.5354561 \tTrain Acc: 95.36859% \tVal Acc: 88.3974352%\n",
      "Validation Loss decreased from 0.573595 to 0.535456, saving the model weights\n",
      "Epoch: 479\tTrain Loss: 0.1411548 \tVal Loss:0.7237544 \tTrain Acc: 95.99359% \tVal Acc: 84.3589738%\n",
      "Epoch: 480\tTrain Loss: 0.1459728 \tVal Loss:0.6711199 \tTrain Acc: 95.44872% \tVal Acc: 85.6410253%\n",
      "Epoch: 481\tTrain Loss: 0.1566419 \tVal Loss:0.6715689 \tTrain Acc: 95.30449% \tVal Acc: 85.8974352%\n",
      "Epoch: 482\tTrain Loss: 0.1893434 \tVal Loss:0.8014993 \tTrain Acc: 94.27884% \tVal Acc: 82.3076913%\n",
      "Epoch: 483\tTrain Loss: 0.2045489 \tVal Loss:0.7338952 \tTrain Acc: 93.75% \tVal Acc: 84.7435895%\n",
      "Epoch: 484\tTrain Loss: 0.2174257 \tVal Loss:0.7107803 \tTrain Acc: 93.54167% \tVal Acc: 84.6153840%\n",
      "Epoch: 485\tTrain Loss: 0.1848801 \tVal Loss:0.6146931 \tTrain Acc: 94.27884% \tVal Acc: 87.4999991%\n",
      "Epoch: 486\tTrain Loss: 0.1662064 \tVal Loss:0.7028630 \tTrain Acc: 95.03205% \tVal Acc: 85.1282045%\n",
      "Epoch: 487\tTrain Loss: 0.2169131 \tVal Loss:0.5365829 \tTrain Acc: 93.94231% \tVal Acc: 88.5256404%\n",
      "Epoch: 488\tTrain Loss: 0.4739364 \tVal Loss:0.8365674 \tTrain Acc: 87.91667% \tVal Acc: 81.2820512%\n",
      "Epoch: 489\tTrain Loss: 0.6289754 \tVal Loss:0.8881594 \tTrain Acc: 82.14744% \tVal Acc: 79.2307684%\n",
      "Epoch: 490\tTrain Loss: 0.4508572 \tVal Loss:0.8147027 \tTrain Acc: 86.52243% \tVal Acc: 80.8974348%\n",
      "Epoch: 491\tTrain Loss: 0.2987325 \tVal Loss:0.8190605 \tTrain Acc: 90.72115% \tVal Acc: 82.4999994%\n",
      "Epoch: 492\tTrain Loss: 0.2141748 \tVal Loss:0.7421541 \tTrain Acc: 93.49359% \tVal Acc: 84.0384609%\n",
      "Epoch: 493\tTrain Loss: 0.1753386 \tVal Loss:0.7326885 \tTrain Acc: 94.4391% \tVal Acc: 84.9999991%\n",
      "Epoch: 494\tTrain Loss: 0.1549113 \tVal Loss:0.7328687 \tTrain Acc: 95.41667% \tVal Acc: 84.6153840%\n",
      "Epoch: 495\tTrain Loss: 0.1456082 \tVal Loss:0.7212274 \tTrain Acc: 95.625% \tVal Acc: 84.4230762%\n",
      "Epoch: 496\tTrain Loss: 0.1436504 \tVal Loss:0.5991119 \tTrain Acc: 95.84936% \tVal Acc: 86.9871789%\n",
      "Epoch: 497\tTrain Loss: 0.1415580 \tVal Loss:0.6341877 \tTrain Acc: 95.96154% \tVal Acc: 86.6666666%\n",
      "Epoch: 498\tTrain Loss: 0.1362631 \tVal Loss:0.7476850 \tTrain Acc: 95.94551% \tVal Acc: 84.2948714%\n",
      "Epoch: 499\tTrain Loss: 0.1365195 \tVal Loss:0.6402979 \tTrain Acc: 95.7532% \tVal Acc: 86.2820506%\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    \n",
    "    hidden = model.hidden_init(train_batch_size)    \n",
    "    #print('hidden[0].shape:- ',hidden[0].shape)\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        '''\n",
    "        Creating new variables for the hidden state, otherwise\n",
    "        we'd backprop through the entire training history\n",
    "        '''\n",
    "        h = tuple([each.data for each in hidden])\n",
    "        \n",
    "\n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "       \n",
    "        # get the output from the model\n",
    "        output, h = model.forward(inputs, h,train_batch_size)\n",
    "        #print('OUTPUT', output)\n",
    "        \n",
    "        \n",
    "        #print('Labels Shape :-', (torch.max(labels, 1)[1]).shape)\n",
    "    \n",
    "        # calculate the loss and perform backprop\n",
    "        #print('Labels Long :-', labels.long())\n",
    "        loss = criterion(output,labels.long())\n",
    "        #print('LOSS IS :-', loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #calculate training accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        #logging.debug(' top probab {} top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        #print(train_loss)\n",
    "              \n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "                \n",
    "        val_h = tuple([each.data for each in hidden])\n",
    "        \n",
    "        output, hidden = model.forward(inputs, val_h,val_batch_size)\n",
    "       \n",
    "        loss = criterion(output,labels.long())\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        #calculate validation accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        \n",
    "        #logging.debug(output)\n",
    "        #logging.debug('VALIDATION top probab {} VALIDATION top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        #print('Top Class:- ',top_class)\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        #print('Equals:- ', equals)\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    #Averaging losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    val_accuracy = val_accuracy/len(val_loader)\n",
    "    train_accuracy = train_accuracy/len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}\\tTrain Loss: {:.7f} \\tVal Loss:{:.7f} \\tTrain Acc: {:.7}% \\tVal Acc: {:.7f}%'.format(e, train_loss, val_loss, train_accuracy*100,val_accuracy*100))\n",
    "    \n",
    "    #saving the model if validation loss is decreased\n",
    "    if val_loss <= min_val_loss:\n",
    "        print('Validation Loss decreased from {:6f} to {:6f}, saving the model weights'.format(min_val_loss, val_loss))\n",
    "        torch.save(model.state_dict(), 'lstm_state_256_256_our_normalized_their_bakchodi_Combined_Data.pt')\n",
    "        min_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Music Genaration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=256, out_features=81, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights\n",
    "test_model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "test_model.load_state_dict(torch.load('lstm_state_256_256_our_normalized_their_bakchodi_Combined_Data.pt'))\n",
    "test_model.eval()\n",
    "test_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population database\n",
    "\n",
    "# testing_data = np.ones(500)*0\n",
    "testing_data = list(range(50,90))\n",
    "testing_data.extend(testing_data[::-1])\n",
    "testing_data_rev = testing_data[::-1]\n",
    "testing_data_rev.extend(testing_data)\n",
    "testing_data = testing_data_rev\n",
    "\n",
    "\n",
    "testing_data = np.asarray(testing_data)\n",
    "testing_data = testing_data.reshape(testing_data.shape[0],1)\n",
    "\n",
    "initial_seq = [network_input[0][1:].cpu().numpy().tolist()]\n",
    "\n",
    "testing_data_unnorm = testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "testing_data=testing_data.tolist()\n",
    "for i in range(len(testing_data)):\n",
    "    list1.extend(testing_data[i])\n",
    "\n",
    "#list1\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    list1[i]=(list1[i]-50)/(89-50)\n",
    "#     list1[i]=(list1[i])/(89)\n",
    "\n",
    "\n",
    "list1 = np.asarray(list1)\n",
    "list1 = list1.reshape(list1.shape[0],1)\n",
    "testing_data = list1\n",
    "#list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting\n",
    "def prediction_with_influence(influence,int2note,initial_seq, max_note, test_batch_size = 1):\n",
    "\n",
    "    predicted_notes = []\n",
    "    initial_seq[0].extend([[0]]*len(testing_data))\n",
    "    test_seq = torch.Tensor(initial_seq).cuda()\n",
    "    \n",
    "    test_hidden = test_model.hidden_init(test_batch_size)\n",
    "\n",
    "    \n",
    "    for i in range(len(influence)):\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = float(influence[i])\n",
    "        \n",
    "        test_slice = test_seq[0][i : i + sequence_length]        \n",
    "        test_slice = test_slice.view(1, test_slice.shape[0], test_slice.shape[1])\n",
    "        \n",
    "        test_output,_ = test_model.forward(test_slice, test_hidden, test_batch_size)\n",
    "        test_output = F.softmax(test_output, dim = 1)\n",
    "    \n",
    "        top_p, top_class = test_output.topk(1,dim =1)\n",
    "#         test_seq[0][sequence_length - 1 + i][0] = int2note[top_class.item()]/max_note\n",
    "        test_seq[0][sequence_length - 1 + i][0] = (int2note[top_class.item()] - min_midi_number)/(max_midi_number - min_midi_number)\n",
    "        \n",
    "        predicted_notes.append(int2note[top_class.item()])\n",
    "        \n",
    "    return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_notes_lst = prediction_with_influence(testing_data,int_to_note,initial_seq, max_midi_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_notes_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a06e147208>]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plt.plot(list(np.ones(500)*24))\n",
    "\n",
    "#plt.plot(list(np.ones(200)*50))\n",
    "#plt.plot(list(np.ones(20)*50))\n",
    "plt.plot(testing_data_unnorm)\n",
    "plt.plot(predicted_notes_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({89: 1,\n",
       "         86: 2,\n",
       "         60: 1,\n",
       "         44: 228,\n",
       "         48: 60,\n",
       "         38: 5,\n",
       "         41: 3,\n",
       "         35: 1,\n",
       "         34: 1,\n",
       "         33: 1,\n",
       "         52: 3,\n",
       "         46: 158,\n",
       "         53: 12,\n",
       "         49: 3,\n",
       "         45: 19,\n",
       "         47: 2})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(predicted_notes_lst)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
