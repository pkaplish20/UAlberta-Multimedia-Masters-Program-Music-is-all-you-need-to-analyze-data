{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "#import torch.utils.tensorboard as tb\n",
    "from Preprocessing.preprocessing_sorted import PreprocessingTrainingData\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as  plt\n",
    "import os\n",
    "import logging\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static parameters\n",
    "train_batch_size = 30\n",
    "val_batch_size = 30\n",
    "sequence_length=50\n",
    "test_batch_size = 1\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layer = 2\n",
    "output_size = 38\n",
    "clip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from preprocessing.py\n",
    "dataset_path = os.path.join(os.path.abspath('..'),'Dataset\\\\Clementi dataset\\\\Clementi dataset' )\n",
    "network_input,network_output,max_midi_number,min_midi_number,int_to_note = PreprocessingTrainingData().preprocess_notes(dataset_path)\n",
    "network_input, network_output = network_input.cuda(), network_output.cuda()\n",
    "\n",
    "# print(network_input)\n",
    "#print(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(network_output.max())\n",
    "print(network_output.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor(0.5618)\n",
      "89\n",
      "50\n",
      "{0: 50, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61, 11: 62, 12: 63, 13: 64, 14: 65, 15: 66, 16: 67, 17: 68, 18: 69, 19: 70, 20: 71, 21: 72, 22: 73, 23: 74, 24: 75, 25: 76, 26: 77, 27: 78, 28: 79, 29: 80, 30: 81, 31: 82, 32: 83, 33: 84, 34: 85, 35: 86, 36: 88, 37: 89}\n"
     ]
    }
   ],
   "source": [
    "print(network_input.max())\n",
    "print(network_input.min())\n",
    "print(max_midi_number)\n",
    "print(min_midi_number)\n",
    "print(int_to_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata is highly unbalanced\\n# '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data is highly unbalanced\n",
    "# '''\n",
    "# sns.distplot(torch.tensor(network_output).cpu())\n",
    "# xx = pd.DataFrame(torch.tensor(network_output).cpu())\n",
    "# xx.groupby(0).size().to_frame(name='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1800, 50, 1])\n",
      "torch.Size([1800])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "to make batch of equal sizes\n",
    "Quick Fix\n",
    "'''\n",
    "network_input = network_input[: -29]\n",
    "network_output = network_output[: -29]\n",
    "\n",
    "print(network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bakchodi Normalization\n",
    "# network_input=network_input.cpu().numpy().tolist()\n",
    "# for i in range(len(network_input)):\n",
    "#     for j in range(len(network_input[i])):\n",
    "#         network_input[i][j][0]=((network_input[i][j][0])*(max_midi_number-min_midi_number)+min_midi_number)/max_midi_number\n",
    "# network_input=torch.Tensor(network_input).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_input[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create Stacked LSTM model\n",
    "'''\n",
    "class Stacked_LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size = input_size, hidden_size = hidden_size, batch_first = True)\n",
    "        self.lstm2 = nn.LSTM(input_size = hidden_size, hidden_size = output_size,batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(output_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, batch_size):\n",
    "        \n",
    "        output, _ = self.lstm1(x)        \n",
    "        #print(output.shape)\n",
    "        \n",
    "        #output = self.dropout(output)\n",
    "        \n",
    "        output, _ = self.lstm2(output)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        output = output.contiguous().view(-1, 38)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        #print('Linear Output :-',output.shape)\n",
    "        \n",
    "        #output = F.softmax(output, dim = 1)\n",
    "        #print('SOFTMAX OUTPUT :--', output)\n",
    "        \n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1)\n",
    "        #print('Reshape to batch size first :-',output.shape)\n",
    "        \n",
    "        output = output[:, -self.output_size:] # get last batch of labels\n",
    "        #print('Final Output :-',output)\n",
    "        #print('RESHAPE SIZE :-', output.shape)\n",
    "        \n",
    "        return output\n",
    "\n",
    "#initialize the weights of LSTM using Xavier initialization    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Divide the dataset into train/val \n",
    "'''\n",
    "train_size = 0.8\n",
    "indices = list(range(len(network_input)))\n",
    "split = int(np.floor(train_size*len(network_input)))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SequentialSampler(train_idx)\n",
    "val_sampler = SequentialSampler(val_idx)\n",
    "\n",
    "dataset = TensorDataset(network_input,network_output)\n",
    "train_loader = DataLoader(dataset, batch_size= train_batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size= val_batch_size,sampler= val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm1): LSTM(1, 256, batch_first=True)\n",
       "  (lstm2): LSTM(256, 38, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=38, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optimizer\n",
    "\n",
    "model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer.AdamW(model.parameters())\n",
    "#optimizer = optimizer.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "#make sure to transfer model to GPU after initializing optimizer\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 3.6334269 \tVal Loss:3.4586551 \tTrain Acc: 3.75% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from    inf to 3.458655, saving the model weights\n",
      "Epoch: 1\tTrain Loss: 3.4967090 \tVal Loss:3.4123434 \tTrain Acc: 3.958333% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.458655 to 3.412343, saving the model weights\n",
      "Epoch: 2\tTrain Loss: 3.4343334 \tVal Loss:3.2786045 \tTrain Acc: 5.069445% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.412343 to 3.278604, saving the model weights\n",
      "Epoch: 3\tTrain Loss: 3.3904500 \tVal Loss:3.2091378 \tTrain Acc: 4.375% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.278604 to 3.209138, saving the model weights\n",
      "Epoch: 4\tTrain Loss: 3.3593954 \tVal Loss:3.1820642 \tTrain Acc: 5.0% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.209138 to 3.182064, saving the model weights\n",
      "Epoch: 5\tTrain Loss: 3.3481151 \tVal Loss:3.1723341 \tTrain Acc: 4.791667% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.182064 to 3.172334, saving the model weights\n",
      "Epoch: 6\tTrain Loss: 3.3378173 \tVal Loss:3.1640133 \tTrain Acc: 5.138889% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.172334 to 3.164013, saving the model weights\n",
      "Epoch: 7\tTrain Loss: 3.3417572 \tVal Loss:3.1610940 \tTrain Acc: 4.791667% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.164013 to 3.161094, saving the model weights\n",
      "Epoch: 8\tTrain Loss: 3.3226718 \tVal Loss:3.1490118 \tTrain Acc: 5.138889% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.161094 to 3.149012, saving the model weights\n",
      "Epoch: 9\tTrain Loss: 3.3346002 \tVal Loss:3.1511880 \tTrain Acc: 5.347222% \tVal Acc: 8.8888892%\n",
      "Epoch: 10\tTrain Loss: 3.3281567 \tVal Loss:3.1508130 \tTrain Acc: 4.166667% \tVal Acc: 8.8888892%\n",
      "Epoch: 11\tTrain Loss: 3.3208179 \tVal Loss:3.1483841 \tTrain Acc: 4.722222% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.149012 to 3.148384, saving the model weights\n",
      "Epoch: 12\tTrain Loss: 3.3279931 \tVal Loss:3.1471432 \tTrain Acc: 5.208334% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.148384 to 3.147143, saving the model weights\n",
      "Epoch: 13\tTrain Loss: 3.3113494 \tVal Loss:3.1438479 \tTrain Acc: 5.069445% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.147143 to 3.143848, saving the model weights\n",
      "Epoch: 14\tTrain Loss: 3.3088879 \tVal Loss:3.1447840 \tTrain Acc: 4.166667% \tVal Acc: 8.8888892%\n",
      "Epoch: 15\tTrain Loss: 3.3124208 \tVal Loss:3.1442040 \tTrain Acc: 4.375% \tVal Acc: 8.8888892%\n",
      "Epoch: 16\tTrain Loss: 3.3132800 \tVal Loss:3.1428233 \tTrain Acc: 4.305556% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.143848 to 3.142823, saving the model weights\n",
      "Epoch: 17\tTrain Loss: 3.3009833 \tVal Loss:3.1395284 \tTrain Acc: 4.930556% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.142823 to 3.139528, saving the model weights\n",
      "Epoch: 18\tTrain Loss: 3.3069212 \tVal Loss:3.1382080 \tTrain Acc: 5.0% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.139528 to 3.138208, saving the model weights\n",
      "Epoch: 19\tTrain Loss: 3.2981600 \tVal Loss:3.1437606 \tTrain Acc: 5.763889% \tVal Acc: 8.8888892%\n",
      "Epoch: 20\tTrain Loss: 3.3142878 \tVal Loss:3.1561315 \tTrain Acc: 4.375% \tVal Acc: 8.8888892%\n",
      "Epoch: 21\tTrain Loss: 3.3032610 \tVal Loss:3.1492681 \tTrain Acc: 5.486111% \tVal Acc: 8.8888892%\n",
      "Epoch: 22\tTrain Loss: 3.3005580 \tVal Loss:3.1439706 \tTrain Acc: 4.791667% \tVal Acc: 8.8888892%\n",
      "Epoch: 23\tTrain Loss: 3.2912820 \tVal Loss:3.1398625 \tTrain Acc: 6.25% \tVal Acc: 8.8888892%\n",
      "Epoch: 24\tTrain Loss: 3.2982128 \tVal Loss:3.1306044 \tTrain Acc: 4.930556% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.138208 to 3.130604, saving the model weights\n",
      "Epoch: 25\tTrain Loss: 3.2935092 \tVal Loss:3.1205890 \tTrain Acc: 4.652778% \tVal Acc: 3.0555557%\n",
      "Validation Loss decreased from 3.130604 to 3.120589, saving the model weights\n",
      "Epoch: 26\tTrain Loss: 3.2698161 \tVal Loss:3.1202759 \tTrain Acc: 6.25% \tVal Acc: 2.7777779%\n",
      "Validation Loss decreased from 3.120589 to 3.120276, saving the model weights\n",
      "Epoch: 27\tTrain Loss: 3.2532017 \tVal Loss:3.0699352 \tTrain Acc: 5.833334% \tVal Acc: 8.8888892%\n",
      "Validation Loss decreased from 3.120276 to 3.069935, saving the model weights\n",
      "Epoch: 28\tTrain Loss: 3.3019034 \tVal Loss:3.2015980 \tTrain Acc: 5.0% \tVal Acc: 5.0000001%\n",
      "Epoch: 29\tTrain Loss: 3.2820179 \tVal Loss:3.1086152 \tTrain Acc: 5.416667% \tVal Acc: 8.8888892%\n",
      "Epoch: 30\tTrain Loss: 3.2607413 \tVal Loss:3.1781503 \tTrain Acc: 5.347222% \tVal Acc: 8.3333337%\n",
      "Epoch: 31\tTrain Loss: 3.2656120 \tVal Loss:3.1030672 \tTrain Acc: 5.347222% \tVal Acc: 9.4444448%\n",
      "Epoch: 32\tTrain Loss: 3.2378845 \tVal Loss:3.0815851 \tTrain Acc: 5.694445% \tVal Acc: 9.4444448%\n",
      "Epoch: 33\tTrain Loss: 3.1866281 \tVal Loss:3.0622673 \tTrain Acc: 7.222223% \tVal Acc: 8.8888891%\n",
      "Validation Loss decreased from 3.069935 to 3.062267, saving the model weights\n",
      "Epoch: 34\tTrain Loss: 3.1158823 \tVal Loss:2.9695491 \tTrain Acc: 8.819445% \tVal Acc: 12.5000005%\n",
      "Validation Loss decreased from 3.062267 to 2.969549, saving the model weights\n",
      "Epoch: 35\tTrain Loss: 3.0640159 \tVal Loss:2.9555759 \tTrain Acc: 10.13889% \tVal Acc: 12.5000005%\n",
      "Validation Loss decreased from 2.969549 to 2.955576, saving the model weights\n",
      "Epoch: 36\tTrain Loss: 3.0580393 \tVal Loss:2.8900640 \tTrain Acc: 10.625% \tVal Acc: 14.4444450%\n",
      "Validation Loss decreased from 2.955576 to 2.890064, saving the model weights\n",
      "Epoch: 37\tTrain Loss: 2.9944898 \tVal Loss:2.9368939 \tTrain Acc: 11.18056% \tVal Acc: 11.9444449%\n",
      "Epoch: 38\tTrain Loss: 2.9962852 \tVal Loss:2.9404430 \tTrain Acc: 11.04167% \tVal Acc: 10.5555559%\n",
      "Epoch: 39\tTrain Loss: 2.9556054 \tVal Loss:2.9797516 \tTrain Acc: 12.08333% \tVal Acc: 8.8888892%\n",
      "Epoch: 40\tTrain Loss: 2.9370089 \tVal Loss:3.0138973 \tTrain Acc: 10.83333% \tVal Acc: 9.1666670%\n",
      "Epoch: 41\tTrain Loss: 2.9260155 \tVal Loss:3.0801119 \tTrain Acc: 11.25% \tVal Acc: 8.6111113%\n",
      "Epoch: 42\tTrain Loss: 2.9325483 \tVal Loss:2.8931533 \tTrain Acc: 10.48611% \tVal Acc: 10.0000004%\n",
      "Epoch: 43\tTrain Loss: 2.9303923 \tVal Loss:2.7938182 \tTrain Acc: 9.791667% \tVal Acc: 16.9444451%\n",
      "Validation Loss decreased from 2.890064 to 2.793818, saving the model weights\n",
      "Epoch: 44\tTrain Loss: 2.8494679 \tVal Loss:2.7991488 \tTrain Acc: 12.98611% \tVal Acc: 11.1111115%\n",
      "Epoch: 45\tTrain Loss: 2.7998856 \tVal Loss:2.7917299 \tTrain Acc: 14.86111% \tVal Acc: 9.1666670%\n",
      "Validation Loss decreased from 2.793818 to 2.791730, saving the model weights\n",
      "Epoch: 46\tTrain Loss: 2.7673521 \tVal Loss:2.8918821 \tTrain Acc: 13.61111% \tVal Acc: 9.7222225%\n",
      "Epoch: 47\tTrain Loss: 2.8192907 \tVal Loss:2.7169114 \tTrain Acc: 13.54167% \tVal Acc: 8.0555558%\n",
      "Validation Loss decreased from 2.791730 to 2.716911, saving the model weights\n",
      "Epoch: 48\tTrain Loss: 2.7663013 \tVal Loss:2.7137005 \tTrain Acc: 15.69444% \tVal Acc: 8.6111114%\n",
      "Validation Loss decreased from 2.716911 to 2.713700, saving the model weights\n",
      "Epoch: 49\tTrain Loss: 2.7784737 \tVal Loss:2.7607111 \tTrain Acc: 13.33333% \tVal Acc: 7.5000002%\n",
      "Epoch: 50\tTrain Loss: 2.7453614 \tVal Loss:2.7700424 \tTrain Acc: 15.20833% \tVal Acc: 7.5000002%\n",
      "Epoch: 51\tTrain Loss: 2.6868167 \tVal Loss:2.7702872 \tTrain Acc: 16.59722% \tVal Acc: 6.9444447%\n",
      "Epoch: 52\tTrain Loss: 2.7053083 \tVal Loss:2.7761904 \tTrain Acc: 16.66667% \tVal Acc: 7.5000002%\n",
      "Epoch: 53\tTrain Loss: 2.7366974 \tVal Loss:2.6745598 \tTrain Acc: 13.88889% \tVal Acc: 8.3333336%\n",
      "Validation Loss decreased from 2.713700 to 2.674560, saving the model weights\n",
      "Epoch: 54\tTrain Loss: 2.6174482 \tVal Loss:2.8126582 \tTrain Acc: 17.5% \tVal Acc: 8.0555559%\n",
      "Epoch: 55\tTrain Loss: 2.6671349 \tVal Loss:2.7602777 \tTrain Acc: 16.80556% \tVal Acc: 8.0555558%\n",
      "Epoch: 56\tTrain Loss: 2.6494179 \tVal Loss:2.7226859 \tTrain Acc: 17.36111% \tVal Acc: 8.3333336%\n",
      "Epoch: 57\tTrain Loss: 2.8041117 \tVal Loss:2.5808834 \tTrain Acc: 10.90278% \tVal Acc: 13.0555560%\n",
      "Validation Loss decreased from 2.674560 to 2.580883, saving the model weights\n",
      "Epoch: 58\tTrain Loss: 2.8048675 \tVal Loss:2.6316333 \tTrain Acc: 13.19444% \tVal Acc: 12.2222226%\n",
      "Epoch: 59\tTrain Loss: 2.7191555 \tVal Loss:2.6211869 \tTrain Acc: 13.19444% \tVal Acc: 11.1111114%\n",
      "Epoch: 60\tTrain Loss: 2.7091218 \tVal Loss:2.6222456 \tTrain Acc: 14.65278% \tVal Acc: 11.6666670%\n",
      "Epoch: 61\tTrain Loss: 2.6870884 \tVal Loss:2.6053190 \tTrain Acc: 13.81944% \tVal Acc: 11.1111114%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 62\tTrain Loss: 2.6936753 \tVal Loss:2.6053655 \tTrain Acc: 13.40278% \tVal Acc: 12.5000005%\n",
      "Epoch: 63\tTrain Loss: 2.6747660 \tVal Loss:2.5761303 \tTrain Acc: 15.90278% \tVal Acc: 11.9444449%\n",
      "Validation Loss decreased from 2.580883 to 2.576130, saving the model weights\n",
      "Epoch: 64\tTrain Loss: 2.6596414 \tVal Loss:2.5898408 \tTrain Acc: 15.83333% \tVal Acc: 12.5000005%\n",
      "Epoch: 65\tTrain Loss: 2.6641835 \tVal Loss:2.5824909 \tTrain Acc: 16.11111% \tVal Acc: 13.3333337%\n",
      "Epoch: 66\tTrain Loss: 2.6638518 \tVal Loss:2.5576372 \tTrain Acc: 15.41667% \tVal Acc: 12.5000004%\n",
      "Validation Loss decreased from 2.576130 to 2.557637, saving the model weights\n",
      "Epoch: 67\tTrain Loss: 2.6431375 \tVal Loss:2.5598120 \tTrain Acc: 16.66667% \tVal Acc: 14.4444449%\n",
      "Epoch: 68\tTrain Loss: 2.6338044 \tVal Loss:2.5516669 \tTrain Acc: 16.59722% \tVal Acc: 15.0000005%\n",
      "Validation Loss decreased from 2.557637 to 2.551667, saving the model weights\n",
      "Epoch: 69\tTrain Loss: 2.6372640 \tVal Loss:2.5625014 \tTrain Acc: 16.18056% \tVal Acc: 13.8888894%\n",
      "Epoch: 70\tTrain Loss: 2.6176903 \tVal Loss:2.5498144 \tTrain Acc: 17.70833% \tVal Acc: 14.4444450%\n",
      "Validation Loss decreased from 2.551667 to 2.549814, saving the model weights\n",
      "Epoch: 71\tTrain Loss: 2.6344629 \tVal Loss:2.5552819 \tTrain Acc: 16.18056% \tVal Acc: 16.3888896%\n",
      "Epoch: 72\tTrain Loss: 2.6017575 \tVal Loss:2.5268082 \tTrain Acc: 18.05556% \tVal Acc: 14.4444449%\n",
      "Validation Loss decreased from 2.549814 to 2.526808, saving the model weights\n",
      "Epoch: 73\tTrain Loss: 2.5888189 \tVal Loss:2.5154244 \tTrain Acc: 17.91667% \tVal Acc: 17.2222229%\n",
      "Validation Loss decreased from 2.526808 to 2.515424, saving the model weights\n",
      "Epoch: 74\tTrain Loss: 2.6082693 \tVal Loss:2.5156874 \tTrain Acc: 17.98611% \tVal Acc: 15.0000006%\n",
      "Epoch: 75\tTrain Loss: 2.5845130 \tVal Loss:2.5343210 \tTrain Acc: 17.5% \tVal Acc: 17.7777784%\n",
      "Epoch: 76\tTrain Loss: 2.5797768 \tVal Loss:2.5401026 \tTrain Acc: 18.54167% \tVal Acc: 15.2777783%\n",
      "Epoch: 77\tTrain Loss: 2.5598820 \tVal Loss:2.5124604 \tTrain Acc: 19.51389% \tVal Acc: 16.1111116%\n",
      "Validation Loss decreased from 2.515424 to 2.512460, saving the model weights\n",
      "Epoch: 78\tTrain Loss: 2.5524827 \tVal Loss:2.5591406 \tTrain Acc: 19.375% \tVal Acc: 13.8888892%\n",
      "Epoch: 79\tTrain Loss: 2.5888520 \tVal Loss:2.5117318 \tTrain Acc: 17.56944% \tVal Acc: 16.1111116%\n",
      "Validation Loss decreased from 2.512460 to 2.511732, saving the model weights\n",
      "Epoch: 80\tTrain Loss: 2.5544676 \tVal Loss:2.5134896 \tTrain Acc: 18.95833% \tVal Acc: 15.2777782%\n",
      "Epoch: 81\tTrain Loss: 2.5896406 \tVal Loss:2.4923543 \tTrain Acc: 17.08333% \tVal Acc: 19.1666672%\n",
      "Validation Loss decreased from 2.511732 to 2.492354, saving the model weights\n",
      "Epoch: 82\tTrain Loss: 2.5322457 \tVal Loss:2.5056489 \tTrain Acc: 19.58333% \tVal Acc: 16.3888895%\n",
      "Epoch: 83\tTrain Loss: 2.5298583 \tVal Loss:2.4922375 \tTrain Acc: 19.72222% \tVal Acc: 15.2777782%\n",
      "Validation Loss decreased from 2.492354 to 2.492237, saving the model weights\n",
      "Epoch: 84\tTrain Loss: 2.5641878 \tVal Loss:2.4721131 \tTrain Acc: 18.88889% \tVal Acc: 15.5555560%\n",
      "Validation Loss decreased from 2.492237 to 2.472113, saving the model weights\n",
      "Epoch: 85\tTrain Loss: 2.5220236 \tVal Loss:2.4902965 \tTrain Acc: 20.0% \tVal Acc: 16.6666674%\n",
      "Epoch: 86\tTrain Loss: 2.4951822 \tVal Loss:2.4219661 \tTrain Acc: 20.55556% \tVal Acc: 19.1666672%\n",
      "Validation Loss decreased from 2.472113 to 2.421966, saving the model weights\n",
      "Epoch: 87\tTrain Loss: 2.5357563 \tVal Loss:2.4790225 \tTrain Acc: 18.54167% \tVal Acc: 18.6111116%\n",
      "Epoch: 88\tTrain Loss: 2.5212703 \tVal Loss:2.4102583 \tTrain Acc: 20.90278% \tVal Acc: 21.6666675%\n",
      "Validation Loss decreased from 2.421966 to 2.410258, saving the model weights\n",
      "Epoch: 89\tTrain Loss: 2.5239844 \tVal Loss:2.3965968 \tTrain Acc: 19.93056% \tVal Acc: 22.7777787%\n",
      "Validation Loss decreased from 2.410258 to 2.396597, saving the model weights\n",
      "Epoch: 90\tTrain Loss: 2.4919759 \tVal Loss:2.4244530 \tTrain Acc: 19.79167% \tVal Acc: 20.5555561%\n",
      "Epoch: 91\tTrain Loss: 2.5267472 \tVal Loss:2.3747778 \tTrain Acc: 19.93056% \tVal Acc: 24.7222232%\n",
      "Validation Loss decreased from 2.396597 to 2.374778, saving the model weights\n",
      "Epoch: 92\tTrain Loss: 2.4893602 \tVal Loss:2.4527739 \tTrain Acc: 21.04167% \tVal Acc: 20.8333341%\n",
      "Epoch: 93\tTrain Loss: 2.4763773 \tVal Loss:2.3741210 \tTrain Acc: 20.13889% \tVal Acc: 23.6111119%\n",
      "Validation Loss decreased from 2.374778 to 2.374121, saving the model weights\n",
      "Epoch: 94\tTrain Loss: 2.4602929 \tVal Loss:2.3652782 \tTrain Acc: 22.5% \tVal Acc: 23.3333339%\n",
      "Validation Loss decreased from 2.374121 to 2.365278, saving the model weights\n",
      "Epoch: 95\tTrain Loss: 2.4396286 \tVal Loss:2.4398629 \tTrain Acc: 22.43056% \tVal Acc: 19.7222229%\n",
      "Epoch: 96\tTrain Loss: 2.4505308 \tVal Loss:2.3677909 \tTrain Acc: 22.63889% \tVal Acc: 23.6111119%\n",
      "Epoch: 97\tTrain Loss: 2.4570690 \tVal Loss:2.4255976 \tTrain Acc: 21.66667% \tVal Acc: 18.0555563%\n",
      "Epoch: 98\tTrain Loss: 2.4517469 \tVal Loss:2.4179692 \tTrain Acc: 22.15278% \tVal Acc: 20.8333341%\n",
      "Epoch: 99\tTrain Loss: 2.4478423 \tVal Loss:2.4236610 \tTrain Acc: 22.77778% \tVal Acc: 20.0000008%\n",
      "Epoch: 100\tTrain Loss: 2.4177172 \tVal Loss:2.3827869 \tTrain Acc: 23.75% \tVal Acc: 20.2777784%\n",
      "Epoch: 101\tTrain Loss: 2.4522506 \tVal Loss:2.4236134 \tTrain Acc: 22.22222% \tVal Acc: 19.1666672%\n",
      "Epoch: 102\tTrain Loss: 2.4127430 \tVal Loss:2.3719127 \tTrain Acc: 23.05556% \tVal Acc: 21.1111117%\n",
      "Epoch: 103\tTrain Loss: 2.4244583 \tVal Loss:2.3899300 \tTrain Acc: 21.94445% \tVal Acc: 20.5555563%\n",
      "Epoch: 104\tTrain Loss: 2.4194948 \tVal Loss:2.3672743 \tTrain Acc: 23.61111% \tVal Acc: 21.3888897%\n",
      "Epoch: 105\tTrain Loss: 2.3883752 \tVal Loss:2.3568142 \tTrain Acc: 25.06945% \tVal Acc: 21.9444453%\n",
      "Validation Loss decreased from 2.365278 to 2.356814, saving the model weights\n",
      "Epoch: 106\tTrain Loss: 2.4008240 \tVal Loss:2.3828507 \tTrain Acc: 23.40278% \tVal Acc: 20.5555562%\n",
      "Epoch: 107\tTrain Loss: 2.3864186 \tVal Loss:2.3429090 \tTrain Acc: 24.375% \tVal Acc: 20.8333341%\n",
      "Validation Loss decreased from 2.356814 to 2.342909, saving the model weights\n",
      "Epoch: 108\tTrain Loss: 2.3673623 \tVal Loss:2.4008992 \tTrain Acc: 24.93056% \tVal Acc: 22.5000008%\n",
      "Epoch: 109\tTrain Loss: 2.3777562 \tVal Loss:2.3846651 \tTrain Acc: 24.375% \tVal Acc: 22.5000007%\n",
      "Epoch: 110\tTrain Loss: 2.3810540 \tVal Loss:2.3457418 \tTrain Acc: 23.88889% \tVal Acc: 22.7777785%\n",
      "Epoch: 111\tTrain Loss: 2.3933078 \tVal Loss:2.4261334 \tTrain Acc: 25.13889% \tVal Acc: 21.6666672%\n",
      "Epoch: 112\tTrain Loss: 2.3514728 \tVal Loss:2.3758093 \tTrain Acc: 25.76389% \tVal Acc: 22.5000008%\n",
      "Epoch: 113\tTrain Loss: 2.3530596 \tVal Loss:2.3667434 \tTrain Acc: 25.34722% \tVal Acc: 21.1111118%\n",
      "Epoch: 114\tTrain Loss: 2.3255824 \tVal Loss:2.2879390 \tTrain Acc: 27.36111% \tVal Acc: 21.3888895%\n",
      "Validation Loss decreased from 2.342909 to 2.287939, saving the model weights\n",
      "Epoch: 115\tTrain Loss: 2.3190585 \tVal Loss:2.3283601 \tTrain Acc: 24.93056% \tVal Acc: 21.9444453%\n",
      "Epoch: 116\tTrain Loss: 2.3341763 \tVal Loss:2.2757768 \tTrain Acc: 25.69445% \tVal Acc: 22.2222229%\n",
      "Validation Loss decreased from 2.287939 to 2.275777, saving the model weights\n",
      "Epoch: 117\tTrain Loss: 2.2950333 \tVal Loss:2.3092312 \tTrain Acc: 26.52778% \tVal Acc: 20.5555563%\n",
      "Epoch: 118\tTrain Loss: 2.2877608 \tVal Loss:2.2929200 \tTrain Acc: 26.94445% \tVal Acc: 24.7222230%\n",
      "Epoch: 119\tTrain Loss: 2.3035102 \tVal Loss:2.2561689 \tTrain Acc: 26.04167% \tVal Acc: 22.7777784%\n",
      "Validation Loss decreased from 2.275777 to 2.256169, saving the model weights\n",
      "Epoch: 120\tTrain Loss: 2.2935296 \tVal Loss:2.2386696 \tTrain Acc: 26.18056% \tVal Acc: 24.7222232%\n",
      "Validation Loss decreased from 2.256169 to 2.238670, saving the model weights\n",
      "Epoch: 121\tTrain Loss: 2.3173943 \tVal Loss:2.1731918 \tTrain Acc: 26.18056% \tVal Acc: 26.6666674%\n",
      "Validation Loss decreased from 2.238670 to 2.173192, saving the model weights\n",
      "Epoch: 122\tTrain Loss: 2.2921311 \tVal Loss:2.2536424 \tTrain Acc: 26.45833% \tVal Acc: 24.1666673%\n",
      "Epoch: 123\tTrain Loss: 2.2959457 \tVal Loss:2.2850851 \tTrain Acc: 26.18056% \tVal Acc: 23.3333342%\n",
      "Epoch: 124\tTrain Loss: 2.2948483 \tVal Loss:2.2354275 \tTrain Acc: 27.43056% \tVal Acc: 20.5555564%\n",
      "Epoch: 125\tTrain Loss: 2.2597644 \tVal Loss:2.1175260 \tTrain Acc: 26.52778% \tVal Acc: 33.0555561%\n",
      "Validation Loss decreased from 2.173192 to 2.117526, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 126\tTrain Loss: 2.2637879 \tVal Loss:2.1128978 \tTrain Acc: 27.56945% \tVal Acc: 31.9444449%\n",
      "Validation Loss decreased from 2.117526 to 2.112898, saving the model weights\n",
      "Epoch: 127\tTrain Loss: 2.2608598 \tVal Loss:2.1057869 \tTrain Acc: 26.94445% \tVal Acc: 30.8333341%\n",
      "Validation Loss decreased from 2.112898 to 2.105787, saving the model weights\n",
      "Epoch: 128\tTrain Loss: 2.2643013 \tVal Loss:2.1240274 \tTrain Acc: 26.25% \tVal Acc: 32.2222228%\n",
      "Epoch: 129\tTrain Loss: 2.3341103 \tVal Loss:2.1045726 \tTrain Acc: 26.66667% \tVal Acc: 33.3333341%\n",
      "Validation Loss decreased from 2.105787 to 2.104573, saving the model weights\n",
      "Epoch: 130\tTrain Loss: 2.2619911 \tVal Loss:2.0360711 \tTrain Acc: 27.5% \tVal Acc: 35.8333339%\n",
      "Validation Loss decreased from 2.104573 to 2.036071, saving the model weights\n",
      "Epoch: 131\tTrain Loss: 2.2937876 \tVal Loss:2.0688519 \tTrain Acc: 26.875% \tVal Acc: 32.2222230%\n",
      "Epoch: 132\tTrain Loss: 2.2631938 \tVal Loss:2.0567730 \tTrain Acc: 26.73611% \tVal Acc: 35.0000006%\n",
      "Epoch: 133\tTrain Loss: 2.2605722 \tVal Loss:1.9951636 \tTrain Acc: 27.63889% \tVal Acc: 37.2222229%\n",
      "Validation Loss decreased from 2.036071 to 1.995164, saving the model weights\n",
      "Epoch: 134\tTrain Loss: 2.2961762 \tVal Loss:2.0010288 \tTrain Acc: 27.56945% \tVal Acc: 34.1666674%\n",
      "Epoch: 135\tTrain Loss: 2.2800658 \tVal Loss:2.0200367 \tTrain Acc: 25.55556% \tVal Acc: 36.3888895%\n",
      "Epoch: 136\tTrain Loss: 2.2636826 \tVal Loss:1.9954242 \tTrain Acc: 27.08333% \tVal Acc: 36.6666672%\n",
      "Epoch: 137\tTrain Loss: 2.2585511 \tVal Loss:2.0017226 \tTrain Acc: 28.54167% \tVal Acc: 35.2777784%\n",
      "Epoch: 138\tTrain Loss: 2.2336317 \tVal Loss:1.9713640 \tTrain Acc: 29.44445% \tVal Acc: 36.6666677%\n",
      "Validation Loss decreased from 1.995164 to 1.971364, saving the model weights\n",
      "Epoch: 139\tTrain Loss: 2.2206106 \tVal Loss:1.9773838 \tTrain Acc: 29.72222% \tVal Acc: 37.2222227%\n",
      "Epoch: 140\tTrain Loss: 2.2336745 \tVal Loss:1.9656247 \tTrain Acc: 27.43056% \tVal Acc: 40.5555561%\n",
      "Validation Loss decreased from 1.971364 to 1.965625, saving the model weights\n",
      "Epoch: 141\tTrain Loss: 2.1705524 \tVal Loss:1.8933663 \tTrain Acc: 32.43056% \tVal Acc: 40.2777784%\n",
      "Validation Loss decreased from 1.965625 to 1.893366, saving the model weights\n",
      "Epoch: 142\tTrain Loss: 2.1430446 \tVal Loss:1.8547977 \tTrain Acc: 32.5% \tVal Acc: 41.6666675%\n",
      "Validation Loss decreased from 1.893366 to 1.854798, saving the model weights\n",
      "Epoch: 143\tTrain Loss: 2.1285494 \tVal Loss:1.8170417 \tTrain Acc: 30.83333% \tVal Acc: 41.9444449%\n",
      "Validation Loss decreased from 1.854798 to 1.817042, saving the model weights\n",
      "Epoch: 144\tTrain Loss: 2.1195796 \tVal Loss:1.8307120 \tTrain Acc: 32.29167% \tVal Acc: 41.9444450%\n",
      "Epoch: 145\tTrain Loss: 2.0937273 \tVal Loss:1.8369294 \tTrain Acc: 32.70833% \tVal Acc: 40.2777782%\n",
      "Epoch: 146\tTrain Loss: 2.0937267 \tVal Loss:1.8562601 \tTrain Acc: 33.54167% \tVal Acc: 40.8333343%\n",
      "Epoch: 147\tTrain Loss: 2.0820190 \tVal Loss:1.7768961 \tTrain Acc: 34.02778% \tVal Acc: 43.3333344%\n",
      "Validation Loss decreased from 1.817042 to 1.776896, saving the model weights\n",
      "Epoch: 148\tTrain Loss: 2.0578532 \tVal Loss:1.8362297 \tTrain Acc: 34.65278% \tVal Acc: 44.1666671%\n",
      "Epoch: 149\tTrain Loss: 2.0351076 \tVal Loss:1.7742999 \tTrain Acc: 36.18056% \tVal Acc: 45.5555564%\n",
      "Validation Loss decreased from 1.776896 to 1.774300, saving the model weights\n",
      "Epoch: 150\tTrain Loss: 2.0352304 \tVal Loss:1.8353085 \tTrain Acc: 35.06945% \tVal Acc: 41.9444455%\n",
      "Epoch: 151\tTrain Loss: 2.0515647 \tVal Loss:1.8089853 \tTrain Acc: 35.76389% \tVal Acc: 45.2777783%\n",
      "Epoch: 152\tTrain Loss: 2.0262174 \tVal Loss:1.7580552 \tTrain Acc: 34.30556% \tVal Acc: 48.0555562%\n",
      "Validation Loss decreased from 1.774300 to 1.758055, saving the model weights\n",
      "Epoch: 153\tTrain Loss: 2.0320763 \tVal Loss:1.8332351 \tTrain Acc: 35.27778% \tVal Acc: 45.5555566%\n",
      "Epoch: 154\tTrain Loss: 2.0259260 \tVal Loss:1.6852398 \tTrain Acc: 36.04167% \tVal Acc: 51.9444451%\n",
      "Validation Loss decreased from 1.758055 to 1.685240, saving the model weights\n",
      "Epoch: 155\tTrain Loss: 1.9876716 \tVal Loss:1.7328890 \tTrain Acc: 36.45833% \tVal Acc: 43.0555563%\n",
      "Epoch: 156\tTrain Loss: 1.9239078 \tVal Loss:1.8150453 \tTrain Acc: 38.19445% \tVal Acc: 42.2222228%\n",
      "Epoch: 157\tTrain Loss: 2.0157805 \tVal Loss:1.8150890 \tTrain Acc: 35.625% \tVal Acc: 44.4444453%\n",
      "Epoch: 158\tTrain Loss: 1.9957123 \tVal Loss:1.7800255 \tTrain Acc: 37.77778% \tVal Acc: 48.6111113%\n",
      "Epoch: 159\tTrain Loss: 1.9780848 \tVal Loss:1.7082248 \tTrain Acc: 35.97222% \tVal Acc: 48.8888892%\n",
      "Epoch: 160\tTrain Loss: 1.9438405 \tVal Loss:1.6859846 \tTrain Acc: 37.29167% \tVal Acc: 46.6666674%\n",
      "Epoch: 161\tTrain Loss: 1.9288431 \tVal Loss:1.6998232 \tTrain Acc: 37.77778% \tVal Acc: 47.5000008%\n",
      "Epoch: 162\tTrain Loss: 1.9152060 \tVal Loss:1.6455863 \tTrain Acc: 38.75% \tVal Acc: 50.0000009%\n",
      "Validation Loss decreased from 1.685240 to 1.645586, saving the model weights\n",
      "Epoch: 163\tTrain Loss: 1.9121755 \tVal Loss:1.6371613 \tTrain Acc: 39.86111% \tVal Acc: 52.2222227%\n",
      "Validation Loss decreased from 1.645586 to 1.637161, saving the model weights\n",
      "Epoch: 164\tTrain Loss: 1.9133881 \tVal Loss:1.7013834 \tTrain Acc: 39.51389% \tVal Acc: 47.7777789%\n",
      "Epoch: 165\tTrain Loss: 1.8644039 \tVal Loss:1.6018812 \tTrain Acc: 39.86111% \tVal Acc: 55.0000004%\n",
      "Validation Loss decreased from 1.637161 to 1.601881, saving the model weights\n",
      "Epoch: 166\tTrain Loss: 1.8601380 \tVal Loss:1.5891253 \tTrain Acc: 40.90278% \tVal Acc: 53.0555567%\n",
      "Validation Loss decreased from 1.601881 to 1.589125, saving the model weights\n",
      "Epoch: 167\tTrain Loss: 1.8530070 \tVal Loss:1.6198775 \tTrain Acc: 40.13889% \tVal Acc: 52.2222232%\n",
      "Epoch: 168\tTrain Loss: 1.8552773 \tVal Loss:1.5800986 \tTrain Acc: 40.48611% \tVal Acc: 53.8888897%\n",
      "Validation Loss decreased from 1.589125 to 1.580099, saving the model weights\n",
      "Epoch: 169\tTrain Loss: 1.8185260 \tVal Loss:1.5224719 \tTrain Acc: 42.22222% \tVal Acc: 55.0000014%\n",
      "Validation Loss decreased from 1.580099 to 1.522472, saving the model weights\n",
      "Epoch: 170\tTrain Loss: 1.7927270 \tVal Loss:1.6385627 \tTrain Acc: 43.61111% \tVal Acc: 50.8333339%\n",
      "Epoch: 171\tTrain Loss: 1.8854856 \tVal Loss:1.4657763 \tTrain Acc: 40.20833% \tVal Acc: 58.6111111%\n",
      "Validation Loss decreased from 1.522472 to 1.465776, saving the model weights\n",
      "Epoch: 172\tTrain Loss: 1.7980468 \tVal Loss:1.4907328 \tTrain Acc: 41.59722% \tVal Acc: 57.5000002%\n",
      "Epoch: 173\tTrain Loss: 1.7775366 \tVal Loss:1.4481827 \tTrain Acc: 42.43056% \tVal Acc: 61.1111114%\n",
      "Validation Loss decreased from 1.465776 to 1.448183, saving the model weights\n",
      "Epoch: 174\tTrain Loss: 1.7449628 \tVal Loss:1.4439263 \tTrain Acc: 44.30556% \tVal Acc: 57.7777778%\n",
      "Validation Loss decreased from 1.448183 to 1.443926, saving the model weights\n",
      "Epoch: 175\tTrain Loss: 1.7088914 \tVal Loss:1.4118463 \tTrain Acc: 45.27778% \tVal Acc: 62.5000007%\n",
      "Validation Loss decreased from 1.443926 to 1.411846, saving the model weights\n",
      "Epoch: 176\tTrain Loss: 1.6911223 \tVal Loss:1.3667881 \tTrain Acc: 46.18056% \tVal Acc: 61.3888890%\n",
      "Validation Loss decreased from 1.411846 to 1.366788, saving the model weights\n",
      "Epoch: 177\tTrain Loss: 1.6966453 \tVal Loss:1.3395451 \tTrain Acc: 46.66667% \tVal Acc: 65.0000006%\n",
      "Validation Loss decreased from 1.366788 to 1.339545, saving the model weights\n",
      "Epoch: 178\tTrain Loss: 1.6325643 \tVal Loss:1.3007715 \tTrain Acc: 46.875% \tVal Acc: 62.7777778%\n",
      "Validation Loss decreased from 1.339545 to 1.300771, saving the model weights\n",
      "Epoch: 179\tTrain Loss: 1.6349372 \tVal Loss:1.3921822 \tTrain Acc: 47.15278% \tVal Acc: 61.9444450%\n",
      "Epoch: 180\tTrain Loss: 1.6715457 \tVal Loss:1.3468733 \tTrain Acc: 46.94445% \tVal Acc: 63.8888896%\n",
      "Epoch: 181\tTrain Loss: 1.6989079 \tVal Loss:1.4367415 \tTrain Acc: 45.41667% \tVal Acc: 58.8888897%\n",
      "Epoch: 182\tTrain Loss: 1.6949582 \tVal Loss:1.4002591 \tTrain Acc: 45.41667% \tVal Acc: 60.8333341%\n",
      "Epoch: 183\tTrain Loss: 1.6122416 \tVal Loss:1.2760136 \tTrain Acc: 47.43056% \tVal Acc: 63.3333340%\n",
      "Validation Loss decreased from 1.300771 to 1.276014, saving the model weights\n",
      "Epoch: 184\tTrain Loss: 1.6039961 \tVal Loss:1.2151316 \tTrain Acc: 48.125% \tVal Acc: 69.1666663%\n",
      "Validation Loss decreased from 1.276014 to 1.215132, saving the model weights\n",
      "Epoch: 185\tTrain Loss: 1.5748500 \tVal Loss:1.4402485 \tTrain Acc: 49.65278% \tVal Acc: 58.6111116%\n",
      "Epoch: 186\tTrain Loss: 1.7053988 \tVal Loss:1.2748654 \tTrain Acc: 45.27778% \tVal Acc: 66.9444449%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 187\tTrain Loss: 1.5847565 \tVal Loss:1.2703393 \tTrain Acc: 48.05556% \tVal Acc: 66.6666662%\n",
      "Epoch: 188\tTrain Loss: 1.5461112 \tVal Loss:1.1684767 \tTrain Acc: 50.76389% \tVal Acc: 70.2777786%\n",
      "Validation Loss decreased from 1.215132 to 1.168477, saving the model weights\n",
      "Epoch: 189\tTrain Loss: 1.5133356 \tVal Loss:1.1954581 \tTrain Acc: 52.91667% \tVal Acc: 68.3333335%\n",
      "Epoch: 190\tTrain Loss: 1.5221955 \tVal Loss:1.2206020 \tTrain Acc: 52.84722% \tVal Acc: 66.6666674%\n",
      "Epoch: 191\tTrain Loss: 1.5258612 \tVal Loss:1.1512225 \tTrain Acc: 50.90278% \tVal Acc: 68.6111117%\n",
      "Validation Loss decreased from 1.168477 to 1.151223, saving the model weights\n",
      "Epoch: 192\tTrain Loss: 1.5051169 \tVal Loss:1.1285702 \tTrain Acc: 50.34722% \tVal Acc: 70.0000008%\n",
      "Validation Loss decreased from 1.151223 to 1.128570, saving the model weights\n",
      "Epoch: 193\tTrain Loss: 1.5003233 \tVal Loss:1.1124006 \tTrain Acc: 50.34722% \tVal Acc: 73.3333339%\n",
      "Validation Loss decreased from 1.128570 to 1.112401, saving the model weights\n",
      "Epoch: 194\tTrain Loss: 1.4651270 \tVal Loss:1.0882773 \tTrain Acc: 53.47222% \tVal Acc: 72.2222226%\n",
      "Validation Loss decreased from 1.112401 to 1.088277, saving the model weights\n",
      "Epoch: 195\tTrain Loss: 1.4403499 \tVal Loss:1.0632451 \tTrain Acc: 54.58333% \tVal Acc: 74.9999998%\n",
      "Validation Loss decreased from 1.088277 to 1.063245, saving the model weights\n",
      "Epoch: 196\tTrain Loss: 1.4464217 \tVal Loss:1.0145349 \tTrain Acc: 55.13889% \tVal Acc: 73.6111114%\n",
      "Validation Loss decreased from 1.063245 to 1.014535, saving the model weights\n",
      "Epoch: 197\tTrain Loss: 1.4116304 \tVal Loss:1.0341732 \tTrain Acc: 54.72222% \tVal Acc: 75.5555555%\n",
      "Epoch: 198\tTrain Loss: 1.3888333 \tVal Loss:1.0248396 \tTrain Acc: 55.76389% \tVal Acc: 74.9999998%\n",
      "Epoch: 199\tTrain Loss: 1.3712855 \tVal Loss:1.0353367 \tTrain Acc: 55.34722% \tVal Acc: 72.2222224%\n",
      "Epoch: 200\tTrain Loss: 1.3570987 \tVal Loss:1.0250300 \tTrain Acc: 57.08333% \tVal Acc: 73.3333339%\n",
      "Epoch: 201\tTrain Loss: 1.3807955 \tVal Loss:0.9979818 \tTrain Acc: 56.38889% \tVal Acc: 75.2777775%\n",
      "Validation Loss decreased from 1.014535 to 0.997982, saving the model weights\n",
      "Epoch: 202\tTrain Loss: 1.3282169 \tVal Loss:0.9848269 \tTrain Acc: 56.94445% \tVal Acc: 76.6666666%\n",
      "Validation Loss decreased from 0.997982 to 0.984827, saving the model weights\n",
      "Epoch: 203\tTrain Loss: 1.3425205 \tVal Loss:0.9603872 \tTrain Acc: 56.31944% \tVal Acc: 81.1111117%\n",
      "Validation Loss decreased from 0.984827 to 0.960387, saving the model weights\n",
      "Epoch: 204\tTrain Loss: 1.3229186 \tVal Loss:0.9750149 \tTrain Acc: 57.36111% \tVal Acc: 76.1111105%\n",
      "Epoch: 205\tTrain Loss: 1.2959792 \tVal Loss:0.9164505 \tTrain Acc: 58.54167% \tVal Acc: 78.8888892%\n",
      "Validation Loss decreased from 0.960387 to 0.916451, saving the model weights\n",
      "Epoch: 206\tTrain Loss: 1.2793187 \tVal Loss:0.8710142 \tTrain Acc: 60.20833% \tVal Acc: 80.5555545%\n",
      "Validation Loss decreased from 0.916451 to 0.871014, saving the model weights\n",
      "Epoch: 207\tTrain Loss: 1.2469617 \tVal Loss:0.9534464 \tTrain Acc: 61.04167% \tVal Acc: 76.6666666%\n",
      "Epoch: 208\tTrain Loss: 1.2744496 \tVal Loss:0.9439399 \tTrain Acc: 59.02778% \tVal Acc: 78.3333331%\n",
      "Epoch: 209\tTrain Loss: 1.2419834 \tVal Loss:0.8841123 \tTrain Acc: 61.18056% \tVal Acc: 80.5555547%\n",
      "Epoch: 210\tTrain Loss: 1.2319712 \tVal Loss:1.0823614 \tTrain Acc: 61.45833% \tVal Acc: 74.4444442%\n",
      "Epoch: 211\tTrain Loss: 1.3288652 \tVal Loss:0.9856221 \tTrain Acc: 58.26389% \tVal Acc: 73.8888890%\n",
      "Epoch: 212\tTrain Loss: 1.3616483 \tVal Loss:0.9945158 \tTrain Acc: 55.20833% \tVal Acc: 75.8333330%\n",
      "Epoch: 213\tTrain Loss: 1.2978544 \tVal Loss:0.9120285 \tTrain Acc: 58.81944% \tVal Acc: 81.1111107%\n",
      "Epoch: 214\tTrain Loss: 1.2174437 \tVal Loss:0.8401371 \tTrain Acc: 61.18056% \tVal Acc: 80.0000002%\n",
      "Validation Loss decreased from 0.871014 to 0.840137, saving the model weights\n",
      "Epoch: 215\tTrain Loss: 1.1564849 \tVal Loss:0.8467818 \tTrain Acc: 63.81944% \tVal Acc: 80.2777777%\n",
      "Epoch: 216\tTrain Loss: 1.1559226 \tVal Loss:0.8031960 \tTrain Acc: 64.79167% \tVal Acc: 81.1111112%\n",
      "Validation Loss decreased from 0.840137 to 0.803196, saving the model weights\n",
      "Epoch: 217\tTrain Loss: 1.1358989 \tVal Loss:0.8253501 \tTrain Acc: 64.58333% \tVal Acc: 83.3333333%\n",
      "Epoch: 218\tTrain Loss: 1.1463076 \tVal Loss:0.8084345 \tTrain Acc: 63.81944% \tVal Acc: 81.1111110%\n",
      "Epoch: 219\tTrain Loss: 1.1154071 \tVal Loss:0.7607774 \tTrain Acc: 65.27778% \tVal Acc: 82.4999993%\n",
      "Validation Loss decreased from 0.803196 to 0.760777, saving the model weights\n",
      "Epoch: 220\tTrain Loss: 1.1131785 \tVal Loss:0.7570165 \tTrain Acc: 65.27778% \tVal Acc: 84.7222214%\n",
      "Validation Loss decreased from 0.760777 to 0.757017, saving the model weights\n",
      "Epoch: 221\tTrain Loss: 1.0873462 \tVal Loss:0.7157288 \tTrain Acc: 66.73611% \tVal Acc: 85.8333329%\n",
      "Validation Loss decreased from 0.757017 to 0.715729, saving the model weights\n",
      "Epoch: 222\tTrain Loss: 1.0572552 \tVal Loss:0.7557291 \tTrain Acc: 65.97222% \tVal Acc: 81.6666663%\n",
      "Epoch: 223\tTrain Loss: 1.0681429 \tVal Loss:0.7170254 \tTrain Acc: 67.29167% \tVal Acc: 83.6111113%\n",
      "Epoch: 224\tTrain Loss: 1.0529403 \tVal Loss:0.6674878 \tTrain Acc: 66.18056% \tVal Acc: 85.0000004%\n",
      "Validation Loss decreased from 0.715729 to 0.667488, saving the model weights\n",
      "Epoch: 225\tTrain Loss: 1.0203828 \tVal Loss:0.6928922 \tTrain Acc: 68.19444% \tVal Acc: 84.1666659%\n",
      "Epoch: 226\tTrain Loss: 1.0310623 \tVal Loss:0.6798499 \tTrain Acc: 67.91667% \tVal Acc: 83.0555551%\n",
      "Epoch: 227\tTrain Loss: 1.0628550 \tVal Loss:0.7272143 \tTrain Acc: 66.25% \tVal Acc: 81.9444443%\n",
      "Epoch: 228\tTrain Loss: 1.0216503 \tVal Loss:0.6736140 \tTrain Acc: 67.70833% \tVal Acc: 85.8333329%\n",
      "Epoch: 229\tTrain Loss: 1.0945112 \tVal Loss:0.7067902 \tTrain Acc: 65.55556% \tVal Acc: 83.0555558%\n",
      "Epoch: 230\tTrain Loss: 1.0640842 \tVal Loss:0.6468909 \tTrain Acc: 65.90278% \tVal Acc: 86.1111109%\n",
      "Validation Loss decreased from 0.667488 to 0.646891, saving the model weights\n",
      "Epoch: 231\tTrain Loss: 1.0484592 \tVal Loss:0.6244699 \tTrain Acc: 66.45833% \tVal Acc: 87.2222225%\n",
      "Validation Loss decreased from 0.646891 to 0.624470, saving the model weights\n",
      "Epoch: 232\tTrain Loss: 1.0019048 \tVal Loss:0.6070623 \tTrain Acc: 67.84722% \tVal Acc: 85.5555554%\n",
      "Validation Loss decreased from 0.624470 to 0.607062, saving the model weights\n",
      "Epoch: 233\tTrain Loss: 0.9699689 \tVal Loss:0.5738293 \tTrain Acc: 70.27778% \tVal Acc: 89.4444436%\n",
      "Validation Loss decreased from 0.607062 to 0.573829, saving the model weights\n",
      "Epoch: 234\tTrain Loss: 0.9343344 \tVal Loss:0.5835414 \tTrain Acc: 71.59722% \tVal Acc: 88.3333335%\n",
      "Epoch: 235\tTrain Loss: 0.9255643 \tVal Loss:0.6133591 \tTrain Acc: 72.56944% \tVal Acc: 85.2777774%\n",
      "Epoch: 236\tTrain Loss: 0.9116299 \tVal Loss:0.5691653 \tTrain Acc: 72.84722% \tVal Acc: 87.7777775%\n",
      "Validation Loss decreased from 0.573829 to 0.569165, saving the model weights\n",
      "Epoch: 237\tTrain Loss: 0.8943085 \tVal Loss:0.5206108 \tTrain Acc: 72.91667% \tVal Acc: 90.0000001%\n",
      "Validation Loss decreased from 0.569165 to 0.520611, saving the model weights\n",
      "Epoch: 238\tTrain Loss: 0.8739187 \tVal Loss:0.5720807 \tTrain Acc: 73.33333% \tVal Acc: 87.2222225%\n",
      "Epoch: 239\tTrain Loss: 0.9352707 \tVal Loss:0.5359045 \tTrain Acc: 70.69444% \tVal Acc: 90.2777766%\n",
      "Epoch: 240\tTrain Loss: 0.8551118 \tVal Loss:0.5152019 \tTrain Acc: 73.05556% \tVal Acc: 89.7222216%\n",
      "Validation Loss decreased from 0.520611 to 0.515202, saving the model weights\n",
      "Epoch: 241\tTrain Loss: 0.8862460 \tVal Loss:0.5207179 \tTrain Acc: 72.08333% \tVal Acc: 88.8888876%\n",
      "Epoch: 242\tTrain Loss: 0.8652096 \tVal Loss:0.5615528 \tTrain Acc: 73.75% \tVal Acc: 86.3888880%\n",
      "Epoch: 243\tTrain Loss: 0.9538142 \tVal Loss:0.6279708 \tTrain Acc: 70.27778% \tVal Acc: 82.7777778%\n",
      "Epoch: 244\tTrain Loss: 0.8977916 \tVal Loss:0.5567906 \tTrain Acc: 72.29167% \tVal Acc: 87.7777775%\n",
      "Epoch: 245\tTrain Loss: 0.8891026 \tVal Loss:0.4949374 \tTrain Acc: 71.66667% \tVal Acc: 89.9999991%\n",
      "Validation Loss decreased from 0.515202 to 0.494937, saving the model weights\n",
      "Epoch: 246\tTrain Loss: 0.8512557 \tVal Loss:0.4581499 \tTrain Acc: 72.98611% \tVal Acc: 89.4444446%\n",
      "Validation Loss decreased from 0.494937 to 0.458150, saving the model weights\n",
      "Epoch: 247\tTrain Loss: 0.8127253 \tVal Loss:0.4718869 \tTrain Acc: 75.20833% \tVal Acc: 90.2777776%\n",
      "Epoch: 248\tTrain Loss: 0.8059201 \tVal Loss:0.4530279 \tTrain Acc: 75.83333% \tVal Acc: 91.3888887%\n",
      "Validation Loss decreased from 0.458150 to 0.453028, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 249\tTrain Loss: 0.7997848 \tVal Loss:0.4448572 \tTrain Acc: 75.90278% \tVal Acc: 91.1111106%\n",
      "Validation Loss decreased from 0.453028 to 0.444857, saving the model weights\n",
      "Epoch: 250\tTrain Loss: 0.7859303 \tVal Loss:0.4464096 \tTrain Acc: 75.0% \tVal Acc: 90.8333331%\n",
      "Epoch: 251\tTrain Loss: 0.7889689 \tVal Loss:0.4297475 \tTrain Acc: 75.27778% \tVal Acc: 91.3888882%\n",
      "Validation Loss decreased from 0.444857 to 0.429747, saving the model weights\n",
      "Epoch: 252\tTrain Loss: 0.7832603 \tVal Loss:0.4018155 \tTrain Acc: 75.34722% \tVal Acc: 91.9444437%\n",
      "Validation Loss decreased from 0.429747 to 0.401815, saving the model weights\n",
      "Epoch: 253\tTrain Loss: 0.7247728 \tVal Loss:0.4085759 \tTrain Acc: 77.84722% \tVal Acc: 90.8333331%\n",
      "Epoch: 254\tTrain Loss: 0.7312702 \tVal Loss:0.3910477 \tTrain Acc: 77.98611% \tVal Acc: 91.6666657%\n",
      "Validation Loss decreased from 0.401815 to 0.391048, saving the model weights\n",
      "Epoch: 255\tTrain Loss: 0.7295601 \tVal Loss:0.4155309 \tTrain Acc: 78.54167% \tVal Acc: 91.6666657%\n",
      "Epoch: 256\tTrain Loss: 0.6968670 \tVal Loss:0.4014323 \tTrain Acc: 79.65278% \tVal Acc: 90.2777771%\n",
      "Epoch: 257\tTrain Loss: 0.6821700 \tVal Loss:0.4289755 \tTrain Acc: 78.68056% \tVal Acc: 89.7222211%\n",
      "Epoch: 258\tTrain Loss: 0.7349597 \tVal Loss:0.4167913 \tTrain Acc: 77.15278% \tVal Acc: 90.8333326%\n",
      "Epoch: 259\tTrain Loss: 0.7743415 \tVal Loss:0.4281748 \tTrain Acc: 75.55556% \tVal Acc: 89.4444441%\n",
      "Epoch: 260\tTrain Loss: 0.8518293 \tVal Loss:0.4037020 \tTrain Acc: 73.61111% \tVal Acc: 89.7222216%\n",
      "Epoch: 261\tTrain Loss: 0.7976137 \tVal Loss:0.3870658 \tTrain Acc: 74.02778% \tVal Acc: 90.5555551%\n",
      "Validation Loss decreased from 0.391048 to 0.387066, saving the model weights\n",
      "Epoch: 262\tTrain Loss: 0.7132389 \tVal Loss:0.3571784 \tTrain Acc: 77.43056% \tVal Acc: 93.6111107%\n",
      "Validation Loss decreased from 0.387066 to 0.357178, saving the model weights\n",
      "Epoch: 263\tTrain Loss: 0.6975960 \tVal Loss:0.3448293 \tTrain Acc: 78.47222% \tVal Acc: 93.6111107%\n",
      "Validation Loss decreased from 0.357178 to 0.344829, saving the model weights\n",
      "Epoch: 264\tTrain Loss: 0.6475016 \tVal Loss:0.3142843 \tTrain Acc: 81.52778% \tVal Acc: 93.6111107%\n",
      "Validation Loss decreased from 0.344829 to 0.314284, saving the model weights\n",
      "Epoch: 265\tTrain Loss: 0.6674505 \tVal Loss:0.3444418 \tTrain Acc: 79.16667% \tVal Acc: 92.4999997%\n",
      "Epoch: 266\tTrain Loss: 0.7051842 \tVal Loss:0.3315662 \tTrain Acc: 77.77778% \tVal Acc: 93.3333327%\n",
      "Epoch: 267\tTrain Loss: 0.6328897 \tVal Loss:0.3395432 \tTrain Acc: 82.08333% \tVal Acc: 92.2222212%\n",
      "Epoch: 268\tTrain Loss: 0.6377241 \tVal Loss:0.3171759 \tTrain Acc: 81.18056% \tVal Acc: 91.9444432%\n",
      "Epoch: 269\tTrain Loss: 0.5892304 \tVal Loss:0.2858846 \tTrain Acc: 82.5% \tVal Acc: 94.7222213%\n",
      "Validation Loss decreased from 0.314284 to 0.285885, saving the model weights\n",
      "Epoch: 270\tTrain Loss: 0.6111502 \tVal Loss:0.3549205 \tTrain Acc: 81.31944% \tVal Acc: 92.4999987%\n",
      "Epoch: 271\tTrain Loss: 0.6431617 \tVal Loss:0.3280768 \tTrain Acc: 80.625% \tVal Acc: 93.0555547%\n",
      "Epoch: 272\tTrain Loss: 0.6185993 \tVal Loss:0.2790942 \tTrain Acc: 80.76389% \tVal Acc: 93.6111107%\n",
      "Validation Loss decreased from 0.285885 to 0.279094, saving the model weights\n",
      "Epoch: 273\tTrain Loss: 0.5951428 \tVal Loss:0.2807557 \tTrain Acc: 81.25% \tVal Acc: 93.6111107%\n",
      "Epoch: 274\tTrain Loss: 0.6315573 \tVal Loss:0.2906703 \tTrain Acc: 80.76389% \tVal Acc: 94.7222218%\n",
      "Epoch: 275\tTrain Loss: 0.6107956 \tVal Loss:0.2804021 \tTrain Acc: 81.59722% \tVal Acc: 93.3333327%\n",
      "Epoch: 276\tTrain Loss: 0.5646712 \tVal Loss:0.2534178 \tTrain Acc: 84.23611% \tVal Acc: 94.1666668%\n",
      "Validation Loss decreased from 0.279094 to 0.253418, saving the model weights\n",
      "Epoch: 277\tTrain Loss: 0.5367190 \tVal Loss:0.2502788 \tTrain Acc: 84.72222% \tVal Acc: 95.8333328%\n",
      "Validation Loss decreased from 0.253418 to 0.250279, saving the model weights\n",
      "Epoch: 278\tTrain Loss: 0.7497385 \tVal Loss:1.0260421 \tTrain Acc: 75.76389% \tVal Acc: 71.3888889%\n",
      "Epoch: 279\tTrain Loss: 0.9415276 \tVal Loss:0.4977104 \tTrain Acc: 72.08333% \tVal Acc: 84.9999999%\n",
      "Epoch: 280\tTrain Loss: 0.7568338 \tVal Loss:0.3577526 \tTrain Acc: 76.31944% \tVal Acc: 92.2222217%\n",
      "Epoch: 281\tTrain Loss: 0.6582754 \tVal Loss:0.2799008 \tTrain Acc: 80.13889% \tVal Acc: 93.8888878%\n",
      "Epoch: 282\tTrain Loss: 0.5496795 \tVal Loss:0.2469819 \tTrain Acc: 83.54167% \tVal Acc: 94.9999998%\n",
      "Validation Loss decreased from 0.250279 to 0.246982, saving the model weights\n",
      "Epoch: 283\tTrain Loss: 0.5542007 \tVal Loss:0.2362058 \tTrain Acc: 83.61111% \tVal Acc: 94.9999993%\n",
      "Validation Loss decreased from 0.246982 to 0.236206, saving the model weights\n",
      "Epoch: 284\tTrain Loss: 0.5159357 \tVal Loss:0.2223492 \tTrain Acc: 84.16667% \tVal Acc: 95.2777773%\n",
      "Validation Loss decreased from 0.236206 to 0.222349, saving the model weights\n",
      "Epoch: 285\tTrain Loss: 0.4850802 \tVal Loss:0.2211056 \tTrain Acc: 86.66667% \tVal Acc: 94.7222213%\n",
      "Validation Loss decreased from 0.222349 to 0.221106, saving the model weights\n",
      "Epoch: 286\tTrain Loss: 0.4891356 \tVal Loss:0.2094469 \tTrain Acc: 85.27778% \tVal Acc: 95.2777773%\n",
      "Validation Loss decreased from 0.221106 to 0.209447, saving the model weights\n",
      "Epoch: 287\tTrain Loss: 0.4672699 \tVal Loss:0.2114181 \tTrain Acc: 86.38889% \tVal Acc: 94.7222218%\n",
      "Epoch: 288\tTrain Loss: 0.4552987 \tVal Loss:0.1957398 \tTrain Acc: 87.43055% \tVal Acc: 96.3888884%\n",
      "Validation Loss decreased from 0.209447 to 0.195740, saving the model weights\n",
      "Epoch: 289\tTrain Loss: 0.4610337 \tVal Loss:0.1910847 \tTrain Acc: 86.52778% \tVal Acc: 96.9444439%\n",
      "Validation Loss decreased from 0.195740 to 0.191085, saving the model weights\n",
      "Epoch: 290\tTrain Loss: 0.4835336 \tVal Loss:0.1907879 \tTrain Acc: 86.25% \tVal Acc: 96.3888884%\n",
      "Validation Loss decreased from 0.191085 to 0.190788, saving the model weights\n",
      "Epoch: 291\tTrain Loss: 0.4632901 \tVal Loss:0.3557629 \tTrain Acc: 86.25% \tVal Acc: 91.9444437%\n",
      "Epoch: 292\tTrain Loss: 0.8505809 \tVal Loss:0.5881905 \tTrain Acc: 72.29167% \tVal Acc: 82.4999993%\n",
      "Epoch: 293\tTrain Loss: 0.8849844 \tVal Loss:0.5878080 \tTrain Acc: 71.94444% \tVal Acc: 81.9444450%\n",
      "Epoch: 294\tTrain Loss: 0.8981685 \tVal Loss:0.4923838 \tTrain Acc: 72.36111% \tVal Acc: 85.5555549%\n",
      "Epoch: 295\tTrain Loss: 0.7549557 \tVal Loss:0.3949366 \tTrain Acc: 75.06944% \tVal Acc: 89.4444441%\n",
      "Epoch: 296\tTrain Loss: 0.6468532 \tVal Loss:0.2469303 \tTrain Acc: 79.16667% \tVal Acc: 94.1666658%\n",
      "Epoch: 297\tTrain Loss: 0.4985296 \tVal Loss:0.2090891 \tTrain Acc: 85.41667% \tVal Acc: 94.4444443%\n",
      "Epoch: 298\tTrain Loss: 0.4765270 \tVal Loss:0.1946286 \tTrain Acc: 86.04167% \tVal Acc: 94.7222218%\n",
      "Epoch: 299\tTrain Loss: 0.4523105 \tVal Loss:0.1877227 \tTrain Acc: 86.875% \tVal Acc: 96.9444439%\n",
      "Validation Loss decreased from 0.190788 to 0.187723, saving the model weights\n",
      "Epoch: 300\tTrain Loss: 0.4221584 \tVal Loss:0.1714772 \tTrain Acc: 88.81944% \tVal Acc: 96.3888884%\n",
      "Validation Loss decreased from 0.187723 to 0.171477, saving the model weights\n",
      "Epoch: 301\tTrain Loss: 0.4202282 \tVal Loss:0.1763354 \tTrain Acc: 88.81944% \tVal Acc: 95.8333333%\n",
      "Epoch: 302\tTrain Loss: 0.4230465 \tVal Loss:0.1722322 \tTrain Acc: 87.98611% \tVal Acc: 96.1111108%\n",
      "Epoch: 303\tTrain Loss: 0.3977272 \tVal Loss:0.1661029 \tTrain Acc: 89.375% \tVal Acc: 96.6666659%\n",
      "Validation Loss decreased from 0.171477 to 0.166103, saving the model weights\n",
      "Epoch: 304\tTrain Loss: 0.3996333 \tVal Loss:0.1898156 \tTrain Acc: 87.5% \tVal Acc: 95.2777773%\n",
      "Epoch: 305\tTrain Loss: 0.4006724 \tVal Loss:0.1597487 \tTrain Acc: 88.54167% \tVal Acc: 96.6666664%\n",
      "Validation Loss decreased from 0.166103 to 0.159749, saving the model weights\n",
      "Epoch: 306\tTrain Loss: 0.3743635 \tVal Loss:0.1490270 \tTrain Acc: 89.375% \tVal Acc: 97.2222214%\n",
      "Validation Loss decreased from 0.159749 to 0.149027, saving the model weights\n",
      "Epoch: 307\tTrain Loss: 0.3542952 \tVal Loss:0.1449551 \tTrain Acc: 90.27778% \tVal Acc: 97.4999994%\n",
      "Validation Loss decreased from 0.149027 to 0.144955, saving the model weights\n",
      "Epoch: 308\tTrain Loss: 0.4128533 \tVal Loss:0.1536548 \tTrain Acc: 87.5% \tVal Acc: 96.6666664%\n",
      "Epoch: 309\tTrain Loss: 0.3648548 \tVal Loss:0.1522792 \tTrain Acc: 90.34722% \tVal Acc: 96.1111108%\n",
      "Epoch: 310\tTrain Loss: 0.3494710 \tVal Loss:0.1357702 \tTrain Acc: 90.20833% \tVal Acc: 97.2222214%\n",
      "Validation Loss decreased from 0.144955 to 0.135770, saving the model weights\n",
      "Epoch: 311\tTrain Loss: 0.3623663 \tVal Loss:0.1297813 \tTrain Acc: 90.20833% \tVal Acc: 97.4999994%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased from 0.135770 to 0.129781, saving the model weights\n",
      "Epoch: 312\tTrain Loss: 0.3402483 \tVal Loss:0.1364914 \tTrain Acc: 90.69444% \tVal Acc: 96.9444434%\n",
      "Epoch: 313\tTrain Loss: 0.3491890 \tVal Loss:0.1330011 \tTrain Acc: 90.625% \tVal Acc: 96.6666659%\n",
      "Epoch: 314\tTrain Loss: 0.3346975 \tVal Loss:0.1482885 \tTrain Acc: 90.06944% \tVal Acc: 96.6666659%\n",
      "Epoch: 315\tTrain Loss: 0.3618614 \tVal Loss:0.1399375 \tTrain Acc: 90.13889% \tVal Acc: 97.2222219%\n",
      "Epoch: 316\tTrain Loss: 0.3548529 \tVal Loss:0.1181016 \tTrain Acc: 90.90278% \tVal Acc: 97.7777774%\n",
      "Validation Loss decreased from 0.129781 to 0.118102, saving the model weights\n",
      "Epoch: 317\tTrain Loss: 0.3439632 \tVal Loss:0.1144071 \tTrain Acc: 89.79167% \tVal Acc: 98.0555549%\n",
      "Validation Loss decreased from 0.118102 to 0.114407, saving the model weights\n",
      "Epoch: 318\tTrain Loss: 0.3621009 \tVal Loss:0.1176705 \tTrain Acc: 90.0% \tVal Acc: 97.4999999%\n",
      "Epoch: 319\tTrain Loss: 0.6131971 \tVal Loss:0.5165411 \tTrain Acc: 80.27778% \tVal Acc: 85.2777779%\n",
      "Epoch: 320\tTrain Loss: 0.7411072 \tVal Loss:0.2499556 \tTrain Acc: 75.34722% \tVal Acc: 93.3333327%\n",
      "Epoch: 321\tTrain Loss: 0.6794019 \tVal Loss:0.2037416 \tTrain Acc: 78.88889% \tVal Acc: 95.2777773%\n",
      "Epoch: 322\tTrain Loss: 0.5393198 \tVal Loss:0.1577944 \tTrain Acc: 82.84722% \tVal Acc: 95.5555553%\n",
      "Epoch: 323\tTrain Loss: 0.4630852 \tVal Loss:0.1430253 \tTrain Acc: 86.59722% \tVal Acc: 96.1111108%\n",
      "Epoch: 324\tTrain Loss: 0.4295128 \tVal Loss:0.1194378 \tTrain Acc: 86.66667% \tVal Acc: 97.7777774%\n",
      "Epoch: 325\tTrain Loss: 0.3472607 \tVal Loss:0.1323310 \tTrain Acc: 89.86111% \tVal Acc: 96.6666659%\n",
      "Epoch: 326\tTrain Loss: 0.3282210 \tVal Loss:0.1252823 \tTrain Acc: 90.97222% \tVal Acc: 96.9444439%\n",
      "Epoch: 327\tTrain Loss: 0.3100002 \tVal Loss:0.1080876 \tTrain Acc: 91.52778% \tVal Acc: 97.7777774%\n",
      "Validation Loss decreased from 0.114407 to 0.108088, saving the model weights\n",
      "Epoch: 328\tTrain Loss: 0.3116750 \tVal Loss:0.1059042 \tTrain Acc: 90.34722% \tVal Acc: 98.3333329%\n",
      "Validation Loss decreased from 0.108088 to 0.105904, saving the model weights\n",
      "Epoch: 329\tTrain Loss: 0.2986186 \tVal Loss:0.1095461 \tTrain Acc: 91.31944% \tVal Acc: 97.4999994%\n",
      "Epoch: 330\tTrain Loss: 0.2840249 \tVal Loss:0.0881255 \tTrain Acc: 92.15278% \tVal Acc: 98.3333329%\n",
      "Validation Loss decreased from 0.105904 to 0.088126, saving the model weights\n",
      "Epoch: 331\tTrain Loss: 0.2814739 \tVal Loss:0.0954562 \tTrain Acc: 92.22222% \tVal Acc: 98.0555549%\n",
      "Epoch: 332\tTrain Loss: 0.2725993 \tVal Loss:0.0954540 \tTrain Acc: 93.05555% \tVal Acc: 97.7777769%\n",
      "Epoch: 333\tTrain Loss: 0.2743910 \tVal Loss:0.1440307 \tTrain Acc: 92.5% \tVal Acc: 95.8333328%\n",
      "Epoch: 334\tTrain Loss: 0.2894427 \tVal Loss:0.1083296 \tTrain Acc: 92.5% \tVal Acc: 97.4999994%\n",
      "Epoch: 335\tTrain Loss: 0.3110427 \tVal Loss:0.1083643 \tTrain Acc: 91.52778% \tVal Acc: 97.4999994%\n",
      "Epoch: 336\tTrain Loss: 0.3075705 \tVal Loss:0.1047397 \tTrain Acc: 90.97222% \tVal Acc: 98.3333329%\n",
      "Epoch: 337\tTrain Loss: 0.3047217 \tVal Loss:0.0923844 \tTrain Acc: 91.11111% \tVal Acc: 98.0555554%\n",
      "Epoch: 338\tTrain Loss: 0.2843859 \tVal Loss:0.0826252 \tTrain Acc: 92.56944% \tVal Acc: 98.6111104%\n",
      "Validation Loss decreased from 0.088126 to 0.082625, saving the model weights\n",
      "Epoch: 339\tTrain Loss: 0.2583562 \tVal Loss:0.0770228 \tTrain Acc: 93.40278% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.082625 to 0.077023, saving the model weights\n",
      "Epoch: 340\tTrain Loss: 0.2984080 \tVal Loss:0.0863422 \tTrain Acc: 91.25% \tVal Acc: 98.6111104%\n",
      "Epoch: 341\tTrain Loss: 0.3204530 \tVal Loss:0.0878781 \tTrain Acc: 90.34722% \tVal Acc: 98.3333329%\n",
      "Epoch: 342\tTrain Loss: 0.2880536 \tVal Loss:0.1207400 \tTrain Acc: 91.52778% \tVal Acc: 96.3888884%\n",
      "Epoch: 343\tTrain Loss: 0.3250437 \tVal Loss:0.2141379 \tTrain Acc: 90.20833% \tVal Acc: 94.4444438%\n",
      "Epoch: 344\tTrain Loss: 0.5311984 \tVal Loss:0.3091815 \tTrain Acc: 83.26389% \tVal Acc: 88.6111115%\n",
      "Epoch: 345\tTrain Loss: 0.6821829 \tVal Loss:0.2396987 \tTrain Acc: 79.16667% \tVal Acc: 92.4999992%\n",
      "Epoch: 346\tTrain Loss: 0.7670930 \tVal Loss:0.2662980 \tTrain Acc: 75.48611% \tVal Acc: 93.3333322%\n",
      "Epoch: 347\tTrain Loss: 0.6018136 \tVal Loss:0.1371924 \tTrain Acc: 80.90278% \tVal Acc: 96.6666659%\n",
      "Epoch: 348\tTrain Loss: 0.4342807 \tVal Loss:0.1021856 \tTrain Acc: 86.875% \tVal Acc: 98.0555549%\n",
      "Epoch: 349\tTrain Loss: 0.3469874 \tVal Loss:0.0914866 \tTrain Acc: 89.93055% \tVal Acc: 98.6111109%\n",
      "Epoch: 350\tTrain Loss: 0.2928755 \tVal Loss:0.0809777 \tTrain Acc: 91.25% \tVal Acc: 98.6111109%\n",
      "Epoch: 351\tTrain Loss: 0.2702484 \tVal Loss:0.0706976 \tTrain Acc: 93.125% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.077023 to 0.070698, saving the model weights\n",
      "Epoch: 352\tTrain Loss: 0.2734055 \tVal Loss:0.0733950 \tTrain Acc: 92.43055% \tVal Acc: 98.6111109%\n",
      "Epoch: 353\tTrain Loss: 0.2444813 \tVal Loss:0.0723053 \tTrain Acc: 94.16667% \tVal Acc: 99.4444440%\n",
      "Epoch: 354\tTrain Loss: 0.2389693 \tVal Loss:0.0714806 \tTrain Acc: 93.95833% \tVal Acc: 98.3333329%\n",
      "Epoch: 355\tTrain Loss: 0.2425342 \tVal Loss:0.0732115 \tTrain Acc: 93.81944% \tVal Acc: 98.8888885%\n",
      "Epoch: 356\tTrain Loss: 0.2410829 \tVal Loss:0.0642553 \tTrain Acc: 93.125% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.070698 to 0.064255, saving the model weights\n",
      "Epoch: 357\tTrain Loss: 0.7060512 \tVal Loss:0.2443683 \tTrain Acc: 80.27778% \tVal Acc: 93.3333327%\n",
      "Epoch: 358\tTrain Loss: 0.4932430 \tVal Loss:0.1192307 \tTrain Acc: 84.86111% \tVal Acc: 97.4999994%\n",
      "Epoch: 359\tTrain Loss: 0.3471142 \tVal Loss:0.0882816 \tTrain Acc: 89.93055% \tVal Acc: 98.0555549%\n",
      "Epoch: 360\tTrain Loss: 0.3043556 \tVal Loss:0.0788113 \tTrain Acc: 90.27778% \tVal Acc: 98.3333329%\n",
      "Epoch: 361\tTrain Loss: 0.2467847 \tVal Loss:0.0686913 \tTrain Acc: 93.33333% \tVal Acc: 99.1666660%\n",
      "Epoch: 362\tTrain Loss: 0.2511237 \tVal Loss:0.0695689 \tTrain Acc: 93.05555% \tVal Acc: 98.6111104%\n",
      "Epoch: 363\tTrain Loss: 0.2232395 \tVal Loss:0.0659694 \tTrain Acc: 93.95833% \tVal Acc: 98.6111104%\n",
      "Epoch: 364\tTrain Loss: 0.2246085 \tVal Loss:0.0559811 \tTrain Acc: 93.61111% \tVal Acc: 98.6111109%\n",
      "Validation Loss decreased from 0.064255 to 0.055981, saving the model weights\n",
      "Epoch: 365\tTrain Loss: 0.2285043 \tVal Loss:0.0585783 \tTrain Acc: 94.23611% \tVal Acc: 99.1666660%\n",
      "Epoch: 366\tTrain Loss: 0.2361745 \tVal Loss:0.0666589 \tTrain Acc: 93.125% \tVal Acc: 98.8888885%\n",
      "Epoch: 367\tTrain Loss: 0.2505411 \tVal Loss:0.0554107 \tTrain Acc: 94.16667% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.055981 to 0.055411, saving the model weights\n",
      "Epoch: 368\tTrain Loss: 0.2151835 \tVal Loss:0.0532745 \tTrain Acc: 94.02778% \tVal Acc: 98.8888885%\n",
      "Validation Loss decreased from 0.055411 to 0.053275, saving the model weights\n",
      "Epoch: 369\tTrain Loss: 0.2936646 \tVal Loss:0.3681414 \tTrain Acc: 91.94444% \tVal Acc: 89.9999996%\n",
      "Epoch: 370\tTrain Loss: 0.6098635 \tVal Loss:0.5369638 \tTrain Acc: 82.29167% \tVal Acc: 84.9999999%\n",
      "Epoch: 371\tTrain Loss: 0.7345608 \tVal Loss:0.3880910 \tTrain Acc: 77.91667% \tVal Acc: 87.7777770%\n",
      "Epoch: 372\tTrain Loss: 0.5000495 \tVal Loss:0.1031433 \tTrain Acc: 83.05555% \tVal Acc: 98.3333329%\n",
      "Epoch: 373\tTrain Loss: 0.3413831 \tVal Loss:0.0813051 \tTrain Acc: 89.51389% \tVal Acc: 98.0555549%\n",
      "Epoch: 374\tTrain Loss: 0.2513948 \tVal Loss:0.0681189 \tTrain Acc: 93.47222% \tVal Acc: 98.8888885%\n",
      "Epoch: 375\tTrain Loss: 0.2453529 \tVal Loss:0.0610975 \tTrain Acc: 93.05555% \tVal Acc: 99.1666665%\n",
      "Epoch: 376\tTrain Loss: 0.2304626 \tVal Loss:0.0571579 \tTrain Acc: 93.95833% \tVal Acc: 99.1666660%\n",
      "Epoch: 377\tTrain Loss: 0.2229212 \tVal Loss:0.0588435 \tTrain Acc: 94.44444% \tVal Acc: 98.6111109%\n",
      "Epoch: 378\tTrain Loss: 0.2211826 \tVal Loss:0.0536890 \tTrain Acc: 94.375% \tVal Acc: 99.4444445%\n",
      "Epoch: 379\tTrain Loss: 0.1842319 \tVal Loss:0.0510760 \tTrain Acc: 95.27778% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.053275 to 0.051076, saving the model weights\n",
      "Epoch: 380\tTrain Loss: 0.2035763 \tVal Loss:0.0487317 \tTrain Acc: 94.86111% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.051076 to 0.048732, saving the model weights\n",
      "Epoch: 381\tTrain Loss: 0.1875081 \tVal Loss:0.0439680 \tTrain Acc: 95.06944% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.048732 to 0.043968, saving the model weights\n",
      "Epoch: 382\tTrain Loss: 0.1917378 \tVal Loss:0.0449281 \tTrain Acc: 94.44444% \tVal Acc: 99.1666665%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 383\tTrain Loss: 0.1959186 \tVal Loss:0.0489974 \tTrain Acc: 95.06944% \tVal Acc: 99.1666665%\n",
      "Epoch: 384\tTrain Loss: 0.1751861 \tVal Loss:0.0467453 \tTrain Acc: 96.04167% \tVal Acc: 98.8888890%\n",
      "Epoch: 385\tTrain Loss: 0.1778570 \tVal Loss:0.0507377 \tTrain Acc: 95.41667% \tVal Acc: 98.8888885%\n",
      "Epoch: 386\tTrain Loss: 0.1834961 \tVal Loss:0.0512096 \tTrain Acc: 95.41667% \tVal Acc: 98.8888890%\n",
      "Epoch: 387\tTrain Loss: 0.1747073 \tVal Loss:0.0405209 \tTrain Acc: 95.83333% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.043968 to 0.040521, saving the model weights\n",
      "Epoch: 388\tTrain Loss: 0.2047414 \tVal Loss:0.0569365 \tTrain Acc: 94.58333% \tVal Acc: 98.3333329%\n",
      "Epoch: 389\tTrain Loss: 0.2271417 \tVal Loss:0.0423867 \tTrain Acc: 93.61111% \tVal Acc: 99.1666665%\n",
      "Epoch: 390\tTrain Loss: 0.1844900 \tVal Loss:0.0401228 \tTrain Acc: 95.69444% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.040521 to 0.040123, saving the model weights\n",
      "Epoch: 391\tTrain Loss: 0.1767757 \tVal Loss:0.0469129 \tTrain Acc: 94.65278% \tVal Acc: 98.6111109%\n",
      "Epoch: 392\tTrain Loss: 0.1618381 \tVal Loss:0.0378567 \tTrain Acc: 96.38889% \tVal Acc: 99.1666660%\n",
      "Validation Loss decreased from 0.040123 to 0.037857, saving the model weights\n",
      "Epoch: 393\tTrain Loss: 0.1657749 \tVal Loss:0.0429914 \tTrain Acc: 96.11111% \tVal Acc: 98.8888890%\n",
      "Epoch: 394\tTrain Loss: 0.1638292 \tVal Loss:0.0496673 \tTrain Acc: 96.25% \tVal Acc: 98.3333334%\n",
      "Epoch: 395\tTrain Loss: 0.1746360 \tVal Loss:0.0410794 \tTrain Acc: 95.20833% \tVal Acc: 98.8888885%\n",
      "Epoch: 396\tTrain Loss: 0.1707039 \tVal Loss:0.0482087 \tTrain Acc: 96.11111% \tVal Acc: 98.3333329%\n",
      "Epoch: 397\tTrain Loss: 0.1607741 \tVal Loss:0.0620503 \tTrain Acc: 96.45833% \tVal Acc: 98.0555549%\n",
      "Epoch: 398\tTrain Loss: 0.1753040 \tVal Loss:0.0616017 \tTrain Acc: 95.06944% \tVal Acc: 98.6111109%\n",
      "Epoch: 399\tTrain Loss: 0.1643574 \tVal Loss:0.0464414 \tTrain Acc: 95.90278% \tVal Acc: 98.8888890%\n",
      "Epoch: 400\tTrain Loss: 0.1662944 \tVal Loss:0.0395183 \tTrain Acc: 95.06944% \tVal Acc: 98.8888890%\n",
      "Epoch: 401\tTrain Loss: 0.1510604 \tVal Loss:0.0392616 \tTrain Acc: 96.31944% \tVal Acc: 98.8888890%\n",
      "Epoch: 402\tTrain Loss: 0.1692651 \tVal Loss:0.0351941 \tTrain Acc: 95.41667% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.037857 to 0.035194, saving the model weights\n",
      "Epoch: 403\tTrain Loss: 0.1865902 \tVal Loss:0.0318487 \tTrain Acc: 94.58333% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.035194 to 0.031849, saving the model weights\n",
      "Epoch: 404\tTrain Loss: 0.1935650 \tVal Loss:0.0520646 \tTrain Acc: 95.27778% \tVal Acc: 98.3333329%\n",
      "Epoch: 405\tTrain Loss: 0.2562132 \tVal Loss:0.0691536 \tTrain Acc: 92.84722% \tVal Acc: 98.0555549%\n",
      "Epoch: 406\tTrain Loss: 0.3373018 \tVal Loss:0.0720731 \tTrain Acc: 89.65278% \tVal Acc: 98.0555549%\n",
      "Epoch: 407\tTrain Loss: 0.3519050 \tVal Loss:0.0792084 \tTrain Acc: 89.30555% \tVal Acc: 97.4999994%\n",
      "Epoch: 408\tTrain Loss: 0.2966454 \tVal Loss:0.0504902 \tTrain Acc: 90.97222% \tVal Acc: 98.6111104%\n",
      "Epoch: 409\tTrain Loss: 0.2688410 \tVal Loss:0.0486470 \tTrain Acc: 91.66667% \tVal Acc: 99.1666660%\n",
      "Epoch: 410\tTrain Loss: 0.2278719 \tVal Loss:0.0431894 \tTrain Acc: 93.68055% \tVal Acc: 98.8888885%\n",
      "Epoch: 411\tTrain Loss: 0.2160249 \tVal Loss:0.0351603 \tTrain Acc: 94.44444% \tVal Acc: 99.4444440%\n",
      "Epoch: 412\tTrain Loss: 0.1776910 \tVal Loss:0.0380769 \tTrain Acc: 95.0% \tVal Acc: 98.8888885%\n",
      "Epoch: 413\tTrain Loss: 0.1990128 \tVal Loss:0.0345527 \tTrain Acc: 93.125% \tVal Acc: 99.4444440%\n",
      "Epoch: 414\tTrain Loss: 0.2095791 \tVal Loss:0.0421499 \tTrain Acc: 93.95833% \tVal Acc: 98.8888890%\n",
      "Epoch: 415\tTrain Loss: 0.1852023 \tVal Loss:0.0289558 \tTrain Acc: 94.51389% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.031849 to 0.028956, saving the model weights\n",
      "Epoch: 416\tTrain Loss: 0.1590358 \tVal Loss:0.0276219 \tTrain Acc: 95.76389% \tVal Acc: 99.4444440%\n",
      "Validation Loss decreased from 0.028956 to 0.027622, saving the model weights\n",
      "Epoch: 417\tTrain Loss: 0.1605433 \tVal Loss:0.0287140 \tTrain Acc: 95.76389% \tVal Acc: 99.4444440%\n",
      "Epoch: 418\tTrain Loss: 0.1744008 \tVal Loss:0.1533554 \tTrain Acc: 95.90278% \tVal Acc: 96.9444439%\n",
      "Epoch: 419\tTrain Loss: 0.1743603 \tVal Loss:0.0376833 \tTrain Acc: 94.93055% \tVal Acc: 99.1666660%\n",
      "Epoch: 420\tTrain Loss: 0.1690851 \tVal Loss:0.0301465 \tTrain Acc: 95.13889% \tVal Acc: 99.1666660%\n",
      "Epoch: 421\tTrain Loss: 0.1624851 \tVal Loss:0.0336502 \tTrain Acc: 95.625% \tVal Acc: 98.6111109%\n",
      "Epoch: 422\tTrain Loss: 0.1513667 \tVal Loss:0.0350087 \tTrain Acc: 96.38889% \tVal Acc: 98.6111104%\n",
      "Epoch: 423\tTrain Loss: 0.1497103 \tVal Loss:0.0360439 \tTrain Acc: 95.41667% \tVal Acc: 98.3333329%\n",
      "Epoch: 424\tTrain Loss: 0.1518013 \tVal Loss:0.0333718 \tTrain Acc: 96.18055% \tVal Acc: 98.8888885%\n",
      "Epoch: 425\tTrain Loss: 0.2027224 \tVal Loss:0.0344141 \tTrain Acc: 94.23611% \tVal Acc: 99.1666665%\n",
      "Epoch: 426\tTrain Loss: 0.2350628 \tVal Loss:0.0596001 \tTrain Acc: 92.43055% \tVal Acc: 98.6111109%\n",
      "Epoch: 427\tTrain Loss: 0.4351938 \tVal Loss:0.3834158 \tTrain Acc: 87.36111% \tVal Acc: 91.9444437%\n",
      "Epoch: 428\tTrain Loss: 1.0236185 \tVal Loss:1.1834822 \tTrain Acc: 72.08333% \tVal Acc: 72.7777784%\n",
      "Epoch: 429\tTrain Loss: 0.9125837 \tVal Loss:0.2091725 \tTrain Acc: 75.0% \tVal Acc: 94.1666658%\n",
      "Epoch: 430\tTrain Loss: 0.4619568 \tVal Loss:0.1433566 \tTrain Acc: 85.83333% \tVal Acc: 96.3888884%\n",
      "Epoch: 431\tTrain Loss: 0.4235779 \tVal Loss:0.1706423 \tTrain Acc: 87.70833% \tVal Acc: 95.2777773%\n",
      "Epoch: 432\tTrain Loss: 0.3556414 \tVal Loss:0.1660775 \tTrain Acc: 89.16667% \tVal Acc: 96.1111108%\n",
      "Epoch: 433\tTrain Loss: 0.2863081 \tVal Loss:0.0476822 \tTrain Acc: 91.11111% \tVal Acc: 98.8888890%\n",
      "Epoch: 434\tTrain Loss: 0.2026402 \tVal Loss:0.0415251 \tTrain Acc: 94.51389% \tVal Acc: 98.8888890%\n",
      "Epoch: 435\tTrain Loss: 0.1752612 \tVal Loss:0.0384309 \tTrain Acc: 95.55555% \tVal Acc: 99.1666665%\n",
      "Epoch: 436\tTrain Loss: 0.1612295 \tVal Loss:0.0309946 \tTrain Acc: 95.34722% \tVal Acc: 99.4444445%\n",
      "Epoch: 437\tTrain Loss: 0.1295667 \tVal Loss:0.0300744 \tTrain Acc: 97.15278% \tVal Acc: 99.1666660%\n",
      "Epoch: 438\tTrain Loss: 0.1559004 \tVal Loss:0.0329412 \tTrain Acc: 95.34722% \tVal Acc: 98.6111104%\n",
      "Epoch: 439\tTrain Loss: 0.1503652 \tVal Loss:0.0280558 \tTrain Acc: 95.97222% \tVal Acc: 99.1666665%\n",
      "Epoch: 440\tTrain Loss: 0.1325052 \tVal Loss:0.0295386 \tTrain Acc: 96.875% \tVal Acc: 99.1666665%\n",
      "Epoch: 441\tTrain Loss: 0.1320345 \tVal Loss:0.0261248 \tTrain Acc: 96.73611% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.027622 to 0.026125, saving the model weights\n",
      "Epoch: 442\tTrain Loss: 0.1238576 \tVal Loss:0.0298281 \tTrain Acc: 96.59722% \tVal Acc: 98.6111104%\n",
      "Epoch: 443\tTrain Loss: 0.1103661 \tVal Loss:0.0290455 \tTrain Acc: 97.01389% \tVal Acc: 99.4444445%\n",
      "Epoch: 444\tTrain Loss: 0.1446941 \tVal Loss:0.1046912 \tTrain Acc: 95.83333% \tVal Acc: 96.6666669%\n",
      "Epoch: 445\tTrain Loss: 0.1715178 \tVal Loss:0.0784397 \tTrain Acc: 95.13889% \tVal Acc: 97.2222214%\n",
      "Epoch: 446\tTrain Loss: 0.1447474 \tVal Loss:0.0343125 \tTrain Acc: 96.38889% \tVal Acc: 98.6111109%\n",
      "Epoch: 447\tTrain Loss: 0.1348374 \tVal Loss:0.0340835 \tTrain Acc: 96.66667% \tVal Acc: 98.8888890%\n",
      "Epoch: 448\tTrain Loss: 0.1121338 \tVal Loss:0.0250251 \tTrain Acc: 97.43056% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.026125 to 0.025025, saving the model weights\n",
      "Epoch: 449\tTrain Loss: 0.1368379 \tVal Loss:0.0274777 \tTrain Acc: 97.01389% \tVal Acc: 99.1666660%\n",
      "Epoch: 450\tTrain Loss: 0.1291351 \tVal Loss:0.0267512 \tTrain Acc: 97.29167% \tVal Acc: 99.1666665%\n",
      "Epoch: 451\tTrain Loss: 0.1143299 \tVal Loss:0.0245289 \tTrain Acc: 97.08333% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.025025 to 0.024529, saving the model weights\n",
      "Epoch: 452\tTrain Loss: 0.1092156 \tVal Loss:0.0257476 \tTrain Acc: 97.43055% \tVal Acc: 99.1666660%\n",
      "Epoch: 453\tTrain Loss: 0.1247898 \tVal Loss:0.0290064 \tTrain Acc: 96.73611% \tVal Acc: 98.8888885%\n",
      "Epoch: 454\tTrain Loss: 0.1261106 \tVal Loss:0.0282238 \tTrain Acc: 97.01389% \tVal Acc: 98.6111109%\n",
      "Epoch: 455\tTrain Loss: 0.1224328 \tVal Loss:0.0277590 \tTrain Acc: 96.31944% \tVal Acc: 98.6111109%\n",
      "Epoch: 456\tTrain Loss: 0.1099170 \tVal Loss:0.0208365 \tTrain Acc: 96.38889% \tVal Acc: 99.1666665%\n",
      "Validation Loss decreased from 0.024529 to 0.020837, saving the model weights\n",
      "Epoch: 457\tTrain Loss: 0.0960956 \tVal Loss:0.0208637 \tTrain Acc: 97.77778% \tVal Acc: 99.4444440%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 458\tTrain Loss: 0.1144196 \tVal Loss:0.0217776 \tTrain Acc: 97.5% \tVal Acc: 99.1666665%\n",
      "Epoch: 459\tTrain Loss: 0.1105025 \tVal Loss:0.0211817 \tTrain Acc: 97.15278% \tVal Acc: 99.1666665%\n",
      "Epoch: 460\tTrain Loss: 0.1194557 \tVal Loss:0.0215966 \tTrain Acc: 96.45833% \tVal Acc: 99.1666665%\n",
      "Epoch: 461\tTrain Loss: 0.1255931 \tVal Loss:0.0219419 \tTrain Acc: 96.73611% \tVal Acc: 98.8888885%\n",
      "Epoch: 462\tTrain Loss: 0.1281422 \tVal Loss:0.0259165 \tTrain Acc: 96.66667% \tVal Acc: 98.8888885%\n",
      "Epoch: 463\tTrain Loss: 0.1075597 \tVal Loss:0.0259850 \tTrain Acc: 97.43056% \tVal Acc: 99.1666665%\n",
      "Epoch: 464\tTrain Loss: 0.1146186 \tVal Loss:0.0246508 \tTrain Acc: 96.73611% \tVal Acc: 99.1666660%\n",
      "Epoch: 465\tTrain Loss: 0.1072948 \tVal Loss:0.0238099 \tTrain Acc: 97.15278% \tVal Acc: 99.1666665%\n",
      "Epoch: 466\tTrain Loss: 0.1007921 \tVal Loss:0.0215547 \tTrain Acc: 97.84722% \tVal Acc: 99.1666660%\n",
      "Epoch: 467\tTrain Loss: 0.1049922 \tVal Loss:0.0203328 \tTrain Acc: 97.36111% \tVal Acc: 99.1666660%\n",
      "Validation Loss decreased from 0.020837 to 0.020333, saving the model weights\n",
      "Epoch: 468\tTrain Loss: 0.0970777 \tVal Loss:0.0208236 \tTrain Acc: 97.77778% \tVal Acc: 99.1666665%\n",
      "Epoch: 469\tTrain Loss: 0.1108572 \tVal Loss:0.0203408 \tTrain Acc: 96.94444% \tVal Acc: 99.1666665%\n",
      "Epoch: 470\tTrain Loss: 0.0932523 \tVal Loss:0.0228847 \tTrain Acc: 97.77778% \tVal Acc: 98.8888890%\n",
      "Epoch: 471\tTrain Loss: 0.1039864 \tVal Loss:0.0216959 \tTrain Acc: 97.22222% \tVal Acc: 98.8888885%\n",
      "Epoch: 472\tTrain Loss: 0.0913269 \tVal Loss:0.0195667 \tTrain Acc: 98.125% \tVal Acc: 99.1666660%\n",
      "Validation Loss decreased from 0.020333 to 0.019567, saving the model weights\n",
      "Epoch: 473\tTrain Loss: 0.1064124 \tVal Loss:0.0191135 \tTrain Acc: 96.94444% \tVal Acc: 99.4444445%\n",
      "Validation Loss decreased from 0.019567 to 0.019114, saving the model weights\n",
      "Epoch: 474\tTrain Loss: 0.1021154 \tVal Loss:0.0248855 \tTrain Acc: 97.22222% \tVal Acc: 98.6111109%\n",
      "Epoch: 475\tTrain Loss: 0.1125807 \tVal Loss:0.0253033 \tTrain Acc: 97.43055% \tVal Acc: 99.1666665%\n",
      "Epoch: 476\tTrain Loss: 0.6617930 \tVal Loss:1.3116316 \tTrain Acc: 81.66667% \tVal Acc: 71.6666661%\n",
      "Epoch: 477\tTrain Loss: 1.1837887 \tVal Loss:0.9415656 \tTrain Acc: 68.05556% \tVal Acc: 73.6111109%\n",
      "Epoch: 478\tTrain Loss: 3.5765931 \tVal Loss:6.5452195 \tTrain Acc: 40.69444% \tVal Acc: 8.6111114%\n",
      "Epoch: 479\tTrain Loss: 5.3928414 \tVal Loss:3.8337559 \tTrain Acc: 3.75% \tVal Acc: 2.7777779%\n",
      "Epoch: 480\tTrain Loss: 4.0866250 \tVal Loss:3.2042278 \tTrain Acc: 4.513889% \tVal Acc: 4.7222224%\n",
      "Epoch: 481\tTrain Loss: 3.5076174 \tVal Loss:3.1746370 \tTrain Acc: 5.625% \tVal Acc: 6.6666670%\n",
      "Epoch: 482\tTrain Loss: 3.3465135 \tVal Loss:3.1408735 \tTrain Acc: 7.152778% \tVal Acc: 9.1666670%\n",
      "Epoch: 483\tTrain Loss: 3.2962600 \tVal Loss:3.0749834 \tTrain Acc: 7.847222% \tVal Acc: 11.6666670%\n",
      "Epoch: 484\tTrain Loss: 3.2569297 \tVal Loss:2.9977880 \tTrain Acc: 7.777778% \tVal Acc: 13.8888893%\n",
      "Epoch: 485\tTrain Loss: 3.1897387 \tVal Loss:2.9504515 \tTrain Acc: 7.361111% \tVal Acc: 11.1111115%\n",
      "Epoch: 486\tTrain Loss: 3.1816830 \tVal Loss:2.8544800 \tTrain Acc: 9.236111% \tVal Acc: 15.8333339%\n",
      "Epoch: 487\tTrain Loss: 3.1366039 \tVal Loss:2.8920017 \tTrain Acc: 8.472223% \tVal Acc: 15.5555562%\n",
      "Epoch: 488\tTrain Loss: 3.0871268 \tVal Loss:2.8358869 \tTrain Acc: 9.236111% \tVal Acc: 16.3888896%\n",
      "Epoch: 489\tTrain Loss: 3.0672189 \tVal Loss:2.8728465 \tTrain Acc: 8.194445% \tVal Acc: 16.1111117%\n",
      "Epoch: 490\tTrain Loss: 3.0155779 \tVal Loss:2.7447719 \tTrain Acc: 10.48611% \tVal Acc: 15.2777782%\n",
      "Epoch: 491\tTrain Loss: 2.9996587 \tVal Loss:2.8045766 \tTrain Acc: 10.0% \tVal Acc: 15.2777783%\n",
      "Epoch: 492\tTrain Loss: 2.9974032 \tVal Loss:2.7278499 \tTrain Acc: 10.55556% \tVal Acc: 15.2777782%\n",
      "Epoch: 493\tTrain Loss: 2.9986026 \tVal Loss:3.4739134 \tTrain Acc: 10.76389% \tVal Acc: 8.8888890%\n",
      "Epoch: 494\tTrain Loss: 3.1900457 \tVal Loss:2.8316124 \tTrain Acc: 8.263889% \tVal Acc: 15.0000005%\n",
      "Epoch: 495\tTrain Loss: 2.9830147 \tVal Loss:2.7323127 \tTrain Acc: 11.94444% \tVal Acc: 19.7222230%\n",
      "Epoch: 496\tTrain Loss: 2.9869345 \tVal Loss:2.6738207 \tTrain Acc: 11.875% \tVal Acc: 18.0555562%\n",
      "Epoch: 497\tTrain Loss: 2.9398238 \tVal Loss:2.6700666 \tTrain Acc: 11.25% \tVal Acc: 18.3333339%\n",
      "Epoch: 498\tTrain Loss: 2.9007682 \tVal Loss:2.6620798 \tTrain Acc: 12.01389% \tVal Acc: 18.0555561%\n",
      "Epoch: 499\tTrain Loss: 2.9101149 \tVal Loss:2.6306837 \tTrain Acc: 11.94444% \tVal Acc: 20.0000005%\n"
     ]
    }
   ],
   "source": [
    "epochs = 700\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        '''\n",
    "        Creating new variables for the hidden state, otherwise\n",
    "        we'd backprop through the entire training history\n",
    "        '''\n",
    "        \n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "       \n",
    "        # get the output from the model\n",
    "        output = model.forward(inputs, train_batch_size)\n",
    "        #print('OUTPUT', output)\n",
    "        \n",
    "        \n",
    "        #print('Labels Shape :-', (torch.max(labels, 1)[1]).shape)\n",
    "    \n",
    "        # calculate the loss and perform backprop\n",
    "        #print('Labels Long :-', labels.long())\n",
    "        loss = criterion(output,labels.long())\n",
    "        #print('LOSS IS :-', loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #calculate training accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        logging.debug(' top probab {} top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        #print(train_loss)\n",
    "              \n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "                \n",
    "        output = model.forward(inputs, val_batch_size)\n",
    "       \n",
    "        loss = criterion(output,labels.long())\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        #calculate validation accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        \n",
    "        #logging.debug(output)\n",
    "        #logging.debug('VALIDATION top probab {} VALIDATION top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        #print('Top Class:- ',top_class)\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        #print('Equals:- ', equals)\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    #Averaging losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    val_accuracy = val_accuracy/len(val_loader)\n",
    "    train_accuracy = train_accuracy/len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}\\tTrain Loss: {:.7f} \\tVal Loss:{:.7f} \\tTrain Acc: {:.7}% \\tVal Acc: {:.7f}%'.format(e, train_loss, val_loss, train_accuracy*100,val_accuracy*100))\n",
    "    \n",
    "    #saving the model if validation loss is decreased\n",
    "    if val_loss <= min_val_loss:\n",
    "        print('Validation Loss decreased from {:6f} to {:6f}, saving the model weights'.format(min_val_loss, val_loss))\n",
    "        torch.save(model.state_dict(), 'lstm_state_256-38-our_normalization_bakchodi.pt')\n",
    "        min_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUSIC GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm1): LSTM(1, 256, batch_first=True)\n",
       "  (lstm2): LSTM(256, 38, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=38, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights\n",
    "test_model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "test_model.load_state_dict(torch.load('lstm_state_256-38-our_normalization_bakchodi.pt'))\n",
    "test_model.eval()\n",
    "test_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population database\n",
    "#testing_data = np.ones(200)*1\n",
    "# testing_data = list(range(50,90))\n",
    "# testing_data.extend(testing_data[::-1])\n",
    "# testing_data_rev = testing_data[::-1]\n",
    "# testing_data_rev.extend(testing_data)\n",
    "# testing_data_rev.extend(testing_data_rev)\n",
    "# testing_data = testing_data_rev\n",
    "\n",
    "\n",
    "# testing_data = np.asarray(testing_data)\n",
    "# testing_data = testing_data.reshape(testing_data.shape[0],1)\n",
    "\n",
    "initial_seq = [network_input[0][1:].cpu().numpy().tolist()]\n",
    "\n",
    "testing_data_unnorm = testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "testing_data=testing_data.tolist()\n",
    "for i in range(len(testing_data)):\n",
    "    list1.extend(testing_data[i])\n",
    "\n",
    "#list1\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    list1[i]=(list1[i]-50)/(89-50)\n",
    "#     list1[i]=(list1[i])/(89)\n",
    "\n",
    "list1 = np.asarray(list1)\n",
    "list1 = list1.reshape(list1.shape[0],1)\n",
    "testing_data = list1\n",
    "#list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting\n",
    "def prediction_with_influence(influence,int2note,initial_seq, max_note, min_note,test_batch_size = 1):\n",
    "\n",
    "    predicted_notes = []\n",
    "    initial_seq[0].extend([[0]]*len(testing_data))\n",
    "    test_seq = torch.Tensor(initial_seq).cuda()\n",
    "    \n",
    "    for i in range(len(influence)):\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = float(influence[i])\n",
    "        \n",
    "        test_slice = test_seq[0][i : i + sequence_length]        \n",
    "        test_slice = test_slice.view(1, test_slice.shape[0], test_slice.shape[1])\n",
    "        \n",
    "        test_output = test_model.forward(test_slice, test_batch_size)\n",
    "    \n",
    "        test_output = F.softmax(test_output, dim = 1)\n",
    "        top_p, top_class = test_output.topk(1,dim =1)\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = (int2note[top_class.item()] - min_note)/(max_note - min_note)\n",
    "#         test_seq[0][sequence_length - 1 + i][0] = int2note[top_class.item()]/max_note\n",
    "        \n",
    "        predicted_notes.append(int2note[top_class.item()])\n",
    "        \n",
    "    return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_notes_lst = prediction_with_influence(testing_data,int_to_note,initial_seq, max_midi_number, min_midi_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_notes_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2656d369bc8>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(list(np.ones(200)*89))\n",
    "\n",
    "#plt.plot(list(np.ones(200)*50))\n",
    "#plt.plot(list(np.ones(20)*50))\n",
    "plt.plot(testing_data_unnorm*89)\n",
    "plt.plot(predicted_notes_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({72: 15,\n",
       "         74: 22,\n",
       "         76: 17,\n",
       "         78: 10,\n",
       "         79: 10,\n",
       "         81: 19,\n",
       "         83: 9,\n",
       "         67: 7,\n",
       "         77: 82,\n",
       "         88: 10,\n",
       "         86: 19,\n",
       "         82: 22,\n",
       "         89: 5,\n",
       "         84: 45,\n",
       "         71: 14,\n",
       "         73: 7,\n",
       "         69: 5,\n",
       "         75: 1,\n",
       "         80: 1})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(predicted_notes_lst)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Postprocessing.postprocessing import PostProcessing\n",
    "PostProcessing().generate_midi_file('hello.midi', predicted_notes_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int_to_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
