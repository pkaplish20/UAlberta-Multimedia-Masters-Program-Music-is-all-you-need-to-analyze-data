{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "#import torch.utils.tensorboard as tb\n",
    "from Preprocessing.preprocessing_reading_text_files import PreprocessingTrainingData\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as  plt\n",
    "import os\n",
    "import logging\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static parameters\n",
    "train_batch_size = 60\n",
    "val_batch_size = 60\n",
    "sequence_length=50\n",
    "test_batch_size = 1\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layer = 2\n",
    "output_size = 41\n",
    "clip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n",
      "[50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90]\n",
      "{127: 'G9', 126: 'F#9/G-9', 125: 'F9', 124: 'E9', 123: 'D#9/E-9', 122: 'D9', 121: 'C#9/D-9', 120: 'C9', 119: 'B8', 118: 'A#8/B-8', 117: 'A8', 116: 'G#8/A-8', 115: 'G8', 114: 'F#8/G-8', 113: 'F8', 112: 'E8', 111: 'D#8/E-8', 110: 'D8', 109: 'C#8/D-8', 108: 'C8', 107: 'B7', 106: 'A#7/B-7', 105: 'A7', 104: 'G#7/A-7', 103: 'G7', 102: 'F#7/G-7', 101: 'F7', 100: 'E7', 99: 'D#7/E-7', 98: 'D7', 97: 'C#7/D-7', 96: 'C7', 95: 'B6', 94: 'A#6/B-6', 93: 'A6', 92: 'G#6/A-6', 91: 'G6', 90: 'F#6/G-6', 89: 'F6', 88: 'E6', 87: 'D#6/E-6', 86: 'D6', 85: 'C#6/D-6', 84: 'C6', 83: 'B5', 82: 'A#5/B-5', 81: 'A5', 80: 'G#5/A-5', 79: 'G5', 78: 'F#5/G-5', 77: 'F5', 76: 'E5', 75: 'D#5/E-5', 74: 'D5', 73: 'C#5/D-5', 72: 'C5', 71: 'B4', 70: 'A#4/B-4', 69: 'A4', 68: 'G#4/A-4', 67: 'G4', 66: 'F#4/G-4', 65: 'F4', 64: 'E4', 63: 'D#4/E-4', 62: 'D4', 61: 'C#4/D-4', 60: 'C4', 59: 'B3', 58: 'A#3/B-3', 57: 'A3', 56: 'G#3/A-3', 55: 'G3', 54: 'F#3/G-3', 53: 'F3', 52: 'E3', 51: 'D#3/E-3', 50: 'D3', 49: 'C#3/D-3', 48: 'C3', 47: 'B2', 46: 'A#2/B-2', 45: 'A2', 44: 'G#2/A-2', 43: 'G2', 42: 'F#2/G-2', 41: 'F2', 40: 'E2', 39: 'D#2/E-2', 38: 'D2', 37: 'C#2/D-2', 36: 'C2', 35: 'B1', 34: 'A#1/B-1', 33: 'A1', 32: 'G#1/A-1', 31: 'G1', 30: 'F#1/G-1', 29: 'F1', 28: 'E1', 27: 'D#1/E-1', 26: 'D1', 25: 'C#1/D-1', 24: 'C1', 23: 'B0', 22: 'A#0/B-0', 21: 'A0'}\n"
     ]
    }
   ],
   "source": [
    "#get data from preprocessing.py\n",
    "dataset_path = os.path.join(os.path.abspath('..'),'Dataset\\\\traj' )\n",
    "network_input,network_output,max_midi_number,min_midi_number,int_to_note = PreprocessingTrainingData().preprocess_notes(dataset_path)\n",
    "network_input, network_output = network_input.cuda(), network_output.cuda()\n",
    "\n",
    "# print(network_input)\n",
    "#print(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(40, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(network_output.max())\n",
    "print(network_output.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "network_input=(network_input.cpu().numpy().tolist())\n",
    "for i in range(len(network_input)):\n",
    "    for j in range(len(network_input[i])):\n",
    "        network_input[i][j][0]=((network_input[i][j][0])*(max_midi_number-min_midi_number)+min_midi_number)/max_midi_number\n",
    "network_input=torch.Tensor(network_input).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0')\n",
      "tensor(0.5556, device='cuda:0')\n",
      "90\n",
      "50\n",
      "{0: 50, 1: 51, 2: 52, 3: 53, 4: 54, 5: 55, 6: 56, 7: 57, 8: 58, 9: 59, 10: 60, 11: 61, 12: 62, 13: 63, 14: 64, 15: 65, 16: 66, 17: 67, 18: 68, 19: 69, 20: 70, 21: 71, 22: 72, 23: 73, 24: 74, 25: 75, 26: 76, 27: 77, 28: 78, 29: 79, 30: 80, 31: 81, 32: 82, 33: 83, 34: 84, 35: 85, 36: 86, 37: 87, 38: 88, 39: 89, 40: 90}\n"
     ]
    }
   ],
   "source": [
    "print(network_input.max())\n",
    "print(network_input.min())\n",
    "print(max_midi_number)\n",
    "print(min_midi_number)\n",
    "print(int_to_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata is highly unbalanced\\n# '"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data is highly unbalanced\n",
    "# '''\n",
    "# sns.distplot(torch.tensor(network_output).cpu())\n",
    "# xx = pd.DataFrame(torch.tensor(network_output).cpu())\n",
    "# xx.groupby(0).size().to_frame(name='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3000, 50, 1])\n",
      "torch.Size([3000])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "to make batch of equal sizes\n",
    "Quick Fix\n",
    "'''\n",
    "network_input = network_input[: -58]\n",
    "network_output = network_output[: -58]\n",
    "\n",
    "print(network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Bakchodi Normalization\n",
    "# network_input=network_input.cpu().numpy().tolist()\n",
    "# for i in range(len(network_input)):\n",
    "#     for j in range(len(network_input[i])):\n",
    "#         network_input[i][j][0]=((network_input[i][j][0])*(max_midi_number-min_midi_number)+min_midi_number)/max_midi_number\n",
    "# network_input=torch.Tensor(network_input).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_input[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create Stacked LSTM model\n",
    "'''\n",
    "class Stacked_LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size = input_size, hidden_size = hidden_size, batch_first = True)\n",
    "        self.lstm2 = nn.LSTM(input_size = hidden_size, hidden_size = output_size,batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(output_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, batch_size):\n",
    "        \n",
    "        output, _ = self.lstm1(x)        \n",
    "        #print(output.shape)\n",
    "        \n",
    "        #output = self.dropout(output)\n",
    "        \n",
    "        output, _ = self.lstm2(output)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        output = output.contiguous().view(-1, 41)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        #print('Linear Output :-',output.shape)\n",
    "        \n",
    "        #output = F.softmax(output, dim = 1)\n",
    "        #print('SOFTMAX OUTPUT :--', output)\n",
    "        \n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1)\n",
    "        #print('Reshape to batch size first :-',output.shape)\n",
    "        \n",
    "        output = output[:, -self.output_size:] # get last batch of labels\n",
    "        #print('Final Output :-',output)\n",
    "        #print('RESHAPE SIZE :-', output.shape)\n",
    "        \n",
    "        return output\n",
    "\n",
    "#initialize the weights of LSTM using Xavier initialization    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Divide the dataset into train/val \n",
    "'''\n",
    "train_size = 0.8\n",
    "indices = list(range(len(network_input)))\n",
    "split = int(np.floor(train_size*len(network_input)))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SequentialSampler(train_idx)\n",
    "val_sampler = SequentialSampler(val_idx)\n",
    "\n",
    "dataset = TensorDataset(network_input,network_output)\n",
    "train_loader = DataLoader(dataset, batch_size= train_batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size= val_batch_size,sampler= val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm1): LSTM(1, 256, batch_first=True)\n",
       "  (lstm2): LSTM(256, 41, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=41, out_features=41, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optimizer\n",
    "\n",
    "model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer.AdamW(model.parameters())\n",
    "#optimizer = optimizer.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "#make sure to transfer model to GPU after initializing optimizer\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 3.7069556 \tVal Loss:3.6594327 \tTrain Acc: 3.5% \tVal Acc: 6.3333336%\n",
      "Validation Loss decreased from    inf to 3.659433, saving the model weights\n",
      "Epoch: 1\tTrain Loss: 3.6978627 \tVal Loss:3.6284817 \tTrain Acc: 2.291667% \tVal Acc: 4.3333335%\n",
      "Validation Loss decreased from 3.659433 to 3.628482, saving the model weights\n",
      "Epoch: 2\tTrain Loss: 3.6786136 \tVal Loss:3.6172207 \tTrain Acc: 2.75% \tVal Acc: 1.5000001%\n",
      "Validation Loss decreased from 3.628482 to 3.617221, saving the model weights\n",
      "Epoch: 3\tTrain Loss: 3.6682724 \tVal Loss:3.6143053 \tTrain Acc: 2.5% \tVal Acc: 2.3333334%\n",
      "Validation Loss decreased from 3.617221 to 3.614305, saving the model weights\n",
      "Epoch: 4\tTrain Loss: 3.6661001 \tVal Loss:3.6111070 \tTrain Acc: 2.208333% \tVal Acc: 1.5000001%\n",
      "Validation Loss decreased from 3.614305 to 3.611107, saving the model weights\n",
      "Epoch: 5\tTrain Loss: 3.6622595 \tVal Loss:3.6104720 \tTrain Acc: 2.125% \tVal Acc: 1.5000001%\n",
      "Validation Loss decreased from 3.611107 to 3.610472, saving the model weights\n",
      "Epoch: 6\tTrain Loss: 3.6575665 \tVal Loss:3.6099244 \tTrain Acc: 2.541667% \tVal Acc: 2.3333334%\n",
      "Validation Loss decreased from 3.610472 to 3.609924, saving the model weights\n",
      "Epoch: 7\tTrain Loss: 3.6539440 \tVal Loss:3.6066383 \tTrain Acc: 2.25% \tVal Acc: 1.5000001%\n",
      "Validation Loss decreased from 3.609924 to 3.606638, saving the model weights\n",
      "Epoch: 8\tTrain Loss: 3.6510822 \tVal Loss:3.6056064 \tTrain Acc: 2.75% \tVal Acc: 2.3333334%\n",
      "Validation Loss decreased from 3.606638 to 3.605606, saving the model weights\n",
      "Epoch: 9\tTrain Loss: 3.6522547 \tVal Loss:3.6023467 \tTrain Acc: 2.458333% \tVal Acc: 2.3333334%\n",
      "Validation Loss decreased from 3.605606 to 3.602347, saving the model weights\n",
      "Epoch: 10\tTrain Loss: 3.6455830 \tVal Loss:3.6021908 \tTrain Acc: 3.083333% \tVal Acc: 1.5000001%\n",
      "Validation Loss decreased from 3.602347 to 3.602191, saving the model weights\n",
      "Epoch: 11\tTrain Loss: 3.6467350 \tVal Loss:3.6009897 \tTrain Acc: 2.458333% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.602191 to 3.600990, saving the model weights\n",
      "Epoch: 12\tTrain Loss: 3.6485357 \tVal Loss:3.6000985 \tTrain Acc: 2.166667% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.600990 to 3.600099, saving the model weights\n",
      "Epoch: 13\tTrain Loss: 3.6482883 \tVal Loss:3.5991657 \tTrain Acc: 2.083333% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.600099 to 3.599166, saving the model weights\n",
      "Epoch: 14\tTrain Loss: 3.6476731 \tVal Loss:3.5989071 \tTrain Acc: 2.333333% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.599166 to 3.598907, saving the model weights\n",
      "Epoch: 15\tTrain Loss: 3.6462934 \tVal Loss:3.5978723 \tTrain Acc: 2.375% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.598907 to 3.597872, saving the model weights\n",
      "Epoch: 16\tTrain Loss: 3.6485137 \tVal Loss:3.5982005 \tTrain Acc: 2.333333% \tVal Acc: 2.3333334%\n",
      "Epoch: 17\tTrain Loss: 3.6464374 \tVal Loss:3.5971807 \tTrain Acc: 2.208333% \tVal Acc: 2.3333334%\n",
      "Validation Loss decreased from 3.597872 to 3.597181, saving the model weights\n",
      "Epoch: 18\tTrain Loss: 3.6418109 \tVal Loss:3.5954188 \tTrain Acc: 2.083333% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.597181 to 3.595419, saving the model weights\n",
      "Epoch: 19\tTrain Loss: 3.6450888 \tVal Loss:3.5948037 \tTrain Acc: 2.0% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.595419 to 3.594804, saving the model weights\n",
      "Epoch: 20\tTrain Loss: 3.6428571 \tVal Loss:3.5956943 \tTrain Acc: 2.208333% \tVal Acc: 0.5000000%\n",
      "Epoch: 21\tTrain Loss: 3.6434219 \tVal Loss:3.5951346 \tTrain Acc: 2.333333% \tVal Acc: 0.5000000%\n",
      "Epoch: 22\tTrain Loss: 3.6457135 \tVal Loss:3.5952551 \tTrain Acc: 2.0% \tVal Acc: 0.5000000%\n",
      "Epoch: 23\tTrain Loss: 3.6454204 \tVal Loss:3.5951360 \tTrain Acc: 1.375% \tVal Acc: 0.8333334%\n",
      "Epoch: 24\tTrain Loss: 3.6407876 \tVal Loss:3.5940859 \tTrain Acc: 2.291667% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.594804 to 3.594086, saving the model weights\n",
      "Epoch: 25\tTrain Loss: 3.6416162 \tVal Loss:3.5936200 \tTrain Acc: 2.333333% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.594086 to 3.593620, saving the model weights\n",
      "Epoch: 26\tTrain Loss: 3.6417911 \tVal Loss:3.5941818 \tTrain Acc: 2.25% \tVal Acc: 0.5000000%\n",
      "Epoch: 27\tTrain Loss: 3.6400564 \tVal Loss:3.5946264 \tTrain Acc: 1.833333% \tVal Acc: 0.5000000%\n",
      "Epoch: 28\tTrain Loss: 3.6413344 \tVal Loss:3.5936691 \tTrain Acc: 1.833333% \tVal Acc: 0.5000000%\n",
      "Epoch: 29\tTrain Loss: 3.6425037 \tVal Loss:3.5934897 \tTrain Acc: 2.0% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.593620 to 3.593490, saving the model weights\n",
      "Epoch: 30\tTrain Loss: 3.6398991 \tVal Loss:3.5934183 \tTrain Acc: 2.416667% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.593490 to 3.593418, saving the model weights\n",
      "Epoch: 31\tTrain Loss: 3.6405428 \tVal Loss:3.5932945 \tTrain Acc: 2.458333% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.593418 to 3.593295, saving the model weights\n",
      "Epoch: 32\tTrain Loss: 3.6400114 \tVal Loss:3.5932678 \tTrain Acc: 2.125% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.593295 to 3.593268, saving the model weights\n",
      "Epoch: 33\tTrain Loss: 3.6402041 \tVal Loss:3.5934750 \tTrain Acc: 1.875% \tVal Acc: 0.5000000%\n",
      "Epoch: 34\tTrain Loss: 3.6392922 \tVal Loss:3.5930794 \tTrain Acc: 1.958333% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.593268 to 3.593079, saving the model weights\n",
      "Epoch: 35\tTrain Loss: 3.6398625 \tVal Loss:3.5922847 \tTrain Acc: 1.916667% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.593079 to 3.592285, saving the model weights\n",
      "Epoch: 36\tTrain Loss: 3.6388030 \tVal Loss:3.5916628 \tTrain Acc: 2.958333% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.592285 to 3.591663, saving the model weights\n",
      "Epoch: 37\tTrain Loss: 3.6383465 \tVal Loss:3.5915897 \tTrain Acc: 1.833333% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.591663 to 3.591590, saving the model weights\n",
      "Epoch: 38\tTrain Loss: 3.6398438 \tVal Loss:3.5912885 \tTrain Acc: 1.833333% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.591590 to 3.591288, saving the model weights\n",
      "Epoch: 39\tTrain Loss: 3.6386329 \tVal Loss:3.5916411 \tTrain Acc: 2.5% \tVal Acc: 0.8333334%\n",
      "Epoch: 40\tTrain Loss: 3.6361514 \tVal Loss:3.5914263 \tTrain Acc: 3.083333% \tVal Acc: 0.8333334%\n",
      "Epoch: 41\tTrain Loss: 3.6397840 \tVal Loss:3.5910664 \tTrain Acc: 2.708333% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.591288 to 3.591066, saving the model weights\n",
      "Epoch: 42\tTrain Loss: 3.6389980 \tVal Loss:3.5910218 \tTrain Acc: 2.666667% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.591066 to 3.591022, saving the model weights\n",
      "Epoch: 43\tTrain Loss: 3.6379868 \tVal Loss:3.5909315 \tTrain Acc: 2.333333% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.591022 to 3.590932, saving the model weights\n",
      "Epoch: 44\tTrain Loss: 3.6385889 \tVal Loss:3.5905990 \tTrain Acc: 2.291667% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.590932 to 3.590599, saving the model weights\n",
      "Epoch: 45\tTrain Loss: 3.6358751 \tVal Loss:3.5902653 \tTrain Acc: 2.416667% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.590599 to 3.590265, saving the model weights\n",
      "Epoch: 46\tTrain Loss: 3.6373039 \tVal Loss:3.5903444 \tTrain Acc: 2.166667% \tVal Acc: 0.8333334%\n",
      "Epoch: 47\tTrain Loss: 3.6385875 \tVal Loss:3.5900436 \tTrain Acc: 2.166667% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.590265 to 3.590044, saving the model weights\n",
      "Epoch: 48\tTrain Loss: 3.6372182 \tVal Loss:3.5899544 \tTrain Acc: 2.375% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.590044 to 3.589954, saving the model weights\n",
      "Epoch: 49\tTrain Loss: 3.6384183 \tVal Loss:3.5898036 \tTrain Acc: 2.375% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.589954 to 3.589804, saving the model weights\n",
      "Epoch: 50\tTrain Loss: 3.6381555 \tVal Loss:3.5893723 \tTrain Acc: 2.208333% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.589804 to 3.589372, saving the model weights\n",
      "Epoch: 51\tTrain Loss: 3.6377596 \tVal Loss:3.5895785 \tTrain Acc: 2.291667% \tVal Acc: 0.8333334%\n",
      "Epoch: 52\tTrain Loss: 3.6377349 \tVal Loss:3.5891742 \tTrain Acc: 2.125% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.589372 to 3.589174, saving the model weights\n",
      "Epoch: 53\tTrain Loss: 3.6361573 \tVal Loss:3.5886952 \tTrain Acc: 2.208333% \tVal Acc: 0.8333334%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased from 3.589174 to 3.588695, saving the model weights\n",
      "Epoch: 54\tTrain Loss: 3.6380342 \tVal Loss:3.5887082 \tTrain Acc: 2.166667% \tVal Acc: 0.0000000%\n",
      "Epoch: 55\tTrain Loss: 3.6380529 \tVal Loss:3.5888958 \tTrain Acc: 2.041667% \tVal Acc: 0.0000000%\n",
      "Epoch: 56\tTrain Loss: 3.6363619 \tVal Loss:3.5890409 \tTrain Acc: 2.583333% \tVal Acc: 0.5000000%\n",
      "Epoch: 57\tTrain Loss: 3.6359763 \tVal Loss:3.5893891 \tTrain Acc: 2.625% \tVal Acc: 0.0000000%\n",
      "Epoch: 58\tTrain Loss: 3.6384891 \tVal Loss:3.5893646 \tTrain Acc: 2.208333% \tVal Acc: 0.5000000%\n",
      "Epoch: 59\tTrain Loss: 3.6371856 \tVal Loss:3.5888720 \tTrain Acc: 2.708333% \tVal Acc: 0.5000000%\n",
      "Epoch: 60\tTrain Loss: 3.6381938 \tVal Loss:3.5886773 \tTrain Acc: 2.5% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.588695 to 3.588677, saving the model weights\n",
      "Epoch: 61\tTrain Loss: 3.6372207 \tVal Loss:3.5880928 \tTrain Acc: 2.208333% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.588677 to 3.588093, saving the model weights\n",
      "Epoch: 62\tTrain Loss: 3.6368081 \tVal Loss:3.5879485 \tTrain Acc: 2.416667% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.588093 to 3.587948, saving the model weights\n",
      "Epoch: 63\tTrain Loss: 3.6345758 \tVal Loss:3.5876410 \tTrain Acc: 2.333333% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.587948 to 3.587641, saving the model weights\n",
      "Epoch: 64\tTrain Loss: 3.6358108 \tVal Loss:3.5873600 \tTrain Acc: 2.791667% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.587641 to 3.587360, saving the model weights\n",
      "Epoch: 65\tTrain Loss: 3.6363207 \tVal Loss:3.5875891 \tTrain Acc: 2.958333% \tVal Acc: 0.8333334%\n",
      "Epoch: 66\tTrain Loss: 3.6347106 \tVal Loss:3.5875228 \tTrain Acc: 2.041667% \tVal Acc: 0.0000000%\n",
      "Epoch: 67\tTrain Loss: 3.6381671 \tVal Loss:3.5874440 \tTrain Acc: 2.041667% \tVal Acc: 0.0000000%\n",
      "Epoch: 68\tTrain Loss: 3.6334686 \tVal Loss:3.5869778 \tTrain Acc: 2.791667% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.587360 to 3.586978, saving the model weights\n",
      "Epoch: 69\tTrain Loss: 3.6348575 \tVal Loss:3.5868203 \tTrain Acc: 2.041667% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.586978 to 3.586820, saving the model weights\n",
      "Epoch: 70\tTrain Loss: 3.6373234 \tVal Loss:3.5866438 \tTrain Acc: 1.875% \tVal Acc: 1.8333334%\n",
      "Validation Loss decreased from 3.586820 to 3.586644, saving the model weights\n",
      "Epoch: 71\tTrain Loss: 3.6361761 \tVal Loss:3.5868042 \tTrain Acc: 2.666667% \tVal Acc: 1.8333334%\n",
      "Epoch: 72\tTrain Loss: 3.6362834 \tVal Loss:3.5871083 \tTrain Acc: 1.833333% \tVal Acc: 1.8333334%\n",
      "Epoch: 73\tTrain Loss: 3.6369162 \tVal Loss:3.5871212 \tTrain Acc: 1.916667% \tVal Acc: 0.5000000%\n",
      "Epoch: 74\tTrain Loss: 3.6349753 \tVal Loss:3.5870414 \tTrain Acc: 2.833333% \tVal Acc: 0.8333334%\n",
      "Epoch: 75\tTrain Loss: 3.6358248 \tVal Loss:3.5870560 \tTrain Acc: 2.25% \tVal Acc: 0.5000000%\n",
      "Epoch: 76\tTrain Loss: 3.6349794 \tVal Loss:3.5870196 \tTrain Acc: 1.791667% \tVal Acc: 0.8333334%\n",
      "Epoch: 77\tTrain Loss: 3.6358211 \tVal Loss:3.5864258 \tTrain Acc: 2.25% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.586644 to 3.586426, saving the model weights\n",
      "Epoch: 78\tTrain Loss: 3.6342981 \tVal Loss:3.5862668 \tTrain Acc: 2.541667% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.586426 to 3.586267, saving the model weights\n",
      "Epoch: 79\tTrain Loss: 3.6355880 \tVal Loss:3.5862956 \tTrain Acc: 1.541667% \tVal Acc: 0.5000000%\n",
      "Epoch: 80\tTrain Loss: 3.6341843 \tVal Loss:3.5864869 \tTrain Acc: 2.0% \tVal Acc: 0.5000000%\n",
      "Epoch: 81\tTrain Loss: 3.6345426 \tVal Loss:3.5865323 \tTrain Acc: 2.041667% \tVal Acc: 0.8333334%\n",
      "Epoch: 82\tTrain Loss: 3.6340943 \tVal Loss:3.5863494 \tTrain Acc: 2.791667% \tVal Acc: 0.8333334%\n",
      "Epoch: 83\tTrain Loss: 3.6356920 \tVal Loss:3.5862306 \tTrain Acc: 2.291667% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.586267 to 3.586231, saving the model weights\n",
      "Epoch: 84\tTrain Loss: 3.6360128 \tVal Loss:3.5862774 \tTrain Acc: 2.625% \tVal Acc: 1.8333334%\n",
      "Epoch: 85\tTrain Loss: 3.6352468 \tVal Loss:3.5860290 \tTrain Acc: 2.708333% \tVal Acc: 1.8333334%\n",
      "Validation Loss decreased from 3.586231 to 3.586029, saving the model weights\n",
      "Epoch: 86\tTrain Loss: 3.6345587 \tVal Loss:3.5858319 \tTrain Acc: 2.583333% \tVal Acc: 1.8333334%\n",
      "Validation Loss decreased from 3.586029 to 3.585832, saving the model weights\n",
      "Epoch: 87\tTrain Loss: 3.6347635 \tVal Loss:3.5859638 \tTrain Acc: 2.125% \tVal Acc: 1.8333334%\n",
      "Epoch: 88\tTrain Loss: 3.6341060 \tVal Loss:3.5855206 \tTrain Acc: 2.291667% \tVal Acc: 1.8333334%\n",
      "Validation Loss decreased from 3.585832 to 3.585521, saving the model weights\n",
      "Epoch: 89\tTrain Loss: 3.6349960 \tVal Loss:3.5859162 \tTrain Acc: 2.333333% \tVal Acc: 0.0000000%\n",
      "Epoch: 90\tTrain Loss: 3.6359275 \tVal Loss:3.5859002 \tTrain Acc: 2.166667% \tVal Acc: 0.0000000%\n",
      "Epoch: 91\tTrain Loss: 3.6352219 \tVal Loss:3.5855594 \tTrain Acc: 2.0% \tVal Acc: 1.8333334%\n",
      "Epoch: 92\tTrain Loss: 3.6334196 \tVal Loss:3.5852062 \tTrain Acc: 2.458333% \tVal Acc: 1.8333334%\n",
      "Validation Loss decreased from 3.585521 to 3.585206, saving the model weights\n",
      "Epoch: 93\tTrain Loss: 3.6352520 \tVal Loss:3.5853560 \tTrain Acc: 2.5% \tVal Acc: 0.5000000%\n",
      "Epoch: 94\tTrain Loss: 3.6349451 \tVal Loss:3.5851662 \tTrain Acc: 2.666667% \tVal Acc: 1.8333334%\n",
      "Validation Loss decreased from 3.585206 to 3.585166, saving the model weights\n",
      "Epoch: 95\tTrain Loss: 3.6346375 \tVal Loss:3.5850632 \tTrain Acc: 2.166667% \tVal Acc: 1.8333334%\n",
      "Validation Loss decreased from 3.585166 to 3.585063, saving the model weights\n",
      "Epoch: 96\tTrain Loss: 3.6324716 \tVal Loss:3.5850884 \tTrain Acc: 2.625% \tVal Acc: 0.5000000%\n",
      "Epoch: 97\tTrain Loss: 3.6334661 \tVal Loss:3.5854950 \tTrain Acc: 2.791667% \tVal Acc: 0.0000000%\n",
      "Epoch: 98\tTrain Loss: 3.6348511 \tVal Loss:3.5855354 \tTrain Acc: 2.125% \tVal Acc: 0.0000000%\n",
      "Epoch: 99\tTrain Loss: 3.6348646 \tVal Loss:3.5854997 \tTrain Acc: 2.166667% \tVal Acc: 2.3333334%\n",
      "Epoch: 100\tTrain Loss: 3.6345449 \tVal Loss:3.5853697 \tTrain Acc: 1.916667% \tVal Acc: 0.0000000%\n",
      "Epoch: 101\tTrain Loss: 3.6340862 \tVal Loss:3.5850848 \tTrain Acc: 2.333333% \tVal Acc: 1.8333334%\n",
      "Epoch: 102\tTrain Loss: 3.6338625 \tVal Loss:3.5847840 \tTrain Acc: 1.916667% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.585063 to 3.584784, saving the model weights\n",
      "Epoch: 103\tTrain Loss: 3.6345503 \tVal Loss:3.5848795 \tTrain Acc: 2.25% \tVal Acc: 0.5000000%\n",
      "Epoch: 104\tTrain Loss: 3.6339523 \tVal Loss:3.5851343 \tTrain Acc: 2.541667% \tVal Acc: 2.3333334%\n",
      "Epoch: 105\tTrain Loss: 3.6319961 \tVal Loss:3.5849928 \tTrain Acc: 2.625% \tVal Acc: 2.3333334%\n",
      "Epoch: 106\tTrain Loss: 3.6338300 \tVal Loss:3.5845291 \tTrain Acc: 2.291667% \tVal Acc: 1.5000001%\n",
      "Validation Loss decreased from 3.584784 to 3.584529, saving the model weights\n",
      "Epoch: 107\tTrain Loss: 3.6339796 \tVal Loss:3.5844967 \tTrain Acc: 2.625% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.584529 to 3.584497, saving the model weights\n",
      "Epoch: 108\tTrain Loss: 3.6333515 \tVal Loss:3.5847427 \tTrain Acc: 2.25% \tVal Acc: 0.5000000%\n",
      "Epoch: 109\tTrain Loss: 3.6321455 \tVal Loss:3.5845697 \tTrain Acc: 2.416667% \tVal Acc: 0.5000000%\n",
      "Epoch: 110\tTrain Loss: 3.6341857 \tVal Loss:3.5843827 \tTrain Acc: 2.458333% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.584497 to 3.584383, saving the model weights\n",
      "Epoch: 111\tTrain Loss: 3.6348261 \tVal Loss:3.5841181 \tTrain Acc: 2.166667% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.584383 to 3.584118, saving the model weights\n",
      "Epoch: 112\tTrain Loss: 3.6322309 \tVal Loss:3.5842214 \tTrain Acc: 2.333333% \tVal Acc: 0.8333334%\n",
      "Epoch: 113\tTrain Loss: 3.6321566 \tVal Loss:3.5838794 \tTrain Acc: 2.625% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.584118 to 3.583879, saving the model weights\n",
      "Epoch: 114\tTrain Loss: 3.6340206 \tVal Loss:3.5838186 \tTrain Acc: 1.958333% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.583879 to 3.583819, saving the model weights\n",
      "Epoch: 115\tTrain Loss: 3.6329805 \tVal Loss:3.5838292 \tTrain Acc: 2.0% \tVal Acc: 0.8333334%\n",
      "Epoch: 116\tTrain Loss: 3.6335559 \tVal Loss:3.5840902 \tTrain Acc: 2.041667% \tVal Acc: 0.8333334%\n",
      "Epoch: 117\tTrain Loss: 3.6327770 \tVal Loss:3.5841302 \tTrain Acc: 1.666667% \tVal Acc: 0.8333334%\n",
      "Epoch: 118\tTrain Loss: 3.6330850 \tVal Loss:3.5839353 \tTrain Acc: 2.416667% \tVal Acc: 1.8333334%\n",
      "Epoch: 119\tTrain Loss: 3.6316337 \tVal Loss:3.5837752 \tTrain Acc: 2.375% \tVal Acc: 0.8333334%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased from 3.583819 to 3.583775, saving the model weights\n",
      "Epoch: 120\tTrain Loss: 3.6343553 \tVal Loss:3.5838497 \tTrain Acc: 1.958333% \tVal Acc: 0.8333334%\n",
      "Epoch: 121\tTrain Loss: 3.6340623 \tVal Loss:3.5837903 \tTrain Acc: 2.5% \tVal Acc: 0.8333334%\n",
      "Epoch: 122\tTrain Loss: 3.6333428 \tVal Loss:3.5839995 \tTrain Acc: 2.25% \tVal Acc: 0.8333334%\n",
      "Epoch: 123\tTrain Loss: 3.6327974 \tVal Loss:3.5839385 \tTrain Acc: 2.625% \tVal Acc: 0.5000000%\n",
      "Epoch: 124\tTrain Loss: 3.6329280 \tVal Loss:3.5839886 \tTrain Acc: 2.166667% \tVal Acc: 0.5000000%\n",
      "Epoch: 125\tTrain Loss: 3.6324182 \tVal Loss:3.5837543 \tTrain Acc: 2.291667% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.583775 to 3.583754, saving the model weights\n",
      "Epoch: 126\tTrain Loss: 3.6322647 \tVal Loss:3.5834317 \tTrain Acc: 2.208333% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.583754 to 3.583432, saving the model weights\n",
      "Epoch: 127\tTrain Loss: 3.6337856 \tVal Loss:3.5835385 \tTrain Acc: 1.791667% \tVal Acc: 0.5000000%\n",
      "Epoch: 128\tTrain Loss: 3.6331123 \tVal Loss:3.5835294 \tTrain Acc: 1.708333% \tVal Acc: 0.5000000%\n",
      "Epoch: 129\tTrain Loss: 3.6339745 \tVal Loss:3.5831514 \tTrain Acc: 2.083333% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.583432 to 3.583151, saving the model weights\n",
      "Epoch: 130\tTrain Loss: 3.6329369 \tVal Loss:3.5830382 \tTrain Acc: 2.208333% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.583151 to 3.583038, saving the model weights\n",
      "Epoch: 131\tTrain Loss: 3.6344268 \tVal Loss:3.5828566 \tTrain Acc: 1.75% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.583038 to 3.582857, saving the model weights\n",
      "Epoch: 132\tTrain Loss: 3.6339496 \tVal Loss:3.5827903 \tTrain Acc: 2.291667% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.582857 to 3.582790, saving the model weights\n",
      "Epoch: 133\tTrain Loss: 3.6329807 \tVal Loss:3.5829591 \tTrain Acc: 1.916667% \tVal Acc: 0.5000000%\n",
      "Epoch: 134\tTrain Loss: 3.6316071 \tVal Loss:3.5831497 \tTrain Acc: 2.416667% \tVal Acc: 1.8333334%\n",
      "Epoch: 135\tTrain Loss: 3.6335919 \tVal Loss:3.5829960 \tTrain Acc: 1.833333% \tVal Acc: 1.8333334%\n",
      "Epoch: 136\tTrain Loss: 3.6339966 \tVal Loss:3.5827483 \tTrain Acc: 1.708333% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.582790 to 3.582748, saving the model weights\n",
      "Epoch: 137\tTrain Loss: 3.6334272 \tVal Loss:3.5831507 \tTrain Acc: 1.958333% \tVal Acc: 1.8333334%\n",
      "Epoch: 138\tTrain Loss: 3.6313590 \tVal Loss:3.5831194 \tTrain Acc: 3.083333% \tVal Acc: 1.8333334%\n",
      "Epoch: 139\tTrain Loss: 3.6318012 \tVal Loss:3.5830867 \tTrain Acc: 2.458333% \tVal Acc: 0.5000000%\n",
      "Epoch: 140\tTrain Loss: 3.6338876 \tVal Loss:3.5832788 \tTrain Acc: 2.083333% \tVal Acc: 0.5000000%\n",
      "Epoch: 141\tTrain Loss: 3.6327721 \tVal Loss:3.5833483 \tTrain Acc: 2.208333% \tVal Acc: 0.5000000%\n",
      "Epoch: 142\tTrain Loss: 3.6333216 \tVal Loss:3.5834124 \tTrain Acc: 2.625% \tVal Acc: 2.1666667%\n",
      "Epoch: 143\tTrain Loss: 3.6331846 \tVal Loss:3.5833479 \tTrain Acc: 1.75% \tVal Acc: 0.5000000%\n",
      "Epoch: 144\tTrain Loss: 3.6319168 \tVal Loss:3.5831592 \tTrain Acc: 2.583333% \tVal Acc: 0.5000000%\n",
      "Epoch: 145\tTrain Loss: 3.6329383 \tVal Loss:3.5832351 \tTrain Acc: 2.583333% \tVal Acc: 1.8333334%\n",
      "Epoch: 146\tTrain Loss: 3.6318442 \tVal Loss:3.5831110 \tTrain Acc: 2.0% \tVal Acc: 1.5000001%\n",
      "Epoch: 147\tTrain Loss: 3.6318425 \tVal Loss:3.5830656 \tTrain Acc: 1.791667% \tVal Acc: 2.1666667%\n",
      "Epoch: 148\tTrain Loss: 3.6341335 \tVal Loss:3.5831648 \tTrain Acc: 2.625% \tVal Acc: 0.8333334%\n",
      "Epoch: 149\tTrain Loss: 3.6317248 \tVal Loss:3.5832741 \tTrain Acc: 2.375% \tVal Acc: 1.8333334%\n",
      "Epoch: 150\tTrain Loss: 3.6327110 \tVal Loss:3.5833825 \tTrain Acc: 2.25% \tVal Acc: 2.1666667%\n",
      "Epoch: 151\tTrain Loss: 3.6318250 \tVal Loss:3.5834976 \tTrain Acc: 1.833333% \tVal Acc: 0.8333334%\n",
      "Epoch: 152\tTrain Loss: 3.6339311 \tVal Loss:3.5834939 \tTrain Acc: 2.166667% \tVal Acc: 0.8333334%\n",
      "Epoch: 153\tTrain Loss: 3.6336005 \tVal Loss:3.5837168 \tTrain Acc: 1.666667% \tVal Acc: 0.0000000%\n",
      "Epoch: 154\tTrain Loss: 3.6332235 \tVal Loss:3.5835965 \tTrain Acc: 2.0% \tVal Acc: 0.8333334%\n",
      "Epoch: 155\tTrain Loss: 3.6330734 \tVal Loss:3.5835138 \tTrain Acc: 2.0% \tVal Acc: 0.5000000%\n",
      "Epoch: 156\tTrain Loss: 3.6326940 \tVal Loss:3.5834008 \tTrain Acc: 2.125% \tVal Acc: 2.1666668%\n",
      "Epoch: 157\tTrain Loss: 3.6318740 \tVal Loss:3.5830472 \tTrain Acc: 2.291667% \tVal Acc: 0.8333334%\n",
      "Epoch: 158\tTrain Loss: 3.6322024 \tVal Loss:3.5827154 \tTrain Acc: 2.541667% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.582748 to 3.582715, saving the model weights\n",
      "Epoch: 159\tTrain Loss: 3.6315657 \tVal Loss:3.5830126 \tTrain Acc: 2.25% \tVal Acc: 0.5000000%\n",
      "Epoch: 160\tTrain Loss: 3.6321108 \tVal Loss:3.5828270 \tTrain Acc: 2.666667% \tVal Acc: 0.5000000%\n",
      "Epoch: 161\tTrain Loss: 3.6318747 \tVal Loss:3.5828074 \tTrain Acc: 2.0% \tVal Acc: 0.0000000%\n",
      "Epoch: 162\tTrain Loss: 3.6326626 \tVal Loss:3.5828073 \tTrain Acc: 2.333333% \tVal Acc: 0.0000000%\n",
      "Epoch: 163\tTrain Loss: 3.6326360 \tVal Loss:3.5828746 \tTrain Acc: 2.791667% \tVal Acc: 0.0000000%\n",
      "Epoch: 164\tTrain Loss: 3.6339508 \tVal Loss:3.5828461 \tTrain Acc: 2.0% \tVal Acc: 1.3333334%\n",
      "Epoch: 165\tTrain Loss: 3.6323479 \tVal Loss:3.5828248 \tTrain Acc: 1.833333% \tVal Acc: 0.0000000%\n",
      "Epoch: 166\tTrain Loss: 3.6325558 \tVal Loss:3.5827672 \tTrain Acc: 2.375% \tVal Acc: 0.0000000%\n",
      "Epoch: 167\tTrain Loss: 3.6320884 \tVal Loss:3.5825629 \tTrain Acc: 2.458333% \tVal Acc: 0.0000000%\n",
      "Validation Loss decreased from 3.582715 to 3.582563, saving the model weights\n",
      "Epoch: 168\tTrain Loss: 3.6336786 \tVal Loss:3.5824514 \tTrain Acc: 1.791667% \tVal Acc: 2.3333334%\n",
      "Validation Loss decreased from 3.582563 to 3.582451, saving the model weights\n",
      "Epoch: 169\tTrain Loss: 3.6329637 \tVal Loss:3.5824686 \tTrain Acc: 2.333333% \tVal Acc: 0.0000000%\n",
      "Epoch: 170\tTrain Loss: 3.6319617 \tVal Loss:3.5825537 \tTrain Acc: 2.916667% \tVal Acc: 0.0000000%\n",
      "Epoch: 171\tTrain Loss: 3.6314530 \tVal Loss:3.5824214 \tTrain Acc: 1.541667% \tVal Acc: 0.0000000%\n",
      "Validation Loss decreased from 3.582451 to 3.582421, saving the model weights\n",
      "Epoch: 172\tTrain Loss: 3.6320074 \tVal Loss:3.5822830 \tTrain Acc: 2.041667% \tVal Acc: 0.0000000%\n",
      "Validation Loss decreased from 3.582421 to 3.582283, saving the model weights\n",
      "Epoch: 173\tTrain Loss: 3.6324147 \tVal Loss:3.5823138 \tTrain Acc: 2.041667% \tVal Acc: 2.3333334%\n",
      "Epoch: 174\tTrain Loss: 3.6314157 \tVal Loss:3.5823525 \tTrain Acc: 2.166667% \tVal Acc: 2.3333334%\n",
      "Epoch: 175\tTrain Loss: 3.6318884 \tVal Loss:3.5824430 \tTrain Acc: 2.125% \tVal Acc: 2.3333334%\n",
      "Epoch: 176\tTrain Loss: 3.6323033 \tVal Loss:3.5820720 \tTrain Acc: 2.0% \tVal Acc: 2.3333334%\n",
      "Validation Loss decreased from 3.582283 to 3.582072, saving the model weights\n",
      "Epoch: 177\tTrain Loss: 3.6321871 \tVal Loss:3.5819163 \tTrain Acc: 2.291667% \tVal Acc: 2.6666668%\n",
      "Validation Loss decreased from 3.582072 to 3.581916, saving the model weights\n",
      "Epoch: 178\tTrain Loss: 3.6327914 \tVal Loss:3.5820209 \tTrain Acc: 2.125% \tVal Acc: 2.6666668%\n",
      "Epoch: 179\tTrain Loss: 3.6323284 \tVal Loss:3.5820190 \tTrain Acc: 1.958333% \tVal Acc: 0.8333334%\n",
      "Epoch: 180\tTrain Loss: 3.6327067 \tVal Loss:3.5820323 \tTrain Acc: 1.958333% \tVal Acc: 0.8333334%\n",
      "Epoch: 181\tTrain Loss: 3.6318997 \tVal Loss:3.5819061 \tTrain Acc: 1.75% \tVal Acc: 2.3333334%\n",
      "Validation Loss decreased from 3.581916 to 3.581906, saving the model weights\n",
      "Epoch: 182\tTrain Loss: 3.6320204 \tVal Loss:3.5819328 \tTrain Acc: 2.166667% \tVal Acc: 0.0000000%\n",
      "Epoch: 183\tTrain Loss: 3.6323929 \tVal Loss:3.5819773 \tTrain Acc: 2.833333% \tVal Acc: 2.3333334%\n",
      "Epoch: 184\tTrain Loss: 3.6317353 \tVal Loss:3.5819243 \tTrain Acc: 2.25% \tVal Acc: 0.8333334%\n",
      "Epoch: 185\tTrain Loss: 3.6317234 \tVal Loss:3.5818393 \tTrain Acc: 2.583333% \tVal Acc: 0.0000000%\n",
      "Validation Loss decreased from 3.581906 to 3.581839, saving the model weights\n",
      "Epoch: 186\tTrain Loss: 3.6326751 \tVal Loss:3.5817866 \tTrain Acc: 1.666667% \tVal Acc: 2.6666668%\n",
      "Validation Loss decreased from 3.581839 to 3.581787, saving the model weights\n",
      "Epoch: 187\tTrain Loss: 3.6322468 \tVal Loss:3.5818052 \tTrain Acc: 2.833333% \tVal Acc: 5.3333335%\n",
      "Epoch: 188\tTrain Loss: 3.6316081 \tVal Loss:3.5817452 \tTrain Acc: 2.75% \tVal Acc: 2.6666668%\n",
      "Validation Loss decreased from 3.581787 to 3.581745, saving the model weights\n",
      "Epoch: 189\tTrain Loss: 3.6316621 \tVal Loss:3.5817878 \tTrain Acc: 2.375% \tVal Acc: 0.8333334%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 190\tTrain Loss: 3.6318728 \tVal Loss:3.5818631 \tTrain Acc: 2.25% \tVal Acc: 0.5000000%\n",
      "Epoch: 191\tTrain Loss: 3.6328898 \tVal Loss:3.5818761 \tTrain Acc: 1.791667% \tVal Acc: 0.8333334%\n",
      "Epoch: 192\tTrain Loss: 3.6319928 \tVal Loss:3.5819311 \tTrain Acc: 2.708333% \tVal Acc: 0.8333334%\n",
      "Epoch: 193\tTrain Loss: 3.6311407 \tVal Loss:3.5818247 \tTrain Acc: 1.75% \tVal Acc: 0.8333334%\n",
      "Epoch: 194\tTrain Loss: 3.6326557 \tVal Loss:3.5817761 \tTrain Acc: 2.208333% \tVal Acc: 0.8333334%\n",
      "Epoch: 195\tTrain Loss: 3.6318248 \tVal Loss:3.5817305 \tTrain Acc: 2.708333% \tVal Acc: 2.6666668%\n",
      "Validation Loss decreased from 3.581745 to 3.581731, saving the model weights\n",
      "Epoch: 196\tTrain Loss: 3.6323376 \tVal Loss:3.5817662 \tTrain Acc: 2.0% \tVal Acc: 2.6666668%\n",
      "Epoch: 197\tTrain Loss: 3.6322557 \tVal Loss:3.5819615 \tTrain Acc: 1.916667% \tVal Acc: 2.6666668%\n",
      "Epoch: 198\tTrain Loss: 3.6324920 \tVal Loss:3.5820528 \tTrain Acc: 2.041667% \tVal Acc: 2.6666668%\n",
      "Epoch: 199\tTrain Loss: 3.6310541 \tVal Loss:3.5820333 \tTrain Acc: 2.333333% \tVal Acc: 0.8333334%\n",
      "Epoch: 200\tTrain Loss: 3.6319346 \tVal Loss:3.5819649 \tTrain Acc: 2.0% \tVal Acc: 0.8333334%\n",
      "Epoch: 201\tTrain Loss: 3.6324822 \tVal Loss:3.5819031 \tTrain Acc: 2.125% \tVal Acc: 0.8333334%\n",
      "Epoch: 202\tTrain Loss: 3.6323403 \tVal Loss:3.5816900 \tTrain Acc: 1.416667% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.581731 to 3.581690, saving the model weights\n",
      "Epoch: 203\tTrain Loss: 3.6316610 \tVal Loss:3.5817073 \tTrain Acc: 2.458333% \tVal Acc: 0.8333334%\n",
      "Epoch: 204\tTrain Loss: 3.6322973 \tVal Loss:3.5815650 \tTrain Acc: 1.875% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.581690 to 3.581565, saving the model weights\n",
      "Epoch: 205\tTrain Loss: 3.6312402 \tVal Loss:3.5815781 \tTrain Acc: 1.791667% \tVal Acc: 0.8333334%\n",
      "Epoch: 206\tTrain Loss: 3.6310433 \tVal Loss:3.5817199 \tTrain Acc: 1.958333% \tVal Acc: 0.8333334%\n",
      "Epoch: 207\tTrain Loss: 3.6321671 \tVal Loss:3.5818234 \tTrain Acc: 2.208333% \tVal Acc: 0.8333334%\n",
      "Epoch: 208\tTrain Loss: 3.6307087 \tVal Loss:3.5819333 \tTrain Acc: 2.708333% \tVal Acc: 0.8333334%\n",
      "Epoch: 209\tTrain Loss: 3.6314280 \tVal Loss:3.5817655 \tTrain Acc: 1.791667% \tVal Acc: 0.0000000%\n",
      "Epoch: 210\tTrain Loss: 3.6321210 \tVal Loss:3.5817675 \tTrain Acc: 1.625% \tVal Acc: 0.0000000%\n",
      "Epoch: 211\tTrain Loss: 3.6326162 \tVal Loss:3.5817954 \tTrain Acc: 2.166667% \tVal Acc: 2.8333334%\n",
      "Epoch: 212\tTrain Loss: 3.6320517 \tVal Loss:3.5815412 \tTrain Acc: 1.916667% \tVal Acc: 2.8333334%\n",
      "Validation Loss decreased from 3.581565 to 3.581541, saving the model weights\n",
      "Epoch: 213\tTrain Loss: 3.6315291 \tVal Loss:3.5815606 \tTrain Acc: 2.208333% \tVal Acc: 2.8333334%\n",
      "Epoch: 214\tTrain Loss: 3.6320198 \tVal Loss:3.5814730 \tTrain Acc: 2.333333% \tVal Acc: 0.0000000%\n",
      "Validation Loss decreased from 3.581541 to 3.581473, saving the model weights\n",
      "Epoch: 215\tTrain Loss: 3.6313275 \tVal Loss:3.5814919 \tTrain Acc: 1.75% \tVal Acc: 0.0000000%\n",
      "Epoch: 216\tTrain Loss: 3.6315554 \tVal Loss:3.5812705 \tTrain Acc: 2.083333% \tVal Acc: 2.8333334%\n",
      "Validation Loss decreased from 3.581473 to 3.581271, saving the model weights\n",
      "Epoch: 217\tTrain Loss: 3.6310086 \tVal Loss:3.5812979 \tTrain Acc: 2.125% \tVal Acc: 0.8333334%\n",
      "Epoch: 218\tTrain Loss: 3.6320276 \tVal Loss:3.5813390 \tTrain Acc: 2.041667% \tVal Acc: 0.8333334%\n",
      "Epoch: 219\tTrain Loss: 3.6319240 \tVal Loss:3.5815807 \tTrain Acc: 2.375% \tVal Acc: 0.8333334%\n",
      "Epoch: 220\tTrain Loss: 3.6313692 \tVal Loss:3.5817246 \tTrain Acc: 2.791667% \tVal Acc: 0.8333334%\n",
      "Epoch: 221\tTrain Loss: 3.6318371 \tVal Loss:3.5817682 \tTrain Acc: 1.708333% \tVal Acc: 0.8333334%\n",
      "Epoch: 222\tTrain Loss: 3.6316627 \tVal Loss:3.5817330 \tTrain Acc: 2.125% \tVal Acc: 0.0000000%\n",
      "Epoch: 223\tTrain Loss: 3.6318412 \tVal Loss:3.5816793 \tTrain Acc: 1.75% \tVal Acc: 0.8333334%\n",
      "Epoch: 224\tTrain Loss: 3.6306315 \tVal Loss:3.5817546 \tTrain Acc: 2.375% \tVal Acc: 0.8333334%\n",
      "Epoch: 225\tTrain Loss: 3.6316134 \tVal Loss:3.5818051 \tTrain Acc: 2.416667% \tVal Acc: 0.5000000%\n",
      "Epoch: 226\tTrain Loss: 3.6317980 \tVal Loss:3.5818133 \tTrain Acc: 1.291667% \tVal Acc: 0.8333334%\n",
      "Epoch: 227\tTrain Loss: 3.6322466 \tVal Loss:3.5816218 \tTrain Acc: 1.666667% \tVal Acc: 0.8333334%\n",
      "Epoch: 228\tTrain Loss: 3.6318828 \tVal Loss:3.5815174 \tTrain Acc: 1.666667% \tVal Acc: 0.5000000%\n",
      "Epoch: 229\tTrain Loss: 3.6321309 \tVal Loss:3.5814059 \tTrain Acc: 2.041667% \tVal Acc: 0.5000000%\n",
      "Epoch: 230\tTrain Loss: 3.6316480 \tVal Loss:3.5812531 \tTrain Acc: 2.125% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.581271 to 3.581253, saving the model weights\n",
      "Epoch: 231\tTrain Loss: 3.6320751 \tVal Loss:3.5811973 \tTrain Acc: 1.666667% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.581253 to 3.581197, saving the model weights\n",
      "Epoch: 232\tTrain Loss: 3.6305708 \tVal Loss:3.5811876 \tTrain Acc: 2.375% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.581197 to 3.581188, saving the model weights\n",
      "Epoch: 233\tTrain Loss: 3.6313684 \tVal Loss:3.5813220 \tTrain Acc: 2.291667% \tVal Acc: 0.5000000%\n",
      "Epoch: 234\tTrain Loss: 3.6319790 \tVal Loss:3.5813632 \tTrain Acc: 1.916667% \tVal Acc: 0.5000000%\n",
      "Epoch: 235\tTrain Loss: 3.6320265 \tVal Loss:3.5815297 \tTrain Acc: 2.083333% \tVal Acc: 0.5000000%\n",
      "Epoch: 236\tTrain Loss: 3.6314468 \tVal Loss:3.5815638 \tTrain Acc: 2.083333% \tVal Acc: 0.5000000%\n",
      "Epoch: 237\tTrain Loss: 3.6313369 \tVal Loss:3.5816520 \tTrain Acc: 2.666667% \tVal Acc: 2.1666668%\n",
      "Epoch: 238\tTrain Loss: 3.6318597 \tVal Loss:3.5815873 \tTrain Acc: 2.166667% \tVal Acc: 2.1666668%\n",
      "Epoch: 239\tTrain Loss: 3.6318941 \tVal Loss:3.5814213 \tTrain Acc: 2.291667% \tVal Acc: 2.1666668%\n",
      "Epoch: 240\tTrain Loss: 3.6314185 \tVal Loss:3.5810672 \tTrain Acc: 1.583333% \tVal Acc: 0.0000000%\n",
      "Validation Loss decreased from 3.581188 to 3.581067, saving the model weights\n",
      "Epoch: 241\tTrain Loss: 3.6307527 \tVal Loss:3.5809833 \tTrain Acc: 2.166667% \tVal Acc: 0.0000000%\n",
      "Validation Loss decreased from 3.581067 to 3.580983, saving the model weights\n",
      "Epoch: 242\tTrain Loss: 3.6313022 \tVal Loss:3.5810213 \tTrain Acc: 2.291667% \tVal Acc: 0.0000000%\n",
      "Epoch: 243\tTrain Loss: 3.6311223 \tVal Loss:3.5809144 \tTrain Acc: 2.125% \tVal Acc: 2.3333334%\n",
      "Validation Loss decreased from 3.580983 to 3.580914, saving the model weights\n",
      "Epoch: 244\tTrain Loss: 3.6318577 \tVal Loss:3.5808689 \tTrain Acc: 1.291667% \tVal Acc: 2.3333334%\n",
      "Validation Loss decreased from 3.580914 to 3.580869, saving the model weights\n",
      "Epoch: 245\tTrain Loss: 3.6315448 \tVal Loss:3.5809388 \tTrain Acc: 1.916667% \tVal Acc: 2.1666668%\n",
      "Epoch: 246\tTrain Loss: 3.6307814 \tVal Loss:3.5809707 \tTrain Acc: 2.583333% \tVal Acc: 0.5000000%\n",
      "Epoch: 247\tTrain Loss: 3.6311349 \tVal Loss:3.5810294 \tTrain Acc: 2.0% \tVal Acc: 0.5000000%\n",
      "Epoch: 248\tTrain Loss: 3.6312439 \tVal Loss:3.5809927 \tTrain Acc: 1.916667% \tVal Acc: 0.5000000%\n",
      "Epoch: 249\tTrain Loss: 3.6309020 \tVal Loss:3.5809440 \tTrain Acc: 2.5% \tVal Acc: 0.5000000%\n",
      "Epoch: 250\tTrain Loss: 3.6306570 \tVal Loss:3.5808325 \tTrain Acc: 2.166667% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.580869 to 3.580833, saving the model weights\n",
      "Epoch: 251\tTrain Loss: 3.6320353 \tVal Loss:3.5808577 \tTrain Acc: 2.125% \tVal Acc: 0.5000000%\n",
      "Epoch: 252\tTrain Loss: 3.6311657 \tVal Loss:3.5808848 \tTrain Acc: 2.0% \tVal Acc: 1.3333334%\n",
      "Epoch: 253\tTrain Loss: 3.6313533 \tVal Loss:3.5809655 \tTrain Acc: 2.375% \tVal Acc: 2.6666668%\n",
      "Epoch: 254\tTrain Loss: 3.6317785 \tVal Loss:3.5809596 \tTrain Acc: 2.125% \tVal Acc: 2.1666667%\n",
      "Epoch: 255\tTrain Loss: 3.6307207 \tVal Loss:3.5808029 \tTrain Acc: 2.458333% \tVal Acc: 2.1666667%\n",
      "Validation Loss decreased from 3.580833 to 3.580803, saving the model weights\n",
      "Epoch: 256\tTrain Loss: 3.6309448 \tVal Loss:3.5808699 \tTrain Acc: 2.458333% \tVal Acc: 2.1666667%\n",
      "Epoch: 257\tTrain Loss: 3.6312710 \tVal Loss:3.5807756 \tTrain Acc: 1.916667% \tVal Acc: 2.1666667%\n",
      "Validation Loss decreased from 3.580803 to 3.580776, saving the model weights\n",
      "Epoch: 258\tTrain Loss: 3.6311362 \tVal Loss:3.5807451 \tTrain Acc: 2.458333% \tVal Acc: 2.1666667%\n",
      "Validation Loss decreased from 3.580776 to 3.580745, saving the model weights\n",
      "Epoch: 259\tTrain Loss: 3.6313323 \tVal Loss:3.5807280 \tTrain Acc: 1.791667% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.580745 to 3.580728, saving the model weights\n",
      "Epoch: 260\tTrain Loss: 3.6311790 \tVal Loss:3.5806347 \tTrain Acc: 2.041667% \tVal Acc: 0.8333334%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased from 3.580728 to 3.580635, saving the model weights\n",
      "Epoch: 261\tTrain Loss: 3.6310046 \tVal Loss:3.5806179 \tTrain Acc: 2.125% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.580635 to 3.580618, saving the model weights\n",
      "Epoch: 262\tTrain Loss: 3.6307534 \tVal Loss:3.5808099 \tTrain Acc: 2.333333% \tVal Acc: 0.5000000%\n",
      "Epoch: 263\tTrain Loss: 3.6311805 \tVal Loss:3.5809652 \tTrain Acc: 2.208333% \tVal Acc: 0.5000000%\n",
      "Epoch: 264\tTrain Loss: 3.6309352 \tVal Loss:3.5808415 \tTrain Acc: 2.5% \tVal Acc: 0.5000000%\n",
      "Epoch: 265\tTrain Loss: 3.6315739 \tVal Loss:3.5807003 \tTrain Acc: 2.0% \tVal Acc: 1.3333334%\n",
      "Epoch: 266\tTrain Loss: 3.6311818 \tVal Loss:3.5808140 \tTrain Acc: 2.666667% \tVal Acc: 1.3333334%\n",
      "Epoch: 267\tTrain Loss: 3.6312041 \tVal Loss:3.5807532 \tTrain Acc: 2.375% \tVal Acc: 2.6666668%\n",
      "Epoch: 268\tTrain Loss: 3.6316473 \tVal Loss:3.5807943 \tTrain Acc: 1.916667% \tVal Acc: 2.6666668%\n",
      "Epoch: 269\tTrain Loss: 3.6317037 \tVal Loss:3.5808993 \tTrain Acc: 1.958333% \tVal Acc: 2.6666668%\n",
      "Epoch: 270\tTrain Loss: 3.6308669 \tVal Loss:3.5808036 \tTrain Acc: 2.041667% \tVal Acc: 2.6666668%\n",
      "Epoch: 271\tTrain Loss: 3.6306111 \tVal Loss:3.5807042 \tTrain Acc: 1.583333% \tVal Acc: 2.6666668%\n",
      "Epoch: 272\tTrain Loss: 3.6325278 \tVal Loss:3.5806160 \tTrain Acc: 1.875% \tVal Acc: 2.6666668%\n",
      "Validation Loss decreased from 3.580618 to 3.580616, saving the model weights\n",
      "Epoch: 273\tTrain Loss: 3.6309212 \tVal Loss:3.5805841 \tTrain Acc: 1.791667% \tVal Acc: 2.6666668%\n",
      "Validation Loss decreased from 3.580616 to 3.580584, saving the model weights\n",
      "Epoch: 274\tTrain Loss: 3.6317142 \tVal Loss:3.5804770 \tTrain Acc: 1.875% \tVal Acc: 2.6666668%\n",
      "Validation Loss decreased from 3.580584 to 3.580477, saving the model weights\n",
      "Epoch: 275\tTrain Loss: 3.6304043 \tVal Loss:3.5805832 \tTrain Acc: 2.041667% \tVal Acc: 2.6666668%\n",
      "Epoch: 276\tTrain Loss: 3.6309463 \tVal Loss:3.5806581 \tTrain Acc: 1.75% \tVal Acc: 2.6666668%\n",
      "Epoch: 277\tTrain Loss: 3.6315645 \tVal Loss:3.5807402 \tTrain Acc: 2.541667% \tVal Acc: 1.3333334%\n",
      "Epoch: 278\tTrain Loss: 3.6314236 \tVal Loss:3.5807767 \tTrain Acc: 2.416667% \tVal Acc: 1.3333334%\n",
      "Epoch: 279\tTrain Loss: 3.6306489 \tVal Loss:3.5807074 \tTrain Acc: 2.041667% \tVal Acc: 1.3333334%\n",
      "Epoch: 280\tTrain Loss: 3.6297972 \tVal Loss:3.5806611 \tTrain Acc: 2.0% \tVal Acc: 0.8333334%\n",
      "Epoch: 281\tTrain Loss: 3.6305349 \tVal Loss:3.5806558 \tTrain Acc: 1.875% \tVal Acc: 0.8333334%\n",
      "Epoch: 282\tTrain Loss: 3.6309638 \tVal Loss:3.5806811 \tTrain Acc: 1.666667% \tVal Acc: 2.8333334%\n",
      "Epoch: 283\tTrain Loss: 3.6308101 \tVal Loss:3.5807144 \tTrain Acc: 1.958333% \tVal Acc: 0.0000000%\n",
      "Epoch: 284\tTrain Loss: 3.6322069 \tVal Loss:3.5805476 \tTrain Acc: 2.291667% \tVal Acc: 2.8333334%\n",
      "Epoch: 285\tTrain Loss: 3.6306876 \tVal Loss:3.5806114 \tTrain Acc: 2.208333% \tVal Acc: 2.8333334%\n",
      "Epoch: 286\tTrain Loss: 3.6301531 \tVal Loss:3.5807154 \tTrain Acc: 1.625% \tVal Acc: 2.8333334%\n",
      "Epoch: 287\tTrain Loss: 3.6316590 \tVal Loss:3.5806458 \tTrain Acc: 1.541667% \tVal Acc: 2.8333334%\n",
      "Epoch: 288\tTrain Loss: 3.6310119 \tVal Loss:3.5806180 \tTrain Acc: 2.25% \tVal Acc: 2.8333334%\n",
      "Epoch: 289\tTrain Loss: 3.6309426 \tVal Loss:3.5804743 \tTrain Acc: 2.083333% \tVal Acc: 2.8333334%\n",
      "Validation Loss decreased from 3.580477 to 3.580474, saving the model weights\n",
      "Epoch: 290\tTrain Loss: 3.6309342 \tVal Loss:3.5805335 \tTrain Acc: 1.875% \tVal Acc: 2.8333334%\n",
      "Epoch: 291\tTrain Loss: 3.6310350 \tVal Loss:3.5804799 \tTrain Acc: 2.25% \tVal Acc: 2.8333334%\n",
      "Epoch: 292\tTrain Loss: 3.6307783 \tVal Loss:3.5804368 \tTrain Acc: 2.5% \tVal Acc: 0.5000000%\n",
      "Validation Loss decreased from 3.580474 to 3.580437, saving the model weights\n",
      "Epoch: 293\tTrain Loss: 3.6307903 \tVal Loss:3.5805128 \tTrain Acc: 2.208333% \tVal Acc: 0.5000000%\n",
      "Epoch: 294\tTrain Loss: 3.6307086 \tVal Loss:3.5805886 \tTrain Acc: 2.25% \tVal Acc: 0.5000000%\n",
      "Epoch: 295\tTrain Loss: 3.6310666 \tVal Loss:3.5804876 \tTrain Acc: 1.666667% \tVal Acc: 0.5000000%\n",
      "Epoch: 296\tTrain Loss: 3.6304920 \tVal Loss:3.5804645 \tTrain Acc: 1.958333% \tVal Acc: 0.5000000%\n",
      "Epoch: 297\tTrain Loss: 3.6300753 \tVal Loss:3.5804979 \tTrain Acc: 2.208333% \tVal Acc: 0.5000000%\n",
      "Epoch: 298\tTrain Loss: 3.6311607 \tVal Loss:3.5806231 \tTrain Acc: 2.125% \tVal Acc: 2.1666667%\n",
      "Epoch: 299\tTrain Loss: 3.6310116 \tVal Loss:3.5806847 \tTrain Acc: 2.708333% \tVal Acc: 2.1666667%\n",
      "Epoch: 300\tTrain Loss: 3.6308196 \tVal Loss:3.5806101 \tTrain Acc: 2.375% \tVal Acc: 0.5000000%\n",
      "Epoch: 301\tTrain Loss: 3.6309184 \tVal Loss:3.5806704 \tTrain Acc: 2.125% \tVal Acc: 0.5000000%\n",
      "Epoch: 302\tTrain Loss: 3.6314885 \tVal Loss:3.5807020 \tTrain Acc: 2.125% \tVal Acc: 2.1666667%\n",
      "Epoch: 303\tTrain Loss: 3.6305877 \tVal Loss:3.5805938 \tTrain Acc: 2.041667% \tVal Acc: 2.8333334%\n",
      "Epoch: 304\tTrain Loss: 3.6308642 \tVal Loss:3.5806308 \tTrain Acc: 1.833333% \tVal Acc: 2.8333334%\n",
      "Epoch: 305\tTrain Loss: 3.6308746 \tVal Loss:3.5805856 \tTrain Acc: 1.875% \tVal Acc: 3.6666668%\n",
      "Epoch: 306\tTrain Loss: 3.6310627 \tVal Loss:3.5804006 \tTrain Acc: 1.791667% \tVal Acc: 2.3333335%\n",
      "Validation Loss decreased from 3.580437 to 3.580401, saving the model weights\n",
      "Epoch: 307\tTrain Loss: 3.6309997 \tVal Loss:3.5802879 \tTrain Acc: 2.166667% \tVal Acc: 2.8333334%\n",
      "Validation Loss decreased from 3.580401 to 3.580288, saving the model weights\n",
      "Epoch: 308\tTrain Loss: 3.6305140 \tVal Loss:3.5802082 \tTrain Acc: 2.5% \tVal Acc: 2.8333334%\n",
      "Validation Loss decreased from 3.580288 to 3.580208, saving the model weights\n",
      "Epoch: 309\tTrain Loss: 3.6303477 \tVal Loss:3.5803101 \tTrain Acc: 2.375% \tVal Acc: 2.8333334%\n",
      "Epoch: 310\tTrain Loss: 3.6297230 \tVal Loss:3.5804048 \tTrain Acc: 2.333333% \tVal Acc: 0.5000000%\n",
      "Epoch: 311\tTrain Loss: 3.6313935 \tVal Loss:3.5803012 \tTrain Acc: 2.041667% \tVal Acc: 1.3333334%\n",
      "Epoch: 312\tTrain Loss: 3.6303853 \tVal Loss:3.5801355 \tTrain Acc: 2.166667% \tVal Acc: 0.8333334%\n",
      "Validation Loss decreased from 3.580208 to 3.580136, saving the model weights\n",
      "Epoch: 313\tTrain Loss: 3.6301471 \tVal Loss:3.5802723 \tTrain Acc: 2.083333% \tVal Acc: 1.3333334%\n",
      "Epoch: 314\tTrain Loss: 3.6305686 \tVal Loss:3.5803262 \tTrain Acc: 2.291667% \tVal Acc: 1.5000001%\n",
      "Epoch: 315\tTrain Loss: 3.6312707 \tVal Loss:3.5802974 \tTrain Acc: 1.583333% \tVal Acc: 0.8333334%\n",
      "Epoch: 316\tTrain Loss: 3.6302856 \tVal Loss:3.5803441 \tTrain Acc: 1.625% \tVal Acc: 0.8333334%\n",
      "Epoch: 317\tTrain Loss: 3.6308132 \tVal Loss:3.5803954 \tTrain Acc: 1.916667% \tVal Acc: 0.8333334%\n",
      "Epoch: 318\tTrain Loss: 3.6308513 \tVal Loss:3.5804346 \tTrain Acc: 1.833333% \tVal Acc: 1.3333334%\n",
      "Epoch: 319\tTrain Loss: 3.6300908 \tVal Loss:3.5805449 \tTrain Acc: 2.0% \tVal Acc: 1.3333334%\n",
      "Epoch: 320\tTrain Loss: 3.6309260 \tVal Loss:3.5804621 \tTrain Acc: 1.75% \tVal Acc: 1.3333334%\n",
      "Epoch: 321\tTrain Loss: 3.6308750 \tVal Loss:3.5804414 \tTrain Acc: 1.916667% \tVal Acc: 1.3333334%\n",
      "Epoch: 322\tTrain Loss: 3.6305358 \tVal Loss:3.5804538 \tTrain Acc: 2.458333% \tVal Acc: 5.1666669%\n",
      "Epoch: 323\tTrain Loss: 3.6313152 \tVal Loss:3.5804389 \tTrain Acc: 2.0% \tVal Acc: 0.5000000%\n",
      "Epoch: 324\tTrain Loss: 3.6312959 \tVal Loss:3.5803962 \tTrain Acc: 1.833333% \tVal Acc: 3.6666668%\n",
      "Epoch: 325\tTrain Loss: 3.6303975 \tVal Loss:3.5802756 \tTrain Acc: 2.083333% \tVal Acc: 0.5000000%\n",
      "Epoch: 326\tTrain Loss: 3.6308830 \tVal Loss:3.5802148 \tTrain Acc: 1.958333% \tVal Acc: 0.5000000%\n",
      "Epoch: 327\tTrain Loss: 3.6304640 \tVal Loss:3.5801493 \tTrain Acc: 1.75% \tVal Acc: 3.6666668%\n",
      "Epoch: 328\tTrain Loss: 3.6308689 \tVal Loss:3.5801288 \tTrain Acc: 2.125% \tVal Acc: 1.3333334%\n",
      "Validation Loss decreased from 3.580136 to 3.580129, saving the model weights\n",
      "Epoch: 329\tTrain Loss: 3.6303321 \tVal Loss:3.5800985 \tTrain Acc: 1.75% \tVal Acc: 1.3333334%\n",
      "Validation Loss decreased from 3.580129 to 3.580099, saving the model weights\n",
      "Epoch: 330\tTrain Loss: 3.6304440 \tVal Loss:3.5800848 \tTrain Acc: 2.291667% \tVal Acc: 1.3333334%\n",
      "Validation Loss decreased from 3.580099 to 3.580085, saving the model weights\n",
      "Epoch: 331\tTrain Loss: 3.6309782 \tVal Loss:3.5800812 \tTrain Acc: 2.291667% \tVal Acc: 1.3333334%\n",
      "Validation Loss decreased from 3.580085 to 3.580081, saving the model weights\n",
      "Epoch: 332\tTrain Loss: 3.6312607 \tVal Loss:3.5800797 \tTrain Acc: 1.75% \tVal Acc: 1.3333334%\n",
      "Validation Loss decreased from 3.580081 to 3.580080, saving the model weights\n",
      "Epoch: 333\tTrain Loss: 3.6306130 \tVal Loss:3.5801939 \tTrain Acc: 2.083333% \tVal Acc: 1.3333334%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 334\tTrain Loss: 3.6308693 \tVal Loss:3.5801732 \tTrain Acc: 2.166667% \tVal Acc: 2.6666668%\n",
      "Epoch: 335\tTrain Loss: 3.6303726 \tVal Loss:3.5801649 \tTrain Acc: 2.166667% \tVal Acc: 1.3333334%\n",
      "Epoch: 336\tTrain Loss: 3.6306365 \tVal Loss:3.5800734 \tTrain Acc: 1.875% \tVal Acc: 5.1666669%\n",
      "Validation Loss decreased from 3.580080 to 3.580073, saving the model weights\n",
      "Epoch: 337\tTrain Loss: 3.6302610 \tVal Loss:3.5801145 \tTrain Acc: 2.416667% \tVal Acc: 5.1666669%\n",
      "Epoch: 338\tTrain Loss: 3.6303809 \tVal Loss:3.5799491 \tTrain Acc: 2.333333% \tVal Acc: 5.1666669%\n",
      "Validation Loss decreased from 3.580073 to 3.579949, saving the model weights\n",
      "Epoch: 339\tTrain Loss: 3.6302283 \tVal Loss:3.5799295 \tTrain Acc: 1.958333% \tVal Acc: 1.3333334%\n",
      "Validation Loss decreased from 3.579949 to 3.579930, saving the model weights\n",
      "Epoch: 340\tTrain Loss: 3.6304677 \tVal Loss:3.5799155 \tTrain Acc: 2.291667% \tVal Acc: 1.3333334%\n",
      "Validation Loss decreased from 3.579930 to 3.579915, saving the model weights\n",
      "Epoch: 341\tTrain Loss: 3.6300202 \tVal Loss:3.5799510 \tTrain Acc: 1.791667% \tVal Acc: 1.3333334%\n",
      "Epoch: 342\tTrain Loss: 3.6303740 \tVal Loss:3.5800946 \tTrain Acc: 2.041667% \tVal Acc: 1.3333334%\n",
      "Epoch: 343\tTrain Loss: 3.6305012 \tVal Loss:3.5801549 \tTrain Acc: 1.833333% \tVal Acc: 1.3333334%\n",
      "Epoch: 344\tTrain Loss: 3.6309137 \tVal Loss:3.5799888 \tTrain Acc: 1.625% \tVal Acc: 1.3333334%\n",
      "Epoch: 345\tTrain Loss: 3.6310991 \tVal Loss:3.5800934 \tTrain Acc: 2.041667% \tVal Acc: 2.6666668%\n",
      "Epoch: 346\tTrain Loss: 3.6306885 \tVal Loss:3.5801218 \tTrain Acc: 2.125% \tVal Acc: 2.6666668%\n",
      "Epoch: 347\tTrain Loss: 3.6311456 \tVal Loss:3.5800741 \tTrain Acc: 1.708333% \tVal Acc: 4.5000002%\n",
      "Epoch: 348\tTrain Loss: 3.6312034 \tVal Loss:3.5800483 \tTrain Acc: 1.625% \tVal Acc: 4.5000002%\n",
      "Epoch: 349\tTrain Loss: 3.6304845 \tVal Loss:3.5799238 \tTrain Acc: 2.0% \tVal Acc: 5.1666669%\n",
      "Epoch: 350\tTrain Loss: 3.6310543 \tVal Loss:3.5798941 \tTrain Acc: 1.791667% \tVal Acc: 5.1666669%\n",
      "Validation Loss decreased from 3.579915 to 3.579894, saving the model weights\n",
      "Epoch: 351\tTrain Loss: 3.6305564 \tVal Loss:3.5800121 \tTrain Acc: 2.083333% \tVal Acc: 5.1666669%\n",
      "Epoch: 352\tTrain Loss: 3.6307949 \tVal Loss:3.5800211 \tTrain Acc: 2.208333% \tVal Acc: 2.1666668%\n",
      "Epoch: 353\tTrain Loss: 3.6300501 \tVal Loss:3.5799647 \tTrain Acc: 2.041667% \tVal Acc: 5.1666669%\n",
      "Epoch: 354\tTrain Loss: 3.6307039 \tVal Loss:3.5800048 \tTrain Acc: 2.25% \tVal Acc: 2.8333334%\n",
      "Epoch: 355\tTrain Loss: 3.6304674 \tVal Loss:3.5799232 \tTrain Acc: 1.416667% \tVal Acc: 2.8333334%\n",
      "Epoch: 356\tTrain Loss: 3.6301253 \tVal Loss:3.5799677 \tTrain Acc: 2.166667% \tVal Acc: 0.8333334%\n",
      "Epoch: 357\tTrain Loss: 3.6310317 \tVal Loss:3.5799794 \tTrain Acc: 1.458333% \tVal Acc: 0.8333334%\n",
      "Epoch: 358\tTrain Loss: 3.6303982 \tVal Loss:3.5800315 \tTrain Acc: 1.416667% \tVal Acc: 0.8333334%\n",
      "Epoch: 359\tTrain Loss: 3.6301905 \tVal Loss:3.5800506 \tTrain Acc: 1.875% \tVal Acc: 0.8333334%\n",
      "Epoch: 360\tTrain Loss: 3.6304230 \tVal Loss:3.5800203 \tTrain Acc: 1.791667% \tVal Acc: 0.8333334%\n",
      "Epoch: 361\tTrain Loss: 3.6303178 \tVal Loss:3.5800559 \tTrain Acc: 2.0% \tVal Acc: 0.8333334%\n",
      "Epoch: 362\tTrain Loss: 3.6306990 \tVal Loss:3.5800383 \tTrain Acc: 2.083333% \tVal Acc: 2.8333334%\n",
      "Epoch: 363\tTrain Loss: 3.6310685 \tVal Loss:3.5800208 \tTrain Acc: 1.875% \tVal Acc: 2.8333334%\n",
      "Epoch: 364\tTrain Loss: 3.6308421 \tVal Loss:3.5799558 \tTrain Acc: 1.833333% \tVal Acc: 1.3333334%\n",
      "Epoch: 365\tTrain Loss: 3.6300784 \tVal Loss:3.5799101 \tTrain Acc: 1.666667% \tVal Acc: 2.8333334%\n",
      "Epoch: 366\tTrain Loss: 3.6302281 \tVal Loss:3.5798439 \tTrain Acc: 1.75% \tVal Acc: 1.3333334%\n",
      "Validation Loss decreased from 3.579894 to 3.579844, saving the model weights\n",
      "Epoch: 367\tTrain Loss: 3.6311234 \tVal Loss:3.5798931 \tTrain Acc: 1.833333% \tVal Acc: 1.3333334%\n",
      "Epoch: 368\tTrain Loss: 3.6305883 \tVal Loss:3.5798121 \tTrain Acc: 1.833333% \tVal Acc: 2.8333334%\n",
      "Validation Loss decreased from 3.579844 to 3.579812, saving the model weights\n",
      "Epoch: 369\tTrain Loss: 3.6306837 \tVal Loss:3.5798307 \tTrain Acc: 1.75% \tVal Acc: 2.8333334%\n",
      "Epoch: 370\tTrain Loss: 3.6310548 \tVal Loss:3.5798309 \tTrain Acc: 1.75% \tVal Acc: 2.8333334%\n",
      "Epoch: 371\tTrain Loss: 3.6306391 \tVal Loss:3.5799084 \tTrain Acc: 1.75% \tVal Acc: 2.8333334%\n",
      "Epoch: 372\tTrain Loss: 3.6304181 \tVal Loss:3.5799797 \tTrain Acc: 2.291667% \tVal Acc: 2.8333334%\n",
      "Epoch: 373\tTrain Loss: 3.6305898 \tVal Loss:3.5800240 \tTrain Acc: 1.458333% \tVal Acc: 2.8333334%\n",
      "Epoch: 374\tTrain Loss: 3.6306756 \tVal Loss:3.5800093 \tTrain Acc: 1.625% \tVal Acc: 2.8333334%\n",
      "Epoch: 375\tTrain Loss: 3.6298941 \tVal Loss:3.5800471 \tTrain Acc: 2.125% \tVal Acc: 2.6666668%\n",
      "Epoch: 376\tTrain Loss: 3.6300542 \tVal Loss:3.5800302 \tTrain Acc: 2.333333% \tVal Acc: 0.8333334%\n",
      "Epoch: 377\tTrain Loss: 3.6305953 \tVal Loss:3.5800153 \tTrain Acc: 2.166667% \tVal Acc: 0.8333334%\n",
      "Epoch: 378\tTrain Loss: 3.6306575 \tVal Loss:3.5801094 \tTrain Acc: 1.625% \tVal Acc: 1.3333334%\n",
      "Epoch: 379\tTrain Loss: 3.6308622 \tVal Loss:3.5799933 \tTrain Acc: 2.291667% \tVal Acc: 1.3333334%\n",
      "Epoch: 380\tTrain Loss: 3.6300811 \tVal Loss:3.5798965 \tTrain Acc: 2.125% \tVal Acc: 1.3333334%\n",
      "Epoch: 381\tTrain Loss: 3.6308382 \tVal Loss:3.5797771 \tTrain Acc: 1.375% \tVal Acc: 2.3333335%\n",
      "Validation Loss decreased from 3.579812 to 3.579777, saving the model weights\n",
      "Epoch: 382\tTrain Loss: 3.6308626 \tVal Loss:3.5797520 \tTrain Acc: 1.791667% \tVal Acc: 2.3333335%\n",
      "Validation Loss decreased from 3.579777 to 3.579752, saving the model weights\n",
      "Epoch: 383\tTrain Loss: 3.6306559 \tVal Loss:3.5797710 \tTrain Acc: 1.708333% \tVal Acc: 2.3333335%\n",
      "Epoch: 384\tTrain Loss: 3.6306552 \tVal Loss:3.5798071 \tTrain Acc: 2.125% \tVal Acc: 1.3333334%\n",
      "Epoch: 385\tTrain Loss: 3.6303296 \tVal Loss:3.5797618 \tTrain Acc: 1.416667% \tVal Acc: 1.3333334%\n",
      "Epoch: 386\tTrain Loss: 3.6302798 \tVal Loss:3.5798101 \tTrain Acc: 1.75% \tVal Acc: 2.1666667%\n",
      "Epoch: 387\tTrain Loss: 3.6301654 \tVal Loss:3.5798795 \tTrain Acc: 1.916667% \tVal Acc: 0.8333334%\n",
      "Epoch: 388\tTrain Loss: 3.6307479 \tVal Loss:3.5798745 \tTrain Acc: 1.833333% \tVal Acc: 5.0000001%\n",
      "Epoch: 389\tTrain Loss: 3.6303918 \tVal Loss:3.5798796 \tTrain Acc: 1.833333% \tVal Acc: 2.8333334%\n",
      "Epoch: 390\tTrain Loss: 3.6301292 \tVal Loss:3.5798011 \tTrain Acc: 1.666667% \tVal Acc: 2.8333334%\n",
      "Epoch: 391\tTrain Loss: 3.6301797 \tVal Loss:3.5798126 \tTrain Acc: 2.208333% \tVal Acc: 2.8333334%\n",
      "Epoch: 392\tTrain Loss: 3.6310059 \tVal Loss:3.5796662 \tTrain Acc: 1.791667% \tVal Acc: 2.8333334%\n",
      "Validation Loss decreased from 3.579752 to 3.579666, saving the model weights\n",
      "Epoch: 393\tTrain Loss: 3.6300441 \tVal Loss:3.5796480 \tTrain Acc: 2.083333% \tVal Acc: 2.6666668%\n",
      "Validation Loss decreased from 3.579666 to 3.579648, saving the model weights\n",
      "Epoch: 394\tTrain Loss: 3.6301952 \tVal Loss:3.5796311 \tTrain Acc: 2.208333% \tVal Acc: 2.6666668%\n",
      "Validation Loss decreased from 3.579648 to 3.579631, saving the model weights\n",
      "Epoch: 395\tTrain Loss: 3.6302409 \tVal Loss:3.5796307 \tTrain Acc: 1.625% \tVal Acc: 2.1666668%\n",
      "Validation Loss decreased from 3.579631 to 3.579631, saving the model weights\n",
      "Epoch: 396\tTrain Loss: 3.6302310 \tVal Loss:3.5796657 \tTrain Acc: 1.375% \tVal Acc: 2.1666668%\n",
      "Epoch: 397\tTrain Loss: 3.6306460 \tVal Loss:3.5795987 \tTrain Acc: 1.541667% \tVal Acc: 2.1666668%\n",
      "Validation Loss decreased from 3.579631 to 3.579599, saving the model weights\n",
      "Epoch: 398\tTrain Loss: 3.6301934 \tVal Loss:3.5796809 \tTrain Acc: 2.333333% \tVal Acc: 2.1666668%\n",
      "Epoch: 399\tTrain Loss: 3.6301453 \tVal Loss:3.5797211 \tTrain Acc: 1.75% \tVal Acc: 2.1666668%\n",
      "Epoch: 400\tTrain Loss: 3.6299932 \tVal Loss:3.5797204 \tTrain Acc: 1.75% \tVal Acc: 2.1666668%\n",
      "Epoch: 401\tTrain Loss: 3.6302158 \tVal Loss:3.5796670 \tTrain Acc: 1.916667% \tVal Acc: 2.1666668%\n",
      "Epoch: 402\tTrain Loss: 3.6305766 \tVal Loss:3.5796887 \tTrain Acc: 2.333333% \tVal Acc: 2.1666668%\n",
      "Epoch: 403\tTrain Loss: 3.6304895 \tVal Loss:3.5796046 \tTrain Acc: 1.875% \tVal Acc: 2.1666668%\n",
      "Epoch: 404\tTrain Loss: 3.6299924 \tVal Loss:3.5796125 \tTrain Acc: 2.291667% \tVal Acc: 2.1666668%\n",
      "Epoch: 405\tTrain Loss: 3.6302801 \tVal Loss:3.5796360 \tTrain Acc: 1.625% \tVal Acc: 2.1666668%\n",
      "Epoch: 406\tTrain Loss: 3.6305564 \tVal Loss:3.5795739 \tTrain Acc: 2.208333% \tVal Acc: 2.1666668%\n",
      "Validation Loss decreased from 3.579599 to 3.579574, saving the model weights\n",
      "Epoch: 407\tTrain Loss: 3.6309098 \tVal Loss:3.5795430 \tTrain Acc: 1.583333% \tVal Acc: 2.1666668%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased from 3.579574 to 3.579543, saving the model weights\n",
      "Epoch: 408\tTrain Loss: 3.6298565 \tVal Loss:3.5796134 \tTrain Acc: 1.875% \tVal Acc: 2.3333335%\n",
      "Epoch: 409\tTrain Loss: 3.6302967 \tVal Loss:3.5796317 \tTrain Acc: 1.625% \tVal Acc: 2.3333335%\n",
      "Epoch: 410\tTrain Loss: 3.6310464 \tVal Loss:3.5795971 \tTrain Acc: 1.625% \tVal Acc: 2.1666668%\n",
      "Epoch: 411\tTrain Loss: 3.6308770 \tVal Loss:3.5796083 \tTrain Acc: 1.666667% \tVal Acc: 2.8333334%\n",
      "Epoch: 412\tTrain Loss: 3.6305317 \tVal Loss:3.5796350 \tTrain Acc: 1.625% \tVal Acc: 2.1666668%\n",
      "Epoch: 413\tTrain Loss: 3.6305663 \tVal Loss:3.5796603 \tTrain Acc: 2.041667% \tVal Acc: 2.1666668%\n",
      "Epoch: 414\tTrain Loss: 3.6302233 \tVal Loss:3.5796624 \tTrain Acc: 1.958333% \tVal Acc: 2.1666668%\n",
      "Epoch: 415\tTrain Loss: 3.6304238 \tVal Loss:3.5796878 \tTrain Acc: 1.708333% \tVal Acc: 2.1666668%\n",
      "Epoch: 416\tTrain Loss: 3.6302563 \tVal Loss:3.5796606 \tTrain Acc: 2.166667% \tVal Acc: 2.1666668%\n",
      "Epoch: 417\tTrain Loss: 3.6304022 \tVal Loss:3.5796508 \tTrain Acc: 2.125% \tVal Acc: 2.1666668%\n",
      "Epoch: 418\tTrain Loss: 3.6295191 \tVal Loss:3.5795956 \tTrain Acc: 1.958333% \tVal Acc: 0.5000000%\n",
      "Epoch: 419\tTrain Loss: 3.6302704 \tVal Loss:3.5796292 \tTrain Acc: 1.791667% \tVal Acc: 0.5000000%\n",
      "Epoch: 420\tTrain Loss: 3.6298998 \tVal Loss:3.5796220 \tTrain Acc: 1.583333% \tVal Acc: 2.1666668%\n",
      "Epoch: 421\tTrain Loss: 3.6305846 \tVal Loss:3.5795406 \tTrain Acc: 1.833333% \tVal Acc: 2.1666668%\n",
      "Validation Loss decreased from 3.579543 to 3.579541, saving the model weights\n",
      "Epoch: 422\tTrain Loss: 3.6303190 \tVal Loss:3.5795042 \tTrain Acc: 2.458333% \tVal Acc: 2.1666668%\n",
      "Validation Loss decreased from 3.579541 to 3.579504, saving the model weights\n",
      "Epoch: 423\tTrain Loss: 3.6299172 \tVal Loss:3.5793832 \tTrain Acc: 1.875% \tVal Acc: 2.1666668%\n",
      "Validation Loss decreased from 3.579504 to 3.579383, saving the model weights\n",
      "Epoch: 424\tTrain Loss: 3.6306393 \tVal Loss:3.5793184 \tTrain Acc: 1.958333% \tVal Acc: 2.1666668%\n",
      "Validation Loss decreased from 3.579383 to 3.579318, saving the model weights\n",
      "Epoch: 425\tTrain Loss: 3.6302625 \tVal Loss:3.5793422 \tTrain Acc: 2.208333% \tVal Acc: 0.5000000%\n",
      "Epoch: 426\tTrain Loss: 3.6306884 \tVal Loss:3.5793696 \tTrain Acc: 2.375% \tVal Acc: 2.8333334%\n",
      "Epoch: 427\tTrain Loss: 3.6304389 \tVal Loss:3.5794077 \tTrain Acc: 2.0% \tVal Acc: 2.8333334%\n",
      "Epoch: 428\tTrain Loss: 3.6305505 \tVal Loss:3.5794470 \tTrain Acc: 1.708333% \tVal Acc: 2.8333334%\n",
      "Epoch: 429\tTrain Loss: 3.6298109 \tVal Loss:3.5795555 \tTrain Acc: 2.0% \tVal Acc: 1.3333334%\n",
      "Epoch: 430\tTrain Loss: 3.6303100 \tVal Loss:3.5796209 \tTrain Acc: 2.958333% \tVal Acc: 0.5000000%\n",
      "Epoch: 431\tTrain Loss: 3.6301326 \tVal Loss:3.5796694 \tTrain Acc: 2.041667% \tVal Acc: 2.8333334%\n",
      "Epoch: 432\tTrain Loss: 3.6299447 \tVal Loss:3.5797619 \tTrain Acc: 2.166667% \tVal Acc: 2.8333334%\n",
      "Epoch: 433\tTrain Loss: 3.6304839 \tVal Loss:3.5797537 \tTrain Acc: 2.25% \tVal Acc: 3.6666668%\n",
      "Epoch: 434\tTrain Loss: 3.6293445 \tVal Loss:3.5797585 \tTrain Acc: 2.125% \tVal Acc: 3.6666668%\n",
      "Epoch: 435\tTrain Loss: 3.6300356 \tVal Loss:3.5796422 \tTrain Acc: 2.5% \tVal Acc: 3.6666668%\n",
      "Epoch: 436\tTrain Loss: 3.6301106 \tVal Loss:3.5797043 \tTrain Acc: 1.708333% \tVal Acc: 3.6666668%\n",
      "Epoch: 437\tTrain Loss: 3.6299697 \tVal Loss:3.5797521 \tTrain Acc: 2.416667% \tVal Acc: 3.6666668%\n",
      "Epoch: 438\tTrain Loss: 3.6300618 \tVal Loss:3.5797939 \tTrain Acc: 1.583333% \tVal Acc: 1.3333334%\n",
      "Epoch: 439\tTrain Loss: 3.6301396 \tVal Loss:3.5797928 \tTrain Acc: 2.208333% \tVal Acc: 2.1666668%\n",
      "Epoch: 440\tTrain Loss: 3.6299704 \tVal Loss:3.5798904 \tTrain Acc: 1.875% \tVal Acc: 2.1666668%\n",
      "Epoch: 441\tTrain Loss: 3.6298800 \tVal Loss:3.5798847 \tTrain Acc: 2.083333% \tVal Acc: 2.1666668%\n",
      "Epoch: 442\tTrain Loss: 3.6300525 \tVal Loss:3.5799150 \tTrain Acc: 2.166667% \tVal Acc: 1.3333334%\n",
      "Epoch: 443\tTrain Loss: 3.6290694 \tVal Loss:3.5800179 \tTrain Acc: 2.458333% \tVal Acc: 1.3333334%\n",
      "Epoch: 444\tTrain Loss: 3.6299002 \tVal Loss:3.5798925 \tTrain Acc: 1.791667% \tVal Acc: 2.1666668%\n",
      "Epoch: 445\tTrain Loss: 3.6303465 \tVal Loss:3.5798532 \tTrain Acc: 1.75% \tVal Acc: 2.1666668%\n",
      "Epoch: 446\tTrain Loss: 3.6301800 \tVal Loss:3.5798200 \tTrain Acc: 2.0% \tVal Acc: 2.1666668%\n",
      "Epoch: 447\tTrain Loss: 3.6301092 \tVal Loss:3.5797567 \tTrain Acc: 1.916667% \tVal Acc: 1.3333334%\n",
      "Epoch: 448\tTrain Loss: 3.6302821 \tVal Loss:3.5796649 \tTrain Acc: 2.0% \tVal Acc: 1.3333334%\n",
      "Epoch: 449\tTrain Loss: 3.6303535 \tVal Loss:3.5796833 \tTrain Acc: 1.416667% \tVal Acc: 2.8333334%\n",
      "Epoch: 450\tTrain Loss: 3.6301742 \tVal Loss:3.5796877 \tTrain Acc: 1.833333% \tVal Acc: 2.8333334%\n",
      "Epoch: 451\tTrain Loss: 3.6297163 \tVal Loss:3.5796809 \tTrain Acc: 1.958333% \tVal Acc: 2.8333334%\n",
      "Epoch: 452\tTrain Loss: 3.6303697 \tVal Loss:3.5796662 \tTrain Acc: 2.125% \tVal Acc: 2.3333335%\n",
      "Epoch: 453\tTrain Loss: 3.6293321 \tVal Loss:3.5796151 \tTrain Acc: 2.333333% \tVal Acc: 2.1666667%\n",
      "Epoch: 454\tTrain Loss: 3.6302450 \tVal Loss:3.5795710 \tTrain Acc: 2.375% \tVal Acc: 2.1666667%\n",
      "Epoch: 455\tTrain Loss: 3.6302270 \tVal Loss:3.5795946 \tTrain Acc: 1.791667% \tVal Acc: 2.3333335%\n",
      "Epoch: 456\tTrain Loss: 3.6300616 \tVal Loss:3.5795916 \tTrain Acc: 1.333333% \tVal Acc: 2.6666668%\n",
      "Epoch: 457\tTrain Loss: 3.6300199 \tVal Loss:3.5795426 \tTrain Acc: 2.208333% \tVal Acc: 2.6666668%\n",
      "Epoch: 458\tTrain Loss: 3.6301662 \tVal Loss:3.5794773 \tTrain Acc: 2.25% \tVal Acc: 2.6666668%\n",
      "Epoch: 459\tTrain Loss: 3.6298740 \tVal Loss:3.5794432 \tTrain Acc: 2.083333% \tVal Acc: 2.6666668%\n",
      "Epoch: 460\tTrain Loss: 3.6304254 \tVal Loss:3.5795369 \tTrain Acc: 1.75% \tVal Acc: 2.6666668%\n",
      "Epoch: 461\tTrain Loss: 3.6307675 \tVal Loss:3.5795620 \tTrain Acc: 1.375% \tVal Acc: 2.6666668%\n",
      "Epoch: 462\tTrain Loss: 3.6300210 \tVal Loss:3.5795884 \tTrain Acc: 1.375% \tVal Acc: 2.6666668%\n",
      "Epoch: 463\tTrain Loss: 3.6298530 \tVal Loss:3.5795164 \tTrain Acc: 2.541667% \tVal Acc: 2.6666668%\n",
      "Epoch: 464\tTrain Loss: 3.6296036 \tVal Loss:3.5795038 \tTrain Acc: 1.791667% \tVal Acc: 4.3333334%\n",
      "Epoch: 465\tTrain Loss: 3.6303523 \tVal Loss:3.5795452 \tTrain Acc: 1.708333% \tVal Acc: 1.3333334%\n",
      "Epoch: 466\tTrain Loss: 3.6296946 \tVal Loss:3.5795458 \tTrain Acc: 1.416667% \tVal Acc: 1.3333334%\n",
      "Epoch: 467\tTrain Loss: 3.6305486 \tVal Loss:3.5795746 \tTrain Acc: 1.25% \tVal Acc: 1.3333334%\n",
      "Epoch: 468\tTrain Loss: 3.6302252 \tVal Loss:3.5795892 \tTrain Acc: 1.583333% \tVal Acc: 1.3333334%\n",
      "Epoch: 469\tTrain Loss: 3.6299542 \tVal Loss:3.5796263 \tTrain Acc: 2.041667% \tVal Acc: 1.3333334%\n",
      "Epoch: 470\tTrain Loss: 3.6302454 \tVal Loss:3.5796389 \tTrain Acc: 1.833333% \tVal Acc: 1.3333334%\n",
      "Epoch: 471\tTrain Loss: 3.6304433 \tVal Loss:3.5796633 \tTrain Acc: 1.75% \tVal Acc: 2.1666668%\n",
      "Epoch: 472\tTrain Loss: 3.6298324 \tVal Loss:3.5795945 \tTrain Acc: 1.5% \tVal Acc: 2.1666668%\n",
      "Epoch: 473\tTrain Loss: 3.6300855 \tVal Loss:3.5795817 \tTrain Acc: 1.541667% \tVal Acc: 2.1666668%\n",
      "Epoch: 474\tTrain Loss: 3.6297286 \tVal Loss:3.5795965 \tTrain Acc: 1.666667% \tVal Acc: 2.1666668%\n",
      "Epoch: 475\tTrain Loss: 3.6300133 \tVal Loss:3.5796641 \tTrain Acc: 2.583333% \tVal Acc: 2.6666668%\n",
      "Epoch: 476\tTrain Loss: 3.6299131 \tVal Loss:3.5796447 \tTrain Acc: 1.5% \tVal Acc: 1.3333334%\n",
      "Epoch: 477\tTrain Loss: 3.6304358 \tVal Loss:3.5796213 \tTrain Acc: 1.916667% \tVal Acc: 0.8333334%\n",
      "Epoch: 478\tTrain Loss: 3.6298355 \tVal Loss:3.5796217 \tTrain Acc: 1.875% \tVal Acc: 1.3333334%\n",
      "Epoch: 479\tTrain Loss: 3.6298063 \tVal Loss:3.5797278 \tTrain Acc: 1.916667% \tVal Acc: 2.3333335%\n",
      "Epoch: 480\tTrain Loss: 3.6292972 \tVal Loss:3.5797180 \tTrain Acc: 1.291667% \tVal Acc: 2.1666668%\n",
      "Epoch: 481\tTrain Loss: 3.6298607 \tVal Loss:3.5797407 \tTrain Acc: 2.041667% \tVal Acc: 1.0000000%\n",
      "Epoch: 482\tTrain Loss: 3.6299745 \tVal Loss:3.5797023 \tTrain Acc: 2.166667% \tVal Acc: 1.0000000%\n",
      "Epoch: 483\tTrain Loss: 3.6302939 \tVal Loss:3.5796729 \tTrain Acc: 1.916667% \tVal Acc: 1.3333334%\n",
      "Epoch: 484\tTrain Loss: 3.6294730 \tVal Loss:3.5796958 \tTrain Acc: 1.625% \tVal Acc: 2.6666668%\n",
      "Epoch: 485\tTrain Loss: 3.6303914 \tVal Loss:3.5797523 \tTrain Acc: 1.916667% \tVal Acc: 2.6666668%\n",
      "Epoch: 486\tTrain Loss: 3.6300514 \tVal Loss:3.5797328 \tTrain Acc: 2.291667% \tVal Acc: 2.1666668%\n",
      "Epoch: 487\tTrain Loss: 3.6298634 \tVal Loss:3.5796414 \tTrain Acc: 2.166667% \tVal Acc: 0.5000000%\n",
      "Epoch: 488\tTrain Loss: 3.6296023 \tVal Loss:3.5796529 \tTrain Acc: 1.75% \tVal Acc: 2.6666668%\n",
      "Epoch: 489\tTrain Loss: 3.6298142 \tVal Loss:3.5795713 \tTrain Acc: 2.041667% \tVal Acc: 0.5000000%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 490\tTrain Loss: 3.6297173 \tVal Loss:3.5795497 \tTrain Acc: 2.0% \tVal Acc: 0.5000000%\n",
      "Epoch: 491\tTrain Loss: 3.6300544 \tVal Loss:3.5796095 \tTrain Acc: 1.333333% \tVal Acc: 0.5000000%\n",
      "Epoch: 492\tTrain Loss: 3.6302157 \tVal Loss:3.5795926 \tTrain Acc: 2.041667% \tVal Acc: 0.5000000%\n",
      "Epoch: 493\tTrain Loss: 3.6298420 \tVal Loss:3.5796008 \tTrain Acc: 1.125% \tVal Acc: 0.5000000%\n",
      "Epoch: 494\tTrain Loss: 3.6299347 \tVal Loss:3.5795783 \tTrain Acc: 2.166667% \tVal Acc: 0.5000000%\n",
      "Epoch: 495\tTrain Loss: 3.6304671 \tVal Loss:3.5796066 \tTrain Acc: 1.291667% \tVal Acc: 2.3333335%\n",
      "Epoch: 496\tTrain Loss: 3.6299846 \tVal Loss:3.5795741 \tTrain Acc: 1.958333% \tVal Acc: 2.1666668%\n",
      "Epoch: 497\tTrain Loss: 3.6298080 \tVal Loss:3.5795478 \tTrain Acc: 1.541667% \tVal Acc: 0.5000000%\n",
      "Epoch: 498\tTrain Loss: 3.6299034 \tVal Loss:3.5795937 \tTrain Acc: 1.791667% \tVal Acc: 2.1666668%\n",
      "Epoch: 499\tTrain Loss: 3.6298010 \tVal Loss:3.5796261 \tTrain Acc: 1.375% \tVal Acc: 2.1666668%\n",
      "Epoch: 500\tTrain Loss: 3.6297373 \tVal Loss:3.5795950 \tTrain Acc: 1.75% \tVal Acc: 2.1666668%\n",
      "Epoch: 501\tTrain Loss: 3.6305869 \tVal Loss:3.5795880 \tTrain Acc: 1.5% \tVal Acc: 2.1666668%\n",
      "Epoch: 502\tTrain Loss: 3.6295240 \tVal Loss:3.5795769 \tTrain Acc: 1.916667% \tVal Acc: 2.1666668%\n",
      "Epoch: 503\tTrain Loss: 3.6300641 \tVal Loss:3.5795446 \tTrain Acc: 1.916667% \tVal Acc: 0.5000000%\n",
      "Epoch: 504\tTrain Loss: 3.6297476 \tVal Loss:3.5795680 \tTrain Acc: 1.958333% \tVal Acc: 2.6666668%\n",
      "Epoch: 505\tTrain Loss: 3.6300480 \tVal Loss:3.5795700 \tTrain Acc: 1.916667% \tVal Acc: 2.1666667%\n",
      "Epoch: 506\tTrain Loss: 3.6300889 \tVal Loss:3.5795729 \tTrain Acc: 2.041667% \tVal Acc: 2.8333334%\n",
      "Epoch: 507\tTrain Loss: 3.6303058 \tVal Loss:3.5795997 \tTrain Acc: 1.5% \tVal Acc: 2.8333334%\n",
      "Epoch: 508\tTrain Loss: 3.6300989 \tVal Loss:3.5795902 \tTrain Acc: 1.666667% \tVal Acc: 2.1666668%\n",
      "Epoch: 509\tTrain Loss: 3.6301561 \tVal Loss:3.5795918 \tTrain Acc: 1.25% \tVal Acc: 2.1666668%\n",
      "Epoch: 510\tTrain Loss: 3.6294660 \tVal Loss:3.5796456 \tTrain Acc: 2.041667% \tVal Acc: 2.1666668%\n",
      "Epoch: 511\tTrain Loss: 3.6296943 \tVal Loss:3.5795573 \tTrain Acc: 1.958333% \tVal Acc: 2.1666668%\n",
      "Epoch: 512\tTrain Loss: 3.6299731 \tVal Loss:3.5795700 \tTrain Acc: 1.875% \tVal Acc: 2.1666668%\n",
      "Epoch: 513\tTrain Loss: 3.6298245 \tVal Loss:3.5795810 \tTrain Acc: 1.541667% \tVal Acc: 2.8333334%\n",
      "Epoch: 514\tTrain Loss: 3.6297794 \tVal Loss:3.5796175 \tTrain Acc: 2.25% \tVal Acc: 2.8333334%\n",
      "Epoch: 515\tTrain Loss: 3.6296122 \tVal Loss:3.5796777 \tTrain Acc: 1.666667% \tVal Acc: 2.6666668%\n",
      "Epoch: 516\tTrain Loss: 3.6299682 \tVal Loss:3.5796272 \tTrain Acc: 1.708333% \tVal Acc: 2.6666668%\n",
      "Epoch: 517\tTrain Loss: 3.6302842 \tVal Loss:3.5795679 \tTrain Acc: 1.791667% \tVal Acc: 2.8333334%\n",
      "Epoch: 518\tTrain Loss: 3.6299749 \tVal Loss:3.5795911 \tTrain Acc: 2.125% \tVal Acc: 2.8333334%\n",
      "Epoch: 519\tTrain Loss: 3.6300054 \tVal Loss:3.5797033 \tTrain Acc: 1.791667% \tVal Acc: 2.6666668%\n",
      "Epoch: 520\tTrain Loss: 3.6295816 \tVal Loss:3.5797196 \tTrain Acc: 1.458333% \tVal Acc: 2.1666668%\n",
      "Epoch: 521\tTrain Loss: 3.6299260 \tVal Loss:3.5797262 \tTrain Acc: 1.875% \tVal Acc: 2.5000001%\n",
      "Epoch: 522\tTrain Loss: 3.6297718 \tVal Loss:3.5796644 \tTrain Acc: 1.958333% \tVal Acc: 2.5000001%\n",
      "Epoch: 523\tTrain Loss: 3.6299469 \tVal Loss:3.5796667 \tTrain Acc: 1.333333% \tVal Acc: 2.5000001%\n",
      "Epoch: 524\tTrain Loss: 3.6296997 \tVal Loss:3.5796372 \tTrain Acc: 2.0% \tVal Acc: 2.5000001%\n",
      "Epoch: 525\tTrain Loss: 3.6296806 \tVal Loss:3.5795738 \tTrain Acc: 1.5% \tVal Acc: 2.5000001%\n",
      "Epoch: 526\tTrain Loss: 3.6299932 \tVal Loss:3.5795337 \tTrain Acc: 2.0% \tVal Acc: 2.6666668%\n",
      "Epoch: 527\tTrain Loss: 3.6301160 \tVal Loss:3.5794673 \tTrain Acc: 1.333333% \tVal Acc: 2.6666668%\n",
      "Epoch: 528\tTrain Loss: 3.6297386 \tVal Loss:3.5794425 \tTrain Acc: 1.875% \tVal Acc: 2.6666668%\n",
      "Epoch: 529\tTrain Loss: 3.6299475 \tVal Loss:3.5794641 \tTrain Acc: 1.833333% \tVal Acc: 2.6666668%\n",
      "Epoch: 530\tTrain Loss: 3.6299003 \tVal Loss:3.5794392 \tTrain Acc: 2.125% \tVal Acc: 4.5000002%\n",
      "Epoch: 531\tTrain Loss: 3.6295027 \tVal Loss:3.5794356 \tTrain Acc: 2.041667% \tVal Acc: 2.6666668%\n",
      "Epoch: 532\tTrain Loss: 3.6296892 \tVal Loss:3.5794420 \tTrain Acc: 1.75% \tVal Acc: 2.6666668%\n",
      "Epoch: 533\tTrain Loss: 3.6301945 \tVal Loss:3.5794964 \tTrain Acc: 1.916667% \tVal Acc: 1.3333334%\n",
      "Epoch: 534\tTrain Loss: 3.6303179 \tVal Loss:3.5795173 \tTrain Acc: 1.458333% \tVal Acc: 1.3333334%\n",
      "Epoch: 535\tTrain Loss: 3.6297982 \tVal Loss:3.5795009 \tTrain Acc: 2.0% \tVal Acc: 1.3333334%\n",
      "Epoch: 536\tTrain Loss: 3.6296706 \tVal Loss:3.5794749 \tTrain Acc: 1.75% \tVal Acc: 2.6666668%\n",
      "Epoch: 537\tTrain Loss: 3.6296938 \tVal Loss:3.5794083 \tTrain Acc: 2.041667% \tVal Acc: 2.6666668%\n",
      "Epoch: 538\tTrain Loss: 3.6301394 \tVal Loss:3.5793272 \tTrain Acc: 1.958333% \tVal Acc: 2.6666668%\n",
      "Epoch: 539\tTrain Loss: 3.6297347 \tVal Loss:3.5793300 \tTrain Acc: 1.791667% \tVal Acc: 2.5000001%\n",
      "Epoch: 540\tTrain Loss: 3.6301628 \tVal Loss:3.5793268 \tTrain Acc: 1.916667% \tVal Acc: 2.8333334%\n",
      "Epoch: 541\tTrain Loss: 3.6296584 \tVal Loss:3.5794213 \tTrain Acc: 1.875% \tVal Acc: 2.8333334%\n",
      "Epoch: 542\tTrain Loss: 3.6301573 \tVal Loss:3.5794584 \tTrain Acc: 1.958333% \tVal Acc: 2.8333334%\n",
      "Epoch: 543\tTrain Loss: 3.6298670 \tVal Loss:3.5794581 \tTrain Acc: 1.75% \tVal Acc: 2.8333334%\n",
      "Epoch: 544\tTrain Loss: 3.6294829 \tVal Loss:3.5794180 \tTrain Acc: 1.916667% \tVal Acc: 2.8333334%\n",
      "Epoch: 545\tTrain Loss: 3.6295359 \tVal Loss:3.5793662 \tTrain Acc: 2.0% \tVal Acc: 1.3333334%\n",
      "Epoch: 546\tTrain Loss: 3.6294672 \tVal Loss:3.5792730 \tTrain Acc: 2.0% \tVal Acc: 1.3333334%\n",
      "Validation Loss decreased from 3.579318 to 3.579273, saving the model weights\n",
      "Epoch: 547\tTrain Loss: 3.6293975 \tVal Loss:3.5792728 \tTrain Acc: 1.708333% \tVal Acc: 2.5000001%\n",
      "Validation Loss decreased from 3.579273 to 3.579273, saving the model weights\n",
      "Epoch: 548\tTrain Loss: 3.6296530 \tVal Loss:3.5792082 \tTrain Acc: 1.583333% \tVal Acc: 2.5000001%\n",
      "Validation Loss decreased from 3.579273 to 3.579208, saving the model weights\n",
      "Epoch: 549\tTrain Loss: 3.6300055 \tVal Loss:3.5792092 \tTrain Acc: 1.625% \tVal Acc: 2.5000001%\n",
      "Epoch: 550\tTrain Loss: 3.6296449 \tVal Loss:3.5792450 \tTrain Acc: 1.625% \tVal Acc: 2.8333334%\n",
      "Epoch: 551\tTrain Loss: 3.6300180 \tVal Loss:3.5792403 \tTrain Acc: 1.916667% \tVal Acc: 1.3333334%\n",
      "Epoch: 552\tTrain Loss: 3.6298035 \tVal Loss:3.5792620 \tTrain Acc: 1.416667% \tVal Acc: 1.3333334%\n",
      "Epoch: 553\tTrain Loss: 3.6295553 \tVal Loss:3.5792452 \tTrain Acc: 2.416667% \tVal Acc: 1.3333334%\n",
      "Epoch: 554\tTrain Loss: 3.6299474 \tVal Loss:3.5792646 \tTrain Acc: 1.708333% \tVal Acc: 1.3333334%\n",
      "Epoch: 555\tTrain Loss: 3.6296730 \tVal Loss:3.5793702 \tTrain Acc: 1.791667% \tVal Acc: 1.3333334%\n",
      "Epoch: 556\tTrain Loss: 3.6297652 \tVal Loss:3.5793518 \tTrain Acc: 1.75% \tVal Acc: 1.3333334%\n",
      "Epoch: 557\tTrain Loss: 3.6295935 \tVal Loss:3.5793946 \tTrain Acc: 2.166667% \tVal Acc: 1.3333334%\n",
      "Epoch: 558\tTrain Loss: 3.6297983 \tVal Loss:3.5794080 \tTrain Acc: 1.916667% \tVal Acc: 1.3333334%\n",
      "Epoch: 559\tTrain Loss: 3.6297792 \tVal Loss:3.5794214 \tTrain Acc: 1.458333% \tVal Acc: 1.3333334%\n",
      "Epoch: 560\tTrain Loss: 3.6300196 \tVal Loss:3.5794131 \tTrain Acc: 1.708333% \tVal Acc: 1.3333334%\n",
      "Epoch: 561\tTrain Loss: 3.6293516 \tVal Loss:3.5793743 \tTrain Acc: 2.041667% \tVal Acc: 1.3333334%\n",
      "Epoch: 562\tTrain Loss: 3.6296809 \tVal Loss:3.5793906 \tTrain Acc: 1.666667% \tVal Acc: 1.3333334%\n",
      "Epoch: 563\tTrain Loss: 3.6297913 \tVal Loss:3.5793499 \tTrain Acc: 1.333333% \tVal Acc: 5.0000001%\n",
      "Epoch: 564\tTrain Loss: 3.6299190 \tVal Loss:3.5793275 \tTrain Acc: 1.916667% \tVal Acc: 1.3333334%\n",
      "Epoch: 565\tTrain Loss: 3.6295882 \tVal Loss:3.5793541 \tTrain Acc: 1.791667% \tVal Acc: 2.3333335%\n",
      "Epoch: 566\tTrain Loss: 3.6298856 \tVal Loss:3.5794030 \tTrain Acc: 1.916667% \tVal Acc: 1.3333334%\n",
      "Epoch: 567\tTrain Loss: 3.6299172 \tVal Loss:3.5793801 \tTrain Acc: 1.833333% \tVal Acc: 1.3333334%\n",
      "Epoch: 568\tTrain Loss: 3.6303465 \tVal Loss:3.5795120 \tTrain Acc: 1.375% \tVal Acc: 1.3333334%\n",
      "Epoch: 569\tTrain Loss: 3.6294968 \tVal Loss:3.5795400 \tTrain Acc: 1.666667% \tVal Acc: 1.3333334%\n",
      "Epoch: 570\tTrain Loss: 3.6298318 \tVal Loss:3.5795692 \tTrain Acc: 2.25% \tVal Acc: 1.3333334%\n",
      "Epoch: 571\tTrain Loss: 3.6294915 \tVal Loss:3.5796064 \tTrain Acc: 1.708333% \tVal Acc: 1.3333334%\n",
      "Epoch: 572\tTrain Loss: 3.6300208 \tVal Loss:3.5795616 \tTrain Acc: 1.541667% \tVal Acc: 2.1666668%\n",
      "Epoch: 573\tTrain Loss: 3.6294467 \tVal Loss:3.5795847 \tTrain Acc: 1.708333% \tVal Acc: 2.1666668%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 574\tTrain Loss: 3.6294864 \tVal Loss:3.5795708 \tTrain Acc: 1.958333% \tVal Acc: 2.3333335%\n",
      "Epoch: 575\tTrain Loss: 3.6300241 \tVal Loss:3.5795642 \tTrain Acc: 1.708333% \tVal Acc: 2.3333335%\n",
      "Epoch: 576\tTrain Loss: 3.6295375 \tVal Loss:3.5795787 \tTrain Acc: 2.041667% \tVal Acc: 2.1666668%\n",
      "Epoch: 577\tTrain Loss: 3.6295000 \tVal Loss:3.5796273 \tTrain Acc: 1.791667% \tVal Acc: 2.1666668%\n",
      "Epoch: 578\tTrain Loss: 3.6294456 \tVal Loss:3.5796036 \tTrain Acc: 1.541667% \tVal Acc: 1.3333334%\n",
      "Epoch: 579\tTrain Loss: 3.6297910 \tVal Loss:3.5795806 \tTrain Acc: 2.0% \tVal Acc: 1.3333334%\n",
      "Epoch: 580\tTrain Loss: 3.6297835 \tVal Loss:3.5796671 \tTrain Acc: 1.625% \tVal Acc: 1.3333334%\n",
      "Epoch: 581\tTrain Loss: 3.6300852 \tVal Loss:3.5796657 \tTrain Acc: 1.708333% \tVal Acc: 1.3333334%\n",
      "Epoch: 582\tTrain Loss: 3.6295461 \tVal Loss:3.5796501 \tTrain Acc: 1.708333% \tVal Acc: 1.3333334%\n",
      "Epoch: 583\tTrain Loss: 3.6296892 \tVal Loss:3.5796557 \tTrain Acc: 1.958333% \tVal Acc: 1.3333334%\n",
      "Epoch: 584\tTrain Loss: 3.6295868 \tVal Loss:3.5796997 \tTrain Acc: 1.875% \tVal Acc: 1.3333334%\n",
      "Epoch: 585\tTrain Loss: 3.6295613 \tVal Loss:3.5796304 \tTrain Acc: 2.291667% \tVal Acc: 1.3333334%\n",
      "Epoch: 586\tTrain Loss: 3.6298289 \tVal Loss:3.5796357 \tTrain Acc: 1.958333% \tVal Acc: 1.3333334%\n",
      "Epoch: 587\tTrain Loss: 3.6294985 \tVal Loss:3.5796987 \tTrain Acc: 1.458333% \tVal Acc: 1.3333334%\n",
      "Epoch: 588\tTrain Loss: 3.6299042 \tVal Loss:3.5796668 \tTrain Acc: 2.333333% \tVal Acc: 1.3333334%\n",
      "Epoch: 589\tTrain Loss: 3.6297613 \tVal Loss:3.5796565 \tTrain Acc: 2.125% \tVal Acc: 1.3333334%\n",
      "Epoch: 590\tTrain Loss: 3.6296000 \tVal Loss:3.5796396 \tTrain Acc: 1.75% \tVal Acc: 2.1666667%\n",
      "Epoch: 591\tTrain Loss: 3.6297246 \tVal Loss:3.5796693 \tTrain Acc: 1.041667% \tVal Acc: 2.1666667%\n",
      "Epoch: 592\tTrain Loss: 3.6298568 \tVal Loss:3.5796001 \tTrain Acc: 2.0% \tVal Acc: 2.1666667%\n",
      "Epoch: 593\tTrain Loss: 3.6297128 \tVal Loss:3.5796127 \tTrain Acc: 1.333333% \tVal Acc: 2.1666668%\n",
      "Epoch: 594\tTrain Loss: 3.6301321 \tVal Loss:3.5795717 \tTrain Acc: 1.958333% \tVal Acc: 2.6666668%\n",
      "Epoch: 595\tTrain Loss: 3.6296511 \tVal Loss:3.5796341 \tTrain Acc: 1.875% \tVal Acc: 2.6666668%\n",
      "Epoch: 596\tTrain Loss: 3.6299405 \tVal Loss:3.5796855 \tTrain Acc: 1.958333% \tVal Acc: 2.6666668%\n",
      "Epoch: 597\tTrain Loss: 3.6295662 \tVal Loss:3.5797021 \tTrain Acc: 1.875% \tVal Acc: 2.6666668%\n",
      "Epoch: 598\tTrain Loss: 3.6295446 \tVal Loss:3.5797484 \tTrain Acc: 2.083333% \tVal Acc: 2.1666668%\n",
      "Epoch: 599\tTrain Loss: 3.6296809 \tVal Loss:3.5798285 \tTrain Acc: 1.708333% \tVal Acc: 2.1666668%\n",
      "Epoch: 600\tTrain Loss: 3.6295592 \tVal Loss:3.5798910 \tTrain Acc: 1.416667% \tVal Acc: 2.1666668%\n",
      "Epoch: 601\tTrain Loss: 3.6300284 \tVal Loss:3.5798607 \tTrain Acc: 1.583333% \tVal Acc: 2.1666668%\n",
      "Epoch: 602\tTrain Loss: 3.6295970 \tVal Loss:3.5797968 \tTrain Acc: 1.5% \tVal Acc: 2.6666668%\n",
      "Epoch: 603\tTrain Loss: 3.6293231 \tVal Loss:3.5798088 \tTrain Acc: 1.416667% \tVal Acc: 2.6666668%\n",
      "Epoch: 604\tTrain Loss: 3.6295373 \tVal Loss:3.5797359 \tTrain Acc: 1.5% \tVal Acc: 2.6666668%\n",
      "Epoch: 605\tTrain Loss: 3.6294020 \tVal Loss:3.5796820 \tTrain Acc: 1.625% \tVal Acc: 1.3333334%\n",
      "Epoch: 606\tTrain Loss: 3.6296354 \tVal Loss:3.5796739 \tTrain Acc: 1.458333% \tVal Acc: 1.3333334%\n",
      "Epoch: 607\tTrain Loss: 3.6295382 \tVal Loss:3.5796543 \tTrain Acc: 1.916667% \tVal Acc: 1.3333334%\n",
      "Epoch: 608\tTrain Loss: 3.6295872 \tVal Loss:3.5796988 \tTrain Acc: 1.875% \tVal Acc: 1.3333334%\n",
      "Epoch: 609\tTrain Loss: 3.6300358 \tVal Loss:3.5797211 \tTrain Acc: 1.083333% \tVal Acc: 1.3333334%\n",
      "Epoch: 610\tTrain Loss: 3.6297920 \tVal Loss:3.5796763 \tTrain Acc: 1.541667% \tVal Acc: 1.3333334%\n",
      "Epoch: 611\tTrain Loss: 3.6294747 \tVal Loss:3.5796736 \tTrain Acc: 1.666667% \tVal Acc: 1.3333334%\n",
      "Epoch: 612\tTrain Loss: 3.6298944 \tVal Loss:3.5796212 \tTrain Acc: 1.916667% \tVal Acc: 1.3333334%\n",
      "Epoch: 613\tTrain Loss: 3.6297725 \tVal Loss:3.5795862 \tTrain Acc: 1.541667% \tVal Acc: 1.3333334%\n",
      "Epoch: 614\tTrain Loss: 3.6295580 \tVal Loss:3.5795319 \tTrain Acc: 1.916667% \tVal Acc: 1.3333334%\n",
      "Epoch: 615\tTrain Loss: 3.6299692 \tVal Loss:3.5794930 \tTrain Acc: 1.583333% \tVal Acc: 2.8333334%\n",
      "Epoch: 616\tTrain Loss: 3.6297847 \tVal Loss:3.5794763 \tTrain Acc: 1.916667% \tVal Acc: 2.8333334%\n",
      "Epoch: 617\tTrain Loss: 3.6299964 \tVal Loss:3.5794803 \tTrain Acc: 1.75% \tVal Acc: 2.8333334%\n",
      "Epoch: 618\tTrain Loss: 3.6298463 \tVal Loss:3.5795408 \tTrain Acc: 1.791667% \tVal Acc: 2.8333334%\n",
      "Epoch: 619\tTrain Loss: 3.6296902 \tVal Loss:3.5796045 \tTrain Acc: 1.708333% \tVal Acc: 2.8333334%\n",
      "Epoch: 620\tTrain Loss: 3.6294081 \tVal Loss:3.5796314 \tTrain Acc: 1.291667% \tVal Acc: 2.8333334%\n",
      "Epoch: 621\tTrain Loss: 3.6290345 \tVal Loss:3.5795812 \tTrain Acc: 1.958333% \tVal Acc: 2.8333334%\n",
      "Epoch: 622\tTrain Loss: 3.6297497 \tVal Loss:3.5795375 \tTrain Acc: 1.5% \tVal Acc: 2.3333335%\n",
      "Epoch: 623\tTrain Loss: 3.6295703 \tVal Loss:3.5795503 \tTrain Acc: 1.625% \tVal Acc: 2.3333335%\n",
      "Epoch: 624\tTrain Loss: 3.6297678 \tVal Loss:3.5795440 \tTrain Acc: 1.833333% \tVal Acc: 1.3333334%\n",
      "Epoch: 625\tTrain Loss: 3.6296657 \tVal Loss:3.5795486 \tTrain Acc: 1.791667% \tVal Acc: 2.1666667%\n",
      "Epoch: 626\tTrain Loss: 3.6298173 \tVal Loss:3.5796144 \tTrain Acc: 1.75% \tVal Acc: 2.1666667%\n",
      "Epoch: 627\tTrain Loss: 3.6294926 \tVal Loss:3.5796074 \tTrain Acc: 1.541667% \tVal Acc: 2.1666667%\n",
      "Epoch: 628\tTrain Loss: 3.6300209 \tVal Loss:3.5795789 \tTrain Acc: 2.166667% \tVal Acc: 1.3333334%\n",
      "Epoch: 629\tTrain Loss: 3.6292791 \tVal Loss:3.5796082 \tTrain Acc: 1.833333% \tVal Acc: 1.3333334%\n",
      "Epoch: 630\tTrain Loss: 3.6296678 \tVal Loss:3.5796188 \tTrain Acc: 1.833333% \tVal Acc: 2.1666668%\n",
      "Epoch: 631\tTrain Loss: 3.6293697 \tVal Loss:3.5796235 \tTrain Acc: 2.25% \tVal Acc: 1.3333334%\n",
      "Epoch: 632\tTrain Loss: 3.6299538 \tVal Loss:3.5796337 \tTrain Acc: 1.416667% \tVal Acc: 1.3333334%\n",
      "Epoch: 633\tTrain Loss: 3.6294537 \tVal Loss:3.5796605 \tTrain Acc: 1.666667% \tVal Acc: 1.3333334%\n",
      "Epoch: 634\tTrain Loss: 3.6291972 \tVal Loss:3.5797355 \tTrain Acc: 1.583333% \tVal Acc: 1.3333334%\n",
      "Epoch: 635\tTrain Loss: 3.6296246 \tVal Loss:3.5796880 \tTrain Acc: 1.458333% \tVal Acc: 1.3333334%\n",
      "Epoch: 636\tTrain Loss: 3.6296476 \tVal Loss:3.5797541 \tTrain Acc: 1.416667% \tVal Acc: 1.3333334%\n",
      "Epoch: 637\tTrain Loss: 3.6296183 \tVal Loss:3.5798214 \tTrain Acc: 2.166667% \tVal Acc: 1.3333334%\n",
      "Epoch: 638\tTrain Loss: 3.6298806 \tVal Loss:3.5798128 \tTrain Acc: 2.041667% \tVal Acc: 1.3333334%\n",
      "Epoch: 639\tTrain Loss: 3.6298725 \tVal Loss:3.5797879 \tTrain Acc: 1.291667% \tVal Acc: 1.3333334%\n",
      "Epoch: 640\tTrain Loss: 3.6296118 \tVal Loss:3.5797427 \tTrain Acc: 1.583333% \tVal Acc: 1.3333334%\n",
      "Epoch: 641\tTrain Loss: 3.6297173 \tVal Loss:3.5797143 \tTrain Acc: 1.75% \tVal Acc: 1.3333334%\n",
      "Epoch: 642\tTrain Loss: 3.6295124 \tVal Loss:3.5797799 \tTrain Acc: 1.666667% \tVal Acc: 1.3333334%\n",
      "Epoch: 643\tTrain Loss: 3.6295493 \tVal Loss:3.5797426 \tTrain Acc: 1.875% \tVal Acc: 1.3333334%\n",
      "Epoch: 644\tTrain Loss: 3.6298441 \tVal Loss:3.5797004 \tTrain Acc: 1.708333% \tVal Acc: 2.6666668%\n",
      "Epoch: 645\tTrain Loss: 3.6294397 \tVal Loss:3.5797152 \tTrain Acc: 1.333333% \tVal Acc: 1.3333334%\n",
      "Epoch: 646\tTrain Loss: 3.6292411 \tVal Loss:3.5797597 \tTrain Acc: 1.541667% \tVal Acc: 1.3333334%\n",
      "Epoch: 647\tTrain Loss: 3.6296661 \tVal Loss:3.5797698 \tTrain Acc: 1.666667% \tVal Acc: 1.3333334%\n",
      "Epoch: 648\tTrain Loss: 3.6296959 \tVal Loss:3.5797985 \tTrain Acc: 1.625% \tVal Acc: 1.3333334%\n",
      "Epoch: 649\tTrain Loss: 3.6296315 \tVal Loss:3.5797473 \tTrain Acc: 1.666667% \tVal Acc: 1.3333334%\n",
      "Epoch: 650\tTrain Loss: 3.6291673 \tVal Loss:3.5797659 \tTrain Acc: 1.833333% \tVal Acc: 1.3333334%\n",
      "Epoch: 651\tTrain Loss: 3.6297324 \tVal Loss:3.5797767 \tTrain Acc: 1.458333% \tVal Acc: 1.3333334%\n",
      "Epoch: 652\tTrain Loss: 3.6293314 \tVal Loss:3.5797744 \tTrain Acc: 2.0% \tVal Acc: 1.3333334%\n",
      "Epoch: 653\tTrain Loss: 3.6300164 \tVal Loss:3.5796786 \tTrain Acc: 2.208333% \tVal Acc: 1.3333334%\n",
      "Epoch: 654\tTrain Loss: 3.6298243 \tVal Loss:3.5795759 \tTrain Acc: 1.458333% \tVal Acc: 1.3333334%\n",
      "Epoch: 655\tTrain Loss: 3.6294649 \tVal Loss:3.5795722 \tTrain Acc: 1.416667% \tVal Acc: 1.3333334%\n",
      "Epoch: 656\tTrain Loss: 3.6294347 \tVal Loss:3.5795279 \tTrain Acc: 1.5% \tVal Acc: 1.3333334%\n",
      "Epoch: 657\tTrain Loss: 3.6296130 \tVal Loss:3.5795382 \tTrain Acc: 1.75% \tVal Acc: 1.3333334%\n",
      "Epoch: 658\tTrain Loss: 3.6295939 \tVal Loss:3.5795885 \tTrain Acc: 2.458333% \tVal Acc: 2.1666668%\n",
      "Epoch: 659\tTrain Loss: 3.6294715 \tVal Loss:3.5796149 \tTrain Acc: 1.666667% \tVal Acc: 2.1666668%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 660\tTrain Loss: 3.6297115 \tVal Loss:3.5795929 \tTrain Acc: 1.708333% \tVal Acc: 1.3333334%\n",
      "Epoch: 661\tTrain Loss: 3.6292682 \tVal Loss:3.5795510 \tTrain Acc: 2.041667% \tVal Acc: 1.3333334%\n",
      "Epoch: 662\tTrain Loss: 3.6292096 \tVal Loss:3.5795405 \tTrain Acc: 2.125% \tVal Acc: 1.3333334%\n",
      "Epoch: 663\tTrain Loss: 3.6295506 \tVal Loss:3.5795146 \tTrain Acc: 1.708333% \tVal Acc: 1.3333334%\n",
      "Epoch: 664\tTrain Loss: 3.6294630 \tVal Loss:3.5795213 \tTrain Acc: 1.916667% \tVal Acc: 1.3333334%\n",
      "Epoch: 665\tTrain Loss: 3.6295828 \tVal Loss:3.5795816 \tTrain Acc: 2.083333% \tVal Acc: 1.3333334%\n",
      "Epoch: 666\tTrain Loss: 3.6294885 \tVal Loss:3.5795257 \tTrain Acc: 1.791667% \tVal Acc: 1.3333334%\n",
      "Epoch: 667\tTrain Loss: 3.6292802 \tVal Loss:3.5795166 \tTrain Acc: 1.5% \tVal Acc: 1.3333334%\n",
      "Epoch: 668\tTrain Loss: 3.6296325 \tVal Loss:3.5794614 \tTrain Acc: 1.583333% \tVal Acc: 1.3333334%\n",
      "Epoch: 669\tTrain Loss: 3.6297004 \tVal Loss:3.5794934 \tTrain Acc: 2.458333% \tVal Acc: 1.3333334%\n",
      "Epoch: 670\tTrain Loss: 3.6296039 \tVal Loss:3.5795231 \tTrain Acc: 1.583333% \tVal Acc: 1.3333334%\n",
      "Epoch: 671\tTrain Loss: 3.6296216 \tVal Loss:3.5795583 \tTrain Acc: 1.625% \tVal Acc: 2.6666668%\n",
      "Epoch: 672\tTrain Loss: 3.6298902 \tVal Loss:3.5795892 \tTrain Acc: 1.666667% \tVal Acc: 2.6666668%\n",
      "Epoch: 673\tTrain Loss: 3.6296892 \tVal Loss:3.5796066 \tTrain Acc: 1.708333% \tVal Acc: 1.3333334%\n",
      "Epoch: 674\tTrain Loss: 3.6297096 \tVal Loss:3.5796194 \tTrain Acc: 1.916667% \tVal Acc: 1.3333334%\n",
      "Epoch: 675\tTrain Loss: 3.6291610 \tVal Loss:3.5796122 \tTrain Acc: 1.875% \tVal Acc: 1.3333334%\n",
      "Epoch: 676\tTrain Loss: 3.6294717 \tVal Loss:3.5795597 \tTrain Acc: 1.708333% \tVal Acc: 1.3333334%\n",
      "Epoch: 677\tTrain Loss: 3.6296350 \tVal Loss:3.5795794 \tTrain Acc: 1.541667% \tVal Acc: 2.1666668%\n",
      "Epoch: 678\tTrain Loss: 3.6294897 \tVal Loss:3.5795034 \tTrain Acc: 1.333333% \tVal Acc: 2.1666668%\n",
      "Epoch: 679\tTrain Loss: 3.6298265 \tVal Loss:3.5794900 \tTrain Acc: 1.375% \tVal Acc: 1.3333334%\n",
      "Epoch: 680\tTrain Loss: 3.6299738 \tVal Loss:3.5795176 \tTrain Acc: 1.583333% \tVal Acc: 1.3333334%\n",
      "Epoch: 681\tTrain Loss: 3.6297050 \tVal Loss:3.5795154 \tTrain Acc: 1.375% \tVal Acc: 1.3333334%\n",
      "Epoch: 682\tTrain Loss: 3.6293417 \tVal Loss:3.5794943 \tTrain Acc: 1.75% \tVal Acc: 1.3333334%\n",
      "Epoch: 683\tTrain Loss: 3.6298745 \tVal Loss:3.5794598 \tTrain Acc: 1.666667% \tVal Acc: 1.3333334%\n",
      "Epoch: 684\tTrain Loss: 3.6294569 \tVal Loss:3.5794751 \tTrain Acc: 2.083333% \tVal Acc: 4.5000002%\n",
      "Epoch: 685\tTrain Loss: 3.6293811 \tVal Loss:3.5795014 \tTrain Acc: 1.958333% \tVal Acc: 2.1666668%\n",
      "Epoch: 686\tTrain Loss: 3.6295611 \tVal Loss:3.5794729 \tTrain Acc: 1.625% \tVal Acc: 2.6666668%\n",
      "Epoch: 687\tTrain Loss: 3.6292637 \tVal Loss:3.5794909 \tTrain Acc: 1.75% \tVal Acc: 2.1666668%\n",
      "Epoch: 688\tTrain Loss: 3.6297809 \tVal Loss:3.5795282 \tTrain Acc: 1.791667% \tVal Acc: 2.1666668%\n",
      "Epoch: 689\tTrain Loss: 3.6296697 \tVal Loss:3.5795203 \tTrain Acc: 1.5% \tVal Acc: 1.3333334%\n",
      "Epoch: 690\tTrain Loss: 3.6290945 \tVal Loss:3.5795245 \tTrain Acc: 1.708333% \tVal Acc: 1.3333334%\n",
      "Epoch: 691\tTrain Loss: 3.6298456 \tVal Loss:3.5794900 \tTrain Acc: 1.666667% \tVal Acc: 1.3333334%\n",
      "Epoch: 692\tTrain Loss: 3.6296465 \tVal Loss:3.5794934 \tTrain Acc: 1.583333% \tVal Acc: 1.3333334%\n",
      "Epoch: 693\tTrain Loss: 3.6298074 \tVal Loss:3.5794396 \tTrain Acc: 2.0% \tVal Acc: 1.3333334%\n",
      "Epoch: 694\tTrain Loss: 3.6295760 \tVal Loss:3.5794857 \tTrain Acc: 2.041667% \tVal Acc: 1.3333334%\n",
      "Epoch: 695\tTrain Loss: 3.6294138 \tVal Loss:3.5795375 \tTrain Acc: 1.75% \tVal Acc: 1.3333334%\n",
      "Epoch: 696\tTrain Loss: 3.6291553 \tVal Loss:3.5794801 \tTrain Acc: 1.666667% \tVal Acc: 2.3333335%\n",
      "Epoch: 697\tTrain Loss: 3.6297841 \tVal Loss:3.5794710 \tTrain Acc: 1.458333% \tVal Acc: 2.1666668%\n",
      "Epoch: 698\tTrain Loss: 3.6295042 \tVal Loss:3.5794706 \tTrain Acc: 2.291667% \tVal Acc: 2.1666668%\n",
      "Epoch: 699\tTrain Loss: 3.6293540 \tVal Loss:3.5794815 \tTrain Acc: 1.583333% \tVal Acc: 1.3333334%\n"
     ]
    }
   ],
   "source": [
    "epochs = 700\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        '''\n",
    "        Creating new variables for the hidden state, otherwise\n",
    "        we'd backprop through the entire training history\n",
    "        '''\n",
    "        \n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "       \n",
    "        # get the output from the model\n",
    "        output = model.forward(inputs, train_batch_size)\n",
    "        #print('OUTPUT', output)\n",
    "        \n",
    "        \n",
    "        #print('Labels Shape :-', (torch.max(labels, 1)[1]).shape)\n",
    "    \n",
    "        # calculate the loss and perform backprop\n",
    "        #print('Labels Long :-', labels.long())\n",
    "        loss = criterion(output,labels.long())\n",
    "        #print('LOSS IS :-', loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #calculate training accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        logging.debug(' top probab {} top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        #print(train_loss)\n",
    "              \n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "                \n",
    "        output = model.forward(inputs, val_batch_size)\n",
    "       \n",
    "        loss = criterion(output,labels.long())\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        #calculate validation accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        \n",
    "        #logging.debug(output)\n",
    "        #logging.debug('VALIDATION top probab {} VALIDATION top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        #print('Top Class:- ',top_class)\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        #print('Equals:- ', equals)\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    #Averaging losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    val_accuracy = val_accuracy/len(val_loader)\n",
    "    train_accuracy = train_accuracy/len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}\\tTrain Loss: {:.7f} \\tVal Loss:{:.7f} \\tTrain Acc: {:.7}% \\tVal Acc: {:.7f}%'.format(e, train_loss, val_loss, train_accuracy*100,val_accuracy*100))\n",
    "    \n",
    "    #saving the model if validation loss is decreased\n",
    "    if val_loss <= min_val_loss:\n",
    "        print('Validation Loss decreased from {:6f} to {:6f}, saving the model weights'.format(min_val_loss, val_loss))\n",
    "        torch.save(model.state_dict(), 'lstm_state_256-38-our_normalization_bakchodi_text_files.pt')\n",
    "        min_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUSIC GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm1): LSTM(1, 256, batch_first=True)\n",
       "  (lstm2): LSTM(256, 38, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=38, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights\n",
    "test_model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "test_model.load_state_dict(torch.load('lstm_state_256-38-our_normalization_bakchodi_text_files.pt'))\n",
    "test_model.eval()\n",
    "test_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population database\n",
    "#testing_data = np.ones(200)*1\n",
    "# testing_data = list(range(50,90))\n",
    "# testing_data.extend(testing_data[::-1])\n",
    "# testing_data_rev = testing_data[::-1]\n",
    "# testing_data_rev.extend(testing_data)\n",
    "# testing_data_rev.extend(testing_data_rev)\n",
    "# testing_data = testing_data_rev\n",
    "\n",
    "\n",
    "# testing_data = np.asarray(testing_data)\n",
    "# testing_data = testing_data.reshape(testing_data.shape[0],1)\n",
    "\n",
    "initial_seq = [network_input[0][1:].cpu().numpy().tolist()]\n",
    "\n",
    "testing_data_unnorm = testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "testing_data=testing_data.tolist()\n",
    "for i in range(len(testing_data)):\n",
    "    list1.extend(testing_data[i])\n",
    "\n",
    "#list1\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    list1[i]=(list1[i]-50)/(89-50)\n",
    "#     list1[i]=(list1[i])/(89)\n",
    "\n",
    "list1 = np.asarray(list1)\n",
    "list1 = list1.reshape(list1.shape[0],1)\n",
    "testing_data = list1\n",
    "#list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting\n",
    "def prediction_with_influence(influence,int2note,initial_seq, max_note, min_note,test_batch_size = 1):\n",
    "\n",
    "    predicted_notes = []\n",
    "    initial_seq[0].extend([[0]]*len(testing_data))\n",
    "    test_seq = torch.Tensor(initial_seq).cuda()\n",
    "    \n",
    "    for i in range(len(influence)):\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = float(influence[i])\n",
    "        \n",
    "        test_slice = test_seq[0][i : i + sequence_length]        \n",
    "        test_slice = test_slice.view(1, test_slice.shape[0], test_slice.shape[1])\n",
    "        \n",
    "        test_output = test_model.forward(test_slice, test_batch_size)\n",
    "    \n",
    "        test_output = F.softmax(test_output, dim = 1)\n",
    "        top_p, top_class = test_output.topk(1,dim =1)\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = (int2note[top_class.item()] - min_note)/(max_note - min_note)\n",
    "#         test_seq[0][sequence_length - 1 + i][0] = int2note[top_class.item()]/max_note\n",
    "        \n",
    "        predicted_notes.append(int2note[top_class.item()])\n",
    "        \n",
    "    return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_notes_lst = prediction_with_influence(testing_data,int_to_note,initial_seq, max_midi_number, min_midi_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_notes_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2656d369bc8>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(list(np.ones(200)*89))\n",
    "\n",
    "#plt.plot(list(np.ones(200)*50))\n",
    "#plt.plot(list(np.ones(20)*50))\n",
    "plt.plot(testing_data_unnorm*89)\n",
    "plt.plot(predicted_notes_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({72: 15,\n",
       "         74: 22,\n",
       "         76: 17,\n",
       "         78: 10,\n",
       "         79: 10,\n",
       "         81: 19,\n",
       "         83: 9,\n",
       "         67: 7,\n",
       "         77: 82,\n",
       "         88: 10,\n",
       "         86: 19,\n",
       "         82: 22,\n",
       "         89: 5,\n",
       "         84: 45,\n",
       "         71: 14,\n",
       "         73: 7,\n",
       "         69: 5,\n",
       "         75: 1,\n",
       "         80: 1})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(predicted_notes_lst)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Postprocessing.postprocessing import PostProcessing\n",
    "PostProcessing().generate_midi_file('hello.midi', predicted_notes_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#int_to_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
