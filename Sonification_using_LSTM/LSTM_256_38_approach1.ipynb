{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.tensorboard as tb\n",
    "from Preprocessing.preprocessing import PreprocessingTrainingData\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as  plt\n",
    "import os\n",
    "import logging\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static parameters\n",
    "train_batch_size = 170\n",
    "val_batch_size = 170\n",
    "sequence_length=50\n",
    "test_batch_size = 1\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layer = 2\n",
    "output_size = 38\n",
    "clip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from preprocessing.py\n",
    "dataset_path = os.path.join(os.path.abspath('..'),'Dataset\\\\Clementi dataset\\\\Clementi dataset' )\n",
    "network_input,network_output,max_midi_number,min_midi_number,int_to_note = PreprocessingTrainingData().preprocess_notes(dataset_path)\n",
    "network_input, network_output = network_input.cuda(), network_output.cuda()\n",
    "\n",
    "# print(network_input)\n",
    "#print(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(network_output.max())\n",
    "print(network_output.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "89\n",
      "50\n",
      "{0: 50, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61, 11: 62, 12: 63, 13: 64, 14: 65, 15: 66, 16: 67, 17: 68, 18: 69, 19: 70, 20: 71, 21: 72, 22: 73, 23: 74, 24: 75, 25: 76, 26: 77, 27: 78, 28: 79, 29: 80, 30: 81, 31: 82, 32: 83, 33: 84, 34: 85, 35: 86, 36: 88, 37: 89}\n"
     ]
    }
   ],
   "source": [
    "print(network_input.max())\n",
    "print(network_input.min())\n",
    "print(max_midi_number)\n",
    "print(min_midi_number)\n",
    "print(int_to_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "# data is highly unbalanced\n",
    "# # '''\n",
    "# sns.distplot(torch.tensor(network_output).cpu())\n",
    "# xx = pd.DataFrame(torch.tensor(network_output).cpu())\n",
    "# xx.groupby(0).size().to_frame(name='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8500, 50, 1])\n",
      "torch.Size([8500])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "to make batch of equal sizes\n",
    "Quick Fix\n",
    "'''\n",
    "network_input = network_input[: -117]\n",
    "network_output = network_output[: -117]\n",
    "\n",
    "print(network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create Stacked LSTM model\n",
    "'''\n",
    "class Stacked_LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm1 = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = 1, batch_first = True)\n",
    "        self.lstm2 = nn.LSTM(input_size = hidden_size, hidden_size = output_size, num_layers = 1, batch_first = True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.linear = nn.Linear(output_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden1, hidden2,batch_size):\n",
    "        \n",
    "        output, hidden1 = self.lstm1(x, hidden1)        \n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        output, hidden2 = self.lstm2(output, hidden2)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        output = output.contiguous().view(-1, 38)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        #print('Linear Output :-',output.shape)\n",
    "        \n",
    "        #output = F.softmax(output, dim = 1)\n",
    "        #print('SOFTMAX OUTPUT :--', output)\n",
    "        \n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1)\n",
    "        #print('Reshape to batch size first :-',output.shape)\n",
    "        \n",
    "        output = output[:, -self.output_size:] # get last batch of labels\n",
    "        #print('Final Output :-',output)\n",
    "        #print('RESHAPE SIZE :-', output.shape)\n",
    "        \n",
    "        return output, hidden2\n",
    "    \n",
    "    def hidden_init(self,batch_size):\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden1 = (weight.new(1, batch_size, self.hidden_size).zero_().cuda(),\n",
    "          weight.new(1, batch_size, self.hidden_size).zero_().cuda())\n",
    "        \n",
    "        hidden2 = (weight.new(1, batch_size, 38).zero_().cuda(),\n",
    "          weight.new(1, batch_size, 38).zero_().cuda())\n",
    "        return hidden1,hidden2\n",
    "\n",
    "#initialize the weights of LSTM using Xavier initialization    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Divide the dataset into train/val \n",
    "'''\n",
    "train_size = 0.8\n",
    "indices = list(range(len(network_input)))\n",
    "split = int(np.floor(train_size*len(network_input)))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SequentialSampler(train_idx)\n",
    "val_sampler = SequentialSampler(val_idx)\n",
    "\n",
    "dataset = TensorDataset(network_input,network_output)\n",
    "train_loader = DataLoader(dataset, batch_size= train_batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size= val_batch_size,sampler= val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm1): LSTM(1, 256, batch_first=True)\n",
       "  (lstm2): LSTM(256, 38, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=38, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optimizer\n",
    "\n",
    "model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer.AdamW(model.parameters())\n",
    "#optimizer = optimizer.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "#make sure to transfer model to GPU after initializing optimizer\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hidden = model.hidden_init(train_batch_size) \n",
    "#hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 3.4000490 \tVal Loss:3.0559365 \tTrain Acc: 7.970588% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from    inf to 3.055936, saving the model weights\n",
      "Epoch: 1\tTrain Loss: 3.1698612 \tVal Loss:2.9762021 \tTrain Acc: 8.529412% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 3.055936 to 2.976202, saving the model weights\n",
      "Epoch: 2\tTrain Loss: 3.1311081 \tVal Loss:2.9635554 \tTrain Acc: 8.808824% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.976202 to 2.963555, saving the model weights\n",
      "Epoch: 3\tTrain Loss: 3.1086993 \tVal Loss:2.9558083 \tTrain Acc: 9.382353% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.963555 to 2.955808, saving the model weights\n",
      "Epoch: 4\tTrain Loss: 3.1001165 \tVal Loss:2.9525560 \tTrain Acc: 8.985294% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.955808 to 2.952556, saving the model weights\n",
      "Epoch: 5\tTrain Loss: 3.0925640 \tVal Loss:2.9494552 \tTrain Acc: 9.455883% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.952556 to 2.949455, saving the model weights\n",
      "Epoch: 6\tTrain Loss: 3.0865575 \tVal Loss:2.9470735 \tTrain Acc: 8.867647% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.949455 to 2.947073, saving the model weights\n",
      "Epoch: 7\tTrain Loss: 3.0783701 \tVal Loss:2.9450082 \tTrain Acc: 9.220588% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.947073 to 2.945008, saving the model weights\n",
      "Epoch: 8\tTrain Loss: 3.0668796 \tVal Loss:2.9402927 \tTrain Acc: 9.352941% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.945008 to 2.940293, saving the model weights\n",
      "Epoch: 9\tTrain Loss: 3.0621148 \tVal Loss:2.9371425 \tTrain Acc: 9.485294% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.940293 to 2.937142, saving the model weights\n",
      "Epoch: 10\tTrain Loss: 3.0561255 \tVal Loss:2.9365551 \tTrain Acc: 9.82353% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.937142 to 2.936555, saving the model weights\n",
      "Epoch: 11\tTrain Loss: 3.0586526 \tVal Loss:2.9375003 \tTrain Acc: 10.04412% \tVal Acc: 11.4117651%\n",
      "Epoch: 12\tTrain Loss: 3.0531169 \tVal Loss:2.9376729 \tTrain Acc: 9.985294% \tVal Acc: 11.4117651%\n",
      "Epoch: 13\tTrain Loss: 3.0511815 \tVal Loss:2.9359556 \tTrain Acc: 9.632353% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.936555 to 2.935956, saving the model weights\n",
      "Epoch: 14\tTrain Loss: 3.0461962 \tVal Loss:2.9404297 \tTrain Acc: 9.720589% \tVal Acc: 11.4117651%\n",
      "Epoch: 15\tTrain Loss: 3.0490633 \tVal Loss:2.9378211 \tTrain Acc: 10.07353% \tVal Acc: 11.4117651%\n",
      "Epoch: 16\tTrain Loss: 3.0528532 \tVal Loss:2.9294315 \tTrain Acc: 9.602941% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.935956 to 2.929431, saving the model weights\n",
      "Epoch: 17\tTrain Loss: 3.0418246 \tVal Loss:2.9298113 \tTrain Acc: 10.01471% \tVal Acc: 11.4117651%\n",
      "Epoch: 18\tTrain Loss: 3.0328173 \tVal Loss:2.9277901 \tTrain Acc: 9.911765% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.929431 to 2.927790, saving the model weights\n",
      "Epoch: 19\tTrain Loss: 3.0275033 \tVal Loss:2.9178408 \tTrain Acc: 10.33824% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.927790 to 2.917841, saving the model weights\n",
      "Epoch: 20\tTrain Loss: 3.0209341 \tVal Loss:2.9483740 \tTrain Acc: 10.02941% \tVal Acc: 11.4117651%\n",
      "Epoch: 21\tTrain Loss: 2.9995820 \tVal Loss:2.8517827 \tTrain Acc: 10.47059% \tVal Acc: 11.7058828%\n",
      "Validation Loss decreased from 2.917841 to 2.851783, saving the model weights\n",
      "Epoch: 22\tTrain Loss: 2.9293497 \tVal Loss:2.8315397 \tTrain Acc: 11.47059% \tVal Acc: 12.1764709%\n",
      "Validation Loss decreased from 2.851783 to 2.831540, saving the model weights\n",
      "Epoch: 23\tTrain Loss: 2.8889742 \tVal Loss:2.7424568 \tTrain Acc: 12.23529% \tVal Acc: 14.5294123%\n",
      "Validation Loss decreased from 2.831540 to 2.742457, saving the model weights\n",
      "Epoch: 24\tTrain Loss: 2.8193777 \tVal Loss:2.6965453 \tTrain Acc: 13.80882% \tVal Acc: 14.9411769%\n",
      "Validation Loss decreased from 2.742457 to 2.696545, saving the model weights\n",
      "Epoch: 25\tTrain Loss: 2.7691626 \tVal Loss:2.6508179 \tTrain Acc: 14.14706% \tVal Acc: 13.9411770%\n",
      "Validation Loss decreased from 2.696545 to 2.650818, saving the model weights\n",
      "Epoch: 26\tTrain Loss: 2.7077629 \tVal Loss:2.6176572 \tTrain Acc: 14.85294% \tVal Acc: 12.9411768%\n",
      "Validation Loss decreased from 2.650818 to 2.617657, saving the model weights\n",
      "Epoch: 27\tTrain Loss: 2.6866432 \tVal Loss:2.5726052 \tTrain Acc: 15.26471% \tVal Acc: 13.2352945%\n",
      "Validation Loss decreased from 2.617657 to 2.572605, saving the model weights\n",
      "Epoch: 28\tTrain Loss: 2.6360052 \tVal Loss:2.5441994 \tTrain Acc: 15.05882% \tVal Acc: 15.0588240%\n",
      "Validation Loss decreased from 2.572605 to 2.544199, saving the model weights\n",
      "Epoch: 29\tTrain Loss: 2.6157508 \tVal Loss:2.5350028 \tTrain Acc: 16.20588% \tVal Acc: 15.2352946%\n",
      "Validation Loss decreased from 2.544199 to 2.535003, saving the model weights\n",
      "Epoch: 30\tTrain Loss: 2.6013065 \tVal Loss:2.5199144 \tTrain Acc: 15.80882% \tVal Acc: 16.5294123%\n",
      "Validation Loss decreased from 2.535003 to 2.519914, saving the model weights\n",
      "Epoch: 31\tTrain Loss: 2.5757436 \tVal Loss:2.4862492 \tTrain Acc: 16.33824% \tVal Acc: 15.0000003%\n",
      "Validation Loss decreased from 2.519914 to 2.486249, saving the model weights\n",
      "Epoch: 32\tTrain Loss: 2.5676257 \tVal Loss:2.4680806 \tTrain Acc: 16.14706% \tVal Acc: 15.0000003%\n",
      "Validation Loss decreased from 2.486249 to 2.468081, saving the model weights\n",
      "Epoch: 33\tTrain Loss: 2.5472153 \tVal Loss:2.4569856 \tTrain Acc: 16.95588% \tVal Acc: 14.8235299%\n",
      "Validation Loss decreased from 2.468081 to 2.456986, saving the model weights\n",
      "Epoch: 34\tTrain Loss: 2.5410949 \tVal Loss:2.4505492 \tTrain Acc: 16.58824% \tVal Acc: 14.1176473%\n",
      "Validation Loss decreased from 2.456986 to 2.450549, saving the model weights\n",
      "Epoch: 35\tTrain Loss: 2.5266567 \tVal Loss:2.4412637 \tTrain Acc: 16.69118% \tVal Acc: 14.9411768%\n",
      "Validation Loss decreased from 2.450549 to 2.441264, saving the model weights\n",
      "Epoch: 36\tTrain Loss: 2.5128042 \tVal Loss:2.4360160 \tTrain Acc: 17.75% \tVal Acc: 14.7058827%\n",
      "Validation Loss decreased from 2.441264 to 2.436016, saving the model weights\n",
      "Epoch: 37\tTrain Loss: 2.5101408 \tVal Loss:2.4271366 \tTrain Acc: 16.83824% \tVal Acc: 13.5294120%\n",
      "Validation Loss decreased from 2.436016 to 2.427137, saving the model weights\n",
      "Epoch: 38\tTrain Loss: 2.5003647 \tVal Loss:2.4191110 \tTrain Acc: 17.35294% \tVal Acc: 14.1764709%\n",
      "Validation Loss decreased from 2.427137 to 2.419111, saving the model weights\n",
      "Epoch: 39\tTrain Loss: 2.4901150 \tVal Loss:2.4214607 \tTrain Acc: 17.10294% \tVal Acc: 14.0588239%\n",
      "Epoch: 40\tTrain Loss: 2.4864919 \tVal Loss:2.4128719 \tTrain Acc: 17.66177% \tVal Acc: 15.8235298%\n",
      "Validation Loss decreased from 2.419111 to 2.412872, saving the model weights\n",
      "Epoch: 41\tTrain Loss: 2.4757473 \tVal Loss:2.4048652 \tTrain Acc: 17.19118% \tVal Acc: 14.7647063%\n",
      "Validation Loss decreased from 2.412872 to 2.404865, saving the model weights\n",
      "Epoch: 42\tTrain Loss: 2.4712908 \tVal Loss:2.3996579 \tTrain Acc: 17.66177% \tVal Acc: 14.8823533%\n",
      "Validation Loss decreased from 2.404865 to 2.399658, saving the model weights\n",
      "Epoch: 43\tTrain Loss: 2.4740583 \tVal Loss:2.4008450 \tTrain Acc: 17.02941% \tVal Acc: 15.2352946%\n",
      "Epoch: 44\tTrain Loss: 2.4613449 \tVal Loss:2.4020454 \tTrain Acc: 17.35294% \tVal Acc: 14.5294121%\n",
      "Epoch: 45\tTrain Loss: 2.4634253 \tVal Loss:2.4006568 \tTrain Acc: 17.88235% \tVal Acc: 15.7058828%\n",
      "Epoch: 46\tTrain Loss: 2.4656388 \tVal Loss:2.3910958 \tTrain Acc: 17.25% \tVal Acc: 15.4117651%\n",
      "Validation Loss decreased from 2.399658 to 2.391096, saving the model weights\n",
      "Epoch: 47\tTrain Loss: 2.4504654 \tVal Loss:2.4015258 \tTrain Acc: 17.58824% \tVal Acc: 16.2941182%\n",
      "Epoch: 48\tTrain Loss: 2.4568496 \tVal Loss:2.3881546 \tTrain Acc: 18.35294% \tVal Acc: 15.6470592%\n",
      "Validation Loss decreased from 2.391096 to 2.388155, saving the model weights\n",
      "Epoch: 49\tTrain Loss: 2.4487168 \tVal Loss:2.3906898 \tTrain Acc: 18.30882% \tVal Acc: 16.7647063%\n",
      "Epoch: 50\tTrain Loss: 2.4404995 \tVal Loss:2.3843772 \tTrain Acc: 18.01471% \tVal Acc: 15.7058828%\n",
      "Validation Loss decreased from 2.388155 to 2.384377, saving the model weights\n",
      "Epoch: 51\tTrain Loss: 2.4413603 \tVal Loss:2.3839520 \tTrain Acc: 18.0% \tVal Acc: 16.4705887%\n",
      "Validation Loss decreased from 2.384377 to 2.383952, saving the model weights\n",
      "Epoch: 52\tTrain Loss: 2.4324569 \tVal Loss:2.3887985 \tTrain Acc: 18.19118% \tVal Acc: 15.8823533%\n",
      "Epoch: 53\tTrain Loss: 2.4344292 \tVal Loss:2.3788047 \tTrain Acc: 18.52941% \tVal Acc: 16.6470592%\n",
      "Validation Loss decreased from 2.383952 to 2.378805, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 54\tTrain Loss: 2.4304934 \tVal Loss:2.3940889 \tTrain Acc: 18.33824% \tVal Acc: 15.2941181%\n",
      "Epoch: 55\tTrain Loss: 2.4345036 \tVal Loss:2.3882740 \tTrain Acc: 18.63235% \tVal Acc: 15.7058828%\n",
      "Epoch: 56\tTrain Loss: 2.4228529 \tVal Loss:2.4019753 \tTrain Acc: 19.07353% \tVal Acc: 15.1176474%\n",
      "Epoch: 57\tTrain Loss: 2.4188706 \tVal Loss:2.3827357 \tTrain Acc: 19.17647% \tVal Acc: 15.4117651%\n",
      "Epoch: 58\tTrain Loss: 2.4248259 \tVal Loss:2.3931638 \tTrain Acc: 18.98529% \tVal Acc: 15.4705887%\n",
      "Epoch: 59\tTrain Loss: 2.4326602 \tVal Loss:2.3939703 \tTrain Acc: 18.11765% \tVal Acc: 15.8235298%\n",
      "Epoch: 60\tTrain Loss: 2.4114447 \tVal Loss:2.3693580 \tTrain Acc: 19.26471% \tVal Acc: 16.6470592%\n",
      "Validation Loss decreased from 2.378805 to 2.369358, saving the model weights\n",
      "Epoch: 61\tTrain Loss: 2.4107700 \tVal Loss:2.3849926 \tTrain Acc: 19.64706% \tVal Acc: 16.5882356%\n",
      "Epoch: 62\tTrain Loss: 2.4065645 \tVal Loss:2.3802968 \tTrain Acc: 19.66177% \tVal Acc: 17.1176475%\n",
      "Epoch: 63\tTrain Loss: 2.4063226 \tVal Loss:2.3931825 \tTrain Acc: 19.88235% \tVal Acc: 16.4117651%\n",
      "Epoch: 64\tTrain Loss: 2.3979608 \tVal Loss:2.3788883 \tTrain Acc: 20.20588% \tVal Acc: 17.2352946%\n",
      "Epoch: 65\tTrain Loss: 2.3959034 \tVal Loss:2.3650347 \tTrain Acc: 20.26471% \tVal Acc: 17.8235298%\n",
      "Validation Loss decreased from 2.369358 to 2.365035, saving the model weights\n",
      "Epoch: 66\tTrain Loss: 2.3968941 \tVal Loss:2.3653973 \tTrain Acc: 19.77941% \tVal Acc: 17.3529416%\n",
      "Epoch: 67\tTrain Loss: 2.3924924 \tVal Loss:2.3737446 \tTrain Acc: 20.42647% \tVal Acc: 16.5882358%\n",
      "Epoch: 68\tTrain Loss: 2.3911961 \tVal Loss:2.3625205 \tTrain Acc: 20.52941% \tVal Acc: 16.9411769%\n",
      "Validation Loss decreased from 2.365035 to 2.362521, saving the model weights\n",
      "Epoch: 69\tTrain Loss: 2.3835421 \tVal Loss:2.3760733 \tTrain Acc: 19.51471% \tVal Acc: 16.5882358%\n",
      "Epoch: 70\tTrain Loss: 2.3831864 \tVal Loss:2.3493274 \tTrain Acc: 20.14706% \tVal Acc: 17.0000005%\n",
      "Validation Loss decreased from 2.362521 to 2.349327, saving the model weights\n",
      "Epoch: 71\tTrain Loss: 2.3779750 \tVal Loss:2.3509780 \tTrain Acc: 20.39706% \tVal Acc: 17.3529416%\n",
      "Epoch: 72\tTrain Loss: 2.3782549 \tVal Loss:2.3499120 \tTrain Acc: 20.20588% \tVal Acc: 16.5294123%\n",
      "Epoch: 73\tTrain Loss: 2.3658976 \tVal Loss:2.3548447 \tTrain Acc: 20.83824% \tVal Acc: 16.8823534%\n",
      "Epoch: 74\tTrain Loss: 2.3660426 \tVal Loss:2.3356426 \tTrain Acc: 20.58824% \tVal Acc: 19.4117650%\n",
      "Validation Loss decreased from 2.349327 to 2.335643, saving the model weights\n",
      "Epoch: 75\tTrain Loss: 2.3533156 \tVal Loss:2.3224066 \tTrain Acc: 20.98529% \tVal Acc: 18.9411768%\n",
      "Validation Loss decreased from 2.335643 to 2.322407, saving the model weights\n",
      "Epoch: 76\tTrain Loss: 2.3554008 \tVal Loss:2.3447186 \tTrain Acc: 21.27941% \tVal Acc: 17.0588240%\n",
      "Epoch: 77\tTrain Loss: 2.3556190 \tVal Loss:2.3315262 \tTrain Acc: 20.92647% \tVal Acc: 18.0588239%\n",
      "Epoch: 78\tTrain Loss: 2.3626325 \tVal Loss:2.3521366 \tTrain Acc: 21.25% \tVal Acc: 17.2941181%\n",
      "Epoch: 79\tTrain Loss: 2.3475935 \tVal Loss:2.3209114 \tTrain Acc: 21.48529% \tVal Acc: 19.2352945%\n",
      "Validation Loss decreased from 2.322407 to 2.320911, saving the model weights\n",
      "Epoch: 80\tTrain Loss: 2.3411568 \tVal Loss:2.3288598 \tTrain Acc: 21.41177% \tVal Acc: 17.8235298%\n",
      "Epoch: 81\tTrain Loss: 2.3381163 \tVal Loss:2.3241611 \tTrain Acc: 22.54412% \tVal Acc: 17.8823534%\n",
      "Epoch: 82\tTrain Loss: 2.3295369 \tVal Loss:2.3078115 \tTrain Acc: 22.13235% \tVal Acc: 19.4705886%\n",
      "Validation Loss decreased from 2.320911 to 2.307811, saving the model weights\n",
      "Epoch: 83\tTrain Loss: 2.3242224 \tVal Loss:2.3104218 \tTrain Acc: 21.73529% \tVal Acc: 18.6470592%\n",
      "Epoch: 84\tTrain Loss: 2.3330146 \tVal Loss:2.3053160 \tTrain Acc: 22.0% \tVal Acc: 18.7058827%\n",
      "Validation Loss decreased from 2.307811 to 2.305316, saving the model weights\n",
      "Epoch: 85\tTrain Loss: 2.3333456 \tVal Loss:2.3265970 \tTrain Acc: 22.05882% \tVal Acc: 16.7647064%\n",
      "Epoch: 86\tTrain Loss: 2.3192852 \tVal Loss:2.3043190 \tTrain Acc: 22.42647% \tVal Acc: 18.0000004%\n",
      "Validation Loss decreased from 2.305316 to 2.304319, saving the model weights\n",
      "Epoch: 87\tTrain Loss: 2.3129085 \tVal Loss:2.2966167 \tTrain Acc: 21.52941% \tVal Acc: 19.1764711%\n",
      "Validation Loss decreased from 2.304319 to 2.296617, saving the model weights\n",
      "Epoch: 88\tTrain Loss: 2.3051324 \tVal Loss:2.2957194 \tTrain Acc: 22.63235% \tVal Acc: 19.1176474%\n",
      "Validation Loss decreased from 2.296617 to 2.295719, saving the model weights\n",
      "Epoch: 89\tTrain Loss: 2.3097989 \tVal Loss:2.2861346 \tTrain Acc: 22.51471% \tVal Acc: 19.5294122%\n",
      "Validation Loss decreased from 2.295719 to 2.286135, saving the model weights\n",
      "Epoch: 90\tTrain Loss: 2.2919762 \tVal Loss:2.2830895 \tTrain Acc: 23.05882% \tVal Acc: 19.4117650%\n",
      "Validation Loss decreased from 2.286135 to 2.283089, saving the model weights\n",
      "Epoch: 91\tTrain Loss: 2.3031397 \tVal Loss:2.2580882 \tTrain Acc: 23.14706% \tVal Acc: 20.7058828%\n",
      "Validation Loss decreased from 2.283089 to 2.258088, saving the model weights\n",
      "Epoch: 92\tTrain Loss: 2.3195217 \tVal Loss:2.2523452 \tTrain Acc: 22.57353% \tVal Acc: 21.5294123%\n",
      "Validation Loss decreased from 2.258088 to 2.252345, saving the model weights\n",
      "Epoch: 93\tTrain Loss: 2.2928753 \tVal Loss:2.2423787 \tTrain Acc: 22.97059% \tVal Acc: 22.2941181%\n",
      "Validation Loss decreased from 2.252345 to 2.242379, saving the model weights\n",
      "Epoch: 94\tTrain Loss: 2.2849557 \tVal Loss:2.2357767 \tTrain Acc: 23.07353% \tVal Acc: 22.1764711%\n",
      "Validation Loss decreased from 2.242379 to 2.235777, saving the model weights\n",
      "Epoch: 95\tTrain Loss: 2.2747870 \tVal Loss:2.2320686 \tTrain Acc: 24.07353% \tVal Acc: 23.1176476%\n",
      "Validation Loss decreased from 2.235777 to 2.232069, saving the model weights\n",
      "Epoch: 96\tTrain Loss: 2.2797117 \tVal Loss:2.2264542 \tTrain Acc: 23.79412% \tVal Acc: 22.8823535%\n",
      "Validation Loss decreased from 2.232069 to 2.226454, saving the model weights\n",
      "Epoch: 97\tTrain Loss: 2.2818551 \tVal Loss:2.2181376 \tTrain Acc: 23.05882% \tVal Acc: 24.0588242%\n",
      "Validation Loss decreased from 2.226454 to 2.218138, saving the model weights\n",
      "Epoch: 98\tTrain Loss: 2.2757721 \tVal Loss:2.2445940 \tTrain Acc: 23.77941% \tVal Acc: 22.1176475%\n",
      "Epoch: 99\tTrain Loss: 2.2750647 \tVal Loss:2.2294107 \tTrain Acc: 22.82353% \tVal Acc: 23.6470595%\n",
      "Epoch: 100\tTrain Loss: 2.2659051 \tVal Loss:2.2173874 \tTrain Acc: 23.82353% \tVal Acc: 23.7647064%\n",
      "Validation Loss decreased from 2.218138 to 2.217387, saving the model weights\n",
      "Epoch: 101\tTrain Loss: 2.2517790 \tVal Loss:2.2062005 \tTrain Acc: 24.72059% \tVal Acc: 24.2941183%\n",
      "Validation Loss decreased from 2.217387 to 2.206201, saving the model weights\n",
      "Epoch: 102\tTrain Loss: 2.2433567 \tVal Loss:2.2000736 \tTrain Acc: 24.98529% \tVal Acc: 24.5882359%\n",
      "Validation Loss decreased from 2.206201 to 2.200074, saving the model weights\n",
      "Epoch: 103\tTrain Loss: 2.2529619 \tVal Loss:2.2272256 \tTrain Acc: 24.16177% \tVal Acc: 23.0588238%\n",
      "Epoch: 104\tTrain Loss: 2.2492407 \tVal Loss:2.1908981 \tTrain Acc: 24.76471% \tVal Acc: 25.3529420%\n",
      "Validation Loss decreased from 2.200074 to 2.190898, saving the model weights\n",
      "Epoch: 105\tTrain Loss: 2.2274480 \tVal Loss:2.1859276 \tTrain Acc: 25.36765% \tVal Acc: 25.4705890%\n",
      "Validation Loss decreased from 2.190898 to 2.185928, saving the model weights\n",
      "Epoch: 106\tTrain Loss: 2.2186309 \tVal Loss:2.1905375 \tTrain Acc: 25.26471% \tVal Acc: 24.4117652%\n",
      "Epoch: 107\tTrain Loss: 2.2168733 \tVal Loss:2.1925386 \tTrain Acc: 25.39706% \tVal Acc: 26.0000010%\n",
      "Epoch: 108\tTrain Loss: 2.2285961 \tVal Loss:2.1836645 \tTrain Acc: 25.60294% \tVal Acc: 27.2941184%\n",
      "Validation Loss decreased from 2.185928 to 2.183664, saving the model weights\n",
      "Epoch: 109\tTrain Loss: 2.2282279 \tVal Loss:2.1966918 \tTrain Acc: 25.89706% \tVal Acc: 25.7647067%\n",
      "Epoch: 110\tTrain Loss: 2.2503428 \tVal Loss:2.2078506 \tTrain Acc: 24.95588% \tVal Acc: 26.8823540%\n",
      "Epoch: 111\tTrain Loss: 2.2443826 \tVal Loss:2.1781347 \tTrain Acc: 25.73529% \tVal Acc: 27.8235304%\n",
      "Validation Loss decreased from 2.183664 to 2.178135, saving the model weights\n",
      "Epoch: 112\tTrain Loss: 2.2059259 \tVal Loss:2.1566683 \tTrain Acc: 26.77941% \tVal Acc: 28.9411776%\n",
      "Validation Loss decreased from 2.178135 to 2.156668, saving the model weights\n",
      "Epoch: 113\tTrain Loss: 2.1987627 \tVal Loss:2.1401642 \tTrain Acc: 26.75% \tVal Acc: 30.1176482%\n",
      "Validation Loss decreased from 2.156668 to 2.140164, saving the model weights\n",
      "Epoch: 114\tTrain Loss: 2.1921415 \tVal Loss:2.1339457 \tTrain Acc: 27.27941% \tVal Acc: 30.3529423%\n",
      "Validation Loss decreased from 2.140164 to 2.133946, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 115\tTrain Loss: 2.1974439 \tVal Loss:2.1343771 \tTrain Acc: 26.45588% \tVal Acc: 30.1176481%\n",
      "Epoch: 116\tTrain Loss: 2.1713290 \tVal Loss:2.1286201 \tTrain Acc: 27.75% \tVal Acc: 29.8823540%\n",
      "Validation Loss decreased from 2.133946 to 2.128620, saving the model weights\n",
      "Epoch: 117\tTrain Loss: 2.1668577 \tVal Loss:2.1661314 \tTrain Acc: 27.80882% \tVal Acc: 25.7647066%\n",
      "Epoch: 118\tTrain Loss: 2.2605281 \tVal Loss:2.1595932 \tTrain Acc: 25.58824% \tVal Acc: 26.7058833%\n",
      "Epoch: 119\tTrain Loss: 2.2003328 \tVal Loss:2.1455364 \tTrain Acc: 26.41177% \tVal Acc: 29.1764717%\n",
      "Epoch: 120\tTrain Loss: 2.1796994 \tVal Loss:2.1078176 \tTrain Acc: 27.72059% \tVal Acc: 30.8235304%\n",
      "Validation Loss decreased from 2.128620 to 2.107818, saving the model weights\n",
      "Epoch: 121\tTrain Loss: 2.1572470 \tVal Loss:2.1022468 \tTrain Acc: 28.02941% \tVal Acc: 31.0588245%\n",
      "Validation Loss decreased from 2.107818 to 2.102247, saving the model weights\n",
      "Epoch: 122\tTrain Loss: 2.1715047 \tVal Loss:2.1123985 \tTrain Acc: 27.72059% \tVal Acc: 31.4117657%\n",
      "Epoch: 123\tTrain Loss: 2.1537187 \tVal Loss:2.1248906 \tTrain Acc: 28.82353% \tVal Acc: 28.4705894%\n",
      "Epoch: 124\tTrain Loss: 2.1521028 \tVal Loss:2.1247889 \tTrain Acc: 28.80882% \tVal Acc: 28.8823539%\n",
      "Epoch: 125\tTrain Loss: 2.1350078 \tVal Loss:2.1297834 \tTrain Acc: 28.75% \tVal Acc: 29.3529421%\n",
      "Epoch: 126\tTrain Loss: 2.1451995 \tVal Loss:2.1063751 \tTrain Acc: 29.0% \tVal Acc: 29.5294127%\n",
      "Epoch: 127\tTrain Loss: 2.1361598 \tVal Loss:2.1007012 \tTrain Acc: 29.14706% \tVal Acc: 28.2941188%\n",
      "Validation Loss decreased from 2.102247 to 2.100701, saving the model weights\n",
      "Epoch: 128\tTrain Loss: 2.1326737 \tVal Loss:2.0984111 \tTrain Acc: 29.55882% \tVal Acc: 30.8823539%\n",
      "Validation Loss decreased from 2.100701 to 2.098411, saving the model weights\n",
      "Epoch: 129\tTrain Loss: 2.1177315 \tVal Loss:2.1084218 \tTrain Acc: 29.51471% \tVal Acc: 29.3529421%\n",
      "Epoch: 130\tTrain Loss: 2.1270218 \tVal Loss:2.1062131 \tTrain Acc: 28.89706% \tVal Acc: 29.8823538%\n",
      "Epoch: 131\tTrain Loss: 2.1194393 \tVal Loss:2.1144470 \tTrain Acc: 29.57353% \tVal Acc: 29.4117656%\n",
      "Epoch: 132\tTrain Loss: 2.1133578 \tVal Loss:2.0696694 \tTrain Acc: 29.13235% \tVal Acc: 30.5882365%\n",
      "Validation Loss decreased from 2.098411 to 2.069669, saving the model weights\n",
      "Epoch: 133\tTrain Loss: 2.0801617 \tVal Loss:2.0934005 \tTrain Acc: 31.23529% \tVal Acc: 30.1764715%\n",
      "Epoch: 134\tTrain Loss: 2.0898569 \tVal Loss:2.0522250 \tTrain Acc: 30.27941% \tVal Acc: 32.1176480%\n",
      "Validation Loss decreased from 2.069669 to 2.052225, saving the model weights\n",
      "Epoch: 135\tTrain Loss: 2.0904512 \tVal Loss:2.0604728 \tTrain Acc: 30.07353% \tVal Acc: 30.3529419%\n",
      "Epoch: 136\tTrain Loss: 2.0974709 \tVal Loss:2.0893304 \tTrain Acc: 30.23529% \tVal Acc: 29.4705892%\n",
      "Epoch: 137\tTrain Loss: 2.1123924 \tVal Loss:2.1094995 \tTrain Acc: 29.16177% \tVal Acc: 26.8823536%\n",
      "Epoch: 138\tTrain Loss: 2.0887854 \tVal Loss:2.1385676 \tTrain Acc: 30.80882% \tVal Acc: 28.1176478%\n",
      "Epoch: 139\tTrain Loss: 2.0820171 \tVal Loss:2.1019939 \tTrain Acc: 31.70588% \tVal Acc: 28.8235299%\n",
      "Epoch: 140\tTrain Loss: 2.1094839 \tVal Loss:2.1146979 \tTrain Acc: 29.97059% \tVal Acc: 27.8823537%\n",
      "Epoch: 141\tTrain Loss: 2.1533908 \tVal Loss:2.1592701 \tTrain Acc: 28.22059% \tVal Acc: 27.1176477%\n",
      "Epoch: 142\tTrain Loss: 2.1347876 \tVal Loss:2.0353750 \tTrain Acc: 29.05882% \tVal Acc: 33.0000007%\n",
      "Validation Loss decreased from 2.052225 to 2.035375, saving the model weights\n",
      "Epoch: 143\tTrain Loss: 2.1234476 \tVal Loss:2.0431985 \tTrain Acc: 30.02941% \tVal Acc: 33.5294127%\n",
      "Epoch: 144\tTrain Loss: 2.1122315 \tVal Loss:1.9454023 \tTrain Acc: 29.26471% \tVal Acc: 37.4705890%\n",
      "Validation Loss decreased from 2.035375 to 1.945402, saving the model weights\n",
      "Epoch: 145\tTrain Loss: 2.0773560 \tVal Loss:1.9624091 \tTrain Acc: 30.32353% \tVal Acc: 39.1764712%\n",
      "Epoch: 146\tTrain Loss: 2.0457401 \tVal Loss:1.9409459 \tTrain Acc: 32.13235% \tVal Acc: 39.4117656%\n",
      "Validation Loss decreased from 1.945402 to 1.940946, saving the model weights\n",
      "Epoch: 147\tTrain Loss: 2.0350669 \tVal Loss:1.9550161 \tTrain Acc: 32.70588% \tVal Acc: 38.0588245%\n",
      "Epoch: 148\tTrain Loss: 2.0154721 \tVal Loss:1.8927692 \tTrain Acc: 32.88235% \tVal Acc: 40.8235303%\n",
      "Validation Loss decreased from 1.940946 to 1.892769, saving the model weights\n",
      "Epoch: 149\tTrain Loss: 1.9944306 \tVal Loss:1.8931461 \tTrain Acc: 34.05882% \tVal Acc: 39.7058830%\n",
      "Epoch: 150\tTrain Loss: 1.9713594 \tVal Loss:1.8607393 \tTrain Acc: 34.05882% \tVal Acc: 41.1764711%\n",
      "Validation Loss decreased from 1.892769 to 1.860739, saving the model weights\n",
      "Epoch: 151\tTrain Loss: 1.9589034 \tVal Loss:1.8695102 \tTrain Acc: 35.60294% \tVal Acc: 41.7058831%\n",
      "Epoch: 152\tTrain Loss: 1.9670418 \tVal Loss:1.8776075 \tTrain Acc: 35.47059% \tVal Acc: 39.5882359%\n",
      "Epoch: 153\tTrain Loss: 1.9864911 \tVal Loss:1.8556705 \tTrain Acc: 34.14706% \tVal Acc: 41.2941182%\n",
      "Validation Loss decreased from 1.860739 to 1.855671, saving the model weights\n",
      "Epoch: 154\tTrain Loss: 1.9450907 \tVal Loss:1.8332547 \tTrain Acc: 35.77941% \tVal Acc: 41.1176476%\n",
      "Validation Loss decreased from 1.855671 to 1.833255, saving the model weights\n",
      "Epoch: 155\tTrain Loss: 1.9236070 \tVal Loss:1.8053616 \tTrain Acc: 36.58824% \tVal Acc: 44.0000007%\n",
      "Validation Loss decreased from 1.833255 to 1.805362, saving the model weights\n",
      "Epoch: 156\tTrain Loss: 1.9121184 \tVal Loss:1.8157051 \tTrain Acc: 36.98529% \tVal Acc: 43.7058830%\n",
      "Epoch: 157\tTrain Loss: 1.9134474 \tVal Loss:1.7860743 \tTrain Acc: 36.51471% \tVal Acc: 43.2941186%\n",
      "Validation Loss decreased from 1.805362 to 1.786074, saving the model weights\n",
      "Epoch: 158\tTrain Loss: 1.9046808 \tVal Loss:1.7995236 \tTrain Acc: 37.25% \tVal Acc: 41.7058831%\n",
      "Epoch: 159\tTrain Loss: 1.9252449 \tVal Loss:1.7957310 \tTrain Acc: 37.19118% \tVal Acc: 41.5882358%\n",
      "Epoch: 160\tTrain Loss: 1.8891785 \tVal Loss:1.7740434 \tTrain Acc: 38.10294% \tVal Acc: 44.5294124%\n",
      "Validation Loss decreased from 1.786074 to 1.774043, saving the model weights\n",
      "Epoch: 161\tTrain Loss: 1.8842792 \tVal Loss:1.7625925 \tTrain Acc: 37.80882% \tVal Acc: 42.4117652%\n",
      "Validation Loss decreased from 1.774043 to 1.762593, saving the model weights\n",
      "Epoch: 162\tTrain Loss: 1.8689863 \tVal Loss:1.7825409 \tTrain Acc: 38.23529% \tVal Acc: 41.8235302%\n",
      "Epoch: 163\tTrain Loss: 1.8720108 \tVal Loss:1.7629914 \tTrain Acc: 38.57353% \tVal Acc: 42.4705887%\n",
      "Epoch: 164\tTrain Loss: 1.8499955 \tVal Loss:1.7271711 \tTrain Acc: 39.26471% \tVal Acc: 45.7058829%\n",
      "Validation Loss decreased from 1.762593 to 1.727171, saving the model weights\n",
      "Epoch: 165\tTrain Loss: 1.8602861 \tVal Loss:1.7107159 \tTrain Acc: 38.63235% \tVal Acc: 44.3529415%\n",
      "Validation Loss decreased from 1.727171 to 1.710716, saving the model weights\n",
      "Epoch: 166\tTrain Loss: 1.8373149 \tVal Loss:1.6975235 \tTrain Acc: 39.60294% \tVal Acc: 46.2352949%\n",
      "Validation Loss decreased from 1.710716 to 1.697523, saving the model weights\n",
      "Epoch: 167\tTrain Loss: 1.8251758 \tVal Loss:1.7049446 \tTrain Acc: 39.67647% \tVal Acc: 46.9411772%\n",
      "Epoch: 168\tTrain Loss: 1.8315246 \tVal Loss:1.7110659 \tTrain Acc: 38.76471% \tVal Acc: 44.8235300%\n",
      "Epoch: 169\tTrain Loss: 1.8157218 \tVal Loss:1.6757193 \tTrain Acc: 39.89706% \tVal Acc: 45.8235300%\n",
      "Validation Loss decreased from 1.697523 to 1.675719, saving the model weights\n",
      "Epoch: 170\tTrain Loss: 1.8104817 \tVal Loss:1.6505579 \tTrain Acc: 39.69118% \tVal Acc: 48.5294124%\n",
      "Validation Loss decreased from 1.675719 to 1.650558, saving the model weights\n",
      "Epoch: 171\tTrain Loss: 1.7695200 \tVal Loss:1.6581146 \tTrain Acc: 41.54412% \tVal Acc: 48.2941183%\n",
      "Epoch: 172\tTrain Loss: 1.7633256 \tVal Loss:1.6655965 \tTrain Acc: 41.76471% \tVal Acc: 47.8235298%\n",
      "Epoch: 173\tTrain Loss: 1.7624118 \tVal Loss:1.6440560 \tTrain Acc: 41.77941% \tVal Acc: 47.7058828%\n",
      "Validation Loss decreased from 1.650558 to 1.644056, saving the model weights\n",
      "Epoch: 174\tTrain Loss: 1.7623187 \tVal Loss:1.6280240 \tTrain Acc: 42.02941% \tVal Acc: 49.2941180%\n",
      "Validation Loss decreased from 1.644056 to 1.628024, saving the model weights\n",
      "Epoch: 175\tTrain Loss: 1.7449955 \tVal Loss:1.5856499 \tTrain Acc: 42.77941% \tVal Acc: 51.0588244%\n",
      "Validation Loss decreased from 1.628024 to 1.585650, saving the model weights\n",
      "Epoch: 176\tTrain Loss: 1.7340719 \tVal Loss:1.6362513 \tTrain Acc: 42.67647% \tVal Acc: 49.0000007%\n",
      "Epoch: 177\tTrain Loss: 1.7180895 \tVal Loss:1.5491958 \tTrain Acc: 43.52941% \tVal Acc: 51.0588244%\n",
      "Validation Loss decreased from 1.585650 to 1.549196, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 178\tTrain Loss: 1.6911992 \tVal Loss:1.5788969 \tTrain Acc: 45.61765% \tVal Acc: 49.4705886%\n",
      "Epoch: 179\tTrain Loss: 1.7049458 \tVal Loss:1.5906754 \tTrain Acc: 44.0% \tVal Acc: 50.1764709%\n",
      "Epoch: 180\tTrain Loss: 1.6977980 \tVal Loss:1.5548712 \tTrain Acc: 44.23529% \tVal Acc: 52.2941184%\n",
      "Epoch: 181\tTrain Loss: 1.6886170 \tVal Loss:1.5412841 \tTrain Acc: 44.73529% \tVal Acc: 52.3529416%\n",
      "Validation Loss decreased from 1.549196 to 1.541284, saving the model weights\n",
      "Epoch: 182\tTrain Loss: 1.6833182 \tVal Loss:1.5204603 \tTrain Acc: 45.16177% \tVal Acc: 52.2941187%\n",
      "Validation Loss decreased from 1.541284 to 1.520460, saving the model weights\n",
      "Epoch: 183\tTrain Loss: 1.6824629 \tVal Loss:1.5230789 \tTrain Acc: 44.48529% \tVal Acc: 53.0588242%\n",
      "Epoch: 184\tTrain Loss: 1.6593805 \tVal Loss:1.4685029 \tTrain Acc: 45.89706% \tVal Acc: 53.5882363%\n",
      "Validation Loss decreased from 1.520460 to 1.468503, saving the model weights\n",
      "Epoch: 185\tTrain Loss: 1.6422966 \tVal Loss:1.4582294 \tTrain Acc: 46.36765% \tVal Acc: 53.9411774%\n",
      "Validation Loss decreased from 1.468503 to 1.458229, saving the model weights\n",
      "Epoch: 186\tTrain Loss: 1.6315167 \tVal Loss:1.4797804 \tTrain Acc: 46.70588% \tVal Acc: 54.9411786%\n",
      "Epoch: 187\tTrain Loss: 1.6204901 \tVal Loss:1.4833505 \tTrain Acc: 47.47059% \tVal Acc: 51.9411770%\n",
      "Epoch: 188\tTrain Loss: 1.6340912 \tVal Loss:1.4579027 \tTrain Acc: 46.20588% \tVal Acc: 54.2352948%\n",
      "Validation Loss decreased from 1.458229 to 1.457903, saving the model weights\n",
      "Epoch: 189\tTrain Loss: 1.6205647 \tVal Loss:1.4618326 \tTrain Acc: 47.07353% \tVal Acc: 55.2941179%\n",
      "Epoch: 190\tTrain Loss: 1.6230169 \tVal Loss:1.4291491 \tTrain Acc: 46.83824% \tVal Acc: 54.4117647%\n",
      "Validation Loss decreased from 1.457903 to 1.429149, saving the model weights\n",
      "Epoch: 191\tTrain Loss: 1.6547888 \tVal Loss:1.5090565 \tTrain Acc: 46.79412% \tVal Acc: 51.1176485%\n",
      "Epoch: 192\tTrain Loss: 1.6336118 \tVal Loss:1.4485174 \tTrain Acc: 46.95588% \tVal Acc: 54.3529421%\n",
      "Epoch: 193\tTrain Loss: 1.6294191 \tVal Loss:1.5064331 \tTrain Acc: 47.23529% \tVal Acc: 52.3529419%\n",
      "Epoch: 194\tTrain Loss: 1.5941937 \tVal Loss:1.4686057 \tTrain Acc: 48.14706% \tVal Acc: 53.2941180%\n",
      "Epoch: 195\tTrain Loss: 1.6054051 \tVal Loss:1.4616701 \tTrain Acc: 47.66177% \tVal Acc: 53.9411771%\n",
      "Epoch: 196\tTrain Loss: 1.5709765 \tVal Loss:1.4737178 \tTrain Acc: 49.22059% \tVal Acc: 53.5882354%\n",
      "Epoch: 197\tTrain Loss: 1.5729318 \tVal Loss:1.3784904 \tTrain Acc: 48.94118% \tVal Acc: 59.0000004%\n",
      "Validation Loss decreased from 1.429149 to 1.378490, saving the model weights\n",
      "Epoch: 198\tTrain Loss: 1.5605163 \tVal Loss:1.4019868 \tTrain Acc: 49.25% \tVal Acc: 55.3529423%\n",
      "Epoch: 199\tTrain Loss: 1.5491408 \tVal Loss:1.4207018 \tTrain Acc: 49.98529% \tVal Acc: 54.2352948%\n",
      "Epoch: 200\tTrain Loss: 1.5325359 \tVal Loss:1.3684805 \tTrain Acc: 49.98529% \tVal Acc: 56.8235296%\n",
      "Validation Loss decreased from 1.378490 to 1.368480, saving the model weights\n",
      "Epoch: 201\tTrain Loss: 1.4963171 \tVal Loss:1.2840953 \tTrain Acc: 51.67647% \tVal Acc: 60.8235306%\n",
      "Validation Loss decreased from 1.368480 to 1.284095, saving the model weights\n",
      "Epoch: 202\tTrain Loss: 1.4510967 \tVal Loss:1.2314451 \tTrain Acc: 52.89706% \tVal Acc: 63.3529419%\n",
      "Validation Loss decreased from 1.284095 to 1.231445, saving the model weights\n",
      "Epoch: 203\tTrain Loss: 1.4712892 \tVal Loss:1.2247247 \tTrain Acc: 52.82353% \tVal Acc: 63.0000013%\n",
      "Validation Loss decreased from 1.231445 to 1.224725, saving the model weights\n",
      "Epoch: 204\tTrain Loss: 1.4665782 \tVal Loss:1.2350539 \tTrain Acc: 52.41177% \tVal Acc: 61.7058831%\n",
      "Epoch: 205\tTrain Loss: 1.4452754 \tVal Loss:1.1995976 \tTrain Acc: 53.52941% \tVal Acc: 64.0588242%\n",
      "Validation Loss decreased from 1.224725 to 1.199598, saving the model weights\n",
      "Epoch: 206\tTrain Loss: 1.4160919 \tVal Loss:1.1907484 \tTrain Acc: 54.52941% \tVal Acc: 64.3529415%\n",
      "Validation Loss decreased from 1.199598 to 1.190748, saving the model weights\n",
      "Epoch: 207\tTrain Loss: 1.4070497 \tVal Loss:1.2149467 \tTrain Acc: 53.80882% \tVal Acc: 63.0000007%\n",
      "Epoch: 208\tTrain Loss: 1.4000290 \tVal Loss:1.2241460 \tTrain Acc: 54.82353% \tVal Acc: 61.9411775%\n",
      "Epoch: 209\tTrain Loss: 1.3968216 \tVal Loss:1.1575193 \tTrain Acc: 54.77941% \tVal Acc: 65.2352950%\n",
      "Validation Loss decreased from 1.190748 to 1.157519, saving the model weights\n",
      "Epoch: 210\tTrain Loss: 1.4044925 \tVal Loss:1.2092587 \tTrain Acc: 54.63235% \tVal Acc: 62.0588234%\n",
      "Epoch: 211\tTrain Loss: 1.4049728 \tVal Loss:1.1831890 \tTrain Acc: 54.80882% \tVal Acc: 64.1764718%\n",
      "Epoch: 212\tTrain Loss: 1.3780143 \tVal Loss:1.1196586 \tTrain Acc: 55.29412% \tVal Acc: 66.0588235%\n",
      "Validation Loss decreased from 1.157519 to 1.119659, saving the model weights\n",
      "Epoch: 213\tTrain Loss: 1.3574091 \tVal Loss:1.1222811 \tTrain Acc: 56.44118% \tVal Acc: 66.9999999%\n",
      "Epoch: 214\tTrain Loss: 1.3316464 \tVal Loss:1.1132049 \tTrain Acc: 56.82353% \tVal Acc: 66.2352937%\n",
      "Validation Loss decreased from 1.119659 to 1.113205, saving the model weights\n",
      "Epoch: 215\tTrain Loss: 1.3163126 \tVal Loss:1.1051630 \tTrain Acc: 57.07353% \tVal Acc: 67.1764708%\n",
      "Validation Loss decreased from 1.113205 to 1.105163, saving the model weights\n",
      "Epoch: 216\tTrain Loss: 1.3099061 \tVal Loss:1.0678088 \tTrain Acc: 57.88235% \tVal Acc: 68.4117645%\n",
      "Validation Loss decreased from 1.105163 to 1.067809, saving the model weights\n",
      "Epoch: 217\tTrain Loss: 1.2942073 \tVal Loss:1.0402073 \tTrain Acc: 57.41177% \tVal Acc: 68.2352954%\n",
      "Validation Loss decreased from 1.067809 to 1.040207, saving the model weights\n",
      "Epoch: 218\tTrain Loss: 1.2928227 \tVal Loss:1.0811575 \tTrain Acc: 58.73529% \tVal Acc: 67.9411757%\n",
      "Epoch: 219\tTrain Loss: 1.2905331 \tVal Loss:1.0062710 \tTrain Acc: 58.04412% \tVal Acc: 70.2352941%\n",
      "Validation Loss decreased from 1.040207 to 1.006271, saving the model weights\n",
      "Epoch: 220\tTrain Loss: 1.2650352 \tVal Loss:0.9758432 \tTrain Acc: 58.72059% \tVal Acc: 72.1176463%\n",
      "Validation Loss decreased from 1.006271 to 0.975843, saving the model weights\n",
      "Epoch: 221\tTrain Loss: 1.2450073 \tVal Loss:0.9749354 \tTrain Acc: 60.02941% \tVal Acc: 71.4705878%\n",
      "Validation Loss decreased from 0.975843 to 0.974935, saving the model weights\n",
      "Epoch: 222\tTrain Loss: 1.2379433 \tVal Loss:0.9742898 \tTrain Acc: 60.61765% \tVal Acc: 71.4705878%\n",
      "Validation Loss decreased from 0.974935 to 0.974290, saving the model weights\n",
      "Epoch: 223\tTrain Loss: 1.2335118 \tVal Loss:1.0333561 \tTrain Acc: 59.52941% \tVal Acc: 67.3529416%\n",
      "Epoch: 224\tTrain Loss: 1.2122957 \tVal Loss:1.0219528 \tTrain Acc: 60.27941% \tVal Acc: 70.1764715%\n",
      "Epoch: 225\tTrain Loss: 1.2600152 \tVal Loss:0.9927667 \tTrain Acc: 59.94118% \tVal Acc: 70.1764715%\n",
      "Epoch: 226\tTrain Loss: 1.2065296 \tVal Loss:0.9548320 \tTrain Acc: 61.44118% \tVal Acc: 71.7058831%\n",
      "Validation Loss decreased from 0.974290 to 0.954832, saving the model weights\n",
      "Epoch: 227\tTrain Loss: 1.1906410 \tVal Loss:0.9808052 \tTrain Acc: 62.16177% \tVal Acc: 70.7058829%\n",
      "Epoch: 228\tTrain Loss: 1.1620996 \tVal Loss:0.9255960 \tTrain Acc: 63.58824% \tVal Acc: 73.4705889%\n",
      "Validation Loss decreased from 0.954832 to 0.925596, saving the model weights\n",
      "Epoch: 229\tTrain Loss: 1.1475959 \tVal Loss:0.9159438 \tTrain Acc: 63.32353% \tVal Acc: 73.9999998%\n",
      "Validation Loss decreased from 0.925596 to 0.915944, saving the model weights\n",
      "Epoch: 230\tTrain Loss: 1.1463945 \tVal Loss:0.9255521 \tTrain Acc: 63.58824% \tVal Acc: 72.7647048%\n",
      "Epoch: 231\tTrain Loss: 1.1357344 \tVal Loss:0.9319089 \tTrain Acc: 63.58824% \tVal Acc: 71.6470587%\n",
      "Epoch: 232\tTrain Loss: 1.1246501 \tVal Loss:0.9547705 \tTrain Acc: 63.86765% \tVal Acc: 70.5294120%\n",
      "Epoch: 233\tTrain Loss: 1.1370375 \tVal Loss:0.8620812 \tTrain Acc: 63.86765% \tVal Acc: 74.9999994%\n",
      "Validation Loss decreased from 0.915944 to 0.862081, saving the model weights\n",
      "Epoch: 234\tTrain Loss: 1.1058233 \tVal Loss:0.9496888 \tTrain Acc: 65.17647% \tVal Acc: 71.1764705%\n",
      "Epoch: 235\tTrain Loss: 1.0925357 \tVal Loss:0.9367824 \tTrain Acc: 65.32353% \tVal Acc: 71.5294117%\n",
      "Epoch: 236\tTrain Loss: 1.0938353 \tVal Loss:0.9291342 \tTrain Acc: 65.11765% \tVal Acc: 72.1764708%\n",
      "Epoch: 237\tTrain Loss: 1.1199136 \tVal Loss:0.8463225 \tTrain Acc: 64.25% \tVal Acc: 75.6470591%\n",
      "Validation Loss decreased from 0.862081 to 0.846323, saving the model weights\n",
      "Epoch: 238\tTrain Loss: 1.0958224 \tVal Loss:0.8064669 \tTrain Acc: 65.19118% \tVal Acc: 76.4705873%\n",
      "Validation Loss decreased from 0.846323 to 0.806467, saving the model weights\n",
      "Epoch: 239\tTrain Loss: 1.1142433 \tVal Loss:0.9129955 \tTrain Acc: 65.0% \tVal Acc: 73.0588233%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 240\tTrain Loss: 1.1181034 \tVal Loss:0.8613408 \tTrain Acc: 64.38235% \tVal Acc: 74.1176468%\n",
      "Epoch: 241\tTrain Loss: 1.1320483 \tVal Loss:0.9089491 \tTrain Acc: 63.30882% \tVal Acc: 72.1764708%\n",
      "Epoch: 242\tTrain Loss: 1.0692608 \tVal Loss:0.8681310 \tTrain Acc: 66.41177% \tVal Acc: 74.4705874%\n",
      "Epoch: 243\tTrain Loss: 1.0249518 \tVal Loss:0.8316382 \tTrain Acc: 67.61765% \tVal Acc: 75.2941173%\n",
      "Epoch: 244\tTrain Loss: 1.0387921 \tVal Loss:0.8811407 \tTrain Acc: 67.10294% \tVal Acc: 73.7647063%\n",
      "Epoch: 245\tTrain Loss: 1.0038891 \tVal Loss:0.7906848 \tTrain Acc: 68.0% \tVal Acc: 77.8235286%\n",
      "Validation Loss decreased from 0.806467 to 0.790685, saving the model weights\n",
      "Epoch: 246\tTrain Loss: 1.0287823 \tVal Loss:1.0003546 \tTrain Acc: 67.08824% \tVal Acc: 69.5882350%\n",
      "Epoch: 247\tTrain Loss: 1.0168363 \tVal Loss:0.8162936 \tTrain Acc: 67.77941% \tVal Acc: 76.7647058%\n",
      "Epoch: 248\tTrain Loss: 1.0033751 \tVal Loss:0.7600625 \tTrain Acc: 68.39706% \tVal Acc: 78.0588239%\n",
      "Validation Loss decreased from 0.790685 to 0.760062, saving the model weights\n",
      "Epoch: 249\tTrain Loss: 0.9944546 \tVal Loss:0.7100499 \tTrain Acc: 68.32353% \tVal Acc: 80.9999990%\n",
      "Validation Loss decreased from 0.760062 to 0.710050, saving the model weights\n",
      "Epoch: 250\tTrain Loss: 0.9789519 \tVal Loss:0.6946086 \tTrain Acc: 69.14706% \tVal Acc: 80.7647049%\n",
      "Validation Loss decreased from 0.710050 to 0.694609, saving the model weights\n",
      "Epoch: 251\tTrain Loss: 0.9619780 \tVal Loss:0.6623294 \tTrain Acc: 69.44118% \tVal Acc: 81.5294117%\n",
      "Validation Loss decreased from 0.694609 to 0.662329, saving the model weights\n",
      "Epoch: 252\tTrain Loss: 0.9721402 \tVal Loss:0.6405786 \tTrain Acc: 69.05882% \tVal Acc: 82.2352946%\n",
      "Validation Loss decreased from 0.662329 to 0.640579, saving the model weights\n",
      "Epoch: 253\tTrain Loss: 0.9676514 \tVal Loss:0.6451067 \tTrain Acc: 68.54412% \tVal Acc: 81.4117640%\n",
      "Epoch: 254\tTrain Loss: 0.9203296 \tVal Loss:0.6148328 \tTrain Acc: 71.0% \tVal Acc: 81.9999999%\n",
      "Validation Loss decreased from 0.640579 to 0.614833, saving the model weights\n",
      "Epoch: 255\tTrain Loss: 0.9137623 \tVal Loss:0.6575632 \tTrain Acc: 71.13235% \tVal Acc: 80.8823526%\n",
      "Epoch: 256\tTrain Loss: 0.9139410 \tVal Loss:0.6306656 \tTrain Acc: 71.73529% \tVal Acc: 81.2352937%\n",
      "Epoch: 257\tTrain Loss: 0.9146617 \tVal Loss:0.6618990 \tTrain Acc: 71.11765% \tVal Acc: 81.2352943%\n",
      "Epoch: 258\tTrain Loss: 0.9330616 \tVal Loss:0.6487441 \tTrain Acc: 70.89706% \tVal Acc: 81.3529408%\n",
      "Epoch: 259\tTrain Loss: 0.9110083 \tVal Loss:0.6222883 \tTrain Acc: 71.54412% \tVal Acc: 81.9999993%\n",
      "Epoch: 260\tTrain Loss: 0.8974526 \tVal Loss:0.5876687 \tTrain Acc: 72.04412% \tVal Acc: 83.2941175%\n",
      "Validation Loss decreased from 0.614833 to 0.587669, saving the model weights\n",
      "Epoch: 261\tTrain Loss: 0.8787883 \tVal Loss:0.6837652 \tTrain Acc: 71.85294% \tVal Acc: 79.7647053%\n",
      "Epoch: 262\tTrain Loss: 0.8777730 \tVal Loss:0.5996022 \tTrain Acc: 72.23529% \tVal Acc: 82.5294113%\n",
      "Epoch: 263\tTrain Loss: 0.8753407 \tVal Loss:0.6173244 \tTrain Acc: 72.48529% \tVal Acc: 83.1764704%\n",
      "Epoch: 264\tTrain Loss: 0.8566618 \tVal Loss:0.6398033 \tTrain Acc: 72.92647% \tVal Acc: 81.5294117%\n",
      "Epoch: 265\tTrain Loss: 0.8540708 \tVal Loss:0.5735247 \tTrain Acc: 73.0% \tVal Acc: 83.5882336%\n",
      "Validation Loss decreased from 0.587669 to 0.573525, saving the model weights\n",
      "Epoch: 266\tTrain Loss: 0.8608114 \tVal Loss:0.5617850 \tTrain Acc: 72.79412% \tVal Acc: 83.7058818%\n",
      "Validation Loss decreased from 0.573525 to 0.561785, saving the model weights\n",
      "Epoch: 267\tTrain Loss: 0.8438053 \tVal Loss:0.5251264 \tTrain Acc: 73.5% \tVal Acc: 85.1176465%\n",
      "Validation Loss decreased from 0.561785 to 0.525126, saving the model weights\n",
      "Epoch: 268\tTrain Loss: 0.8203583 \tVal Loss:0.5477657 \tTrain Acc: 74.39706% \tVal Acc: 83.9411759%\n",
      "Epoch: 269\tTrain Loss: 0.8254898 \tVal Loss:0.5544276 \tTrain Acc: 73.77941% \tVal Acc: 84.5294112%\n",
      "Epoch: 270\tTrain Loss: 0.8065859 \tVal Loss:0.5210586 \tTrain Acc: 74.69118% \tVal Acc: 84.8823524%\n",
      "Validation Loss decreased from 0.525126 to 0.521059, saving the model weights\n",
      "Epoch: 271\tTrain Loss: 0.7868448 \tVal Loss:0.5291917 \tTrain Acc: 75.01471% \tVal Acc: 84.5294118%\n",
      "Epoch: 272\tTrain Loss: 0.7864622 \tVal Loss:0.5649273 \tTrain Acc: 75.01471% \tVal Acc: 83.9411759%\n",
      "Epoch: 273\tTrain Loss: 0.8104094 \tVal Loss:0.7626780 \tTrain Acc: 74.36765% \tVal Acc: 77.5882351%\n",
      "Epoch: 274\tTrain Loss: 0.9575934 \tVal Loss:0.7012706 \tTrain Acc: 69.88235% \tVal Acc: 78.9411759%\n",
      "Epoch: 275\tTrain Loss: 0.8925367 \tVal Loss:0.5359899 \tTrain Acc: 72.19118% \tVal Acc: 85.5294102%\n",
      "Epoch: 276\tTrain Loss: 0.8165585 \tVal Loss:0.4735472 \tTrain Acc: 73.92647% \tVal Acc: 87.0588231%\n",
      "Validation Loss decreased from 0.521059 to 0.473547, saving the model weights\n",
      "Epoch: 277\tTrain Loss: 0.7778535 \tVal Loss:0.4362479 \tTrain Acc: 75.86765% \tVal Acc: 87.6470578%\n",
      "Validation Loss decreased from 0.473547 to 0.436248, saving the model weights\n",
      "Epoch: 278\tTrain Loss: 0.7307245 \tVal Loss:0.4213608 \tTrain Acc: 76.95588% \tVal Acc: 88.1176466%\n",
      "Validation Loss decreased from 0.436248 to 0.421361, saving the model weights\n",
      "Epoch: 279\tTrain Loss: 0.7141341 \tVal Loss:0.3951399 \tTrain Acc: 78.04412% \tVal Acc: 90.0588220%\n",
      "Validation Loss decreased from 0.421361 to 0.395140, saving the model weights\n",
      "Epoch: 280\tTrain Loss: 0.6957371 \tVal Loss:0.3880938 \tTrain Acc: 78.47059% \tVal Acc: 89.2941171%\n",
      "Validation Loss decreased from 0.395140 to 0.388094, saving the model weights\n",
      "Epoch: 281\tTrain Loss: 0.6974890 \tVal Loss:0.3785627 \tTrain Acc: 78.13235% \tVal Acc: 90.4705888%\n",
      "Validation Loss decreased from 0.388094 to 0.378563, saving the model weights\n",
      "Epoch: 282\tTrain Loss: 0.6870138 \tVal Loss:0.3574784 \tTrain Acc: 78.61765% \tVal Acc: 90.5882347%\n",
      "Validation Loss decreased from 0.378563 to 0.357478, saving the model weights\n",
      "Epoch: 283\tTrain Loss: 0.6749179 \tVal Loss:0.3732929 \tTrain Acc: 78.94118% \tVal Acc: 90.4705876%\n",
      "Epoch: 284\tTrain Loss: 0.6809207 \tVal Loss:0.3688767 \tTrain Acc: 79.47059% \tVal Acc: 90.5294114%\n",
      "Epoch: 285\tTrain Loss: 0.6842768 \tVal Loss:0.3626588 \tTrain Acc: 78.60294% \tVal Acc: 90.3529400%\n",
      "Epoch: 286\tTrain Loss: 0.6724659 \tVal Loss:0.3651714 \tTrain Acc: 78.61765% \tVal Acc: 91.4117634%\n",
      "Epoch: 287\tTrain Loss: 0.6644133 \tVal Loss:0.3438915 \tTrain Acc: 79.19118% \tVal Acc: 90.2941173%\n",
      "Validation Loss decreased from 0.357478 to 0.343891, saving the model weights\n",
      "Epoch: 288\tTrain Loss: 0.6650995 \tVal Loss:0.3824720 \tTrain Acc: 79.20588% \tVal Acc: 89.5294118%\n",
      "Epoch: 289\tTrain Loss: 0.6524013 \tVal Loss:0.3545567 \tTrain Acc: 79.54412% \tVal Acc: 90.1764703%\n",
      "Epoch: 290\tTrain Loss: 0.6637384 \tVal Loss:0.3921212 \tTrain Acc: 79.23529% \tVal Acc: 88.7647057%\n",
      "Epoch: 291\tTrain Loss: 0.6432532 \tVal Loss:0.3525421 \tTrain Acc: 79.45588% \tVal Acc: 90.6470579%\n",
      "Epoch: 292\tTrain Loss: 0.6676444 \tVal Loss:0.3727440 \tTrain Acc: 78.57353% \tVal Acc: 89.1176468%\n",
      "Epoch: 293\tTrain Loss: 0.6599919 \tVal Loss:0.3543968 \tTrain Acc: 78.75% \tVal Acc: 90.7647049%\n",
      "Epoch: 294\tTrain Loss: 0.6357838 \tVal Loss:0.3911170 \tTrain Acc: 80.08823% \tVal Acc: 88.8823527%\n",
      "Epoch: 295\tTrain Loss: 0.6525401 \tVal Loss:0.3342497 \tTrain Acc: 79.5% \tVal Acc: 91.2352931%\n",
      "Validation Loss decreased from 0.343891 to 0.334250, saving the model weights\n",
      "Epoch: 296\tTrain Loss: 0.6104191 \tVal Loss:0.3935867 \tTrain Acc: 81.19118% \tVal Acc: 88.3529413%\n",
      "Epoch: 297\tTrain Loss: 0.6170935 \tVal Loss:0.3510510 \tTrain Acc: 80.79412% \tVal Acc: 90.6470585%\n",
      "Epoch: 298\tTrain Loss: 0.6146821 \tVal Loss:0.3599339 \tTrain Acc: 81.19118% \tVal Acc: 90.0588232%\n",
      "Epoch: 299\tTrain Loss: 0.6146276 \tVal Loss:0.3468628 \tTrain Acc: 80.72059% \tVal Acc: 90.5294102%\n",
      "Epoch: 300\tTrain Loss: 0.6317142 \tVal Loss:0.3599384 \tTrain Acc: 80.10294% \tVal Acc: 89.7058821%\n",
      "Epoch: 301\tTrain Loss: 0.6148236 \tVal Loss:0.3910464 \tTrain Acc: 80.58824% \tVal Acc: 87.9411751%\n",
      "Epoch: 302\tTrain Loss: 0.6167516 \tVal Loss:0.2794510 \tTrain Acc: 80.60294% \tVal Acc: 94.1176456%\n",
      "Validation Loss decreased from 0.334250 to 0.279451, saving the model weights\n",
      "Epoch: 303\tTrain Loss: 0.5994463 \tVal Loss:0.3534253 \tTrain Acc: 80.92647% \tVal Acc: 89.4117641%\n",
      "Epoch: 304\tTrain Loss: 0.6062338 \tVal Loss:0.3098117 \tTrain Acc: 81.29412% \tVal Acc: 91.4705878%\n",
      "Epoch: 305\tTrain Loss: 0.5866921 \tVal Loss:0.4275852 \tTrain Acc: 81.22059% \tVal Acc: 88.2352930%\n",
      "Epoch: 306\tTrain Loss: 0.5919501 \tVal Loss:0.3877549 \tTrain Acc: 80.75% \tVal Acc: 88.7058812%\n",
      "Epoch: 307\tTrain Loss: 0.5693411 \tVal Loss:0.4528589 \tTrain Acc: 82.05882% \tVal Acc: 85.3529406%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 308\tTrain Loss: 0.5735785 \tVal Loss:0.3937229 \tTrain Acc: 81.67647% \tVal Acc: 88.5294110%\n",
      "Epoch: 309\tTrain Loss: 0.5697362 \tVal Loss:0.3677353 \tTrain Acc: 82.11765% \tVal Acc: 89.9411756%\n",
      "Epoch: 310\tTrain Loss: 0.5465684 \tVal Loss:0.3663172 \tTrain Acc: 82.67647% \tVal Acc: 90.0588232%\n",
      "Epoch: 311\tTrain Loss: 0.5591593 \tVal Loss:0.3041286 \tTrain Acc: 81.82353% \tVal Acc: 91.8823510%\n",
      "Epoch: 312\tTrain Loss: 0.5725167 \tVal Loss:0.3946509 \tTrain Acc: 82.05882% \tVal Acc: 87.8235292%\n",
      "Epoch: 313\tTrain Loss: 0.5620019 \tVal Loss:0.3933960 \tTrain Acc: 82.33823% \tVal Acc: 89.0588224%\n",
      "Epoch: 314\tTrain Loss: 0.5339812 \tVal Loss:0.3507890 \tTrain Acc: 83.07353% \tVal Acc: 90.5294114%\n",
      "Epoch: 315\tTrain Loss: 0.5352818 \tVal Loss:0.3133107 \tTrain Acc: 83.04412% \tVal Acc: 92.2352934%\n",
      "Epoch: 316\tTrain Loss: 0.5293194 \tVal Loss:0.2932958 \tTrain Acc: 83.63235% \tVal Acc: 92.5882351%\n",
      "Epoch: 317\tTrain Loss: 0.5275598 \tVal Loss:0.3143024 \tTrain Acc: 83.42647% \tVal Acc: 91.8235278%\n",
      "Epoch: 318\tTrain Loss: 0.5240046 \tVal Loss:0.2425582 \tTrain Acc: 83.72059% \tVal Acc: 93.9411753%\n",
      "Validation Loss decreased from 0.279451 to 0.242558, saving the model weights\n",
      "Epoch: 319\tTrain Loss: 0.5418670 \tVal Loss:0.2192752 \tTrain Acc: 82.38235% \tVal Acc: 94.7647047%\n",
      "Validation Loss decreased from 0.242558 to 0.219275, saving the model weights\n",
      "Epoch: 320\tTrain Loss: 0.5269835 \tVal Loss:0.2237762 \tTrain Acc: 83.55882% \tVal Acc: 95.0588226%\n",
      "Epoch: 321\tTrain Loss: 0.5082760 \tVal Loss:0.2318340 \tTrain Acc: 83.70588% \tVal Acc: 93.9999980%\n",
      "Epoch: 322\tTrain Loss: 0.5187894 \tVal Loss:0.2281229 \tTrain Acc: 84.05882% \tVal Acc: 94.5294100%\n",
      "Epoch: 323\tTrain Loss: 0.5293615 \tVal Loss:0.2216056 \tTrain Acc: 83.38235% \tVal Acc: 94.4705868%\n",
      "Epoch: 324\tTrain Loss: 0.5556487 \tVal Loss:0.2753725 \tTrain Acc: 81.80882% \tVal Acc: 93.3529401%\n",
      "Epoch: 325\tTrain Loss: 0.5388684 \tVal Loss:0.2169400 \tTrain Acc: 82.95588% \tVal Acc: 94.4705874%\n",
      "Validation Loss decreased from 0.219275 to 0.216940, saving the model weights\n",
      "Epoch: 326\tTrain Loss: 0.5192509 \tVal Loss:0.2156141 \tTrain Acc: 84.35294% \tVal Acc: 94.4117635%\n",
      "Validation Loss decreased from 0.216940 to 0.215614, saving the model weights\n",
      "Epoch: 327\tTrain Loss: 0.4916342 \tVal Loss:0.1918991 \tTrain Acc: 84.60294% \tVal Acc: 95.4705864%\n",
      "Validation Loss decreased from 0.215614 to 0.191899, saving the model weights\n",
      "Epoch: 328\tTrain Loss: 0.4709954 \tVal Loss:0.2269404 \tTrain Acc: 85.20588% \tVal Acc: 94.7058815%\n",
      "Epoch: 329\tTrain Loss: 0.4762359 \tVal Loss:0.1916610 \tTrain Acc: 85.52941% \tVal Acc: 95.8823508%\n",
      "Validation Loss decreased from 0.191899 to 0.191661, saving the model weights\n",
      "Epoch: 330\tTrain Loss: 0.4743153 \tVal Loss:0.2038107 \tTrain Acc: 85.11765% \tVal Acc: 94.9411750%\n",
      "Epoch: 331\tTrain Loss: 0.4594470 \tVal Loss:0.1724448 \tTrain Acc: 85.88235% \tVal Acc: 96.4117634%\n",
      "Validation Loss decreased from 0.191661 to 0.172445, saving the model weights\n",
      "Epoch: 332\tTrain Loss: 0.4210944 \tVal Loss:0.1723958 \tTrain Acc: 86.80882% \tVal Acc: 96.0588229%\n",
      "Validation Loss decreased from 0.172445 to 0.172396, saving the model weights\n",
      "Epoch: 333\tTrain Loss: 0.4377024 \tVal Loss:0.1640553 \tTrain Acc: 86.38235% \tVal Acc: 96.7058814%\n",
      "Validation Loss decreased from 0.172396 to 0.164055, saving the model weights\n",
      "Epoch: 334\tTrain Loss: 0.4056931 \tVal Loss:0.1543728 \tTrain Acc: 87.22059% \tVal Acc: 96.8235284%\n",
      "Validation Loss decreased from 0.164055 to 0.154373, saving the model weights\n",
      "Epoch: 335\tTrain Loss: 0.4037191 \tVal Loss:0.1743836 \tTrain Acc: 87.58823% \tVal Acc: 95.9411758%\n",
      "Epoch: 336\tTrain Loss: 0.4066319 \tVal Loss:0.1500363 \tTrain Acc: 87.32353% \tVal Acc: 96.7647040%\n",
      "Validation Loss decreased from 0.154373 to 0.150036, saving the model weights\n",
      "Epoch: 337\tTrain Loss: 0.3753393 \tVal Loss:0.1310925 \tTrain Acc: 88.45588% \tVal Acc: 97.2941160%\n",
      "Validation Loss decreased from 0.150036 to 0.131092, saving the model weights\n",
      "Epoch: 338\tTrain Loss: 0.3910038 \tVal Loss:0.1158081 \tTrain Acc: 87.44118% \tVal Acc: 97.9411751%\n",
      "Validation Loss decreased from 0.131092 to 0.115808, saving the model weights\n",
      "Epoch: 339\tTrain Loss: 0.3847402 \tVal Loss:0.1292510 \tTrain Acc: 87.89706% \tVal Acc: 97.4117643%\n",
      "Epoch: 340\tTrain Loss: 0.3892002 \tVal Loss:0.1251907 \tTrain Acc: 87.69118% \tVal Acc: 97.7647036%\n",
      "Epoch: 341\tTrain Loss: 0.3783935 \tVal Loss:0.1221277 \tTrain Acc: 88.27941% \tVal Acc: 97.7647042%\n",
      "Epoch: 342\tTrain Loss: 0.3798734 \tVal Loss:0.1462661 \tTrain Acc: 88.35294% \tVal Acc: 96.6470569%\n",
      "Epoch: 343\tTrain Loss: 0.4004324 \tVal Loss:0.1682481 \tTrain Acc: 87.67647% \tVal Acc: 95.7647049%\n",
      "Epoch: 344\tTrain Loss: 0.4477244 \tVal Loss:0.1742498 \tTrain Acc: 86.02941% \tVal Acc: 95.8823526%\n",
      "Epoch: 345\tTrain Loss: 0.4614695 \tVal Loss:0.1658756 \tTrain Acc: 85.13235% \tVal Acc: 95.6470573%\n",
      "Epoch: 346\tTrain Loss: 0.4536367 \tVal Loss:0.1615182 \tTrain Acc: 85.77941% \tVal Acc: 96.5294111%\n",
      "Epoch: 347\tTrain Loss: 0.4464599 \tVal Loss:0.1579274 \tTrain Acc: 86.10294% \tVal Acc: 96.1764693%\n",
      "Epoch: 348\tTrain Loss: 0.4284521 \tVal Loss:0.1243071 \tTrain Acc: 86.92647% \tVal Acc: 97.3529404%\n",
      "Epoch: 349\tTrain Loss: 0.3955969 \tVal Loss:0.1349155 \tTrain Acc: 87.73529% \tVal Acc: 96.9999993%\n",
      "Epoch: 350\tTrain Loss: 0.3843401 \tVal Loss:0.1168759 \tTrain Acc: 88.13235% \tVal Acc: 97.5882339%\n",
      "Epoch: 351\tTrain Loss: 0.3828383 \tVal Loss:0.1036354 \tTrain Acc: 88.17647% \tVal Acc: 98.1764692%\n",
      "Validation Loss decreased from 0.115808 to 0.103635, saving the model weights\n",
      "Epoch: 352\tTrain Loss: 0.3568642 \tVal Loss:0.0980738 \tTrain Acc: 89.16176% \tVal Acc: 98.1764692%\n",
      "Validation Loss decreased from 0.103635 to 0.098074, saving the model weights\n",
      "Epoch: 353\tTrain Loss: 0.3502344 \tVal Loss:0.1149981 \tTrain Acc: 89.58823% \tVal Acc: 97.1764690%\n",
      "Epoch: 354\tTrain Loss: 0.3525313 \tVal Loss:0.1067660 \tTrain Acc: 89.0% \tVal Acc: 97.7647042%\n",
      "Epoch: 355\tTrain Loss: 0.3226456 \tVal Loss:0.1154907 \tTrain Acc: 90.27941% \tVal Acc: 97.4705869%\n",
      "Epoch: 356\tTrain Loss: 0.3641447 \tVal Loss:0.1164658 \tTrain Acc: 88.70588% \tVal Acc: 97.7647042%\n",
      "Epoch: 357\tTrain Loss: 0.3562941 \tVal Loss:0.0997907 \tTrain Acc: 88.52941% \tVal Acc: 98.2352924%\n",
      "Epoch: 358\tTrain Loss: 0.3258192 \tVal Loss:0.0908680 \tTrain Acc: 90.0147% \tVal Acc: 98.1764698%\n",
      "Validation Loss decreased from 0.098074 to 0.090868, saving the model weights\n",
      "Epoch: 359\tTrain Loss: 0.3306144 \tVal Loss:0.0991075 \tTrain Acc: 89.61765% \tVal Acc: 97.9999983%\n",
      "Epoch: 360\tTrain Loss: 0.3181439 \tVal Loss:0.0988908 \tTrain Acc: 90.29412% \tVal Acc: 97.8823519%\n",
      "Epoch: 361\tTrain Loss: 0.3105014 \tVal Loss:0.0842129 \tTrain Acc: 91.05882% \tVal Acc: 98.2352924%\n",
      "Validation Loss decreased from 0.090868 to 0.084213, saving the model weights\n",
      "Epoch: 362\tTrain Loss: 0.3109010 \tVal Loss:0.0766142 \tTrain Acc: 90.33823% \tVal Acc: 98.8235283%\n",
      "Validation Loss decreased from 0.084213 to 0.076614, saving the model weights\n",
      "Epoch: 363\tTrain Loss: 0.2982167 \tVal Loss:0.0719968 \tTrain Acc: 91.25% \tVal Acc: 98.9411753%\n",
      "Validation Loss decreased from 0.076614 to 0.071997, saving the model weights\n",
      "Epoch: 364\tTrain Loss: 0.2912994 \tVal Loss:0.0752606 \tTrain Acc: 91.11765% \tVal Acc: 98.5294104%\n",
      "Epoch: 365\tTrain Loss: 0.2844966 \tVal Loss:0.0807020 \tTrain Acc: 91.36765% \tVal Acc: 98.3529395%\n",
      "Epoch: 366\tTrain Loss: 0.2837943 \tVal Loss:0.0767795 \tTrain Acc: 91.16176% \tVal Acc: 98.3529401%\n",
      "Epoch: 367\tTrain Loss: 0.2893063 \tVal Loss:0.0858754 \tTrain Acc: 90.77941% \tVal Acc: 98.3529401%\n",
      "Epoch: 368\tTrain Loss: 0.2933516 \tVal Loss:0.0925122 \tTrain Acc: 91.13235% \tVal Acc: 97.9999989%\n",
      "Epoch: 369\tTrain Loss: 0.2865404 \tVal Loss:0.0749630 \tTrain Acc: 90.98529% \tVal Acc: 98.7058806%\n",
      "Epoch: 370\tTrain Loss: 0.2908276 \tVal Loss:0.0749507 \tTrain Acc: 90.89706% \tVal Acc: 98.6470574%\n",
      "Epoch: 371\tTrain Loss: 0.2742886 \tVal Loss:0.0783921 \tTrain Acc: 91.42647% \tVal Acc: 98.4117633%\n",
      "Epoch: 372\tTrain Loss: 0.2875661 \tVal Loss:0.0737454 \tTrain Acc: 91.13235% \tVal Acc: 98.2941163%\n",
      "Epoch: 373\tTrain Loss: 0.2806197 \tVal Loss:0.0682067 \tTrain Acc: 91.33823% \tVal Acc: 98.9411759%\n",
      "Validation Loss decreased from 0.071997 to 0.068207, saving the model weights\n",
      "Epoch: 374\tTrain Loss: 0.2866844 \tVal Loss:0.0814840 \tTrain Acc: 91.05882% \tVal Acc: 98.0588216%\n",
      "Epoch: 375\tTrain Loss: 0.2825880 \tVal Loss:0.0778029 \tTrain Acc: 91.26471% \tVal Acc: 98.4117639%\n",
      "Epoch: 376\tTrain Loss: 0.2857997 \tVal Loss:0.0684354 \tTrain Acc: 91.39706% \tVal Acc: 98.6470574%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 377\tTrain Loss: 0.2937135 \tVal Loss:0.0772119 \tTrain Acc: 90.77941% \tVal Acc: 98.4117627%\n",
      "Epoch: 378\tTrain Loss: 0.2803327 \tVal Loss:0.0662584 \tTrain Acc: 91.70588% \tVal Acc: 98.7058812%\n",
      "Validation Loss decreased from 0.068207 to 0.066258, saving the model weights\n",
      "Epoch: 379\tTrain Loss: 0.2903469 \tVal Loss:0.0821125 \tTrain Acc: 90.92647% \tVal Acc: 98.2941163%\n",
      "Epoch: 380\tTrain Loss: 0.2950115 \tVal Loss:0.0976557 \tTrain Acc: 90.66176% \tVal Acc: 97.7647054%\n",
      "Epoch: 381\tTrain Loss: 0.3162737 \tVal Loss:0.0736185 \tTrain Acc: 90.20588% \tVal Acc: 98.7058806%\n",
      "Epoch: 382\tTrain Loss: 0.3037619 \tVal Loss:0.0724589 \tTrain Acc: 90.67647% \tVal Acc: 98.5882336%\n",
      "Epoch: 383\tTrain Loss: 0.2711745 \tVal Loss:0.0608888 \tTrain Acc: 91.76471% \tVal Acc: 98.7647051%\n",
      "Validation Loss decreased from 0.066258 to 0.060889, saving the model weights\n",
      "Epoch: 384\tTrain Loss: 0.2705927 \tVal Loss:0.0559531 \tTrain Acc: 91.92647% \tVal Acc: 98.7058812%\n",
      "Validation Loss decreased from 0.060889 to 0.055953, saving the model weights\n",
      "Epoch: 385\tTrain Loss: 0.2467959 \tVal Loss:0.0561642 \tTrain Acc: 92.55882% \tVal Acc: 98.8823521%\n",
      "Epoch: 386\tTrain Loss: 0.2766878 \tVal Loss:0.0673218 \tTrain Acc: 91.27941% \tVal Acc: 98.8823515%\n",
      "Epoch: 387\tTrain Loss: 0.2687124 \tVal Loss:0.0553803 \tTrain Acc: 91.55882% \tVal Acc: 98.8235283%\n",
      "Validation Loss decreased from 0.055953 to 0.055380, saving the model weights\n",
      "Epoch: 388\tTrain Loss: 0.2846006 \tVal Loss:0.0722136 \tTrain Acc: 91.05882% \tVal Acc: 98.4705871%\n",
      "Epoch: 389\tTrain Loss: 0.2994374 \tVal Loss:0.0874966 \tTrain Acc: 90.91176% \tVal Acc: 97.4117643%\n",
      "Epoch: 390\tTrain Loss: 0.3651820 \tVal Loss:0.1586803 \tTrain Acc: 88.63235% \tVal Acc: 95.7647043%\n",
      "Epoch: 391\tTrain Loss: 0.4206757 \tVal Loss:0.1781983 \tTrain Acc: 86.98529% \tVal Acc: 94.9411750%\n",
      "Epoch: 392\tTrain Loss: 0.5371049 \tVal Loss:0.2674221 \tTrain Acc: 83.89706% \tVal Acc: 92.5294107%\n",
      "Epoch: 393\tTrain Loss: 0.4615188 \tVal Loss:0.1306180 \tTrain Acc: 84.97059% \tVal Acc: 96.5294105%\n",
      "Epoch: 394\tTrain Loss: 0.3861626 \tVal Loss:0.1191139 \tTrain Acc: 87.94117% \tVal Acc: 96.7058808%\n",
      "Epoch: 395\tTrain Loss: 0.3453774 \tVal Loss:0.1290405 \tTrain Acc: 89.2647% \tVal Acc: 96.5882331%\n",
      "Epoch: 396\tTrain Loss: 0.3197435 \tVal Loss:0.0782803 \tTrain Acc: 90.13235% \tVal Acc: 98.2352918%\n",
      "Epoch: 397\tTrain Loss: 0.2797904 \tVal Loss:0.0630892 \tTrain Acc: 91.58823% \tVal Acc: 98.8823521%\n",
      "Epoch: 398\tTrain Loss: 0.2662055 \tVal Loss:0.1052012 \tTrain Acc: 91.7647% \tVal Acc: 97.7647048%\n",
      "Epoch: 399\tTrain Loss: 0.2726944 \tVal Loss:0.0604421 \tTrain Acc: 92.29412% \tVal Acc: 98.8235289%\n",
      "Epoch: 400\tTrain Loss: 0.2425137 \tVal Loss:0.0477349 \tTrain Acc: 92.57353% \tVal Acc: 98.9411759%\n",
      "Validation Loss decreased from 0.055380 to 0.047735, saving the model weights\n",
      "Epoch: 401\tTrain Loss: 0.2351405 \tVal Loss:0.0470630 \tTrain Acc: 92.94118% \tVal Acc: 99.1176456%\n",
      "Validation Loss decreased from 0.047735 to 0.047063, saving the model weights\n",
      "Epoch: 402\tTrain Loss: 0.2257855 \tVal Loss:0.0439382 \tTrain Acc: 92.91176% \tVal Acc: 99.1176462%\n",
      "Validation Loss decreased from 0.047063 to 0.043938, saving the model weights\n",
      "Epoch: 403\tTrain Loss: 0.2034512 \tVal Loss:0.0444917 \tTrain Acc: 94.17647% \tVal Acc: 99.0588224%\n",
      "Epoch: 404\tTrain Loss: 0.1980386 \tVal Loss:0.0413027 \tTrain Acc: 94.04412% \tVal Acc: 99.1764688%\n",
      "Validation Loss decreased from 0.043938 to 0.041303, saving the model weights\n",
      "Epoch: 405\tTrain Loss: 0.1954475 \tVal Loss:0.0463161 \tTrain Acc: 94.25% \tVal Acc: 99.0588224%\n",
      "Epoch: 406\tTrain Loss: 0.1846744 \tVal Loss:0.0432588 \tTrain Acc: 94.54412% \tVal Acc: 99.0588230%\n",
      "Epoch: 407\tTrain Loss: 0.1923397 \tVal Loss:0.0350013 \tTrain Acc: 94.5% \tVal Acc: 99.1764694%\n",
      "Validation Loss decreased from 0.041303 to 0.035001, saving the model weights\n",
      "Epoch: 408\tTrain Loss: 0.1961236 \tVal Loss:0.0356577 \tTrain Acc: 94.35294% \tVal Acc: 99.2352933%\n",
      "Epoch: 409\tTrain Loss: 0.1917025 \tVal Loss:0.0354871 \tTrain Acc: 94.5147% \tVal Acc: 99.2352933%\n",
      "Epoch: 410\tTrain Loss: 0.1825882 \tVal Loss:0.0312280 \tTrain Acc: 94.61765% \tVal Acc: 99.3529409%\n",
      "Validation Loss decreased from 0.035001 to 0.031228, saving the model weights\n",
      "Epoch: 411\tTrain Loss: 0.1971603 \tVal Loss:0.0345259 \tTrain Acc: 94.0% \tVal Acc: 99.4117635%\n",
      "Epoch: 412\tTrain Loss: 0.1935332 \tVal Loss:0.0403451 \tTrain Acc: 94.10294% \tVal Acc: 98.9999980%\n",
      "Epoch: 413\tTrain Loss: 0.1876883 \tVal Loss:0.0336043 \tTrain Acc: 94.60294% \tVal Acc: 99.2941165%\n",
      "Epoch: 414\tTrain Loss: 0.1988102 \tVal Loss:0.0380437 \tTrain Acc: 93.66176% \tVal Acc: 99.2352933%\n",
      "Epoch: 415\tTrain Loss: 0.1905782 \tVal Loss:0.0396810 \tTrain Acc: 94.5147% \tVal Acc: 99.1176462%\n",
      "Epoch: 416\tTrain Loss: 0.1969803 \tVal Loss:0.0422148 \tTrain Acc: 94.0% \tVal Acc: 99.1176462%\n",
      "Epoch: 417\tTrain Loss: 0.2339333 \tVal Loss:0.0634643 \tTrain Acc: 92.70588% \tVal Acc: 98.4117639%\n",
      "Epoch: 418\tTrain Loss: 0.2897722 \tVal Loss:0.1025395 \tTrain Acc: 91.08823% \tVal Acc: 97.4705863%\n",
      "Epoch: 419\tTrain Loss: 0.3351094 \tVal Loss:0.1077498 \tTrain Acc: 89.5147% \tVal Acc: 96.9999993%\n",
      "Epoch: 420\tTrain Loss: 0.3299897 \tVal Loss:0.0874807 \tTrain Acc: 89.26471% \tVal Acc: 97.8235281%\n",
      "Epoch: 421\tTrain Loss: 0.2941603 \tVal Loss:0.0636313 \tTrain Acc: 90.27941% \tVal Acc: 98.6470580%\n",
      "Epoch: 422\tTrain Loss: 0.2621319 \tVal Loss:0.0529427 \tTrain Acc: 91.55882% \tVal Acc: 98.5882342%\n",
      "Epoch: 423\tTrain Loss: 0.2335845 \tVal Loss:0.0447762 \tTrain Acc: 92.52941% \tVal Acc: 98.9411753%\n",
      "Epoch: 424\tTrain Loss: 0.2343288 \tVal Loss:0.0447175 \tTrain Acc: 92.55882% \tVal Acc: 98.7647051%\n",
      "Epoch: 425\tTrain Loss: 0.2149756 \tVal Loss:0.0442075 \tTrain Acc: 93.35294% \tVal Acc: 99.0588224%\n",
      "Epoch: 426\tTrain Loss: 0.1982477 \tVal Loss:0.0402578 \tTrain Acc: 93.89706% \tVal Acc: 98.9999992%\n",
      "Epoch: 427\tTrain Loss: 0.1909312 \tVal Loss:0.0677142 \tTrain Acc: 94.30882% \tVal Acc: 98.5882336%\n",
      "Epoch: 428\tTrain Loss: 0.1897310 \tVal Loss:0.0350269 \tTrain Acc: 94.58823% \tVal Acc: 99.2941171%\n",
      "Epoch: 429\tTrain Loss: 0.1782422 \tVal Loss:0.0302819 \tTrain Acc: 94.7647% \tVal Acc: 99.2941171%\n",
      "Validation Loss decreased from 0.031228 to 0.030282, saving the model weights\n",
      "Epoch: 430\tTrain Loss: 0.1714787 \tVal Loss:0.0260068 \tTrain Acc: 94.70588% \tVal Acc: 99.4117641%\n",
      "Validation Loss decreased from 0.030282 to 0.026007, saving the model weights\n",
      "Epoch: 431\tTrain Loss: 0.1690448 \tVal Loss:0.0318096 \tTrain Acc: 94.75% \tVal Acc: 99.1764694%\n",
      "Epoch: 432\tTrain Loss: 0.1901734 \tVal Loss:0.0454431 \tTrain Acc: 94.41176% \tVal Acc: 98.8823515%\n",
      "Epoch: 433\tTrain Loss: 0.2412524 \tVal Loss:0.0503521 \tTrain Acc: 92.60294% \tVal Acc: 98.7647057%\n",
      "Epoch: 434\tTrain Loss: 0.2074475 \tVal Loss:0.0425034 \tTrain Acc: 93.52941% \tVal Acc: 99.0588218%\n",
      "Epoch: 435\tTrain Loss: 0.1958349 \tVal Loss:0.0355165 \tTrain Acc: 93.89706% \tVal Acc: 98.9999986%\n",
      "Epoch: 436\tTrain Loss: 0.1979085 \tVal Loss:0.0377430 \tTrain Acc: 93.85294% \tVal Acc: 98.9411753%\n",
      "Epoch: 437\tTrain Loss: 0.2046260 \tVal Loss:0.0550437 \tTrain Acc: 93.91176% \tVal Acc: 98.9999986%\n",
      "Epoch: 438\tTrain Loss: 0.2264732 \tVal Loss:0.0495397 \tTrain Acc: 92.94118% \tVal Acc: 98.7058818%\n",
      "Epoch: 439\tTrain Loss: 0.2325283 \tVal Loss:0.0736943 \tTrain Acc: 92.82353% \tVal Acc: 98.1176454%\n",
      "Epoch: 440\tTrain Loss: 0.2918174 \tVal Loss:0.1202475 \tTrain Acc: 90.57353% \tVal Acc: 96.7058814%\n",
      "Epoch: 441\tTrain Loss: 0.3477424 \tVal Loss:0.1197257 \tTrain Acc: 89.58823% \tVal Acc: 96.4117634%\n",
      "Epoch: 442\tTrain Loss: 0.3054886 \tVal Loss:0.0749996 \tTrain Acc: 90.25% \tVal Acc: 98.3529401%\n",
      "Epoch: 443\tTrain Loss: 0.2600357 \tVal Loss:0.0589667 \tTrain Acc: 92.0147% \tVal Acc: 98.3529401%\n",
      "Epoch: 444\tTrain Loss: 0.2226729 \tVal Loss:0.0326440 \tTrain Acc: 92.69118% \tVal Acc: 99.2941165%\n",
      "Epoch: 445\tTrain Loss: 0.1873143 \tVal Loss:0.0351968 \tTrain Acc: 94.20588% \tVal Acc: 99.1176462%\n",
      "Epoch: 446\tTrain Loss: 0.1711972 \tVal Loss:0.0266310 \tTrain Acc: 94.82353% \tVal Acc: 99.4117641%\n",
      "Epoch: 447\tTrain Loss: 0.1588564 \tVal Loss:0.0310618 \tTrain Acc: 95.29412% \tVal Acc: 99.1764694%\n",
      "Epoch: 448\tTrain Loss: 0.1529613 \tVal Loss:0.0314060 \tTrain Acc: 95.33823% \tVal Acc: 99.1176462%\n",
      "Epoch: 449\tTrain Loss: 0.1533311 \tVal Loss:0.0368796 \tTrain Acc: 95.7647% \tVal Acc: 99.0588224%\n",
      "Epoch: 450\tTrain Loss: 0.1673418 \tVal Loss:0.0326155 \tTrain Acc: 94.98529% \tVal Acc: 99.2352939%\n",
      "Epoch: 451\tTrain Loss: 0.1563326 \tVal Loss:0.0239774 \tTrain Acc: 95.27941% \tVal Acc: 99.4705880%\n",
      "Validation Loss decreased from 0.026007 to 0.023977, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 452\tTrain Loss: 0.1461499 \tVal Loss:0.0244066 \tTrain Acc: 95.66176% \tVal Acc: 99.3529403%\n",
      "Epoch: 453\tTrain Loss: 0.1531582 \tVal Loss:0.0230362 \tTrain Acc: 95.29412% \tVal Acc: 99.3529403%\n",
      "Validation Loss decreased from 0.023977 to 0.023036, saving the model weights\n",
      "Epoch: 454\tTrain Loss: 0.1533784 \tVal Loss:0.0272051 \tTrain Acc: 95.23529% \tVal Acc: 99.3529397%\n",
      "Epoch: 455\tTrain Loss: 0.1613232 \tVal Loss:0.0248017 \tTrain Acc: 95.0% \tVal Acc: 99.4117635%\n",
      "Epoch: 456\tTrain Loss: 0.1502834 \tVal Loss:0.0192170 \tTrain Acc: 95.5% \tVal Acc: 99.5882344%\n",
      "Validation Loss decreased from 0.023036 to 0.019217, saving the model weights\n",
      "Epoch: 457\tTrain Loss: 0.1412588 \tVal Loss:0.0212054 \tTrain Acc: 95.88235% \tVal Acc: 99.4117635%\n",
      "Epoch: 458\tTrain Loss: 0.1800799 \tVal Loss:0.0268418 \tTrain Acc: 94.55882% \tVal Acc: 99.2941165%\n",
      "Epoch: 459\tTrain Loss: 0.1692574 \tVal Loss:0.0283794 \tTrain Acc: 94.75% \tVal Acc: 99.2941165%\n",
      "Epoch: 460\tTrain Loss: 0.1516984 \tVal Loss:0.0283722 \tTrain Acc: 95.42647% \tVal Acc: 99.1176462%\n",
      "Epoch: 461\tTrain Loss: 0.1681529 \tVal Loss:0.0252064 \tTrain Acc: 94.91176% \tVal Acc: 99.3529403%\n",
      "Epoch: 462\tTrain Loss: 0.1614778 \tVal Loss:0.0380709 \tTrain Acc: 95.02941% \tVal Acc: 99.1764694%\n",
      "Epoch: 463\tTrain Loss: 0.1589674 \tVal Loss:0.0263373 \tTrain Acc: 95.27941% \tVal Acc: 99.2352933%\n",
      "Epoch: 464\tTrain Loss: 0.1518243 \tVal Loss:0.0243337 \tTrain Acc: 95.20588% \tVal Acc: 99.3529403%\n",
      "Epoch: 465\tTrain Loss: 0.1626939 \tVal Loss:0.0289604 \tTrain Acc: 94.94118% \tVal Acc: 99.1764694%\n",
      "Epoch: 466\tTrain Loss: 0.1696177 \tVal Loss:0.0273489 \tTrain Acc: 94.82353% \tVal Acc: 99.3529397%\n",
      "Epoch: 467\tTrain Loss: 0.1704228 \tVal Loss:0.0270991 \tTrain Acc: 94.80882% \tVal Acc: 99.2941165%\n",
      "Epoch: 468\tTrain Loss: 0.1659819 \tVal Loss:0.0293035 \tTrain Acc: 95.16176% \tVal Acc: 99.1176456%\n",
      "Epoch: 469\tTrain Loss: 0.1803098 \tVal Loss:0.0443026 \tTrain Acc: 94.39706% \tVal Acc: 98.7058812%\n",
      "Epoch: 470\tTrain Loss: 0.1960886 \tVal Loss:0.0316770 \tTrain Acc: 94.35294% \tVal Acc: 99.1764694%\n",
      "Epoch: 471\tTrain Loss: 0.1839153 \tVal Loss:0.0405605 \tTrain Acc: 94.25% \tVal Acc: 98.8235277%\n",
      "Epoch: 472\tTrain Loss: 0.1864268 \tVal Loss:0.0477270 \tTrain Acc: 94.0% \tVal Acc: 98.6470580%\n",
      "Epoch: 473\tTrain Loss: 0.1976951 \tVal Loss:0.0481290 \tTrain Acc: 93.77941% \tVal Acc: 98.7647051%\n",
      "Epoch: 474\tTrain Loss: 0.2256788 \tVal Loss:0.0545692 \tTrain Acc: 92.91176% \tVal Acc: 98.6470574%\n",
      "Epoch: 475\tTrain Loss: 0.2326943 \tVal Loss:0.0473383 \tTrain Acc: 92.77941% \tVal Acc: 98.6470574%\n",
      "Epoch: 476\tTrain Loss: 0.2245752 \tVal Loss:0.0692490 \tTrain Acc: 93.02941% \tVal Acc: 98.1176448%\n",
      "Epoch: 477\tTrain Loss: 0.2429127 \tVal Loss:0.1097961 \tTrain Acc: 92.23529% \tVal Acc: 97.0588225%\n",
      "Epoch: 478\tTrain Loss: 0.2556099 \tVal Loss:0.0615861 \tTrain Acc: 92.02941% \tVal Acc: 98.2941163%\n",
      "Epoch: 479\tTrain Loss: 0.2499898 \tVal Loss:0.0657051 \tTrain Acc: 92.22059% \tVal Acc: 97.8823513%\n",
      "Epoch: 480\tTrain Loss: 0.2107183 \tVal Loss:0.0416329 \tTrain Acc: 93.17647% \tVal Acc: 98.9411759%\n",
      "Epoch: 481\tTrain Loss: 0.2680813 \tVal Loss:0.0637604 \tTrain Acc: 92.32353% \tVal Acc: 98.1764704%\n",
      "Epoch: 482\tTrain Loss: 0.2303215 \tVal Loss:0.0407782 \tTrain Acc: 92.64706% \tVal Acc: 99.0588230%\n",
      "Epoch: 483\tTrain Loss: 0.2142951 \tVal Loss:0.0451188 \tTrain Acc: 93.57353% \tVal Acc: 98.8823515%\n",
      "Epoch: 484\tTrain Loss: 0.1909221 \tVal Loss:0.0371494 \tTrain Acc: 93.7647% \tVal Acc: 98.9999986%\n",
      "Epoch: 485\tTrain Loss: 0.1611173 \tVal Loss:0.0244526 \tTrain Acc: 95.16176% \tVal Acc: 99.2352927%\n",
      "Epoch: 486\tTrain Loss: 0.1495133 \tVal Loss:0.0340257 \tTrain Acc: 95.64706% \tVal Acc: 98.9999986%\n",
      "Epoch: 487\tTrain Loss: 0.1433197 \tVal Loss:0.0250937 \tTrain Acc: 96.02941% \tVal Acc: 99.1764694%\n",
      "Epoch: 488\tTrain Loss: 0.1549006 \tVal Loss:0.0363989 \tTrain Acc: 95.32353% \tVal Acc: 99.0588224%\n",
      "Epoch: 489\tTrain Loss: 0.1523777 \tVal Loss:0.0174168 \tTrain Acc: 95.32353% \tVal Acc: 99.5294106%\n",
      "Validation Loss decreased from 0.019217 to 0.017417, saving the model weights\n",
      "Epoch: 490\tTrain Loss: 0.1256165 \tVal Loss:0.0139586 \tTrain Acc: 96.25% \tVal Acc: 99.5294112%\n",
      "Validation Loss decreased from 0.017417 to 0.013959, saving the model weights\n",
      "Epoch: 491\tTrain Loss: 0.1194603 \tVal Loss:0.0193804 \tTrain Acc: 96.72059% \tVal Acc: 99.5882344%\n",
      "Epoch: 492\tTrain Loss: 0.1174559 \tVal Loss:0.0253187 \tTrain Acc: 96.42647% \tVal Acc: 99.3529409%\n",
      "Epoch: 493\tTrain Loss: 0.1219954 \tVal Loss:0.0353008 \tTrain Acc: 96.60294% \tVal Acc: 99.1176462%\n",
      "Epoch: 494\tTrain Loss: 0.1209320 \tVal Loss:0.0172595 \tTrain Acc: 96.66176% \tVal Acc: 99.5294106%\n",
      "Epoch: 495\tTrain Loss: 0.1130567 \tVal Loss:0.0154921 \tTrain Acc: 96.52941% \tVal Acc: 99.5882344%\n",
      "Epoch: 496\tTrain Loss: 0.1203936 \tVal Loss:0.0166018 \tTrain Acc: 96.20588% \tVal Acc: 99.5294106%\n",
      "Epoch: 497\tTrain Loss: 0.1102642 \tVal Loss:0.0144843 \tTrain Acc: 96.75% \tVal Acc: 99.5294106%\n",
      "Epoch: 498\tTrain Loss: 0.1153446 \tVal Loss:0.0167734 \tTrain Acc: 96.75% \tVal Acc: 99.2941171%\n",
      "Epoch: 499\tTrain Loss: 0.1048777 \tVal Loss:0.0154696 \tTrain Acc: 96.83823% \tVal Acc: 99.5882344%\n",
      "Epoch: 500\tTrain Loss: 0.1095890 \tVal Loss:0.0164741 \tTrain Acc: 96.80882% \tVal Acc: 99.4705874%\n",
      "Epoch: 501\tTrain Loss: 0.1143969 \tVal Loss:0.0148827 \tTrain Acc: 96.64706% \tVal Acc: 99.5294112%\n",
      "Epoch: 502\tTrain Loss: 0.1163743 \tVal Loss:0.0155627 \tTrain Acc: 96.36765% \tVal Acc: 99.5882344%\n",
      "Epoch: 503\tTrain Loss: 0.1092710 \tVal Loss:0.0182375 \tTrain Acc: 96.83823% \tVal Acc: 99.3529409%\n",
      "Epoch: 504\tTrain Loss: 0.1135478 \tVal Loss:0.0198982 \tTrain Acc: 96.52941% \tVal Acc: 99.2941165%\n",
      "Epoch: 505\tTrain Loss: 0.1581902 \tVal Loss:0.0952802 \tTrain Acc: 95.52941% \tVal Acc: 97.7058810%\n",
      "Epoch: 506\tTrain Loss: 0.3532725 \tVal Loss:0.2041644 \tTrain Acc: 89.29412% \tVal Acc: 94.1764688%\n",
      "Epoch: 507\tTrain Loss: 0.4774899 \tVal Loss:0.2290965 \tTrain Acc: 86.17647% \tVal Acc: 93.2352930%\n",
      "Epoch: 508\tTrain Loss: 0.4651676 \tVal Loss:0.1405053 \tTrain Acc: 86.39706% \tVal Acc: 95.6470579%\n",
      "Epoch: 509\tTrain Loss: 0.4086101 \tVal Loss:0.1409244 \tTrain Acc: 87.48529% \tVal Acc: 95.5882335%\n",
      "Epoch: 510\tTrain Loss: 0.3056139 \tVal Loss:0.0665223 \tTrain Acc: 90.25% \tVal Acc: 98.7647045%\n",
      "Epoch: 511\tTrain Loss: 0.2113624 \tVal Loss:0.0414343 \tTrain Acc: 93.41176% \tVal Acc: 98.8235289%\n",
      "Epoch: 512\tTrain Loss: 0.1946701 \tVal Loss:0.0285353 \tTrain Acc: 93.92647% \tVal Acc: 99.2352939%\n",
      "Epoch: 513\tTrain Loss: 0.1624920 \tVal Loss:0.0191962 \tTrain Acc: 95.13235% \tVal Acc: 99.4705874%\n",
      "Epoch: 514\tTrain Loss: 0.1316329 \tVal Loss:0.0201699 \tTrain Acc: 96.0% \tVal Acc: 99.5294106%\n",
      "Epoch: 515\tTrain Loss: 0.1245867 \tVal Loss:0.0166048 \tTrain Acc: 96.69118% \tVal Acc: 99.5294112%\n",
      "Epoch: 516\tTrain Loss: 0.1184878 \tVal Loss:0.0135837 \tTrain Acc: 96.55882% \tVal Acc: 99.5882344%\n",
      "Validation Loss decreased from 0.013959 to 0.013584, saving the model weights\n",
      "Epoch: 517\tTrain Loss: 0.1508986 \tVal Loss:0.1055469 \tTrain Acc: 95.41176% \tVal Acc: 97.4117631%\n",
      "Epoch: 518\tTrain Loss: 0.2228613 \tVal Loss:0.0544126 \tTrain Acc: 93.08823% \tVal Acc: 98.7647051%\n",
      "Epoch: 519\tTrain Loss: 0.1965979 \tVal Loss:0.0349186 \tTrain Acc: 93.80882% \tVal Acc: 98.9999986%\n",
      "Epoch: 520\tTrain Loss: 0.1509719 \tVal Loss:0.0210088 \tTrain Acc: 95.32353% \tVal Acc: 99.4117641%\n",
      "Epoch: 521\tTrain Loss: 0.1353206 \tVal Loss:0.0161697 \tTrain Acc: 95.7647% \tVal Acc: 99.4117635%\n",
      "Epoch: 522\tTrain Loss: 0.1187798 \tVal Loss:0.0136786 \tTrain Acc: 96.63235% \tVal Acc: 99.5882338%\n",
      "Epoch: 523\tTrain Loss: 0.1165787 \tVal Loss:0.0124482 \tTrain Acc: 96.64706% \tVal Acc: 99.7058815%\n",
      "Validation Loss decreased from 0.013584 to 0.012448, saving the model weights\n",
      "Epoch: 524\tTrain Loss: 0.1120275 \tVal Loss:0.0124404 \tTrain Acc: 96.72059% \tVal Acc: 99.5882350%\n",
      "Validation Loss decreased from 0.012448 to 0.012440, saving the model weights\n",
      "Epoch: 525\tTrain Loss: 0.0950396 \tVal Loss:0.0144658 \tTrain Acc: 97.25% \tVal Acc: 99.4705874%\n",
      "Epoch: 526\tTrain Loss: 0.0965121 \tVal Loss:0.0130384 \tTrain Acc: 97.04412% \tVal Acc: 99.6470577%\n",
      "Epoch: 527\tTrain Loss: 0.0969741 \tVal Loss:0.0124081 \tTrain Acc: 97.38235% \tVal Acc: 99.6470582%\n",
      "Validation Loss decreased from 0.012440 to 0.012408, saving the model weights\n",
      "Epoch: 528\tTrain Loss: 0.0972122 \tVal Loss:0.0133479 \tTrain Acc: 97.27941% \tVal Acc: 99.7058815%\n",
      "Epoch: 529\tTrain Loss: 0.0995025 \tVal Loss:0.0176499 \tTrain Acc: 97.11765% \tVal Acc: 99.4117635%\n",
      "Epoch: 530\tTrain Loss: 0.1486954 \tVal Loss:0.0287153 \tTrain Acc: 95.7647% \tVal Acc: 99.1764688%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 531\tTrain Loss: 0.1734339 \tVal Loss:0.0495225 \tTrain Acc: 94.91176% \tVal Acc: 98.7058812%\n",
      "Epoch: 532\tTrain Loss: 0.2016378 \tVal Loss:0.0241871 \tTrain Acc: 94.19118% \tVal Acc: 99.3529403%\n",
      "Epoch: 533\tTrain Loss: 0.1663997 \tVal Loss:0.0193841 \tTrain Acc: 94.63235% \tVal Acc: 99.4705880%\n",
      "Epoch: 534\tTrain Loss: 0.1563156 \tVal Loss:0.0180090 \tTrain Acc: 95.32353% \tVal Acc: 99.6470577%\n",
      "Epoch: 535\tTrain Loss: 0.1419515 \tVal Loss:0.0221576 \tTrain Acc: 95.64706% \tVal Acc: 99.3529403%\n",
      "Epoch: 536\tTrain Loss: 0.1441956 \tVal Loss:0.0167254 \tTrain Acc: 95.41176% \tVal Acc: 99.4705880%\n",
      "Epoch: 537\tTrain Loss: 0.1340451 \tVal Loss:0.0223833 \tTrain Acc: 96.02941% \tVal Acc: 99.2352927%\n",
      "Epoch: 538\tTrain Loss: 0.1217507 \tVal Loss:0.0223136 \tTrain Acc: 96.42647% \tVal Acc: 99.2941171%\n",
      "Epoch: 539\tTrain Loss: 0.1075294 \tVal Loss:0.0306872 \tTrain Acc: 96.98529% \tVal Acc: 99.2941171%\n",
      "Epoch: 540\tTrain Loss: 0.1181329 \tVal Loss:0.0143601 \tTrain Acc: 96.67647% \tVal Acc: 99.5294112%\n",
      "Epoch: 541\tTrain Loss: 0.1022262 \tVal Loss:0.0159259 \tTrain Acc: 96.7647% \tVal Acc: 99.2352933%\n",
      "Epoch: 542\tTrain Loss: 0.1065479 \tVal Loss:0.0166234 \tTrain Acc: 96.95588% \tVal Acc: 99.5294112%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-b2f81e3835c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mval_h1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meach\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhidden1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset_fetcher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# may raise StopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontainer_abcs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m         \u001b[0mtransposed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdefault_collate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melem_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m     54\u001b[0m             \u001b[0mstorage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0melem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'numpy'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'str_'\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0melem_type\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'string_'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    \n",
    "    hidden1, hidden2 = model.hidden_init(train_batch_size)    \n",
    "    #print('hidden[0].shape:- ',hidden[0].shape)\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        '''\n",
    "        Creating new variables for the hidden state, otherwise\n",
    "        we'd backprop through the entire training history\n",
    "        '''\n",
    "        #h = tuple([each.data for each in hidden])\n",
    "        h1 = tuple([each.data for each in hidden1])\n",
    "        h2 = tuple([each.data for each in hidden2])\n",
    "        \n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "       \n",
    "        # get the output from the model\n",
    "        output, h = model.forward(inputs, h1, h2, train_batch_size)\n",
    "        #print('OUTPUT', output)\n",
    "        \n",
    "        \n",
    "        #print('Labels Shape :-', (torch.max(labels, 1)[1]).shape)\n",
    "    \n",
    "        # calculate the loss and perform backprop\n",
    "        #print('Labels Long :-', labels.long())\n",
    "        loss = criterion(output,labels.long())\n",
    "        #print('LOSS IS :-', loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #calculate training accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        #logging.debug(' top probab {} top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        #print(train_loss)\n",
    "              \n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "                \n",
    "        val_h1 = tuple([each.data for each in hidden1])\n",
    "        val_h2 = tuple([each.data for each in hidden2])\n",
    "        \n",
    "        output, hidden = model.forward(inputs, val_h1, val_h2,val_batch_size)\n",
    "       \n",
    "        loss = criterion(output,labels.long())\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        #calculate validation accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        \n",
    "        #logging.debug(output)\n",
    "        #logging.debug('VALIDATION top probab {} VALIDATION top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        #print('Top Class:- ',top_class)\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        #print('Equals:- ', equals)\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    #Averaging losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    val_accuracy = val_accuracy/len(val_loader)\n",
    "    train_accuracy = train_accuracy/len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}\\tTrain Loss: {:.7f} \\tVal Loss:{:.7f} \\tTrain Acc: {:.7}% \\tVal Acc: {:.7f}%'.format(e, train_loss, val_loss, train_accuracy*100,val_accuracy*100))\n",
    "    \n",
    "    #saving the model if validation loss is decreased\n",
    "    if val_loss <= min_val_loss:\n",
    "        print('Validation Loss decreased from {:6f} to {:6f}, saving the model weights'.format(min_val_loss, val_loss))\n",
    "        torch.save(model.state_dict(), 'lstm_state_256_38.pt')\n",
    "        min_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MUSIC GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm1): LSTM(1, 256, batch_first=True)\n",
       "  (lstm2): LSTM(256, 38, batch_first=True)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear): Linear(in_features=38, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights\n",
    "test_model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "test_model.load_state_dict(torch.load('lstm_state_256_38.pt'))\n",
    "test_model.eval()\n",
    "test_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population database\n",
    "testing_data = np.ones(200)*1\n",
    "# testing_data = list(range(50,90))\n",
    "# testing_data.extend(testing_data[::-1])\n",
    "# testing_data_rev = testing_data[::-1]\n",
    "# testing_data_rev.extend(testing_data)\n",
    "# testing_data = testing_data_rev\n",
    "\n",
    "\n",
    "testing_data = np.asarray(testing_data)\n",
    "testing_data = testing_data.reshape(testing_data.shape[0],1)\n",
    "\n",
    "initial_seq = [network_input[0][1:].cpu().numpy().tolist()]\n",
    "\n",
    "testing_data_unnorm = testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "testing_data=testing_data.tolist()\n",
    "for i in range(len(testing_data)):\n",
    "    list1.extend(testing_data[i])\n",
    "\n",
    "#list1\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    list1[i]=(list1[i]-50)/(89-50)\n",
    "\n",
    "list1 = np.asarray(list1)\n",
    "list1 = list1.reshape(list1.shape[0],1)\n",
    "testing_data = list1\n",
    "#list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting\n",
    "def prediction_with_influence(influence,int2note,initial_seq, max_note, test_batch_size = 1):\n",
    "\n",
    "    predicted_notes = []\n",
    "    initial_seq[0].extend([[0]]*len(testing_data))\n",
    "    test_seq = torch.Tensor(initial_seq).cuda()\n",
    "    \n",
    "    h1, h2 = test_model.hidden_init(test_batch_size)\n",
    "\n",
    "    \n",
    "    for i in range(len(influence)):\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = float(influence[i])\n",
    "        \n",
    "        test_slice = test_seq[0][i : i + sequence_length]        \n",
    "        test_slice = test_slice.view(1, test_slice.shape[0], test_slice.shape[1])\n",
    "                \n",
    "        test_hidden1 = tuple([each.data for each in h1])\n",
    "        test_hidden2 = tuple([each.data for each in h2])\n",
    "        \n",
    "        test_output,_ = test_model.forward(test_slice, test_hidden1, test_hidden2, test_batch_size)\n",
    "    \n",
    "        test_output = F.softmax(test_output, dim = 1)\n",
    "        top_p, top_class = test_output.topk(1,dim =1)\n",
    "        test_seq[0][sequence_length - 1 + i][0] = int2note[top_class.item()]/max_note\n",
    "        \n",
    "        predicted_notes.append(int2note[top_class.item()])\n",
    "        \n",
    "    return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_notes_lst = prediction_with_influence(testing_data,int_to_note,initial_seq, max_midi_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_notes_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x242a2573080>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 369
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(np.ones(200)*89))\n",
    "\n",
    "#plt.plot(list(np.ones(200)*50))\n",
    "#plt.plot(list(np.ones(20)*50))\n",
    "#plt.plot(testing_data_unnorm)\n",
    "plt.plot(predicted_notes_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({72: 1,\n",
       "         77: 1,\n",
       "         76: 4,\n",
       "         81: 38,\n",
       "         79: 15,\n",
       "         83: 33,\n",
       "         84: 50,\n",
       "         69: 2,\n",
       "         67: 1,\n",
       "         74: 1,\n",
       "         85: 7,\n",
       "         86: 4,\n",
       "         78: 3})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(predicted_notes_lst)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-9c99f3e2f730>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
