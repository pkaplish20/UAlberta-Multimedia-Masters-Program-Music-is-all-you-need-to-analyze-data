{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.tensorboard as tb\n",
    "from Preprocessing.preprocessing import PreprocessingTrainingData\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as  plt\n",
    "import os\n",
    "import logging\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static parameters\n",
    "train_batch_size = 170\n",
    "val_batch_size = 170\n",
    "sequence_length=50\n",
    "test_batch_size = 1\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layer = 2\n",
    "output_size = 38\n",
    "clip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from preprocessing.py\n",
    "dataset_path = os.path.join(os.path.abspath('..'),'Dataset\\\\Clementi dataset\\\\Clementi dataset' )\n",
    "network_input,network_output,max_midi_number,min_midi_number,int_to_note = PreprocessingTrainingData().preprocess_notes(dataset_path)\n",
    "network_input, network_output = network_input.cuda(), network_output.cuda()\n",
    "\n",
    "# print(network_input)\n",
    "#print(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(network_output.max())\n",
    "print(network_output.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8617])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "89\n",
      "50\n",
      "{0: 50, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61, 11: 62, 12: 63, 13: 64, 14: 65, 15: 66, 16: 67, 17: 68, 18: 69, 19: 70, 20: 71, 21: 72, 22: 73, 23: 74, 24: 75, 25: 76, 26: 77, 27: 78, 28: 79, 29: 80, 30: 81, 31: 82, 32: 83, 33: 84, 34: 85, 35: 86, 36: 88, 37: 89}\n"
     ]
    }
   ],
   "source": [
    "print(network_input.max())\n",
    "print(network_input.min())\n",
    "print(max_midi_number)\n",
    "print(min_midi_number)\n",
    "print(int_to_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8617, 50, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata is highly unbalanced\\n# '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data is highly unbalanced\n",
    "# '''\n",
    "# sns.distplot(torch.tensor(network_output).cpu())\n",
    "# xx = pd.DataFrame(torch.tensor(network_output).cpu())\n",
    "# xx.groupby(0).size().to_frame(name='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8500, 50, 1])\n",
      "torch.Size([8500])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "to make batch of equal sizes\n",
    "Quick Fix\n",
    "'''\n",
    "network_input = network_input[: -117]\n",
    "network_output = network_output[: -117]\n",
    "\n",
    "print(network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create Stacked LSTM model\n",
    "'''\n",
    "class Stacked_LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True, dropout = 0.5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden,batch_size):\n",
    "        \n",
    "        output, hidden = self.lstm(x, hidden)        \n",
    "        #print(output.shape)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        output = output.contiguous().view(-1, self.hidden_size)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        #print('Linear Output :-',output.shape)\n",
    "        \n",
    "        #output = F.softmax(output, dim = 1)\n",
    "        #print('SOFTMAX OUTPUT :--', output)\n",
    "        \n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1)\n",
    "        #print('Reshape to batch size first :-',output.shape)\n",
    "        \n",
    "        output = output[:, -self.output_size:] # get last batch of labels\n",
    "        #print('Final Output :-',output)\n",
    "        #print('RESHAPE SIZE :-', output.shape)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def hidden_init(self,batch_size):\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_().cuda(),\n",
    "          weight.new(self.num_layers, batch_size, self.hidden_size).zero_().cuda())\n",
    "        return hidden\n",
    "\n",
    "#initialize the weights of LSTM using Xavier initialization    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Divide the dataset into train/val \n",
    "'''\n",
    "train_size = 0.8\n",
    "indices = list(range(len(network_input)))\n",
    "split = int(np.floor(train_size*len(network_input)))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SequentialSampler(train_idx)\n",
    "val_sampler = SequentialSampler(val_idx)\n",
    "\n",
    "dataset = TensorDataset(network_input,network_output)\n",
    "train_loader = DataLoader(dataset, batch_size= train_batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size= val_batch_size,sampler= val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=256, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optimizer\n",
    "\n",
    "model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer.AdamW(model.parameters())\n",
    "#optimizer = optimizer.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "#make sure to transfer model to GPU after initializing optimizer\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 3.2581464 \tVal Loss:2.9775508 \tTrain Acc: 7.544118% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from    inf to 2.977551, saving the model weights\n",
      "Epoch: 1\tTrain Loss: 3.1063758 \tVal Loss:2.9724586 \tTrain Acc: 8.808824% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.977551 to 2.972459, saving the model weights\n",
      "Epoch: 2\tTrain Loss: 3.0548235 \tVal Loss:2.9512067 \tTrain Acc: 9.470588% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.972459 to 2.951207, saving the model weights\n",
      "Epoch: 3\tTrain Loss: 3.0056863 \tVal Loss:2.8510328 \tTrain Acc: 10.35294% \tVal Acc: 13.2352945%\n",
      "Validation Loss decreased from 2.951207 to 2.851033, saving the model weights\n",
      "Epoch: 4\tTrain Loss: 2.9115250 \tVal Loss:2.7754004 \tTrain Acc: 11.57353% \tVal Acc: 13.0000004%\n",
      "Validation Loss decreased from 2.851033 to 2.775400, saving the model weights\n",
      "Epoch: 5\tTrain Loss: 2.8379992 \tVal Loss:2.7353734 \tTrain Acc: 12.72059% \tVal Acc: 12.1176474%\n",
      "Validation Loss decreased from 2.775400 to 2.735373, saving the model weights\n",
      "Epoch: 6\tTrain Loss: 2.7532906 \tVal Loss:2.6444221 \tTrain Acc: 13.44118% \tVal Acc: 14.3529417%\n",
      "Validation Loss decreased from 2.735373 to 2.644422, saving the model weights\n",
      "Epoch: 7\tTrain Loss: 2.7017272 \tVal Loss:2.6138724 \tTrain Acc: 14.36765% \tVal Acc: 14.0000005%\n",
      "Validation Loss decreased from 2.644422 to 2.613872, saving the model weights\n",
      "Epoch: 8\tTrain Loss: 2.6581584 \tVal Loss:2.5570661 \tTrain Acc: 14.66177% \tVal Acc: 13.5294122%\n",
      "Validation Loss decreased from 2.613872 to 2.557066, saving the model weights\n",
      "Epoch: 9\tTrain Loss: 2.6127121 \tVal Loss:2.5150196 \tTrain Acc: 15.08824% \tVal Acc: 13.5882356%\n",
      "Validation Loss decreased from 2.557066 to 2.515020, saving the model weights\n",
      "Epoch: 10\tTrain Loss: 2.5923380 \tVal Loss:2.4998930 \tTrain Acc: 15.29412% \tVal Acc: 13.7647061%\n",
      "Validation Loss decreased from 2.515020 to 2.499893, saving the model weights\n",
      "Epoch: 11\tTrain Loss: 2.5686122 \tVal Loss:2.4802244 \tTrain Acc: 15.70588% \tVal Acc: 14.9411769%\n",
      "Validation Loss decreased from 2.499893 to 2.480224, saving the model weights\n",
      "Epoch: 12\tTrain Loss: 2.5466932 \tVal Loss:2.4945197 \tTrain Acc: 16.19118% \tVal Acc: 15.2352946%\n",
      "Epoch: 13\tTrain Loss: 2.5322888 \tVal Loss:2.4656049 \tTrain Acc: 16.32353% \tVal Acc: 14.3529415%\n",
      "Validation Loss decreased from 2.480224 to 2.465605, saving the model weights\n",
      "Epoch: 14\tTrain Loss: 2.5168304 \tVal Loss:2.4597381 \tTrain Acc: 16.64706% \tVal Acc: 15.1176475%\n",
      "Validation Loss decreased from 2.465605 to 2.459738, saving the model weights\n",
      "Epoch: 15\tTrain Loss: 2.5039494 \tVal Loss:2.4332960 \tTrain Acc: 16.27941% \tVal Acc: 14.4705885%\n",
      "Validation Loss decreased from 2.459738 to 2.433296, saving the model weights\n",
      "Epoch: 16\tTrain Loss: 2.4963141 \tVal Loss:2.4447117 \tTrain Acc: 16.79412% \tVal Acc: 14.8823533%\n",
      "Epoch: 17\tTrain Loss: 2.4811555 \tVal Loss:2.4304648 \tTrain Acc: 16.92647% \tVal Acc: 14.0000004%\n",
      "Validation Loss decreased from 2.433296 to 2.430465, saving the model weights\n",
      "Epoch: 18\tTrain Loss: 2.4757531 \tVal Loss:2.4141522 \tTrain Acc: 17.48529% \tVal Acc: 15.1176475%\n",
      "Validation Loss decreased from 2.430465 to 2.414152, saving the model weights\n",
      "Epoch: 19\tTrain Loss: 2.4713596 \tVal Loss:2.4174933 \tTrain Acc: 17.54412% \tVal Acc: 15.4705887%\n",
      "Epoch: 20\tTrain Loss: 2.4651116 \tVal Loss:2.4232674 \tTrain Acc: 17.72059% \tVal Acc: 15.0588238%\n",
      "Epoch: 21\tTrain Loss: 2.4608151 \tVal Loss:2.4116777 \tTrain Acc: 18.07353% \tVal Acc: 16.5882356%\n",
      "Validation Loss decreased from 2.414152 to 2.411678, saving the model weights\n",
      "Epoch: 22\tTrain Loss: 2.4479691 \tVal Loss:2.3894690 \tTrain Acc: 18.79412% \tVal Acc: 16.7058828%\n",
      "Validation Loss decreased from 2.411678 to 2.389469, saving the model weights\n",
      "Epoch: 23\tTrain Loss: 2.4423002 \tVal Loss:2.4166257 \tTrain Acc: 18.76471% \tVal Acc: 17.0000004%\n",
      "Epoch: 24\tTrain Loss: 2.4344047 \tVal Loss:2.3895384 \tTrain Acc: 19.01471% \tVal Acc: 16.8235298%\n",
      "Epoch: 25\tTrain Loss: 2.4303251 \tVal Loss:2.3966728 \tTrain Acc: 18.64706% \tVal Acc: 17.8235298%\n",
      "Epoch: 26\tTrain Loss: 2.4261535 \tVal Loss:2.3902290 \tTrain Acc: 19.47059% \tVal Acc: 17.7647063%\n",
      "Epoch: 27\tTrain Loss: 2.4170862 \tVal Loss:2.3708976 \tTrain Acc: 19.5% \tVal Acc: 18.4117651%\n",
      "Validation Loss decreased from 2.389469 to 2.370898, saving the model weights\n",
      "Epoch: 28\tTrain Loss: 2.3992056 \tVal Loss:2.3551537 \tTrain Acc: 20.45588% \tVal Acc: 19.7058827%\n",
      "Validation Loss decreased from 2.370898 to 2.355154, saving the model weights\n",
      "Epoch: 29\tTrain Loss: 2.4025957 \tVal Loss:2.4277085 \tTrain Acc: 19.91177% \tVal Acc: 17.0588239%\n",
      "Epoch: 30\tTrain Loss: 2.4272844 \tVal Loss:2.4000661 \tTrain Acc: 19.55882% \tVal Acc: 17.7058827%\n",
      "Epoch: 31\tTrain Loss: 2.4056941 \tVal Loss:2.3785281 \tTrain Acc: 19.17647% \tVal Acc: 19.7647062%\n",
      "Epoch: 32\tTrain Loss: 2.3943842 \tVal Loss:2.3702890 \tTrain Acc: 20.08824% \tVal Acc: 20.9411767%\n",
      "Epoch: 33\tTrain Loss: 2.3770912 \tVal Loss:2.3516796 \tTrain Acc: 19.85294% \tVal Acc: 18.4117652%\n",
      "Validation Loss decreased from 2.355154 to 2.351680, saving the model weights\n",
      "Epoch: 34\tTrain Loss: 2.3629557 \tVal Loss:2.3318512 \tTrain Acc: 20.77941% \tVal Acc: 20.9411769%\n",
      "Validation Loss decreased from 2.351680 to 2.331851, saving the model weights\n",
      "Epoch: 35\tTrain Loss: 2.3563736 \tVal Loss:2.3579896 \tTrain Acc: 21.33824% \tVal Acc: 21.2352943%\n",
      "Epoch: 36\tTrain Loss: 2.3645794 \tVal Loss:2.3374128 \tTrain Acc: 20.60294% \tVal Acc: 20.0000003%\n",
      "Epoch: 37\tTrain Loss: 2.3390974 \tVal Loss:2.3232641 \tTrain Acc: 22.95588% \tVal Acc: 21.1764708%\n",
      "Validation Loss decreased from 2.331851 to 2.323264, saving the model weights\n",
      "Epoch: 38\tTrain Loss: 2.3364041 \tVal Loss:2.2961322 \tTrain Acc: 21.91177% \tVal Acc: 21.6470592%\n",
      "Validation Loss decreased from 2.323264 to 2.296132, saving the model weights\n",
      "Epoch: 39\tTrain Loss: 2.3249092 \tVal Loss:2.3150162 \tTrain Acc: 22.88235% \tVal Acc: 20.8823532%\n",
      "Epoch: 40\tTrain Loss: 2.3092886 \tVal Loss:2.2592169 \tTrain Acc: 22.47059% \tVal Acc: 22.9411769%\n",
      "Validation Loss decreased from 2.296132 to 2.259217, saving the model weights\n",
      "Epoch: 41\tTrain Loss: 2.2953369 \tVal Loss:2.2686913 \tTrain Acc: 23.5% \tVal Acc: 22.0588240%\n",
      "Epoch: 42\tTrain Loss: 2.2927064 \tVal Loss:2.3216173 \tTrain Acc: 24.57353% \tVal Acc: 22.5882356%\n",
      "Epoch: 43\tTrain Loss: 2.2928460 \tVal Loss:2.2446737 \tTrain Acc: 24.44118% \tVal Acc: 24.2941186%\n",
      "Validation Loss decreased from 2.259217 to 2.244674, saving the model weights\n",
      "Epoch: 44\tTrain Loss: 2.2616078 \tVal Loss:2.2370849 \tTrain Acc: 26.02941% \tVal Acc: 21.6470592%\n",
      "Validation Loss decreased from 2.244674 to 2.237085, saving the model weights\n",
      "Epoch: 45\tTrain Loss: 2.2502921 \tVal Loss:2.2132289 \tTrain Acc: 25.82353% \tVal Acc: 23.5882355%\n",
      "Validation Loss decreased from 2.237085 to 2.213229, saving the model weights\n",
      "Epoch: 46\tTrain Loss: 2.2463532 \tVal Loss:2.2978614 \tTrain Acc: 25.88235% \tVal Acc: 22.8235298%\n",
      "Epoch: 47\tTrain Loss: 2.2549136 \tVal Loss:2.2940108 \tTrain Acc: 26.29412% \tVal Acc: 22.7058826%\n",
      "Epoch: 48\tTrain Loss: 2.3038453 \tVal Loss:2.5919576 \tTrain Acc: 25.25% \tVal Acc: 17.7647064%\n",
      "Epoch: 49\tTrain Loss: 2.3990149 \tVal Loss:2.3101345 \tTrain Acc: 23.82353% \tVal Acc: 21.4117651%\n",
      "Epoch: 50\tTrain Loss: 2.2787664 \tVal Loss:2.2533270 \tTrain Acc: 25.25% \tVal Acc: 24.2352945%\n",
      "Epoch: 51\tTrain Loss: 2.2916068 \tVal Loss:2.1609589 \tTrain Acc: 24.42647% \tVal Acc: 29.1176482%\n",
      "Validation Loss decreased from 2.213229 to 2.160959, saving the model weights\n",
      "Epoch: 52\tTrain Loss: 2.1866876 \tVal Loss:2.1324665 \tTrain Acc: 28.25% \tVal Acc: 30.0000010%\n",
      "Validation Loss decreased from 2.160959 to 2.132466, saving the model weights\n",
      "Epoch: 53\tTrain Loss: 2.1773947 \tVal Loss:2.1011373 \tTrain Acc: 28.26471% \tVal Acc: 30.6470598%\n",
      "Validation Loss decreased from 2.132466 to 2.101137, saving the model weights\n",
      "Epoch: 54\tTrain Loss: 2.1322923 \tVal Loss:2.0773297 \tTrain Acc: 29.10294% \tVal Acc: 33.4705892%\n",
      "Validation Loss decreased from 2.101137 to 2.077330, saving the model weights\n",
      "Epoch: 55\tTrain Loss: 2.0925720 \tVal Loss:2.0389132 \tTrain Acc: 31.72059% \tVal Acc: 35.3529420%\n",
      "Validation Loss decreased from 2.077330 to 2.038913, saving the model weights\n",
      "Epoch: 56\tTrain Loss: 2.0525225 \tVal Loss:2.0385706 \tTrain Acc: 31.32353% \tVal Acc: 35.4705891%\n",
      "Validation Loss decreased from 2.038913 to 2.038571, saving the model weights\n",
      "Epoch: 57\tTrain Loss: 2.0593540 \tVal Loss:2.0715015 \tTrain Acc: 32.25% \tVal Acc: 33.2352951%\n",
      "Epoch: 58\tTrain Loss: 2.0519765 \tVal Loss:2.0629718 \tTrain Acc: 33.16177% \tVal Acc: 32.5882362%\n",
      "Epoch: 59\tTrain Loss: 1.9953328 \tVal Loss:2.0944374 \tTrain Acc: 33.58824% \tVal Acc: 32.0588242%\n",
      "Epoch: 60\tTrain Loss: 2.0050606 \tVal Loss:2.0198807 \tTrain Acc: 34.01471% \tVal Acc: 34.4705890%\n",
      "Validation Loss decreased from 2.038571 to 2.019881, saving the model weights\n",
      "Epoch: 61\tTrain Loss: 2.0014060 \tVal Loss:2.0044033 \tTrain Acc: 34.22059% \tVal Acc: 35.5294128%\n",
      "Validation Loss decreased from 2.019881 to 2.004403, saving the model weights\n",
      "Epoch: 62\tTrain Loss: 2.0973299 \tVal Loss:2.0484429 \tTrain Acc: 31.10294% \tVal Acc: 33.2941186%\n",
      "Epoch: 63\tTrain Loss: 2.0302734 \tVal Loss:2.1405578 \tTrain Acc: 33.61765% \tVal Acc: 29.9411772%\n",
      "Epoch: 64\tTrain Loss: 2.0358233 \tVal Loss:2.2986523 \tTrain Acc: 34.42647% \tVal Acc: 25.4705890%\n",
      "Epoch: 65\tTrain Loss: 2.0654175 \tVal Loss:2.3285786 \tTrain Acc: 32.88235% \tVal Acc: 22.1176475%\n",
      "Epoch: 66\tTrain Loss: 2.1688158 \tVal Loss:2.1608984 \tTrain Acc: 29.38235% \tVal Acc: 29.7647068%\n",
      "Epoch: 67\tTrain Loss: 2.1488732 \tVal Loss:2.2651300 \tTrain Acc: 31.02941% \tVal Acc: 27.5294125%\n",
      "Epoch: 68\tTrain Loss: 2.1184524 \tVal Loss:1.9179323 \tTrain Acc: 31.55882% \tVal Acc: 38.8823536%\n",
      "Validation Loss decreased from 2.004403 to 1.917932, saving the model weights\n",
      "Epoch: 69\tTrain Loss: 1.9929592 \tVal Loss:1.8301928 \tTrain Acc: 35.26471% \tVal Acc: 43.4117651%\n",
      "Validation Loss decreased from 1.917932 to 1.830193, saving the model weights\n",
      "Epoch: 70\tTrain Loss: 1.8901118 \tVal Loss:1.7918191 \tTrain Acc: 39.02941% \tVal Acc: 45.4117653%\n",
      "Validation Loss decreased from 1.830193 to 1.791819, saving the model weights\n",
      "Epoch: 71\tTrain Loss: 1.8334647 \tVal Loss:1.7919260 \tTrain Acc: 41.17647% \tVal Acc: 42.7647066%\n",
      "Epoch: 72\tTrain Loss: 1.7854383 \tVal Loss:1.7003636 \tTrain Acc: 42.22059% \tVal Acc: 47.5294119%\n",
      "Validation Loss decreased from 1.791819 to 1.700364, saving the model weights\n",
      "Epoch: 73\tTrain Loss: 1.7469953 \tVal Loss:1.6727516 \tTrain Acc: 44.11765% \tVal Acc: 47.1176475%\n",
      "Validation Loss decreased from 1.700364 to 1.672752, saving the model weights\n",
      "Epoch: 74\tTrain Loss: 1.7108007 \tVal Loss:1.6515774 \tTrain Acc: 44.82353% \tVal Acc: 47.5882357%\n",
      "Validation Loss decreased from 1.672752 to 1.651577, saving the model weights\n",
      "Epoch: 75\tTrain Loss: 1.6773022 \tVal Loss:1.6937886 \tTrain Acc: 46.0% \tVal Acc: 47.4117652%\n",
      "Epoch: 76\tTrain Loss: 1.6337580 \tVal Loss:1.6423868 \tTrain Acc: 47.14706% \tVal Acc: 48.6470592%\n",
      "Validation Loss decreased from 1.651577 to 1.642387, saving the model weights\n",
      "Epoch: 77\tTrain Loss: 1.6093445 \tVal Loss:1.5881989 \tTrain Acc: 48.42647% \tVal Acc: 52.2352946%\n",
      "Validation Loss decreased from 1.642387 to 1.588199, saving the model weights\n",
      "Epoch: 78\tTrain Loss: 1.5801951 \tVal Loss:1.5773326 \tTrain Acc: 49.35294% \tVal Acc: 49.5294124%\n",
      "Validation Loss decreased from 1.588199 to 1.577333, saving the model weights\n",
      "Epoch: 79\tTrain Loss: 1.5512451 \tVal Loss:1.5217335 \tTrain Acc: 50.42647% \tVal Acc: 53.4705892%\n",
      "Validation Loss decreased from 1.577333 to 1.521733, saving the model weights\n",
      "Epoch: 80\tTrain Loss: 1.5409145 \tVal Loss:1.5205677 \tTrain Acc: 50.91177% \tVal Acc: 53.3529416%\n",
      "Validation Loss decreased from 1.521733 to 1.520568, saving the model weights\n",
      "Epoch: 81\tTrain Loss: 1.5496409 \tVal Loss:1.4270971 \tTrain Acc: 49.76471% \tVal Acc: 56.2941188%\n",
      "Validation Loss decreased from 1.520568 to 1.427097, saving the model weights\n",
      "Epoch: 82\tTrain Loss: 1.5099024 \tVal Loss:1.3590063 \tTrain Acc: 51.91177% \tVal Acc: 57.7647063%\n",
      "Validation Loss decreased from 1.427097 to 1.359006, saving the model weights\n",
      "Epoch: 83\tTrain Loss: 1.5314046 \tVal Loss:1.4375531 \tTrain Acc: 50.67647% \tVal Acc: 54.7058833%\n",
      "Epoch: 84\tTrain Loss: 1.5257362 \tVal Loss:1.4137820 \tTrain Acc: 51.85294% \tVal Acc: 56.0588247%\n",
      "Epoch: 85\tTrain Loss: 1.5307945 \tVal Loss:1.4470680 \tTrain Acc: 51.41177% \tVal Acc: 54.0000013%\n",
      "Epoch: 86\tTrain Loss: 1.5763912 \tVal Loss:1.5597172 \tTrain Acc: 49.83824% \tVal Acc: 50.9411773%\n",
      "Epoch: 87\tTrain Loss: 1.4986610 \tVal Loss:1.4496457 \tTrain Acc: 52.61765% \tVal Acc: 54.2352954%\n",
      "Epoch: 88\tTrain Loss: 1.5053409 \tVal Loss:1.3001175 \tTrain Acc: 52.30882% \tVal Acc: 60.7058823%\n",
      "Validation Loss decreased from 1.359006 to 1.300117, saving the model weights\n",
      "Epoch: 89\tTrain Loss: 1.4374371 \tVal Loss:1.2351682 \tTrain Acc: 54.52941% \tVal Acc: 62.8823537%\n",
      "Validation Loss decreased from 1.300117 to 1.235168, saving the model weights\n",
      "Epoch: 90\tTrain Loss: 1.3498267 \tVal Loss:1.3096816 \tTrain Acc: 57.63235% \tVal Acc: 60.7647061%\n",
      "Epoch: 91\tTrain Loss: 1.2900309 \tVal Loss:1.4625207 \tTrain Acc: 59.85294% \tVal Acc: 55.8235300%\n",
      "Epoch: 92\tTrain Loss: 1.2400592 \tVal Loss:1.2312118 \tTrain Acc: 60.66177% \tVal Acc: 62.8823537%\n",
      "Validation Loss decreased from 1.235168 to 1.231212, saving the model weights\n",
      "Epoch: 93\tTrain Loss: 1.1930734 \tVal Loss:1.1680869 \tTrain Acc: 61.89706% \tVal Acc: 65.7058823%\n",
      "Validation Loss decreased from 1.231212 to 1.168087, saving the model weights\n",
      "Epoch: 94\tTrain Loss: 1.1554031 \tVal Loss:1.2268120 \tTrain Acc: 64.20588% \tVal Acc: 63.6470598%\n",
      "Epoch: 95\tTrain Loss: 1.1459417 \tVal Loss:1.2518109 \tTrain Acc: 64.20588% \tVal Acc: 62.5882360%\n",
      "Epoch: 96\tTrain Loss: 1.1179207 \tVal Loss:1.1268960 \tTrain Acc: 65.0% \tVal Acc: 66.1764705%\n",
      "Validation Loss decreased from 1.168087 to 1.126896, saving the model weights\n",
      "Epoch: 97\tTrain Loss: 1.0916869 \tVal Loss:1.1564324 \tTrain Acc: 64.95588% \tVal Acc: 64.9411774%\n",
      "Epoch: 98\tTrain Loss: 1.0805863 \tVal Loss:1.3814172 \tTrain Acc: 66.29412% \tVal Acc: 57.0588246%\n",
      "Epoch: 99\tTrain Loss: 1.0554242 \tVal Loss:1.2720777 \tTrain Acc: 66.89706% \tVal Acc: 61.2941179%\n",
      "Epoch: 100\tTrain Loss: 1.0975434 \tVal Loss:1.2780417 \tTrain Acc: 65.22059% \tVal Acc: 61.7647061%\n",
      "Epoch: 101\tTrain Loss: 1.1026359 \tVal Loss:1.1032518 \tTrain Acc: 65.55882% \tVal Acc: 66.5294117%\n",
      "Validation Loss decreased from 1.126896 to 1.103252, saving the model weights\n",
      "Epoch: 102\tTrain Loss: 1.1030898 \tVal Loss:1.0921078 \tTrain Acc: 64.69118% \tVal Acc: 67.2352946%\n",
      "Validation Loss decreased from 1.103252 to 1.092108, saving the model weights\n",
      "Epoch: 103\tTrain Loss: 1.0776448 \tVal Loss:1.3173051 \tTrain Acc: 65.98529% \tVal Acc: 62.3529422%\n",
      "Epoch: 104\tTrain Loss: 1.0592168 \tVal Loss:1.2249891 \tTrain Acc: 66.26471% \tVal Acc: 63.2352948%\n",
      "Epoch: 105\tTrain Loss: 1.0058702 \tVal Loss:1.0895750 \tTrain Acc: 68.52941% \tVal Acc: 67.5882363%\n",
      "Validation Loss decreased from 1.092108 to 1.089575, saving the model weights\n",
      "Epoch: 106\tTrain Loss: 1.0116436 \tVal Loss:1.0764792 \tTrain Acc: 67.70588% \tVal Acc: 68.4705886%\n",
      "Validation Loss decreased from 1.089575 to 1.076479, saving the model weights\n",
      "Epoch: 107\tTrain Loss: 1.0843840 \tVal Loss:1.1242460 \tTrain Acc: 65.89706% \tVal Acc: 67.8235304%\n",
      "Epoch: 108\tTrain Loss: 1.0380486 \tVal Loss:0.9662218 \tTrain Acc: 67.60294% \tVal Acc: 70.4705876%\n",
      "Validation Loss decreased from 1.076479 to 0.966222, saving the model weights\n",
      "Epoch: 109\tTrain Loss: 1.0245221 \tVal Loss:1.0072626 \tTrain Acc: 68.20588% \tVal Acc: 71.1764699%\n",
      "Epoch: 110\tTrain Loss: 1.0155311 \tVal Loss:0.8218163 \tTrain Acc: 68.05882% \tVal Acc: 76.1764717%\n",
      "Validation Loss decreased from 0.966222 to 0.821816, saving the model weights\n",
      "Epoch: 111\tTrain Loss: 1.0265338 \tVal Loss:0.7263366 \tTrain Acc: 67.69118% \tVal Acc: 81.1764699%\n",
      "Validation Loss decreased from 0.821816 to 0.726337, saving the model weights\n",
      "Epoch: 112\tTrain Loss: 1.0570949 \tVal Loss:0.8064258 \tTrain Acc: 66.70588% \tVal Acc: 76.9411761%\n",
      "Epoch: 113\tTrain Loss: 1.0806845 \tVal Loss:0.8188857 \tTrain Acc: 66.36765% \tVal Acc: 76.2352943%\n",
      "Epoch: 114\tTrain Loss: 0.9782512 \tVal Loss:0.7202117 \tTrain Acc: 69.55882% \tVal Acc: 79.5882338%\n",
      "Validation Loss decreased from 0.726337 to 0.720212, saving the model weights\n",
      "Epoch: 115\tTrain Loss: 0.9227867 \tVal Loss:0.9026363 \tTrain Acc: 70.38235% \tVal Acc: 73.3529413%\n",
      "Epoch: 116\tTrain Loss: 0.9027239 \tVal Loss:0.6678158 \tTrain Acc: 71.58824% \tVal Acc: 82.1764702%\n",
      "Validation Loss decreased from 0.720212 to 0.667816, saving the model weights\n",
      "Epoch: 117\tTrain Loss: 0.9071083 \tVal Loss:0.6388086 \tTrain Acc: 71.72059% \tVal Acc: 82.1764696%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss decreased from 0.667816 to 0.638809, saving the model weights\n",
      "Epoch: 118\tTrain Loss: 0.9387697 \tVal Loss:0.8136419 \tTrain Acc: 70.60294% \tVal Acc: 75.5882353%\n",
      "Epoch: 119\tTrain Loss: 0.9262842 \tVal Loss:0.9804910 \tTrain Acc: 70.79412% \tVal Acc: 71.6470587%\n",
      "Epoch: 120\tTrain Loss: 0.9478057 \tVal Loss:0.8430493 \tTrain Acc: 70.36765% \tVal Acc: 74.9411774%\n",
      "Epoch: 121\tTrain Loss: 0.9037269 \tVal Loss:0.9063616 \tTrain Acc: 71.39706% \tVal Acc: 72.2941178%\n",
      "Epoch: 122\tTrain Loss: 0.9240445 \tVal Loss:0.8555459 \tTrain Acc: 71.30882% \tVal Acc: 74.7647059%\n",
      "Epoch: 123\tTrain Loss: 0.9483112 \tVal Loss:0.8641588 \tTrain Acc: 70.77941% \tVal Acc: 73.7647063%\n",
      "Epoch: 124\tTrain Loss: 0.9206590 \tVal Loss:0.8451085 \tTrain Acc: 70.77941% \tVal Acc: 74.7058827%\n",
      "Epoch: 125\tTrain Loss: 0.8360820 \tVal Loss:0.7896248 \tTrain Acc: 73.16177% \tVal Acc: 76.4705878%\n",
      "Epoch: 126\tTrain Loss: 0.8026342 \tVal Loss:0.5272998 \tTrain Acc: 74.51471% \tVal Acc: 85.9411758%\n",
      "Validation Loss decreased from 0.638809 to 0.527300, saving the model weights\n",
      "Epoch: 127\tTrain Loss: 0.7282000 \tVal Loss:0.4962751 \tTrain Acc: 77.19118% \tVal Acc: 86.2941176%\n",
      "Validation Loss decreased from 0.527300 to 0.496275, saving the model weights\n",
      "Epoch: 128\tTrain Loss: 0.6912366 \tVal Loss:0.4515968 \tTrain Acc: 78.55882% \tVal Acc: 88.2941180%\n",
      "Validation Loss decreased from 0.496275 to 0.451597, saving the model weights\n",
      "Epoch: 129\tTrain Loss: 0.6597485 \tVal Loss:0.4643758 \tTrain Acc: 79.33823% \tVal Acc: 87.7647054%\n",
      "Epoch: 130\tTrain Loss: 0.6408270 \tVal Loss:0.4092811 \tTrain Acc: 79.73529% \tVal Acc: 89.8823512%\n",
      "Validation Loss decreased from 0.451597 to 0.409281, saving the model weights\n",
      "Epoch: 131\tTrain Loss: 0.6154962 \tVal Loss:0.3985812 \tTrain Acc: 80.79412% \tVal Acc: 89.2352939%\n",
      "Validation Loss decreased from 0.409281 to 0.398581, saving the model weights\n",
      "Epoch: 132\tTrain Loss: 0.5998966 \tVal Loss:0.4128385 \tTrain Acc: 81.30882% \tVal Acc: 89.1764700%\n",
      "Epoch: 133\tTrain Loss: 0.5855730 \tVal Loss:0.4143262 \tTrain Acc: 82.29412% \tVal Acc: 88.5882342%\n",
      "Epoch: 134\tTrain Loss: 0.6023724 \tVal Loss:0.4276035 \tTrain Acc: 81.29412% \tVal Acc: 88.7058812%\n",
      "Epoch: 135\tTrain Loss: 0.6110551 \tVal Loss:0.3842399 \tTrain Acc: 80.39706% \tVal Acc: 89.8823518%\n",
      "Validation Loss decreased from 0.398581 to 0.384240, saving the model weights\n",
      "Epoch: 136\tTrain Loss: 0.5864070 \tVal Loss:0.3738908 \tTrain Acc: 81.83823% \tVal Acc: 90.1176459%\n",
      "Validation Loss decreased from 0.384240 to 0.373891, saving the model weights\n",
      "Epoch: 137\tTrain Loss: 0.5766081 \tVal Loss:0.4135420 \tTrain Acc: 82.29412% \tVal Acc: 88.5294110%\n",
      "Epoch: 138\tTrain Loss: 0.5688341 \tVal Loss:0.4677234 \tTrain Acc: 82.39706% \tVal Acc: 86.3529402%\n",
      "Epoch: 139\tTrain Loss: 0.5776990 \tVal Loss:0.3595219 \tTrain Acc: 81.66176% \tVal Acc: 90.7058823%\n",
      "Validation Loss decreased from 0.373891 to 0.359522, saving the model weights\n",
      "Epoch: 140\tTrain Loss: 0.5412651 \tVal Loss:0.3220770 \tTrain Acc: 83.44118% \tVal Acc: 91.4117646%\n",
      "Validation Loss decreased from 0.359522 to 0.322077, saving the model weights\n",
      "Epoch: 141\tTrain Loss: 0.5389681 \tVal Loss:0.4009415 \tTrain Acc: 83.13235% \tVal Acc: 88.9411759%\n",
      "Epoch: 142\tTrain Loss: 0.5186302 \tVal Loss:0.3005008 \tTrain Acc: 83.98529% \tVal Acc: 92.5882345%\n",
      "Validation Loss decreased from 0.322077 to 0.300501, saving the model weights\n",
      "Epoch: 143\tTrain Loss: 0.5024199 \tVal Loss:0.4112978 \tTrain Acc: 84.86765% \tVal Acc: 88.7647045%\n",
      "Epoch: 144\tTrain Loss: 0.4896941 \tVal Loss:0.5413741 \tTrain Acc: 84.82353% \tVal Acc: 83.7647057%\n",
      "Epoch: 145\tTrain Loss: 0.4819085 \tVal Loss:0.5355236 \tTrain Acc: 85.33823% \tVal Acc: 83.9999998%\n",
      "Epoch: 146\tTrain Loss: 0.4943686 \tVal Loss:0.6153175 \tTrain Acc: 84.38235% \tVal Acc: 81.3529414%\n",
      "Epoch: 147\tTrain Loss: 0.4877387 \tVal Loss:0.4104883 \tTrain Acc: 84.42647% \tVal Acc: 88.2352930%\n",
      "Epoch: 148\tTrain Loss: 0.4731898 \tVal Loss:0.3265791 \tTrain Acc: 84.95588% \tVal Acc: 90.7058811%\n",
      "Epoch: 149\tTrain Loss: 0.4715532 \tVal Loss:0.3967048 \tTrain Acc: 85.48529% \tVal Acc: 88.1764704%\n",
      "Epoch: 150\tTrain Loss: 0.4444046 \tVal Loss:0.4477796 \tTrain Acc: 86.51471% \tVal Acc: 86.5294111%\n",
      "Epoch: 151\tTrain Loss: 0.4419028 \tVal Loss:0.3883956 \tTrain Acc: 86.64706% \tVal Acc: 89.9999988%\n",
      "Epoch: 152\tTrain Loss: 0.5329653 \tVal Loss:0.6266909 \tTrain Acc: 83.63235% \tVal Acc: 81.2352931%\n",
      "Epoch: 153\tTrain Loss: 0.5057696 \tVal Loss:0.4628907 \tTrain Acc: 83.83823% \tVal Acc: 87.4705881%\n",
      "Epoch: 154\tTrain Loss: 0.4745623 \tVal Loss:0.4614887 \tTrain Acc: 84.69118% \tVal Acc: 86.4705867%\n",
      "Epoch: 155\tTrain Loss: 0.4386936 \tVal Loss:0.5851016 \tTrain Acc: 85.98529% \tVal Acc: 81.8823522%\n",
      "Epoch: 156\tTrain Loss: 0.4323116 \tVal Loss:0.4019998 \tTrain Acc: 86.72059% \tVal Acc: 87.7647048%\n",
      "Epoch: 157\tTrain Loss: 0.4120190 \tVal Loss:0.3149202 \tTrain Acc: 87.52941% \tVal Acc: 91.9999987%\n",
      "Epoch: 158\tTrain Loss: 0.4056587 \tVal Loss:0.2937113 \tTrain Acc: 87.41176% \tVal Acc: 91.8235290%\n",
      "Validation Loss decreased from 0.300501 to 0.293711, saving the model weights\n",
      "Epoch: 159\tTrain Loss: 0.4071660 \tVal Loss:0.2275061 \tTrain Acc: 87.19118% \tVal Acc: 94.5882344%\n",
      "Validation Loss decreased from 0.293711 to 0.227506, saving the model weights\n",
      "Epoch: 160\tTrain Loss: 0.3836997 \tVal Loss:0.1928807 \tTrain Acc: 87.88235% \tVal Acc: 95.1764685%\n",
      "Validation Loss decreased from 0.227506 to 0.192881, saving the model weights\n",
      "Epoch: 161\tTrain Loss: 0.3611027 \tVal Loss:0.1930476 \tTrain Acc: 88.52941% \tVal Acc: 95.5882341%\n",
      "Epoch: 162\tTrain Loss: 0.3577000 \tVal Loss:0.1779975 \tTrain Acc: 89.35294% \tVal Acc: 94.8823524%\n",
      "Validation Loss decreased from 0.192881 to 0.177998, saving the model weights\n",
      "Epoch: 163\tTrain Loss: 0.3231135 \tVal Loss:0.2056094 \tTrain Acc: 89.89706% \tVal Acc: 95.1176459%\n",
      "Epoch: 164\tTrain Loss: 0.3257278 \tVal Loss:0.1663085 \tTrain Acc: 89.98529% \tVal Acc: 95.7058817%\n",
      "Validation Loss decreased from 0.177998 to 0.166309, saving the model weights\n",
      "Epoch: 165\tTrain Loss: 0.3107049 \tVal Loss:0.1750176 \tTrain Acc: 90.55882% \tVal Acc: 95.2941161%\n",
      "Epoch: 166\tTrain Loss: 0.2996177 \tVal Loss:0.1804268 \tTrain Acc: 91.27941% \tVal Acc: 95.4705876%\n",
      "Epoch: 167\tTrain Loss: 0.2852090 \tVal Loss:0.1590474 \tTrain Acc: 91.41176% \tVal Acc: 96.0588223%\n",
      "Validation Loss decreased from 0.166309 to 0.159047, saving the model weights\n",
      "Epoch: 168\tTrain Loss: 0.2754938 \tVal Loss:0.1520354 \tTrain Acc: 91.82353% \tVal Acc: 95.8823514%\n",
      "Validation Loss decreased from 0.159047 to 0.152035, saving the model weights\n",
      "Epoch: 169\tTrain Loss: 0.2638141 \tVal Loss:0.1335223 \tTrain Acc: 92.20588% \tVal Acc: 96.9999993%\n",
      "Validation Loss decreased from 0.152035 to 0.133522, saving the model weights\n",
      "Epoch: 170\tTrain Loss: 0.2495824 \tVal Loss:0.1589646 \tTrain Acc: 92.33823% \tVal Acc: 96.1764699%\n",
      "Epoch: 171\tTrain Loss: 0.2473412 \tVal Loss:0.1456021 \tTrain Acc: 92.41176% \tVal Acc: 96.5882355%\n",
      "Epoch: 172\tTrain Loss: 0.2540346 \tVal Loss:0.1595126 \tTrain Acc: 92.39706% \tVal Acc: 95.8235282%\n",
      "Epoch: 173\tTrain Loss: 0.2534795 \tVal Loss:0.1597769 \tTrain Acc: 92.42647% \tVal Acc: 95.5294102%\n",
      "Epoch: 174\tTrain Loss: 0.2542596 \tVal Loss:0.1309390 \tTrain Acc: 92.32353% \tVal Acc: 97.1764690%\n",
      "Validation Loss decreased from 0.133522 to 0.130939, saving the model weights\n",
      "Epoch: 175\tTrain Loss: 0.2531786 \tVal Loss:0.1410524 \tTrain Acc: 92.39706% \tVal Acc: 96.2352931%\n",
      "Epoch: 176\tTrain Loss: 0.2351420 \tVal Loss:0.1299258 \tTrain Acc: 92.94118% \tVal Acc: 96.8235284%\n",
      "Validation Loss decreased from 0.130939 to 0.129926, saving the model weights\n",
      "Epoch: 177\tTrain Loss: 0.2308721 \tVal Loss:0.1249297 \tTrain Acc: 93.22059% \tVal Acc: 97.1176457%\n",
      "Validation Loss decreased from 0.129926 to 0.124930, saving the model weights\n",
      "Epoch: 178\tTrain Loss: 0.2191016 \tVal Loss:0.1256655 \tTrain Acc: 93.5% \tVal Acc: 96.8823516%\n",
      "Epoch: 179\tTrain Loss: 0.2101770 \tVal Loss:0.1596900 \tTrain Acc: 93.66176% \tVal Acc: 95.7647049%\n",
      "Epoch: 180\tTrain Loss: 0.2078899 \tVal Loss:0.1278119 \tTrain Acc: 93.82353% \tVal Acc: 96.5294105%\n",
      "Epoch: 181\tTrain Loss: 0.2097743 \tVal Loss:0.1267780 \tTrain Acc: 93.60294% \tVal Acc: 96.8823522%\n",
      "Epoch: 182\tTrain Loss: 0.2028659 \tVal Loss:0.1046208 \tTrain Acc: 93.95588% \tVal Acc: 97.5882334%\n",
      "Validation Loss decreased from 0.124930 to 0.104621, saving the model weights\n",
      "Epoch: 183\tTrain Loss: 0.1915551 \tVal Loss:0.1225932 \tTrain Acc: 94.30882% \tVal Acc: 96.7647046%\n",
      "Epoch: 184\tTrain Loss: 0.1788435 \tVal Loss:0.1084240 \tTrain Acc: 94.88235% \tVal Acc: 97.2352928%\n",
      "Epoch: 185\tTrain Loss: 0.1751163 \tVal Loss:0.0977039 \tTrain Acc: 94.82353% \tVal Acc: 97.9999989%\n",
      "Validation Loss decreased from 0.104621 to 0.097704, saving the model weights\n",
      "Epoch: 186\tTrain Loss: 0.1718211 \tVal Loss:0.1113100 \tTrain Acc: 94.88235% \tVal Acc: 96.8235284%\n",
      "Epoch: 187\tTrain Loss: 0.1695732 \tVal Loss:0.1234920 \tTrain Acc: 95.05882% \tVal Acc: 96.9999987%\n",
      "Epoch: 188\tTrain Loss: 0.1677038 \tVal Loss:0.0990831 \tTrain Acc: 95.32353% \tVal Acc: 97.5294101%\n",
      "Epoch: 189\tTrain Loss: 0.1804425 \tVal Loss:0.1396173 \tTrain Acc: 94.57353% \tVal Acc: 96.2941170%\n",
      "Epoch: 190\tTrain Loss: 0.1726388 \tVal Loss:0.1006338 \tTrain Acc: 94.77941% \tVal Acc: 97.3529410%\n",
      "Epoch: 191\tTrain Loss: 0.1808817 \tVal Loss:0.1290665 \tTrain Acc: 94.73529% \tVal Acc: 97.1176463%\n",
      "Epoch: 192\tTrain Loss: 0.2002892 \tVal Loss:0.2152943 \tTrain Acc: 94.10294% \tVal Acc: 93.8823515%\n",
      "Epoch: 193\tTrain Loss: 0.6603696 \tVal Loss:0.4305210 \tTrain Acc: 80.44118% \tVal Acc: 87.9411763%\n",
      "Epoch: 194\tTrain Loss: 0.6032011 \tVal Loss:0.2135455 \tTrain Acc: 80.67647% \tVal Acc: 93.4705871%\n",
      "Epoch: 195\tTrain Loss: 0.3901448 \tVal Loss:0.1375359 \tTrain Acc: 87.27941% \tVal Acc: 96.5882337%\n",
      "Epoch: 196\tTrain Loss: 0.3066548 \tVal Loss:0.1094685 \tTrain Acc: 90.35294% \tVal Acc: 97.2941166%\n",
      "Epoch: 197\tTrain Loss: 0.2432831 \tVal Loss:0.0951100 \tTrain Acc: 92.66176% \tVal Acc: 97.7647036%\n",
      "Validation Loss decreased from 0.097704 to 0.095110, saving the model weights\n",
      "Epoch: 198\tTrain Loss: 0.1980048 \tVal Loss:0.0852006 \tTrain Acc: 94.2647% \tVal Acc: 98.1176454%\n",
      "Validation Loss decreased from 0.095110 to 0.085201, saving the model weights\n",
      "Epoch: 199\tTrain Loss: 0.1828042 \tVal Loss:0.0847977 \tTrain Acc: 94.97059% \tVal Acc: 97.8823519%\n",
      "Validation Loss decreased from 0.085201 to 0.084798, saving the model weights\n",
      "Epoch: 200\tTrain Loss: 0.1736382 \tVal Loss:0.0944138 \tTrain Acc: 94.95588% \tVal Acc: 97.7058804%\n",
      "Epoch: 201\tTrain Loss: 0.1705684 \tVal Loss:0.0736197 \tTrain Acc: 94.72059% \tVal Acc: 98.3529395%\n",
      "Validation Loss decreased from 0.084798 to 0.073620, saving the model weights\n",
      "Epoch: 202\tTrain Loss: 0.1564003 \tVal Loss:0.0836871 \tTrain Acc: 95.41176% \tVal Acc: 97.9411751%\n",
      "Epoch: 203\tTrain Loss: 0.1588691 \tVal Loss:0.0819376 \tTrain Acc: 95.42647% \tVal Acc: 98.1764692%\n",
      "Epoch: 204\tTrain Loss: 0.1492266 \tVal Loss:0.0729302 \tTrain Acc: 95.77941% \tVal Acc: 98.2941163%\n",
      "Validation Loss decreased from 0.073620 to 0.072930, saving the model weights\n",
      "Epoch: 205\tTrain Loss: 0.1419239 \tVal Loss:0.0921728 \tTrain Acc: 96.0147% \tVal Acc: 97.3529398%\n",
      "Epoch: 206\tTrain Loss: 0.1480518 \tVal Loss:0.0795932 \tTrain Acc: 95.82353% \tVal Acc: 97.7058816%\n",
      "Epoch: 207\tTrain Loss: 0.1342755 \tVal Loss:0.0870709 \tTrain Acc: 95.95588% \tVal Acc: 97.7647048%\n",
      "Epoch: 208\tTrain Loss: 0.1348281 \tVal Loss:0.0628422 \tTrain Acc: 96.16176% \tVal Acc: 98.4705859%\n",
      "Validation Loss decreased from 0.072930 to 0.062842, saving the model weights\n",
      "Epoch: 209\tTrain Loss: 0.1396845 \tVal Loss:0.0823468 \tTrain Acc: 96.07353% \tVal Acc: 97.6470566%\n",
      "Epoch: 210\tTrain Loss: 0.1302210 \tVal Loss:0.0636438 \tTrain Acc: 96.5147% \tVal Acc: 98.1764692%\n",
      "Epoch: 211\tTrain Loss: 0.1367912 \tVal Loss:0.0727537 \tTrain Acc: 95.86765% \tVal Acc: 97.9411751%\n",
      "Epoch: 212\tTrain Loss: 0.1275075 \tVal Loss:0.0745044 \tTrain Acc: 96.16176% \tVal Acc: 97.9999977%\n",
      "Epoch: 213\tTrain Loss: 0.1295471 \tVal Loss:0.0865448 \tTrain Acc: 96.25% \tVal Acc: 97.1176457%\n",
      "Epoch: 214\tTrain Loss: 0.1214710 \tVal Loss:0.0623915 \tTrain Acc: 96.55882% \tVal Acc: 98.0588228%\n",
      "Validation Loss decreased from 0.062842 to 0.062391, saving the model weights\n",
      "Epoch: 215\tTrain Loss: 0.1106257 \tVal Loss:0.0735633 \tTrain Acc: 97.07353% \tVal Acc: 97.9999989%\n",
      "Epoch: 216\tTrain Loss: 0.1054003 \tVal Loss:0.0754516 \tTrain Acc: 96.98529% \tVal Acc: 97.7647048%\n",
      "Epoch: 217\tTrain Loss: 0.1117439 \tVal Loss:0.0625974 \tTrain Acc: 96.7647% \tVal Acc: 98.4705877%\n",
      "Epoch: 218\tTrain Loss: 0.1201412 \tVal Loss:0.0798846 \tTrain Acc: 96.54412% \tVal Acc: 97.3529404%\n",
      "Epoch: 219\tTrain Loss: 0.1182172 \tVal Loss:0.0958323 \tTrain Acc: 96.67647% \tVal Acc: 97.2941160%\n",
      "Epoch: 220\tTrain Loss: 0.1141290 \tVal Loss:0.0756423 \tTrain Acc: 96.77941% \tVal Acc: 97.7058810%\n",
      "Epoch: 221\tTrain Loss: 0.1136664 \tVal Loss:0.0654287 \tTrain Acc: 97.11765% \tVal Acc: 98.3529407%\n",
      "Epoch: 222\tTrain Loss: 0.1267013 \tVal Loss:0.0938527 \tTrain Acc: 96.2647% \tVal Acc: 97.5294101%\n",
      "Epoch: 223\tTrain Loss: 0.1300372 \tVal Loss:0.1031489 \tTrain Acc: 96.20588% \tVal Acc: 96.8235296%\n",
      "Epoch: 224\tTrain Loss: 0.1631306 \tVal Loss:0.1461739 \tTrain Acc: 95.2647% \tVal Acc: 95.9999990%\n",
      "Epoch: 225\tTrain Loss: 0.2229489 \tVal Loss:0.2245451 \tTrain Acc: 93.05882% \tVal Acc: 92.6470584%\n",
      "Epoch: 226\tTrain Loss: 0.2420222 \tVal Loss:0.1591083 \tTrain Acc: 92.22059% \tVal Acc: 95.4117638%\n",
      "Epoch: 227\tTrain Loss: 0.1968800 \tVal Loss:0.0862010 \tTrain Acc: 93.88235% \tVal Acc: 97.3529404%\n",
      "Epoch: 228\tTrain Loss: 0.1342297 \tVal Loss:0.0631983 \tTrain Acc: 96.08823% \tVal Acc: 98.4117639%\n",
      "Epoch: 229\tTrain Loss: 0.1219652 \tVal Loss:0.0509730 \tTrain Acc: 96.32353% \tVal Acc: 98.7647051%\n",
      "Validation Loss decreased from 0.062391 to 0.050973, saving the model weights\n",
      "Epoch: 230\tTrain Loss: 0.1160249 \tVal Loss:0.0813652 \tTrain Acc: 96.5% \tVal Acc: 97.4117637%\n",
      "Epoch: 231\tTrain Loss: 0.1091465 \tVal Loss:0.0624420 \tTrain Acc: 96.89706% \tVal Acc: 97.9411757%\n",
      "Epoch: 232\tTrain Loss: 0.1083796 \tVal Loss:0.0795108 \tTrain Acc: 96.98529% \tVal Acc: 97.4117637%\n",
      "Epoch: 233\tTrain Loss: 0.1080180 \tVal Loss:0.1015781 \tTrain Acc: 97.08823% \tVal Acc: 97.2352922%\n",
      "Epoch: 234\tTrain Loss: 0.1286838 \tVal Loss:0.0788122 \tTrain Acc: 95.97059% \tVal Acc: 97.2941154%\n",
      "Epoch: 235\tTrain Loss: 0.1196912 \tVal Loss:0.0868328 \tTrain Acc: 96.22059% \tVal Acc: 97.5294101%\n",
      "Epoch: 236\tTrain Loss: 0.1208062 \tVal Loss:0.0880961 \tTrain Acc: 96.20588% \tVal Acc: 97.7058810%\n",
      "Epoch: 237\tTrain Loss: 0.1510175 \tVal Loss:0.1072385 \tTrain Acc: 95.30882% \tVal Acc: 96.4117640%\n",
      "Epoch: 238\tTrain Loss: 0.1674999 \tVal Loss:0.1488341 \tTrain Acc: 94.63235% \tVal Acc: 95.9411758%\n",
      "Epoch: 239\tTrain Loss: 0.1695285 \tVal Loss:0.0965731 \tTrain Acc: 94.85294% \tVal Acc: 96.9411749%\n",
      "Epoch: 240\tTrain Loss: 0.1695740 \tVal Loss:0.1413485 \tTrain Acc: 95.04412% \tVal Acc: 95.9999985%\n",
      "Epoch: 241\tTrain Loss: 0.1499042 \tVal Loss:0.1446194 \tTrain Acc: 95.57353% \tVal Acc: 95.7647038%\n",
      "Epoch: 242\tTrain Loss: 0.1411384 \tVal Loss:0.0960344 \tTrain Acc: 95.67647% \tVal Acc: 97.5294101%\n",
      "Epoch: 243\tTrain Loss: 0.1226281 \tVal Loss:0.0609240 \tTrain Acc: 96.60294% \tVal Acc: 98.2941163%\n",
      "Epoch: 244\tTrain Loss: 0.0915176 \tVal Loss:0.0652042 \tTrain Acc: 97.61765% \tVal Acc: 98.0588222%\n",
      "Epoch: 245\tTrain Loss: 0.0964308 \tVal Loss:0.0539894 \tTrain Acc: 97.13235% \tVal Acc: 98.4705871%\n",
      "Epoch: 246\tTrain Loss: 0.0856753 \tVal Loss:0.0462517 \tTrain Acc: 97.52941% \tVal Acc: 98.5294104%\n",
      "Validation Loss decreased from 0.050973 to 0.046252, saving the model weights\n",
      "Epoch: 247\tTrain Loss: 0.0822923 \tVal Loss:0.0547240 \tTrain Acc: 97.94118% \tVal Acc: 98.3529401%\n",
      "Epoch: 248\tTrain Loss: 0.0734096 \tVal Loss:0.0478472 \tTrain Acc: 97.88235% \tVal Acc: 98.4705871%\n",
      "Epoch: 249\tTrain Loss: 0.0709212 \tVal Loss:0.0505256 \tTrain Acc: 97.98529% \tVal Acc: 98.5882342%\n",
      "Epoch: 250\tTrain Loss: 0.0763958 \tVal Loss:0.0519899 \tTrain Acc: 97.73529% \tVal Acc: 98.3529401%\n",
      "Epoch: 251\tTrain Loss: 0.0731431 \tVal Loss:0.0582376 \tTrain Acc: 97.97059% \tVal Acc: 98.0588222%\n",
      "Epoch: 252\tTrain Loss: 0.0786711 \tVal Loss:0.0565509 \tTrain Acc: 97.63235% \tVal Acc: 98.1764698%\n",
      "Epoch: 253\tTrain Loss: 0.0862369 \tVal Loss:0.0668904 \tTrain Acc: 97.57353% \tVal Acc: 97.9411745%\n",
      "Epoch: 254\tTrain Loss: 0.0949822 \tVal Loss:0.0607101 \tTrain Acc: 97.27941% \tVal Acc: 98.2941163%\n",
      "Epoch: 255\tTrain Loss: 0.0895431 \tVal Loss:0.0661919 \tTrain Acc: 97.42647% \tVal Acc: 98.1176454%\n",
      "Epoch: 256\tTrain Loss: 0.0799686 \tVal Loss:0.0538120 \tTrain Acc: 97.83823% \tVal Acc: 98.2941163%\n",
      "Epoch: 257\tTrain Loss: 0.0820172 \tVal Loss:0.0605550 \tTrain Acc: 97.69118% \tVal Acc: 98.1764692%\n",
      "Epoch: 258\tTrain Loss: 0.0830095 \tVal Loss:0.0700824 \tTrain Acc: 97.63235% \tVal Acc: 98.0588228%\n",
      "Epoch: 259\tTrain Loss: 0.0803050 \tVal Loss:0.0572231 \tTrain Acc: 97.86765% \tVal Acc: 98.3529407%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 260\tTrain Loss: 0.0814107 \tVal Loss:0.0633034 \tTrain Acc: 97.86765% \tVal Acc: 97.9411757%\n",
      "Epoch: 261\tTrain Loss: 0.1004882 \tVal Loss:0.0732879 \tTrain Acc: 96.97059% \tVal Acc: 97.8235292%\n",
      "Epoch: 262\tTrain Loss: 0.1155509 \tVal Loss:0.0928915 \tTrain Acc: 96.39706% \tVal Acc: 96.9411743%\n",
      "Epoch: 263\tTrain Loss: 0.1670619 \tVal Loss:0.1359343 \tTrain Acc: 95.17647% \tVal Acc: 96.1764699%\n",
      "Epoch: 264\tTrain Loss: 0.2143920 \tVal Loss:0.1583338 \tTrain Acc: 93.42647% \tVal Acc: 95.0588226%\n",
      "Epoch: 265\tTrain Loss: 0.2623661 \tVal Loss:0.2368511 \tTrain Acc: 92.35294% \tVal Acc: 92.7058816%\n",
      "Epoch: 266\tTrain Loss: 0.2460853 \tVal Loss:0.1084158 \tTrain Acc: 92.44118% \tVal Acc: 96.6470587%\n",
      "Epoch: 267\tTrain Loss: 0.2303385 \tVal Loss:0.1061014 \tTrain Acc: 92.94118% \tVal Acc: 96.6470581%\n",
      "Epoch: 268\tTrain Loss: 0.1585358 \tVal Loss:0.0863132 \tTrain Acc: 95.16176% \tVal Acc: 97.3529410%\n",
      "Epoch: 269\tTrain Loss: 0.1088349 \tVal Loss:0.0596599 \tTrain Acc: 96.70588% \tVal Acc: 98.1176466%\n",
      "Epoch: 270\tTrain Loss: 0.0952821 \tVal Loss:0.0468970 \tTrain Acc: 97.30882% \tVal Acc: 98.3529407%\n",
      "Epoch: 271\tTrain Loss: 0.0869574 \tVal Loss:0.0566859 \tTrain Acc: 97.47059% \tVal Acc: 98.3529395%\n",
      "Epoch: 272\tTrain Loss: 0.0909920 \tVal Loss:0.0547307 \tTrain Acc: 97.33823% \tVal Acc: 98.2352936%\n",
      "Epoch: 273\tTrain Loss: 0.0786209 \tVal Loss:0.0522841 \tTrain Acc: 97.89706% \tVal Acc: 98.1176460%\n",
      "Epoch: 274\tTrain Loss: 0.0694478 \tVal Loss:0.0546960 \tTrain Acc: 98.11765% \tVal Acc: 98.0588222%\n",
      "Epoch: 275\tTrain Loss: 0.0688088 \tVal Loss:0.0385708 \tTrain Acc: 98.14706% \tVal Acc: 98.5882342%\n",
      "Validation Loss decreased from 0.046252 to 0.038571, saving the model weights\n",
      "Epoch: 276\tTrain Loss: 0.0681393 \tVal Loss:0.0492347 \tTrain Acc: 98.17647% \tVal Acc: 98.2352930%\n",
      "Epoch: 277\tTrain Loss: 0.0628244 \tVal Loss:0.0404993 \tTrain Acc: 98.33823% \tVal Acc: 98.4705877%\n",
      "Epoch: 278\tTrain Loss: 0.0552922 \tVal Loss:0.0449192 \tTrain Acc: 98.58823% \tVal Acc: 98.5882342%\n",
      "Epoch: 279\tTrain Loss: 0.0631787 \tVal Loss:0.0454885 \tTrain Acc: 98.20588% \tVal Acc: 98.2941157%\n",
      "Epoch: 280\tTrain Loss: 0.0597502 \tVal Loss:0.0562562 \tTrain Acc: 98.29412% \tVal Acc: 98.0588228%\n",
      "Epoch: 281\tTrain Loss: 0.0647031 \tVal Loss:0.0591426 \tTrain Acc: 98.33823% \tVal Acc: 97.8823519%\n",
      "Epoch: 282\tTrain Loss: 0.0644868 \tVal Loss:0.0520957 \tTrain Acc: 98.20588% \tVal Acc: 98.3529395%\n",
      "Epoch: 283\tTrain Loss: 0.0671528 \tVal Loss:0.0540400 \tTrain Acc: 98.10294% \tVal Acc: 98.2941163%\n",
      "Epoch: 284\tTrain Loss: 0.0564772 \tVal Loss:0.0437433 \tTrain Acc: 98.48529% \tVal Acc: 98.3529401%\n",
      "Epoch: 285\tTrain Loss: 0.0701307 \tVal Loss:0.0498273 \tTrain Acc: 98.05882% \tVal Acc: 98.1764698%\n",
      "Epoch: 286\tTrain Loss: 0.0878689 \tVal Loss:0.1274333 \tTrain Acc: 97.27941% \tVal Acc: 96.0588223%\n",
      "Epoch: 287\tTrain Loss: 0.1059412 \tVal Loss:0.0885920 \tTrain Acc: 96.72059% \tVal Acc: 96.9999987%\n",
      "Epoch: 288\tTrain Loss: 0.0929491 \tVal Loss:0.0988925 \tTrain Acc: 97.08823% \tVal Acc: 96.6470575%\n",
      "Epoch: 289\tTrain Loss: 0.0929793 \tVal Loss:0.0609260 \tTrain Acc: 97.32353% \tVal Acc: 97.9999989%\n",
      "Epoch: 290\tTrain Loss: 0.0783472 \tVal Loss:0.0558513 \tTrain Acc: 97.79412% \tVal Acc: 98.0588222%\n",
      "Epoch: 291\tTrain Loss: 0.0696986 \tVal Loss:0.0503044 \tTrain Acc: 98.0147% \tVal Acc: 98.4705865%\n",
      "Epoch: 292\tTrain Loss: 0.0679955 \tVal Loss:0.0413634 \tTrain Acc: 97.95588% \tVal Acc: 98.6470574%\n",
      "Epoch: 293\tTrain Loss: 0.0829564 \tVal Loss:0.0604207 \tTrain Acc: 97.54412% \tVal Acc: 98.2941151%\n",
      "Epoch: 294\tTrain Loss: 0.0996802 \tVal Loss:0.1118821 \tTrain Acc: 97.39706% \tVal Acc: 96.6470581%\n",
      "Epoch: 295\tTrain Loss: 0.1739176 \tVal Loss:0.0877282 \tTrain Acc: 94.39706% \tVal Acc: 97.0588219%\n",
      "Epoch: 296\tTrain Loss: 0.1605629 \tVal Loss:0.1484511 \tTrain Acc: 94.97059% \tVal Acc: 95.7647043%\n",
      "Epoch: 297\tTrain Loss: 0.4716452 \tVal Loss:0.3174957 \tTrain Acc: 85.97059% \tVal Acc: 89.8823529%\n",
      "Epoch: 298\tTrain Loss: 0.3798138 \tVal Loss:0.1282550 \tTrain Acc: 88.17647% \tVal Acc: 96.5294105%\n",
      "Epoch: 299\tTrain Loss: 0.2111497 \tVal Loss:0.0805668 \tTrain Acc: 93.11765% \tVal Acc: 97.5294101%\n",
      "Epoch: 300\tTrain Loss: 0.1371519 \tVal Loss:0.0754939 \tTrain Acc: 95.83823% \tVal Acc: 97.3529398%\n",
      "Epoch: 301\tTrain Loss: 0.1069816 \tVal Loss:0.0439089 \tTrain Acc: 96.58823% \tVal Acc: 98.4705865%\n",
      "Epoch: 302\tTrain Loss: 0.0828140 \tVal Loss:0.0443747 \tTrain Acc: 97.52941% \tVal Acc: 98.4117633%\n",
      "Epoch: 303\tTrain Loss: 0.0792315 \tVal Loss:0.0471811 \tTrain Acc: 97.88235% \tVal Acc: 98.2352924%\n",
      "Epoch: 304\tTrain Loss: 0.0708787 \tVal Loss:0.0422759 \tTrain Acc: 97.94117% \tVal Acc: 98.5294104%\n",
      "Epoch: 305\tTrain Loss: 0.0593705 \tVal Loss:0.0404468 \tTrain Acc: 98.52941% \tVal Acc: 98.7058812%\n",
      "Epoch: 306\tTrain Loss: 0.0564619 \tVal Loss:0.0399246 \tTrain Acc: 98.52941% \tVal Acc: 98.6470586%\n",
      "Epoch: 307\tTrain Loss: 0.0540206 \tVal Loss:0.0425093 \tTrain Acc: 98.55882% \tVal Acc: 98.4117633%\n",
      "Epoch: 308\tTrain Loss: 0.0522396 \tVal Loss:0.0406357 \tTrain Acc: 98.60294% \tVal Acc: 98.7058812%\n",
      "Epoch: 309\tTrain Loss: 0.0553755 \tVal Loss:0.0369746 \tTrain Acc: 98.41176% \tVal Acc: 98.7647045%\n",
      "Validation Loss decreased from 0.038571 to 0.036975, saving the model weights\n",
      "Epoch: 310\tTrain Loss: 0.0517369 \tVal Loss:0.0469388 \tTrain Acc: 98.66176% \tVal Acc: 98.4705865%\n",
      "Epoch: 311\tTrain Loss: 0.0567588 \tVal Loss:0.0453909 \tTrain Acc: 98.64706% \tVal Acc: 98.7058812%\n",
      "Epoch: 312\tTrain Loss: 0.0490822 \tVal Loss:0.0399885 \tTrain Acc: 98.79412% \tVal Acc: 98.5882342%\n",
      "Epoch: 313\tTrain Loss: 0.0534818 \tVal Loss:0.0362315 \tTrain Acc: 98.48529% \tVal Acc: 98.8823521%\n",
      "Validation Loss decreased from 0.036975 to 0.036231, saving the model weights\n",
      "Epoch: 314\tTrain Loss: 0.0525003 \tVal Loss:0.0352622 \tTrain Acc: 98.5% \tVal Acc: 98.6470574%\n",
      "Validation Loss decreased from 0.036231 to 0.035262, saving the model weights\n",
      "Epoch: 315\tTrain Loss: 0.0492491 \tVal Loss:0.0351974 \tTrain Acc: 98.79412% \tVal Acc: 98.8823515%\n",
      "Validation Loss decreased from 0.035262 to 0.035197, saving the model weights\n",
      "Epoch: 316\tTrain Loss: 0.0496067 \tVal Loss:0.0405740 \tTrain Acc: 98.77941% \tVal Acc: 98.7058818%\n",
      "Epoch: 317\tTrain Loss: 0.0481051 \tVal Loss:0.0399979 \tTrain Acc: 98.69117% \tVal Acc: 98.5294098%\n",
      "Epoch: 318\tTrain Loss: 0.0483699 \tVal Loss:0.0441761 \tTrain Acc: 98.70588% \tVal Acc: 98.5294104%\n",
      "Epoch: 319\tTrain Loss: 0.0437986 \tVal Loss:0.0468354 \tTrain Acc: 98.85294% \tVal Acc: 98.5294110%\n",
      "Epoch: 320\tTrain Loss: 0.0443518 \tVal Loss:0.0426772 \tTrain Acc: 98.83823% \tVal Acc: 98.6470574%\n",
      "Epoch: 321\tTrain Loss: 0.0394141 \tVal Loss:0.0504268 \tTrain Acc: 99.0% \tVal Acc: 98.5882336%\n",
      "Epoch: 322\tTrain Loss: 0.0424533 \tVal Loss:0.0501905 \tTrain Acc: 98.94118% \tVal Acc: 98.1764692%\n",
      "Epoch: 323\tTrain Loss: 0.0472577 \tVal Loss:0.0539390 \tTrain Acc: 98.85294% \tVal Acc: 98.2941163%\n",
      "Epoch: 324\tTrain Loss: 0.0463219 \tVal Loss:0.0528427 \tTrain Acc: 98.73529% \tVal Acc: 98.2941169%\n",
      "Epoch: 325\tTrain Loss: 0.1112083 \tVal Loss:0.1061609 \tTrain Acc: 96.33823% \tVal Acc: 96.4705867%\n",
      "Epoch: 326\tTrain Loss: 0.1506233 \tVal Loss:0.1375562 \tTrain Acc: 95.16176% \tVal Acc: 96.2941164%\n",
      "Epoch: 327\tTrain Loss: 0.1304899 \tVal Loss:0.1012617 \tTrain Acc: 95.94118% \tVal Acc: 96.9999981%\n",
      "Epoch: 328\tTrain Loss: 0.1084381 \tVal Loss:0.0904030 \tTrain Acc: 96.72059% \tVal Acc: 97.4705869%\n",
      "Epoch: 329\tTrain Loss: 0.0973791 \tVal Loss:0.0639109 \tTrain Acc: 97.11765% \tVal Acc: 97.9999977%\n",
      "Epoch: 330\tTrain Loss: 0.0766767 \tVal Loss:0.0466746 \tTrain Acc: 97.77941% \tVal Acc: 98.3529401%\n",
      "Epoch: 331\tTrain Loss: 0.0622866 \tVal Loss:0.0544416 \tTrain Acc: 98.30882% \tVal Acc: 98.1764692%\n",
      "Epoch: 332\tTrain Loss: 0.0691102 \tVal Loss:0.0835353 \tTrain Acc: 98.0147% \tVal Acc: 97.3529404%\n",
      "Epoch: 333\tTrain Loss: 0.0675929 \tVal Loss:0.0719282 \tTrain Acc: 98.04412% \tVal Acc: 97.8823507%\n",
      "Epoch: 334\tTrain Loss: 0.0607104 \tVal Loss:0.0688017 \tTrain Acc: 98.25% \tVal Acc: 97.8235281%\n",
      "Epoch: 335\tTrain Loss: 0.0559872 \tVal Loss:0.0699747 \tTrain Acc: 98.48529% \tVal Acc: 97.8823513%\n",
      "Epoch: 336\tTrain Loss: 0.0555698 \tVal Loss:0.0600568 \tTrain Acc: 98.38235% \tVal Acc: 98.1176460%\n",
      "Epoch: 337\tTrain Loss: 0.0462631 \tVal Loss:0.0617406 \tTrain Acc: 98.82353% \tVal Acc: 97.7647036%\n",
      "Epoch: 338\tTrain Loss: 0.0483243 \tVal Loss:0.0737707 \tTrain Acc: 98.60294% \tVal Acc: 97.9411751%\n",
      "Epoch: 339\tTrain Loss: 0.0553401 \tVal Loss:0.0518618 \tTrain Acc: 98.64706% \tVal Acc: 98.4117627%\n",
      "Epoch: 340\tTrain Loss: 0.0476508 \tVal Loss:0.0574853 \tTrain Acc: 98.70588% \tVal Acc: 97.9999983%\n",
      "Epoch: 341\tTrain Loss: 0.0468445 \tVal Loss:0.0749005 \tTrain Acc: 98.70588% \tVal Acc: 97.8235286%\n",
      "Epoch: 342\tTrain Loss: 0.0444156 \tVal Loss:0.0518449 \tTrain Acc: 98.73529% \tVal Acc: 98.3529401%\n",
      "Epoch: 343\tTrain Loss: 0.0500464 \tVal Loss:0.0739087 \tTrain Acc: 98.63235% \tVal Acc: 97.7647048%\n",
      "Epoch: 344\tTrain Loss: 0.0511260 \tVal Loss:0.0533501 \tTrain Acc: 98.63235% \tVal Acc: 98.2352930%\n",
      "Epoch: 345\tTrain Loss: 0.0599633 \tVal Loss:0.0701004 \tTrain Acc: 98.47059% \tVal Acc: 97.5882334%\n",
      "Epoch: 346\tTrain Loss: 0.0704856 \tVal Loss:0.0941205 \tTrain Acc: 98.14706% \tVal Acc: 96.9411749%\n",
      "Epoch: 347\tTrain Loss: 0.0991083 \tVal Loss:0.0724345 \tTrain Acc: 96.80882% \tVal Acc: 97.7647048%\n",
      "Epoch: 348\tTrain Loss: 0.1187482 \tVal Loss:0.1242240 \tTrain Acc: 96.5% \tVal Acc: 96.8823510%\n",
      "Epoch: 349\tTrain Loss: 0.2178901 \tVal Loss:0.2274213 \tTrain Acc: 93.72059% \tVal Acc: 93.3529401%\n",
      "Epoch: 350\tTrain Loss: 0.2508872 \tVal Loss:0.1496053 \tTrain Acc: 92.0147% \tVal Acc: 95.6470579%\n",
      "Epoch: 351\tTrain Loss: 0.5940326 \tVal Loss:0.6263679 \tTrain Acc: 83.42647% \tVal Acc: 82.9411757%\n",
      "Epoch: 352\tTrain Loss: 0.5497261 \tVal Loss:0.1943703 \tTrain Acc: 83.5% \tVal Acc: 93.9411753%\n",
      "Epoch: 353\tTrain Loss: 0.2566107 \tVal Loss:0.0820138 \tTrain Acc: 91.42647% \tVal Acc: 97.6470584%\n",
      "Epoch: 354\tTrain Loss: 0.1516556 \tVal Loss:0.0420895 \tTrain Acc: 95.22059% \tVal Acc: 98.8235283%\n",
      "Epoch: 355\tTrain Loss: 0.1079776 \tVal Loss:0.0384515 \tTrain Acc: 96.82353% \tVal Acc: 98.4705871%\n",
      "Epoch: 356\tTrain Loss: 0.0869303 \tVal Loss:0.0427237 \tTrain Acc: 97.5% \tVal Acc: 98.5882336%\n",
      "Epoch: 357\tTrain Loss: 0.0737790 \tVal Loss:0.0413318 \tTrain Acc: 97.72059% \tVal Acc: 98.6470574%\n",
      "Epoch: 358\tTrain Loss: 0.0654454 \tVal Loss:0.0391516 \tTrain Acc: 98.08823% \tVal Acc: 98.7647045%\n",
      "Epoch: 359\tTrain Loss: 0.0611407 \tVal Loss:0.0399733 \tTrain Acc: 98.13235% \tVal Acc: 98.7058806%\n",
      "Epoch: 360\tTrain Loss: 0.0590519 \tVal Loss:0.0494246 \tTrain Acc: 98.32353% \tVal Acc: 98.3529395%\n",
      "Epoch: 361\tTrain Loss: 0.0570373 \tVal Loss:0.0393305 \tTrain Acc: 98.41176% \tVal Acc: 98.7647051%\n",
      "Epoch: 362\tTrain Loss: 0.0510583 \tVal Loss:0.0400256 \tTrain Acc: 98.54412% \tVal Acc: 98.7058812%\n",
      "Epoch: 363\tTrain Loss: 0.0514989 \tVal Loss:0.0358238 \tTrain Acc: 98.55882% \tVal Acc: 98.7647033%\n",
      "Epoch: 364\tTrain Loss: 0.0462950 \tVal Loss:0.0394791 \tTrain Acc: 98.7647% \tVal Acc: 98.7647045%\n",
      "Epoch: 365\tTrain Loss: 0.0481505 \tVal Loss:0.0451336 \tTrain Acc: 98.67647% \tVal Acc: 98.5294098%\n",
      "Epoch: 366\tTrain Loss: 0.0413766 \tVal Loss:0.0442245 \tTrain Acc: 98.94118% \tVal Acc: 98.4117627%\n",
      "Epoch: 367\tTrain Loss: 0.0416500 \tVal Loss:0.0481279 \tTrain Acc: 98.7647% \tVal Acc: 98.4705871%\n",
      "Epoch: 368\tTrain Loss: 0.0454260 \tVal Loss:0.0466937 \tTrain Acc: 98.70588% \tVal Acc: 98.3529401%\n",
      "Epoch: 369\tTrain Loss: 0.0451041 \tVal Loss:0.0443622 \tTrain Acc: 98.77941% \tVal Acc: 98.4705877%\n",
      "Epoch: 370\tTrain Loss: 0.0376511 \tVal Loss:0.0447300 \tTrain Acc: 99.0% \tVal Acc: 98.4705865%\n",
      "Epoch: 371\tTrain Loss: 0.0388110 \tVal Loss:0.0474847 \tTrain Acc: 99.04412% \tVal Acc: 98.4705859%\n",
      "Epoch: 372\tTrain Loss: 0.0399537 \tVal Loss:0.0479859 \tTrain Acc: 98.89706% \tVal Acc: 98.4117627%\n",
      "Epoch: 373\tTrain Loss: 0.0420190 \tVal Loss:0.0366459 \tTrain Acc: 98.92647% \tVal Acc: 98.5882324%\n",
      "Epoch: 374\tTrain Loss: 0.0361922 \tVal Loss:0.0499970 \tTrain Acc: 99.02941% \tVal Acc: 98.3529389%\n",
      "Epoch: 375\tTrain Loss: 0.0389023 \tVal Loss:0.0404907 \tTrain Acc: 98.97059% \tVal Acc: 98.6470580%\n",
      "Epoch: 376\tTrain Loss: 0.0401794 \tVal Loss:0.0434975 \tTrain Acc: 98.82353% \tVal Acc: 98.4117639%\n",
      "Epoch: 377\tTrain Loss: 0.0347385 \tVal Loss:0.0438465 \tTrain Acc: 99.11765% \tVal Acc: 98.4705871%\n",
      "Epoch: 378\tTrain Loss: 0.0372882 \tVal Loss:0.0346927 \tTrain Acc: 99.04412% \tVal Acc: 98.7647045%\n",
      "Validation Loss decreased from 0.035197 to 0.034693, saving the model weights\n",
      "Epoch: 379\tTrain Loss: 0.0390478 \tVal Loss:0.0439081 \tTrain Acc: 98.94117% \tVal Acc: 98.5294098%\n",
      "Epoch: 380\tTrain Loss: 0.0371493 \tVal Loss:0.0483510 \tTrain Acc: 98.91176% \tVal Acc: 98.4705865%\n",
      "Epoch: 381\tTrain Loss: 0.0349405 \tVal Loss:0.0523503 \tTrain Acc: 99.08823% \tVal Acc: 98.0588222%\n",
      "Epoch: 382\tTrain Loss: 0.0382999 \tVal Loss:0.0495046 \tTrain Acc: 98.80882% \tVal Acc: 98.1764686%\n",
      "Epoch: 383\tTrain Loss: 0.0368015 \tVal Loss:0.0430111 \tTrain Acc: 98.98529% \tVal Acc: 98.5882342%\n",
      "Epoch: 384\tTrain Loss: 0.0331380 \tVal Loss:0.0469945 \tTrain Acc: 99.02941% \tVal Acc: 98.5294104%\n",
      "Epoch: 385\tTrain Loss: 0.0374955 \tVal Loss:0.0456758 \tTrain Acc: 98.92647% \tVal Acc: 98.4705865%\n",
      "Epoch: 386\tTrain Loss: 0.0368829 \tVal Loss:0.0533441 \tTrain Acc: 99.0% \tVal Acc: 98.4117633%\n",
      "Epoch: 387\tTrain Loss: 0.0305507 \tVal Loss:0.0554931 \tTrain Acc: 99.2647% \tVal Acc: 97.9411757%\n",
      "Epoch: 388\tTrain Loss: 0.0355069 \tVal Loss:0.0444261 \tTrain Acc: 98.88235% \tVal Acc: 98.4117627%\n",
      "Epoch: 389\tTrain Loss: 0.0454365 \tVal Loss:0.0902315 \tTrain Acc: 98.67647% \tVal Acc: 97.1176457%\n",
      "Epoch: 390\tTrain Loss: 0.0802398 \tVal Loss:0.0680132 \tTrain Acc: 97.77941% \tVal Acc: 98.0588222%\n",
      "Epoch: 391\tTrain Loss: 0.0733728 \tVal Loss:0.0423182 \tTrain Acc: 97.73529% \tVal Acc: 98.7647045%\n",
      "Epoch: 392\tTrain Loss: 0.0655379 \tVal Loss:0.0430612 \tTrain Acc: 98.41176% \tVal Acc: 98.3529401%\n",
      "Epoch: 393\tTrain Loss: 0.0514452 \tVal Loss:0.0628156 \tTrain Acc: 98.54412% \tVal Acc: 98.0588222%\n",
      "Epoch: 394\tTrain Loss: 0.0521450 \tVal Loss:0.0663122 \tTrain Acc: 98.52941% \tVal Acc: 97.9999989%\n",
      "Epoch: 395\tTrain Loss: 0.0430142 \tVal Loss:0.0642953 \tTrain Acc: 98.80882% \tVal Acc: 98.2352930%\n",
      "Epoch: 396\tTrain Loss: 0.0398388 \tVal Loss:0.0374099 \tTrain Acc: 98.92647% \tVal Acc: 98.6470580%\n",
      "Epoch: 397\tTrain Loss: 0.0437454 \tVal Loss:0.0582906 \tTrain Acc: 98.70588% \tVal Acc: 98.3529395%\n",
      "Epoch: 398\tTrain Loss: 0.0548388 \tVal Loss:0.0443441 \tTrain Acc: 98.63235% \tVal Acc: 98.4705859%\n",
      "Epoch: 399\tTrain Loss: 0.0465100 \tVal Loss:0.0411545 \tTrain Acc: 98.52941% \tVal Acc: 98.8235283%\n",
      "Epoch: 400\tTrain Loss: 0.0441673 \tVal Loss:0.0489383 \tTrain Acc: 98.63235% \tVal Acc: 98.4117633%\n",
      "Epoch: 401\tTrain Loss: 0.0383458 \tVal Loss:0.0452980 \tTrain Acc: 99.02941% \tVal Acc: 98.6470568%\n",
      "Epoch: 402\tTrain Loss: 0.0444006 \tVal Loss:0.0400753 \tTrain Acc: 98.73529% \tVal Acc: 98.7647045%\n",
      "Epoch: 403\tTrain Loss: 0.0374214 \tVal Loss:0.0806003 \tTrain Acc: 99.04412% \tVal Acc: 97.9411751%\n",
      "Epoch: 404\tTrain Loss: 0.0451186 \tVal Loss:0.0494358 \tTrain Acc: 98.83823% \tVal Acc: 98.4705859%\n",
      "Epoch: 405\tTrain Loss: 0.0451544 \tVal Loss:0.0518621 \tTrain Acc: 98.72059% \tVal Acc: 98.4705865%\n",
      "Epoch: 406\tTrain Loss: 0.0382503 \tVal Loss:0.0476775 \tTrain Acc: 99.0147% \tVal Acc: 98.4705865%\n",
      "Epoch: 407\tTrain Loss: 0.0358549 \tVal Loss:0.0718452 \tTrain Acc: 99.0147% \tVal Acc: 98.2941163%\n",
      "Epoch: 408\tTrain Loss: 0.0440003 \tVal Loss:0.0376252 \tTrain Acc: 98.76471% \tVal Acc: 98.8235271%\n",
      "Epoch: 409\tTrain Loss: 0.0619156 \tVal Loss:0.0401667 \tTrain Acc: 98.27941% \tVal Acc: 98.5882330%\n",
      "Epoch: 410\tTrain Loss: 0.0954158 \tVal Loss:0.1132341 \tTrain Acc: 97.22059% \tVal Acc: 96.4705867%\n",
      "Epoch: 411\tTrain Loss: 0.1449346 \tVal Loss:0.0869775 \tTrain Acc: 95.48529% \tVal Acc: 97.4117625%\n",
      "Epoch: 412\tTrain Loss: 0.1667153 \tVal Loss:0.1461599 \tTrain Acc: 95.23529% \tVal Acc: 95.3529394%\n",
      "Epoch: 413\tTrain Loss: 0.1885654 \tVal Loss:0.2222926 \tTrain Acc: 94.22059% \tVal Acc: 93.4705877%\n",
      "Epoch: 414\tTrain Loss: 0.2682330 \tVal Loss:0.1914199 \tTrain Acc: 92.13235% \tVal Acc: 94.2941171%\n",
      "Epoch: 415\tTrain Loss: 0.4701208 \tVal Loss:0.2065747 \tTrain Acc: 86.79412% \tVal Acc: 93.0588228%\n",
      "Epoch: 416\tTrain Loss: 0.5211183 \tVal Loss:0.1743274 \tTrain Acc: 85.02941% \tVal Acc: 94.7647047%\n",
      "Epoch: 417\tTrain Loss: 0.2562214 \tVal Loss:0.0491059 \tTrain Acc: 91.94118% \tVal Acc: 98.5882348%\n",
      "Epoch: 418\tTrain Loss: 0.1434708 \tVal Loss:0.0387031 \tTrain Acc: 95.25% \tVal Acc: 98.8235283%\n",
      "Epoch: 419\tTrain Loss: 0.1031943 \tVal Loss:0.0314058 \tTrain Acc: 96.83823% \tVal Acc: 99.1764694%\n",
      "Validation Loss decreased from 0.034693 to 0.031406, saving the model weights\n",
      "Epoch: 420\tTrain Loss: 0.0774753 \tVal Loss:0.0230424 \tTrain Acc: 97.63235% \tVal Acc: 99.4705874%\n",
      "Validation Loss decreased from 0.031406 to 0.023042, saving the model weights\n",
      "Epoch: 421\tTrain Loss: 0.0654983 \tVal Loss:0.0283386 \tTrain Acc: 98.13235% \tVal Acc: 99.0588224%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 422\tTrain Loss: 0.0584533 \tVal Loss:0.0248054 \tTrain Acc: 98.30882% \tVal Acc: 99.3529403%\n",
      "Epoch: 423\tTrain Loss: 0.0600928 \tVal Loss:0.0364505 \tTrain Acc: 98.2647% \tVal Acc: 98.6470574%\n",
      "Epoch: 424\tTrain Loss: 0.0628894 \tVal Loss:0.0289501 \tTrain Acc: 98.04412% \tVal Acc: 99.1764700%\n",
      "Epoch: 425\tTrain Loss: 0.0567934 \tVal Loss:0.0314285 \tTrain Acc: 98.32353% \tVal Acc: 98.9999986%\n",
      "Epoch: 426\tTrain Loss: 0.0471218 \tVal Loss:0.0285319 \tTrain Acc: 98.75% \tVal Acc: 99.1764700%\n",
      "Epoch: 427\tTrain Loss: 0.0478415 \tVal Loss:0.0291088 \tTrain Acc: 98.63235% \tVal Acc: 99.1764694%\n",
      "Epoch: 428\tTrain Loss: 0.0490006 \tVal Loss:0.0287097 \tTrain Acc: 98.48529% \tVal Acc: 99.1176462%\n",
      "Epoch: 429\tTrain Loss: 0.0445547 \tVal Loss:0.0271526 \tTrain Acc: 98.70588% \tVal Acc: 99.2352933%\n",
      "Epoch: 430\tTrain Loss: 0.0406305 \tVal Loss:0.0263121 \tTrain Acc: 99.0% \tVal Acc: 99.1176456%\n",
      "Epoch: 431\tTrain Loss: 0.0425855 \tVal Loss:0.0296991 \tTrain Acc: 98.80882% \tVal Acc: 98.9999986%\n",
      "Epoch: 432\tTrain Loss: 0.0397973 \tVal Loss:0.0279805 \tTrain Acc: 98.82353% \tVal Acc: 99.1176456%\n",
      "Epoch: 433\tTrain Loss: 0.0350796 \tVal Loss:0.0278817 \tTrain Acc: 99.02941% \tVal Acc: 99.0588224%\n",
      "Epoch: 434\tTrain Loss: 0.0367039 \tVal Loss:0.0261833 \tTrain Acc: 98.82353% \tVal Acc: 99.1176456%\n",
      "Epoch: 435\tTrain Loss: 0.0359537 \tVal Loss:0.0320630 \tTrain Acc: 99.07353% \tVal Acc: 99.1176462%\n",
      "Epoch: 436\tTrain Loss: 0.0367134 \tVal Loss:0.0404582 \tTrain Acc: 99.0147% \tVal Acc: 98.7647045%\n",
      "Epoch: 437\tTrain Loss: 0.0385039 \tVal Loss:0.0328373 \tTrain Acc: 98.88235% \tVal Acc: 98.8823515%\n",
      "Epoch: 438\tTrain Loss: 0.0316162 \tVal Loss:0.0330311 \tTrain Acc: 99.22059% \tVal Acc: 98.8235271%\n",
      "Epoch: 439\tTrain Loss: 0.0329170 \tVal Loss:0.0339528 \tTrain Acc: 99.14706% \tVal Acc: 98.9411753%\n",
      "Epoch: 440\tTrain Loss: 0.0351350 \tVal Loss:0.0358954 \tTrain Acc: 99.05882% \tVal Acc: 98.8235289%\n",
      "Epoch: 441\tTrain Loss: 0.0345786 \tVal Loss:0.0331209 \tTrain Acc: 99.05882% \tVal Acc: 98.9411747%\n",
      "Epoch: 442\tTrain Loss: 0.0310281 \tVal Loss:0.0262530 \tTrain Acc: 99.07353% \tVal Acc: 99.1176462%\n",
      "Epoch: 443\tTrain Loss: 0.0356473 \tVal Loss:0.0313175 \tTrain Acc: 99.05882% \tVal Acc: 98.8823515%\n",
      "Epoch: 444\tTrain Loss: 0.0336026 \tVal Loss:0.0606508 \tTrain Acc: 99.08823% \tVal Acc: 98.0588222%\n",
      "Epoch: 445\tTrain Loss: 0.0383486 \tVal Loss:0.0429003 \tTrain Acc: 98.91176% \tVal Acc: 98.5882348%\n",
      "Epoch: 446\tTrain Loss: 0.0356013 \tVal Loss:0.0569970 \tTrain Acc: 98.92647% \tVal Acc: 98.1176460%\n",
      "Epoch: 447\tTrain Loss: 0.0438642 \tVal Loss:0.0420489 \tTrain Acc: 98.79412% \tVal Acc: 98.5882336%\n",
      "Epoch: 448\tTrain Loss: 0.0390729 \tVal Loss:0.0353010 \tTrain Acc: 98.92647% \tVal Acc: 98.7058812%\n",
      "Epoch: 449\tTrain Loss: 0.0357280 \tVal Loss:0.0446148 \tTrain Acc: 99.05882% \tVal Acc: 98.7058812%\n",
      "Epoch: 450\tTrain Loss: 0.0377118 \tVal Loss:0.0453632 \tTrain Acc: 99.05882% \tVal Acc: 98.4117633%\n",
      "Epoch: 451\tTrain Loss: 0.0368779 \tVal Loss:0.0291628 \tTrain Acc: 98.97059% \tVal Acc: 99.0588218%\n",
      "Epoch: 452\tTrain Loss: 0.0340704 \tVal Loss:0.0421588 \tTrain Acc: 99.10294% \tVal Acc: 98.8235283%\n",
      "Epoch: 453\tTrain Loss: 0.0347317 \tVal Loss:0.0323065 \tTrain Acc: 99.04412% \tVal Acc: 98.8235283%\n",
      "Epoch: 454\tTrain Loss: 0.0343256 \tVal Loss:0.0275729 \tTrain Acc: 99.05882% \tVal Acc: 98.9999986%\n",
      "Epoch: 455\tTrain Loss: 0.0309346 \tVal Loss:0.0425472 \tTrain Acc: 99.17647% \tVal Acc: 98.7647045%\n",
      "Epoch: 456\tTrain Loss: 0.0342746 \tVal Loss:0.0314046 \tTrain Acc: 99.13235% \tVal Acc: 98.9999992%\n",
      "Epoch: 457\tTrain Loss: 0.0290031 \tVal Loss:0.0361078 \tTrain Acc: 99.14706% \tVal Acc: 98.7058812%\n",
      "Epoch: 458\tTrain Loss: 0.0292745 \tVal Loss:0.0352739 \tTrain Acc: 99.14706% \tVal Acc: 98.8823521%\n",
      "Epoch: 459\tTrain Loss: 0.0319576 \tVal Loss:0.0372848 \tTrain Acc: 99.08823% \tVal Acc: 98.8235283%\n",
      "Epoch: 460\tTrain Loss: 0.0297386 \tVal Loss:0.0418968 \tTrain Acc: 99.16176% \tVal Acc: 98.5882348%\n",
      "Epoch: 461\tTrain Loss: 0.0337506 \tVal Loss:0.0432853 \tTrain Acc: 99.02941% \tVal Acc: 98.5294104%\n",
      "Epoch: 462\tTrain Loss: 0.0433108 \tVal Loss:0.0674476 \tTrain Acc: 98.92647% \tVal Acc: 97.9999983%\n",
      "Epoch: 463\tTrain Loss: 0.0361008 \tVal Loss:0.0500722 \tTrain Acc: 98.91176% \tVal Acc: 98.5294104%\n",
      "Epoch: 464\tTrain Loss: 0.0347680 \tVal Loss:0.0501491 \tTrain Acc: 98.86765% \tVal Acc: 98.2352924%\n",
      "Epoch: 465\tTrain Loss: 0.0397577 \tVal Loss:0.0360811 \tTrain Acc: 98.91176% \tVal Acc: 98.8823521%\n",
      "Epoch: 466\tTrain Loss: 0.0312952 \tVal Loss:0.0377859 \tTrain Acc: 99.04412% \tVal Acc: 98.6470580%\n",
      "Epoch: 467\tTrain Loss: 0.0339029 \tVal Loss:0.0380785 \tTrain Acc: 99.16176% \tVal Acc: 98.6470574%\n",
      "Epoch: 468\tTrain Loss: 0.0338370 \tVal Loss:0.0559259 \tTrain Acc: 99.16176% \tVal Acc: 98.2352930%\n",
      "Epoch: 469\tTrain Loss: 0.0370462 \tVal Loss:0.0589265 \tTrain Acc: 99.0147% \tVal Acc: 98.2941163%\n",
      "Epoch: 470\tTrain Loss: 0.0387129 \tVal Loss:0.0508671 \tTrain Acc: 98.80882% \tVal Acc: 98.4117627%\n",
      "Epoch: 471\tTrain Loss: 0.0363209 \tVal Loss:0.0701991 \tTrain Acc: 98.97059% \tVal Acc: 97.9411751%\n",
      "Epoch: 472\tTrain Loss: 0.0529599 \tVal Loss:0.0857203 \tTrain Acc: 98.5147% \tVal Acc: 97.7647048%\n",
      "Epoch: 473\tTrain Loss: 0.1083926 \tVal Loss:0.1129274 \tTrain Acc: 97.27941% \tVal Acc: 96.6470575%\n",
      "Epoch: 474\tTrain Loss: 0.1648242 \tVal Loss:0.1776671 \tTrain Acc: 95.0% \tVal Acc: 94.5882350%\n",
      "Epoch: 475\tTrain Loss: 0.2271392 \tVal Loss:0.1575904 \tTrain Acc: 93.04412% \tVal Acc: 95.0588226%\n",
      "Epoch: 476\tTrain Loss: 0.2287882 \tVal Loss:0.1731985 \tTrain Acc: 92.77941% \tVal Acc: 94.9999982%\n",
      "Epoch: 477\tTrain Loss: 0.2967424 \tVal Loss:0.2333185 \tTrain Acc: 91.27941% \tVal Acc: 93.4117633%\n",
      "Epoch: 478\tTrain Loss: 0.3274454 \tVal Loss:0.1718024 \tTrain Acc: 89.79412% \tVal Acc: 94.5294106%\n",
      "Epoch: 479\tTrain Loss: 0.2530632 \tVal Loss:0.1003551 \tTrain Acc: 92.14706% \tVal Acc: 96.9411755%\n",
      "Epoch: 480\tTrain Loss: 0.1435776 \tVal Loss:0.0458557 \tTrain Acc: 95.5% \tVal Acc: 98.7058812%\n",
      "Epoch: 481\tTrain Loss: 0.1383617 \tVal Loss:0.2132761 \tTrain Acc: 96.14706% \tVal Acc: 94.2941165%\n",
      "Epoch: 482\tTrain Loss: 0.1190635 \tVal Loss:0.0383975 \tTrain Acc: 96.07353% \tVal Acc: 98.7058812%\n",
      "Epoch: 483\tTrain Loss: 0.1504441 \tVal Loss:0.0841334 \tTrain Acc: 95.54412% \tVal Acc: 97.2941166%\n",
      "Epoch: 484\tTrain Loss: 0.1436001 \tVal Loss:0.0655164 \tTrain Acc: 95.85294% \tVal Acc: 97.9999989%\n",
      "Epoch: 485\tTrain Loss: 0.0840961 \tVal Loss:0.0480693 \tTrain Acc: 97.25% \tVal Acc: 98.4705871%\n",
      "Epoch: 486\tTrain Loss: 0.0673749 \tVal Loss:0.0356053 \tTrain Acc: 97.92647% \tVal Acc: 98.8235283%\n",
      "Epoch: 487\tTrain Loss: 0.0474154 \tVal Loss:0.0307509 \tTrain Acc: 98.79412% \tVal Acc: 99.2352933%\n",
      "Epoch: 488\tTrain Loss: 0.0442258 \tVal Loss:0.0347754 \tTrain Acc: 98.64706% \tVal Acc: 98.9999992%\n",
      "Epoch: 489\tTrain Loss: 0.0421024 \tVal Loss:0.0342315 \tTrain Acc: 98.83823% \tVal Acc: 98.9999992%\n",
      "Epoch: 490\tTrain Loss: 0.0370066 \tVal Loss:0.0325822 \tTrain Acc: 98.85294% \tVal Acc: 98.9999992%\n",
      "Epoch: 491\tTrain Loss: 0.0371246 \tVal Loss:0.0275062 \tTrain Acc: 99.08823% \tVal Acc: 98.9999980%\n",
      "Epoch: 492\tTrain Loss: 0.0371275 \tVal Loss:0.0263285 \tTrain Acc: 98.98529% \tVal Acc: 99.1764694%\n",
      "Epoch: 493\tTrain Loss: 0.0334419 \tVal Loss:0.0306650 \tTrain Acc: 99.0147% \tVal Acc: 98.8823521%\n",
      "Epoch: 494\tTrain Loss: 0.0330499 \tVal Loss:0.0385995 \tTrain Acc: 99.11765% \tVal Acc: 98.6470574%\n",
      "Epoch: 495\tTrain Loss: 0.0335724 \tVal Loss:0.0316664 \tTrain Acc: 99.10294% \tVal Acc: 98.9999992%\n",
      "Epoch: 496\tTrain Loss: 0.0315132 \tVal Loss:0.0266728 \tTrain Acc: 99.19118% \tVal Acc: 99.0588230%\n",
      "Epoch: 497\tTrain Loss: 0.0328462 \tVal Loss:0.0262713 \tTrain Acc: 99.0% \tVal Acc: 99.0588224%\n",
      "Epoch: 498\tTrain Loss: 0.0314614 \tVal Loss:0.0322172 \tTrain Acc: 99.11765% \tVal Acc: 98.9999992%\n",
      "Epoch: 499\tTrain Loss: 0.0317018 \tVal Loss:0.0301035 \tTrain Acc: 99.16176% \tVal Acc: 98.9411759%\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    \n",
    "    hidden = model.hidden_init(train_batch_size)    \n",
    "    #print('hidden[0].shape:- ',hidden[0].shape)\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        '''\n",
    "        Creating new variables for the hidden state, otherwise\n",
    "        we'd backprop through the entire training history\n",
    "        '''\n",
    "        h = tuple([each.data for each in hidden])\n",
    "        \n",
    "\n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "       \n",
    "        # get the output from the model\n",
    "        output, h = model.forward(inputs, h,train_batch_size)\n",
    "        #print('OUTPUT', output)\n",
    "        \n",
    "        \n",
    "        #print('Labels Shape :-', (torch.max(labels, 1)[1]).shape)\n",
    "    \n",
    "        # calculate the loss and perform backprop\n",
    "        #print('Labels Long :-', labels.long())\n",
    "        loss = criterion(output,labels.long())\n",
    "        #print('LOSS IS :-', loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #calculate training accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        #logging.debug(' top probab {} top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        #print(train_loss)\n",
    "              \n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "                \n",
    "        val_h = tuple([each.data for each in hidden])\n",
    "        \n",
    "        output, hidden = model.forward(inputs, val_h,val_batch_size)\n",
    "       \n",
    "        loss = criterion(output,labels.long())\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        #calculate validation accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        \n",
    "        #logging.debug(output)\n",
    "        #logging.debug('VALIDATION top probab {} VALIDATION top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        #print('Top Class:- ',top_class)\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        #print('Equals:- ', equals)\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    #Averaging losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    val_accuracy = val_accuracy/len(val_loader)\n",
    "    train_accuracy = train_accuracy/len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}\\tTrain Loss: {:.7f} \\tVal Loss:{:.7f} \\tTrain Acc: {:.7}% \\tVal Acc: {:.7f}%'.format(e, train_loss, val_loss, train_accuracy*100,val_accuracy*100))\n",
    "    \n",
    "    #saving the model if validation loss is decreased\n",
    "    if val_loss <= min_val_loss:\n",
    "        print('Validation Loss decreased from {:6f} to {:6f}, saving the model weights'.format(min_val_loss, val_loss))\n",
    "        torch.save(model.state_dict(), 'lstm_state_256_256_our_normalization_formula.pt')\n",
    "        min_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Music Genaration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=256, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights\n",
    "test_model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "test_model.load_state_dict(torch.load('lstm_state_256_256_our_normalization_formula.pt'))\n",
    "test_model.cuda()\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #load population database\n",
    "# #testing_data = np.ones(200)*0\n",
    "# testing_data = list(range(50,90))\n",
    "# testing_data.extend(testing_data[::-1])\n",
    "# testing_data = np.asarray(testing_data)\n",
    "# testing_data = testing_data.reshape(testing_data.shape[0],1)\n",
    "\n",
    "# initial_seq = [network_input[0][1:].cpu().numpy().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population database\n",
    "testing_data = np.ones(200)*1\n",
    "# testing_data = list(range(50,90))\n",
    "# testing_data.extend(testing_data[::-1])\n",
    "# testing_data_rev = testing_data[::-1]\n",
    "# testing_data_rev.extend(testing_data)\n",
    "# testing_data = testing_data_rev\n",
    "\n",
    "\n",
    "testing_data = np.asarray(testing_data)\n",
    "testing_data = testing_data.reshape(testing_data.shape[0],1)\n",
    "\n",
    "initial_seq = [network_input[0][1:].cpu().numpy().tolist()]\n",
    "\n",
    "testing_data_unnorm = testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1=[]\n",
    "testing_data=testing_data.tolist()\n",
    "for i in range(len(testing_data)):\n",
    "    list1.extend(testing_data[i])\n",
    "\n",
    "#list1\n",
    "\n",
    "for i in range(len(list1)):\n",
    "    list1[i]=(list1[i]-50)/(89-50)\n",
    "#     list1[i]=(list1[i])/(89)\n",
    "\n",
    "\n",
    "list1 = np.asarray(list1)\n",
    "list1 = list1.reshape(list1.shape[0],1)\n",
    "testing_data = list1\n",
    "#list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testing_data_unnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 50,\n",
       " 1: 52,\n",
       " 2: 53,\n",
       " 3: 54,\n",
       " 4: 55,\n",
       " 5: 56,\n",
       " 6: 57,\n",
       " 7: 58,\n",
       " 8: 59,\n",
       " 9: 60,\n",
       " 10: 61,\n",
       " 11: 62,\n",
       " 12: 63,\n",
       " 13: 64,\n",
       " 14: 65,\n",
       " 15: 66,\n",
       " 16: 67,\n",
       " 17: 68,\n",
       " 18: 69,\n",
       " 19: 70,\n",
       " 20: 71,\n",
       " 21: 72,\n",
       " 22: 73,\n",
       " 23: 74,\n",
       " 24: 75,\n",
       " 25: 76,\n",
       " 26: 77,\n",
       " 27: 78,\n",
       " 28: 79,\n",
       " 29: 80,\n",
       " 30: 81,\n",
       " 31: 82,\n",
       " 32: 83,\n",
       " 33: 84,\n",
       " 34: 85,\n",
       " 35: 86,\n",
       " 36: 88,\n",
       " 37: 89}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_to_note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting\n",
    "def prediction_with_influence(influence,int2note,initial_seq, max_note, test_batch_size = 1):\n",
    "\n",
    "    predicted_notes = []\n",
    "    initial_seq[0].extend([[0]]*len(testing_data))\n",
    "    test_seq = torch.Tensor(initial_seq).cuda()\n",
    "    \n",
    "    test_hidden = test_model.hidden_init(test_batch_size)\n",
    "\n",
    "    \n",
    "    for i in range(len(influence)):\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = float(influence[i])\n",
    "        \n",
    "        test_slice = test_seq[0][i : i + sequence_length]        \n",
    "        test_slice = test_slice.view(1, test_slice.shape[0], test_slice.shape[1])\n",
    "        \n",
    "        test_output,_ = test_model.forward(test_slice, test_hidden, test_batch_size)\n",
    "        print(test_output)\n",
    "        top_p, top_class = test_output.topk(1,dim =1)\n",
    "        print(top_p)\n",
    "        print(top_class)\n",
    "        print(int2note[top_class.item()])\n",
    "#         test_seq[0][sequence_length - 1 + i][0] = int2note[top_class.item()]/max_note\n",
    "        test_seq[0][sequence_length - 1 + i][0] = (int2note[top_class.item()] - min_midi_number)/(max_midi_number - min_midi_number)\n",
    "        \n",
    "        predicted_notes.append(int2note[top_class.item()])\n",
    "        \n",
    "    return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-5.3910, -3.0518, -2.3529, -4.3106, -3.0977, -3.9509, -3.7206, -3.9411,\n",
      "         -3.6954, -7.6719, -1.4397, -3.6105, -3.9444, -8.6946, -5.7710, -3.1181,\n",
      "         -5.5251, -4.7863, -2.8310, -6.6637, -4.3037,  5.5416,  3.1254, -0.2569,\n",
      "         -8.0789, -0.0412,  3.5051,  0.9924, 10.9958,  1.1220,  4.7281, -0.7076,\n",
      "          7.0208,  5.1415, -0.4913,  5.7272,  0.1050, -1.0975]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.9958]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[28]], device='cuda:0')\n",
      "tensor([[-1.6755, -0.7077, -1.1780, -0.0796, -4.5207, -1.3775, -0.4298,  0.2194,\n",
      "         -3.2681, -4.9164, -2.3663, -6.3151, -3.0285, -9.9530, -6.7779, -5.2897,\n",
      "         -2.4221, -4.7705, -6.1670, -1.5615, -2.4391, -5.0023, -2.9753,  6.0620,\n",
      "          0.4434,  6.6230,  1.7800, 12.6993,  4.1079, -0.6712,  7.1380,  1.6138,\n",
      "          8.1547,  6.3010,  7.0034, -0.7452,  0.0598, -1.7965]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[12.6993]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[27]], device='cuda:0')\n",
      "tensor([[-0.7533,  0.0587, -0.1958, -1.1118, -4.1540, -1.4349, -1.7846, -0.6489,\n",
      "         -2.9593, -5.7394, -0.1572, -3.7367, -2.7104, -8.9761, -8.0932, -8.2812,\n",
      "         -8.5048, -5.4909, -4.1766, -1.9988, -6.0429, -5.0441,  0.1797,  6.4796,\n",
      "         -3.2602,  5.8986,  0.4346,  9.6231,  9.9169, -0.2194,  4.2269,  3.0789,\n",
      "         12.9435,  4.8390,  5.8734,  7.3123, -1.3320,  1.4401]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[12.9435]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.0504,  2.3687,  1.4111,  1.3281, -1.2532,  0.4118,  1.5354,  1.4934,\n",
      "          0.7281, -1.8149,  1.7637, -3.5428,  1.0857, -4.0305, -4.9283, -1.5981,\n",
      "         -5.4029, -3.9692, -0.0797, -4.7227, -6.9434, -3.8866, -0.1431, -0.9626,\n",
      "         -3.4381, -1.2253, -1.7321,  7.5432,  7.3312,  4.8070,  3.0269,  6.4521,\n",
      "         11.8556,  7.2749,  9.3906,  8.3604,  2.3966,  2.1247]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.8556]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.8873,  0.2940, -1.3381, -0.7632, -3.2247, -3.4365, -1.1828, -1.4695,\n",
      "         -1.7597, -4.0284, -1.2227, -4.1033,  0.8177, -3.3870, -3.7822, -2.2303,\n",
      "         -1.9996, -6.1549,  0.8245, -2.1910, -3.7588, -2.3104, -1.6008, -2.1012,\n",
      "         -2.5239,  0.4223, -3.9642,  1.0202,  8.0266,  1.8368,  1.8626,  8.0456,\n",
      "          9.3577,  3.2196,  4.8336, 10.2754,  3.2072,  3.4719]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.2754]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.5417, -1.8331, -4.2236, -4.2769, -5.5175, -4.2445, -2.1667, -3.7183,\n",
      "         -5.9698, -2.2889, -2.0651, -4.7216, -2.9482, -3.1802,  1.8074, -2.5443,\n",
      "         -1.0470, -5.5440, -1.1322, -5.9525,  0.7803, -0.0687, -5.8979, -1.2216,\n",
      "         -3.3154, -1.4454, -2.2377,  2.7864,  5.6998,  1.8176,  9.8065, -0.9027,\n",
      "          1.9357,  5.0226,  4.8718,  2.4261,  2.4128, -3.5714]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.8065]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[30]], device='cuda:0')\n",
      "tensor([[-2.5490, -3.4297, -1.8830, -1.9905, -5.2912, -1.6729, -0.2461, -5.0087,\n",
      "         -5.6102, -5.2929, -5.1934, -5.3919, -4.1888, -7.9903, -6.9405, -8.3750,\n",
      "         -1.2477, -5.6782, -0.5863, -4.0248, -3.0079, -5.8012, -2.4649,  1.2913,\n",
      "         -5.6831,  2.1387,  0.9634,  8.2062, 12.6152,  3.2607,  2.4165,  1.7763,\n",
      "         11.3716,  4.0259,  1.7674,  7.1303, -2.0985, -0.6018]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[12.6152]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[28]], device='cuda:0')\n",
      "tensor([[-2.2311, -2.0878, -0.5479, -1.0044, -4.7974, -0.9428,  1.0363, -2.7416,\n",
      "         -5.8196, -1.3030, -5.3223, -8.2141, -6.9231, -7.7796, -8.0224, -7.8821,\n",
      "         -4.2874, -5.8508, -3.6157, -6.2457, -2.1793, -4.8916, -1.3941,  1.5740,\n",
      "         -3.5513, -1.7081,  1.9596,  7.3728, 12.4572,  6.1200,  6.7299,  2.1027,\n",
      "         13.6853,  6.7025,  3.7760,  9.0056, -1.0006,  0.6995]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[13.6853]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.1615,  1.3213,  0.5104,  0.0270, -4.0198, -0.9492,  0.4785, -1.5915,\n",
      "         -1.0328, -0.8425,  0.3419, -8.8702, -2.1483, -5.0768, -5.6119, -3.1582,\n",
      "         -4.1368, -8.6371,  0.3288, -5.3702, -3.7993, -2.1795, -3.3466, -4.5826,\n",
      "         -0.8501, -2.3355, -1.4340,  3.9607,  8.5443,  6.9588,  4.7122,  5.1161,\n",
      "         12.1637,  9.3157,  6.8251, 10.5335,  4.3236,  2.8713]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[12.1637]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.1946,  1.1972,  0.4895,  1.7359, -3.1574, -1.7089, -0.9717, -1.4439,\n",
      "         -1.8130, -1.9112, -0.1939, -8.0770, -1.9333, -4.1011, -5.6038, -3.3366,\n",
      "         -1.6277, -5.7161, -0.5850, -2.6888, -2.3926, -1.7549, -3.5483, -5.4953,\n",
      "         -0.6067, -3.3007,  0.5273,  2.3635,  7.5357,  3.7979,  4.5860,  5.4437,\n",
      "         10.2919,  8.8732,  5.9402, 10.0313,  5.2814,  1.1396]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.2919]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[ 3.3371e-01,  1.2590e+00,  6.7490e-03,  4.9142e-01, -4.3857e+00,\n",
      "         -9.0873e-01, -1.2692e+00,  1.3239e-01, -1.4770e+00, -2.1804e+00,\n",
      "          1.1510e+00, -6.3049e+00, -1.1970e+00, -4.4473e+00, -5.2607e+00,\n",
      "         -3.3836e+00, -2.7514e+00, -4.1845e+00, -2.7348e+00, -2.7918e+00,\n",
      "         -2.6873e+00, -2.8134e+00, -1.1994e+00, -3.6732e+00, -1.6621e+00,\n",
      "         -2.4965e+00, -4.0681e-01,  4.4659e+00,  9.2025e+00,  4.8342e+00,\n",
      "          2.9473e+00,  4.5237e+00,  1.1025e+01,  7.2622e+00,  5.0892e+00,\n",
      "          1.1169e+01,  3.2286e+00,  3.1329e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[11.1691]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.0898e+00,  6.4883e-01, -2.1270e+00, -4.9196e-01, -5.8720e+00,\n",
      "         -3.0690e+00, -1.9145e+00, -1.7008e+00, -2.4124e+00,  1.9101e+00,\n",
      "         -7.5592e-01, -5.1002e+00, -2.9199e+00, -2.1246e+00, -9.2527e-01,\n",
      "         -2.5682e+00, -2.8982e+00, -3.5611e+00, -5.0363e+00, -3.9327e+00,\n",
      "         -9.4281e-01,  2.9081e-03, -1.4566e+00, -2.2758e+00, -6.0457e-01,\n",
      "         -2.1684e+00, -2.2904e-01,  3.7164e+00,  1.7715e+00,  3.3791e+00,\n",
      "          6.3738e+00,  1.9945e-01,  8.5909e+00,  1.1077e+01,  5.9135e+00,\n",
      "          2.0607e+00,  2.4941e+00, -6.7894e-01]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[11.0771]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-4.2738e+00, -3.2213e+00, -1.5620e+00, -2.0635e+00, -8.8131e+00,\n",
      "         -3.4331e+00, -1.7675e+00, -4.8476e+00, -5.3746e+00, -4.2809e-01,\n",
      "         -4.5982e+00, -5.0270e+00, -5.8421e+00, -6.2320e+00, -4.2938e+00,\n",
      "         -7.9488e+00, -3.3983e+00, -7.0732e+00, -3.2546e+00, -2.8665e+00,\n",
      "         -3.1109e+00, -6.2354e-01, -1.9809e+00,  3.1988e+00, -3.1744e+00,\n",
      "         -1.3817e-01,  5.3688e-01,  4.9192e-01,  9.8916e+00,  9.1915e-01,\n",
      "          7.9171e+00, -3.1540e+00,  9.0967e+00,  4.0597e+00, -9.2819e-05,\n",
      "          4.9904e+00, -1.6709e+00, -5.8506e-01]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[9.8916]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[28]], device='cuda:0')\n",
      "tensor([[ -2.7330,  -3.9635,  -1.7758,  -1.6582,  -7.4631,  -1.5471,   0.3276,\n",
      "          -5.7097,  -7.7830,  -2.5233,  -5.0766,  -7.0711,  -7.1402, -11.4592,\n",
      "          -9.7593, -12.1369,  -1.8285,  -7.1273,  -0.3569,  -2.7883,  -3.0887,\n",
      "          -4.1421,  -5.1758,   5.3829,  -3.5039,   0.2045,  -0.0525,   3.8768,\n",
      "          13.2619,   2.6794,   5.9914,  -0.8494,  12.5330,   6.3494,   0.2813,\n",
      "          12.1355,   2.5443,  -0.3524]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[13.2619]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[28]], device='cuda:0')\n",
      "tensor([[ -2.9559,  -1.9113,  -1.1794,  -2.7279,  -6.7876,  -2.1225,   0.9435,\n",
      "          -2.3520,  -1.5157,  -3.4454,  -0.7247,  -6.3823,  -3.3922, -10.7713,\n",
      "          -6.8263,  -4.6758,  -6.3627,  -5.9003,  -1.9489,  -3.4999,  -5.1222,\n",
      "           1.0959,  -2.0857,  -2.5456,  -2.2843,   0.4585,   0.6175,   6.3325,\n",
      "           7.3209,   8.2816,   3.8744,   1.6071,  14.6927,   9.8196,   4.4118,\n",
      "           9.8121,   6.0648,   4.2529]], device='cuda:0',\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[14.6927]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.0839, -0.1938,  1.8519,  2.7461, -2.8604, -1.8725,  1.0805, -0.1017,\n",
      "          2.4150, -1.3814,  1.0917, -4.8165, -1.5527, -5.9151, -5.7082, -4.9445,\n",
      "         -3.5440, -8.5814, -2.0735, -4.5571, -2.9468, -2.5424, -3.5824, -2.9567,\n",
      "          0.6692, -1.4074,  0.7889,  1.0523,  6.1840,  4.3080,  6.3164,  0.1422,\n",
      "          8.2798, 12.4931,  5.4484, 10.4404,  4.4320,  1.1271]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[12.4931]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[ 0.2039,  0.7431,  1.4528,  2.8212, -4.0442, -0.2851,  0.7399,  1.1690,\n",
      "          0.6645,  2.0259,  1.0373, -4.8833, -2.4756, -5.5532, -5.1377, -4.7620,\n",
      "         -2.1356, -3.5986, -4.6016, -1.9104, -1.2353, -2.0756, -4.5983,  0.6865,\n",
      "          0.3676, -4.4379, -1.0850,  4.5341,  3.0157,  1.0658,  8.1355,  2.5221,\n",
      "          8.6052, 15.2225,  6.3134,  6.4547,  6.3850, -1.1210]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[15.2225]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-0.5823, -0.3336, -0.3514,  1.3759, -2.7932, -0.5312, -0.4678,  0.5062,\n",
      "          2.3247,  0.7462,  1.3887, -2.7494, -1.1620, -3.1658, -3.1085, -4.2180,\n",
      "         -2.5166, -3.4808, -2.6907,  0.1007, -2.1389, -1.4760, -2.3042, -1.3029,\n",
      "         -1.4330, -6.2775,  1.7063, -1.4680,  4.6643,  2.8055,  2.3194,  3.0593,\n",
      "          9.7159,  8.4918,  3.7334, 11.1912,  7.7412,  2.4292]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.1912]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-1.7285, -1.0578, -1.0557, -0.0496, -4.1564, -2.4285, -0.9826,  0.0387,\n",
      "          0.5639, -0.1725, -0.6415, -3.1203, -1.4007, -2.1206, -2.2111, -1.5337,\n",
      "         -3.3357, -3.9279, -0.7422, -1.2172, -1.7638, -1.1565, -1.6052, -4.3216,\n",
      "         -3.7881, -3.8073,  0.2762,  1.5768,  4.5249,  5.0776,  0.3072,  1.4039,\n",
      "         10.3718,  7.5657,  4.9179, 10.8891,  5.0177,  4.3224]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.8891]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.8299, -2.6571, -1.9077, -1.8517, -6.4340, -3.0001, -1.2060, -2.0571,\n",
      "         -4.7684, -1.8027, -4.2644, -5.1196, -3.7975, -4.8282, -3.1479, -6.4377,\n",
      "         -2.3614, -8.0932, -2.4651, -2.0339, -3.4034, -2.8273, -5.5354,  0.9258,\n",
      "         -5.8953,  1.1072, -0.3567,  3.2492, 12.0762,  4.7533,  5.4641, -1.8978,\n",
      "          7.4831,  2.4154,  3.8385,  5.9837,  0.9028,  3.6342]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[12.0762]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[28]], device='cuda:0')\n",
      "tensor([[ -3.4112,  -3.9876,  -3.7943,  -1.2106,  -6.6649,  -2.1403,   1.6222,\n",
      "          -4.0355,  -6.9309,  -2.2949,  -4.3898,  -4.2915,  -5.4837, -10.4929,\n",
      "          -8.3816, -12.7363,   0.5377,  -6.8208,  -2.0983,  -2.1091,  -1.8016,\n",
      "          -7.6854,  -6.4619,   5.6854,  -6.1290,   1.9440,  -1.8931,   7.1392,\n",
      "          12.4438,   4.7363,   8.6574,  -1.9112,   7.7229,   6.0628,   3.8185,\n",
      "           7.3404,   3.2426,  -0.8351]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[12.4438]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[28]], device='cuda:0')\n",
      "tensor([[ -1.8690,  -1.6203,  -1.3456,  -0.9258,  -5.2743,  -1.0349,   1.7614,\n",
      "          -2.6358,  -4.3325,  -2.5708,  -2.3988,  -6.5352,  -6.8529, -10.4696,\n",
      "          -8.2442, -10.8863,  -2.7221,  -4.2756,  -3.6932,  -3.5917,  -4.6037,\n",
      "          -5.6287,  -5.3996,   3.9205,  -4.2184,   1.3760,   1.7528,  10.9185,\n",
      "           9.5921,   5.9109,   9.3616,   1.1277,   8.0841,  11.0819,   6.3581,\n",
      "           7.4825,   4.2348,   0.6174]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[11.0819]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[ 1.3229,  2.3460,  3.4521,  3.1993, -1.3169,  0.5542,  1.1791,  2.5616,\n",
      "          1.1813, -1.2837,  2.2901, -4.1518, -1.8173, -5.9380, -6.0341, -4.5539,\n",
      "         -3.7278, -5.5060, -4.7757, -5.0757, -6.0413, -4.1522, -5.2504, -1.3715,\n",
      "         -1.5399, -0.1632,  0.7713,  8.4272,  6.9542,  6.4669,  9.3358,  3.8181,\n",
      "          6.9997, 11.9847, 10.5210,  6.4332,  6.4882,  1.6647]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.9847]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[ 0.6918, -0.5030,  2.0662,  2.7247, -2.3886, -1.6275,  0.2627,  1.6297,\n",
      "          2.0247, -1.8175,  1.8695, -2.8648, -0.4975, -3.6161, -8.5352, -4.5799,\n",
      "         -2.3963, -5.0413, -0.2711, -2.0146, -5.5955, -5.1058, -2.7912, -3.0219,\n",
      "         -0.3340, -1.4126,  0.1063,  3.2344,  9.2657,  5.1663,  0.9470,  4.7845,\n",
      "         10.4803,  6.2858,  7.3955, 14.2802,  3.7975,  2.3471]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[14.2802]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[ 1.0905,  0.6571, -0.5501,  2.1174, -3.5559,  0.4587,  0.0881,  3.2681,\n",
      "          0.3488,  1.1537,  2.0923, -3.9008, -0.2842,  0.1755, -3.3512, -3.8929,\n",
      "         -2.0175, -2.6713, -4.6758, -2.1955, -2.4687, -4.1474, -3.9857, -1.2328,\n",
      "         -1.2886, -4.3306, -0.0472,  2.5917,  5.4071,  3.3373,  6.9407,  3.7475,\n",
      "          9.6544,  9.8078,  7.7162,  6.2763,  5.2606,  0.8672]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.8078]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-2.7422, -1.0953, -2.2633, -1.0097, -4.9098, -2.7259, -0.1573, -0.6411,\n",
      "         -2.7517, -1.4223, -0.4280, -4.5279, -2.7867, -0.4174, -3.2988, -3.3700,\n",
      "         -3.2559, -4.9748, -2.8962, -1.9962, -2.1209, -2.9011, -3.1620, -3.4868,\n",
      "         -3.5236, -1.9267, -1.3324,  0.6740,  8.7346,  4.6842,  6.2270,  0.5825,\n",
      "         10.8219,  5.2623,  5.5589,  9.2051,  2.6898,  2.0889]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.8219]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-3.2164, -3.2688, -2.4812, -1.7110, -6.4641, -1.7631,  1.5734, -3.5143,\n",
      "         -5.9231,  0.0133, -1.8475, -5.4333, -5.9607, -4.2883, -4.4401, -8.3737,\n",
      "         -1.0117, -5.0327, -6.1549, -4.8972, -1.0524, -4.4983, -6.4698,  4.4302,\n",
      "         -5.4501, -1.5306, -2.9040,  4.7446,  9.3969,  3.1377, 11.9408, -3.7547,\n",
      "          4.5696,  8.0996,  5.8729,  4.5911,  2.4578, -1.6877]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.9408]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[30]], device='cuda:0')\n",
      "tensor([[ -1.7013,  -1.6177,   0.0576,  -0.1010,  -4.2337,  -0.8695,   3.2414,\n",
      "          -1.2131,  -6.2816,  -1.1532,  -1.7913,  -5.6949,  -4.0673,  -7.4343,\n",
      "          -8.3353, -10.1649,  -1.2967,  -6.2596,  -2.6028,  -2.6176,  -5.2203,\n",
      "          -7.2095,  -6.4803,   2.1405,  -3.5497,   1.8621,  -0.3377,   6.8337,\n",
      "          12.5522,   7.8890,   6.3203,   1.2059,  10.0323,   6.4022,   7.2808,\n",
      "           9.2030,   3.7358,   2.4015]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[12.5522]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[28]], device='cuda:0')\n",
      "tensor([[-1.1221,  0.1203,  1.1545, -0.1070, -3.3054, -0.9708,  1.2561, -0.0461,\n",
      "         -2.5633, -2.4648,  0.6515, -5.2456, -1.7045, -7.3130, -7.8934, -7.0154,\n",
      "         -0.7922, -2.5871, -1.5483, -3.5148, -6.7497, -3.6831, -5.3061, -1.0162,\n",
      "         -1.6491,  0.4738, -0.1442,  6.2748,  9.9681,  4.9993,  5.9744,  3.2056,\n",
      "          7.0688,  9.3315,  8.3987, 10.5617,  3.7957,  1.0691]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.5617]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[ 2.8828,  1.6784,  3.1340,  4.5463, -1.1584,  0.3352,  0.5753,  4.3295,\n",
      "          3.2164, -0.9075,  2.0155, -3.8681,  0.3041, -3.8716, -7.6109, -3.4035,\n",
      "         -2.6831, -5.9346, -1.7628, -3.5808, -6.6387, -6.1747, -2.5740, -3.8800,\n",
      "         -1.4020, -2.1374,  1.8857,  7.2187, 10.5125,  9.6042,  2.5686,  5.2121,\n",
      "         10.8900,  7.0025,  7.9514, 12.0201,  3.9302,  5.4031]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[12.0201]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[ 4.9742e-01,  1.2732e-01, -1.0913e-01,  2.7464e+00, -4.0353e+00,\n",
      "         -1.1241e+00, -7.8362e-01,  2.7365e+00,  1.1614e+00, -1.1936e+00,\n",
      "          1.8909e+00, -4.9714e+00,  6.9341e-01,  6.3760e-01, -6.4008e+00,\n",
      "         -2.8204e+00, -1.3934e+00, -4.0149e+00, -3.0236e+00, -8.3649e-01,\n",
      "         -4.3100e+00, -6.5905e+00, -3.9214e+00, -4.0983e+00, -9.2842e-01,\n",
      "         -1.3062e+00,  6.7649e-03,  5.4686e+00,  7.6594e+00,  5.9092e+00,\n",
      "          3.5678e+00,  3.8048e+00,  1.1869e+01,  5.7815e+00,  6.7440e+00,\n",
      "          1.0248e+01,  1.6763e+00,  1.5640e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[11.8692]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.5133, -1.9265, -0.9536, -2.1326, -4.4787, -1.3831, -0.4917, -1.1937,\n",
      "         -2.7825, -1.0005, -0.1383, -2.4085, -4.0019, -4.0122, -4.4723, -6.0927,\n",
      "         -1.9054, -3.8581, -0.6749, -2.1942, -2.1360, -3.4271, -3.2449, -1.3190,\n",
      "         -2.7891, -1.1692, -3.4009,  0.2645,  9.7461,  3.2348,  6.0640,  0.1502,\n",
      "          8.6167,  3.9774,  4.8827, 10.5739,  2.9405,  1.3996]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.5739]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-1.3382,  0.1325, -0.7781,  0.2892, -3.6414, -0.8181,  0.8386,  0.3058,\n",
      "         -2.4797,  1.3091,  1.0093, -6.0484, -3.2794, -2.6010, -4.1404, -4.3741,\n",
      "         -2.5480, -4.2500, -5.1684, -5.8637, -2.9311, -2.6224, -3.2578, -0.8733,\n",
      "         -4.8479, -4.2058,  0.1039,  6.4582,  7.3095,  8.1567,  6.1121,  1.7064,\n",
      "          7.1231, 11.7040,  8.9272,  6.3003,  5.5122,  2.4580]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.7040]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-3.3815, -1.0531,  0.5165, -0.8362, -3.3478, -2.4249,  0.9554,  0.4483,\n",
      "         -2.9981, -0.2875,  0.7813, -3.5444, -3.3145, -2.8522, -2.2328, -4.3257,\n",
      "         -2.0831, -3.9559, -4.4808, -3.7524, -3.3277, -3.5092, -5.0300, -1.4770,\n",
      "         -2.9761,  0.3972, -2.1993,  3.6171,  5.7424,  6.0201,  7.8919, -0.4590,\n",
      "          7.9652,  6.3922,  9.2688,  8.2389,  4.6370,  3.0242]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.2688]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-2.1939, -1.9007, -0.9140, -0.5366, -3.8917, -1.4814,  0.5287, -0.7507,\n",
      "         -5.3551, -0.8526, -1.7841, -4.3042, -3.8980, -3.0606, -5.8852, -5.4043,\n",
      "         -1.4165, -5.0944, -1.3761, -3.7115, -3.9968, -4.5027, -7.0476, -1.7987,\n",
      "         -5.1610, -2.2007, -2.1909,  4.5627, 12.3030,  4.8812,  8.5782, -0.9073,\n",
      "          7.7355,  5.6022,  8.2165,  8.7271,  1.8984,  2.9713]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[12.3030]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[28]], device='cuda:0')\n",
      "tensor([[-3.1261, -2.8308, -2.2637, -3.0755, -4.4642, -2.7468, -0.6580, -4.4257,\n",
      "         -6.4964, -1.9261, -2.4095, -6.1962, -4.4946, -7.6869, -7.6878, -9.8178,\n",
      "          1.1373, -3.2211, -0.1762, -2.6597, -2.9472, -5.2747, -6.4962,  2.2067,\n",
      "         -3.6862,  0.3014, -1.0064,  5.4098,  8.8041,  2.5920,  5.8861, -0.1249,\n",
      "          6.4602,  8.0593,  2.6274,  9.3212,  4.7640, -0.6759]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.3212]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[ 0.1297,  1.2715,  2.4144,  0.9280, -1.7413,  0.7542,  2.5082,  3.6752,\n",
      "          0.2957, -2.3089,  0.7983, -4.0832, -1.1259, -4.7884, -5.3955, -4.9462,\n",
      "         -3.1848, -3.7195, -4.6686, -3.3752, -5.9646, -4.6345, -0.8529, -2.2240,\n",
      "         -2.3453, -0.3567,  2.3299,  7.4408,  9.2252, 10.8102,  2.1109,  4.2442,\n",
      "         10.4173,  8.7460,  8.4296,  8.7253,  4.2892,  5.5938]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.8102]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[29]], device='cuda:0')\n",
      "tensor([[-0.8151, -0.7133,  2.8689,  1.1607, -4.0232, -0.9427,  1.7191,  0.9443,\n",
      "         -1.4007, -1.4742,  1.3335, -4.8322, -2.6110, -4.0637, -7.2261, -7.5355,\n",
      "         -2.0357, -3.7253, -3.3543, -1.3941, -3.6102, -6.8144, -5.1606, -0.7647,\n",
      "         -0.9759,  2.5173, -1.6054,  5.8923,  8.5424,  5.5572,  5.6076,  1.7666,\n",
      "          9.4905,  8.5195,  9.0880,  8.8741,  2.6986,  1.4865]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.4905]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[ 0.3537,  1.3933,  2.2059,  1.6395, -2.4724,  0.0276,  1.0060,  2.8130,\n",
      "         -0.1792, -0.7492,  1.5616, -3.9975, -0.2589, -2.6164, -4.7694, -4.8091,\n",
      "         -1.9443, -2.1101, -3.0697, -2.2216, -4.9412, -5.1349, -5.5208, -2.5807,\n",
      "         -1.5116, -1.2096, -1.4510,  5.9569,  7.3837,  6.9741,  5.8371,  3.4221,\n",
      "         10.3451,  9.5104,  9.6179,  9.6124,  4.3513,  3.3221]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.3451]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.1509, -1.4364, -0.4524,  0.1341, -4.0780, -0.8864, -0.3356, -0.1175,\n",
      "         -1.4309,  0.2406,  0.6992, -5.3006, -3.2099, -0.3598, -3.0289, -4.8195,\n",
      "          0.6570, -5.0597, -4.2679, -2.6056, -0.2441, -3.7301, -6.0219, -2.4416,\n",
      "         -3.0514, -4.1712, -0.1497, -0.0227,  6.9833,  3.1874,  7.7399,  0.9065,\n",
      "          7.3099,  9.1108,  4.8770,  8.3971,  6.2938,  0.6541]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.1108]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-1.4845, -0.7935, -1.3815, -0.6515, -4.3546, -1.3834,  0.1135, -0.8820,\n",
      "         -3.2667,  0.4646,  0.4200, -3.2634, -3.4691, -2.0553, -3.4590, -5.1005,\n",
      "         -0.6297, -3.3807, -4.7631, -3.6683, -1.2251, -3.7273, -5.6700,  0.2731,\n",
      "         -3.7695, -1.1303, -2.0984,  4.1047,  6.3453,  4.0551,  7.6466, -1.6101,\n",
      "          7.3674,  6.9016,  6.4101,  5.4109,  2.1446,  0.6043]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[7.6466]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[30]], device='cuda:0')\n",
      "tensor([[-2.1186, -2.4702, -1.4127, -1.8518, -4.9757, -1.2645,  1.7398, -2.9760,\n",
      "         -5.2461, -1.9864, -1.8922, -4.2603, -4.2194, -6.8151, -7.3507, -9.2599,\n",
      "         -0.5799, -5.7884, -1.6778, -2.9271, -3.9032, -5.7980, -4.7952,  0.9704,\n",
      "         -4.0483,  1.5287, -1.8161,  4.8328, 11.9232,  4.9798,  4.9931,  0.7964,\n",
      "          9.1970,  5.2667,  4.2394,  8.8800,  2.2600,  1.3369]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.9232]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[28]], device='cuda:0')\n",
      "tensor([[-1.4947, -0.7812, -0.1663, -0.6329, -3.6713, -0.8549,  2.7362, -1.6368,\n",
      "         -3.8728, -1.4257, -0.6796, -5.3577, -4.5368, -9.0047, -6.9832, -9.7716,\n",
      "         -1.5612, -4.8055, -3.3024, -4.1218, -4.8494, -5.2473, -5.8441,  0.9190,\n",
      "         -2.0870,  1.0673,  0.5572,  7.7863,  8.6694,  6.3491,  7.5138,  2.4908,\n",
      "          9.3746, 10.7737,  6.3975,  8.8801,  5.0211,  0.7307]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.7737]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[ 9.8836e-01,  1.3277e+00,  2.4941e+00,  3.3749e+00, -1.1190e+00,\n",
      "          7.3065e-03,  6.9020e-01,  3.4994e+00,  2.6062e+00, -1.5089e-01,\n",
      "          1.9781e+00, -2.7538e+00, -6.2251e-01, -3.6875e+00, -4.9695e+00,\n",
      "         -6.0015e+00, -2.3972e+00, -6.9843e+00, -3.9714e+00, -4.5497e+00,\n",
      "         -5.3611e+00, -5.6389e+00, -4.1337e+00, -2.8979e+00, -2.2719e+00,\n",
      "         -3.8483e-01,  1.3173e+00,  4.7037e+00,  8.8818e+00,  8.1486e+00,\n",
      "          3.4295e+00,  3.3252e+00,  1.0636e+01,  8.7032e+00,  7.2849e+00,\n",
      "          1.0824e+01,  4.2049e+00,  3.5757e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[10.8238]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-0.0610, -0.0254,  1.3170,  3.0725, -0.8453, -0.7111, -0.5675,  1.7800,\n",
      "          0.9196,  1.6092,  2.8109, -2.9275, -0.4953, -0.9715, -4.8143, -3.4112,\n",
      "         -2.1256, -5.0714, -3.1953, -2.3297, -3.8261, -3.9760, -2.7519, -3.4106,\n",
      "         -0.2680, -2.7937,  1.2035,  1.6355,  4.5764,  4.1455,  3.9342,  4.9142,\n",
      "         10.9169,  9.5134,  7.0788,  8.7340,  6.1508,  0.9110]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.9169]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-3.2665, -1.3390, -2.3977, -1.7119, -3.3927, -2.1367, -0.9285, -1.3691,\n",
      "         -3.5358,  0.7117,  0.1010, -3.9613, -4.7982, -0.9956, -3.5342, -4.9989,\n",
      "         -0.4999, -3.7796, -3.0846, -3.4624, -0.6148, -2.6751, -4.2941, -1.1893,\n",
      "         -2.2545, -3.0142, -1.1341, -1.0792,  6.8739,  0.4344,  7.0133,  0.7375,\n",
      "          8.2059,  7.0769,  3.9979,  7.9391,  4.3632,  0.8920]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.2059]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.0744, -1.3237, -0.9700, -1.1713, -3.5070, -1.7645,  1.7785, -1.9315,\n",
      "         -4.3273, -1.7744, -0.3944, -4.7856, -3.0037, -4.6330, -4.7047, -7.0963,\n",
      "         -1.5401, -5.3969, -3.3291, -3.7373, -1.9125, -3.2869, -3.9611, -1.9519,\n",
      "         -5.6481, -0.5220, -2.0468,  3.7122,  9.3786,  6.8060,  5.3096,  0.9911,\n",
      "          7.6243,  8.0577,  6.1343, 10.1146,  6.2159,  2.3454]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.1146]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.7724, -0.5191,  0.1146, -1.0083, -2.6965, -2.1930,  0.1959,  1.5208,\n",
      "         -2.3896, -0.2454,  1.1741, -2.7723, -3.5441, -3.9407, -2.4001, -2.1306,\n",
      "         -4.4500, -3.7175, -5.2240, -4.4420, -3.7946, -2.2189, -1.9056, -1.7464,\n",
      "         -3.4724,  0.4216, -0.8448,  6.8586,  4.9357,  6.8512,  5.2095, -0.1409,\n",
      "          8.7849,  8.2733, 10.5300,  5.6510,  3.7307,  2.9833]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.5300]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-3.0795, -1.2363, -0.7791, -0.9825, -3.5791, -2.6669, -1.1574, -0.3189,\n",
      "         -3.9502,  0.8434, -0.6177, -2.7297, -4.4584, -2.8191, -4.3748, -7.6125,\n",
      "         -2.0428, -5.0212, -4.3440, -2.9899, -4.5412, -4.5942, -5.4762,  2.2316,\n",
      "         -3.4692,  1.8683, -2.2220,  2.9755,  7.4960,  1.7002,  8.0412, -1.8875,\n",
      "          9.2804,  6.9836,  4.6450,  7.0800,  2.0443,  2.7097]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.2804]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.9367, -1.6284, -2.4587, -0.8552, -2.6600, -1.1433,  1.2498, -1.7035,\n",
      "         -4.8566,  0.5648, -1.2657, -4.0306, -4.9542, -4.6517, -5.2911, -7.9882,\n",
      "          2.1086, -3.5112, -1.2051, -3.9005, -2.3667, -3.4275, -7.3029,  0.0691,\n",
      "         -4.5680, -3.8175, -1.9162,  3.5440,  7.2574,  3.2049,  8.3992,  1.0586,\n",
      "          5.8669,  9.4837,  5.5799,  7.0206,  7.8905,  1.1243]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.4837]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-1.9339, -1.5136,  1.4489, -0.8112, -1.1904, -1.4970,  0.2203,  1.0943,\n",
      "         -1.0686, -3.4705,  0.4488, -3.1813, -2.2223, -2.8836, -4.9773, -4.1169,\n",
      "         -0.1740, -4.0627, -1.7168, -3.3825, -5.1116, -3.1774, -4.4409, -4.7764,\n",
      "         -5.4022, -1.4564, -0.1805,  1.9686, 10.0413,  7.7541,  2.6017,  1.0837,\n",
      "          8.5850,  7.3549,  6.1595, 13.6040,  5.9754,  4.7793]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[13.6040]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-1.6460e+00, -9.1864e-03,  1.4218e-01,  1.3997e+00, -1.7158e+00,\n",
      "         -9.9559e-01, -7.5044e-01,  2.4878e+00, -3.1870e-01,  2.5310e-01,\n",
      "          2.1400e+00, -3.0156e+00, -1.9609e+00, -1.3213e+00, -2.7204e+00,\n",
      "         -1.7257e+00, -2.7503e+00, -3.9106e+00, -4.5556e+00, -3.2858e+00,\n",
      "         -3.7543e+00, -3.1914e+00, -2.7268e+00, -2.8839e+00, -2.6458e+00,\n",
      "         -1.8857e+00, -4.4928e-01,  6.1637e+00,  2.3628e+00,  4.6525e+00,\n",
      "          6.3772e+00,  3.3389e-01,  1.0167e+01,  1.2301e+01,  9.3508e+00,\n",
      "          6.0176e+00,  5.2621e+00,  2.4764e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[12.3012]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-3.7686, -1.8615, -1.5232, -1.6834, -3.4725, -2.8974, -1.8493, -1.0040,\n",
      "         -2.6081,  0.0958, -0.8931, -3.5523, -4.9851, -1.6754, -4.4744, -6.2326,\n",
      "         -1.1216, -3.9690, -3.1232, -1.2364, -2.0689, -3.6035, -3.2851,  0.9538,\n",
      "         -2.0776,  0.1297, -2.0574,  0.5878,  4.9880,  0.5817,  5.9489, -0.8264,\n",
      "         10.3876,  7.9793,  3.5385,  6.3351,  2.0858,  3.1552]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.3876]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.7792, -0.6022, -1.1683, -1.1402, -2.1009, -0.8931,  1.2108, -0.6600,\n",
      "         -4.0835,  0.2457, -0.1625, -2.9928, -4.0414, -4.0869, -4.1263, -7.1883,\n",
      "          0.5989, -3.4664, -0.6453, -3.2418, -2.5123, -4.1038, -4.6197, -0.6860,\n",
      "         -2.7954, -2.6435, -1.7667,  1.4198,  6.5911,  4.1070,  6.2231,  2.2203,\n",
      "          7.2481,  8.0442,  5.5226,  8.0233,  6.7324,  1.2494]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.0442]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-1.8196, -1.8291,  0.0294, -1.0412, -2.6884, -1.8763,  0.8023,  0.8517,\n",
      "         -0.8594, -2.7321,  0.4415, -3.6664, -2.6932, -4.0498, -4.3317, -5.0599,\n",
      "          0.1207, -5.1600, -3.0705, -3.7086, -3.6955, -3.9416, -5.9752, -3.1286,\n",
      "         -6.1674, -1.1827, -0.3901,  3.3556,  9.4012,  7.7804,  4.7796, -0.1106,\n",
      "          6.6555,  8.2969,  6.3358, 11.3613,  6.2251,  2.8354]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.3613]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-1.6519, -0.4626, -0.6316,  1.2966, -3.2405, -1.6422, -1.1865,  2.3929,\n",
      "          0.0630,  1.4814,  1.4297, -3.4414, -2.2070, -1.6636, -3.7946, -2.5741,\n",
      "         -2.0336, -4.6340, -4.8818, -4.0214, -3.7653, -3.9123, -4.8197, -1.5632,\n",
      "         -3.5657, -1.1539, -1.2935,  8.0945,  2.9767,  5.2043,  7.1246, -0.4738,\n",
      "          9.4509, 11.3222,  8.3470,  5.5565,  3.8115,  1.7982]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.3222]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-4.2113e+00, -2.4185e+00, -1.9274e+00, -1.7466e+00, -4.1432e+00,\n",
      "         -3.2304e+00, -1.8915e+00, -1.1670e+00, -2.8232e+00,  1.2091e+00,\n",
      "         -1.4606e+00, -3.9943e+00, -5.1878e+00, -2.6397e+00, -4.6838e+00,\n",
      "         -7.6059e+00,  7.0757e-01, -4.2207e+00, -3.0294e+00, -2.7524e+00,\n",
      "         -1.7551e+00, -3.4395e+00, -4.9181e+00,  2.2791e+00, -2.5046e+00,\n",
      "         -6.4006e-03, -2.7585e+00,  8.9178e-01,  5.1306e+00,  2.2477e-01,\n",
      "          6.8986e+00, -1.5373e+00,  8.3499e+00,  8.4934e+00,  2.9645e+00,\n",
      "          5.9062e+00,  3.1839e+00,  2.7924e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[8.4934]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-2.7903,  0.2467,  0.4844, -0.8756, -1.1217, -1.0050,  0.8766,  0.7098,\n",
      "         -3.1901, -0.1755,  0.4624, -2.2275, -3.0322, -2.7806, -2.8019, -4.8622,\n",
      "         -0.7371, -3.6873, -1.1177, -3.4068, -3.2558, -3.5060, -5.0199, -2.5445,\n",
      "         -2.2192, -2.2416, -1.4030,  1.2495,  5.7420,  4.7552,  6.0746,  2.4433,\n",
      "          7.8382,  8.3715,  7.9112,  8.4100,  7.1589,  2.4184]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.4100]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.7492, -2.2707,  0.0716, -1.6023, -1.4858, -3.2847, -0.9050, -0.1636,\n",
      "         -1.2967, -2.7786,  0.0849, -1.6595, -3.0942, -1.8500, -3.3055, -2.5883,\n",
      "         -1.5818, -4.6744, -0.8863, -3.6158, -3.9697, -1.6459, -3.7399, -4.4759,\n",
      "         -5.9405, -2.5025, -0.7826,  0.3264,  9.1430,  5.8389,  2.0442, -0.4532,\n",
      "          7.9406,  5.4176,  5.5494, 13.0008,  5.1233,  4.2962]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[13.0008]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.7857, -1.7329, -1.1420, -1.1547, -2.0198, -3.0267, -0.8324, -0.0291,\n",
      "         -3.1216, -0.7808,  1.3263, -3.2560, -4.5737, -1.5338, -2.3390, -1.6398,\n",
      "         -1.5507, -2.8057, -2.4002, -2.8674, -1.2333, -2.0283, -3.7175, -2.6559,\n",
      "         -2.7980, -2.5742, -0.1762,  4.3002,  2.6071,  3.3322,  4.9414, -0.9616,\n",
      "          7.5791,  9.5044,  8.2673,  5.6354,  5.4909,  1.6329]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.5044]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-3.6169, -2.1342, -1.0136, -2.1624, -3.1542, -2.7267, -0.7098, -0.6363,\n",
      "         -3.0519, -1.8100, -0.6989, -2.9824, -4.3953, -2.6584, -5.1653, -6.0586,\n",
      "         -0.6954, -3.9936, -2.2366, -2.2686, -2.8279, -3.5591, -3.9171,  0.0282,\n",
      "         -3.2792,  1.0231, -2.7447,  1.2633,  6.8979,  3.9498,  3.7115, -0.0191,\n",
      "          9.8882,  5.8598,  5.3410,  7.7893,  2.9925,  4.1004]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.8882]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-3.1166, -1.3568, -1.1202, -1.1639, -2.4859, -1.5832,  0.9326, -0.6223,\n",
      "         -3.4895, -1.0779, -0.4717, -4.2736, -4.5764, -5.3617, -4.9541, -7.1837,\n",
      "          2.1361, -3.6389, -1.4612, -3.4819, -2.6747, -3.2866, -6.5158, -1.2211,\n",
      "         -3.4543, -2.4404, -1.2037,  3.2477,  6.5905,  4.6314,  5.5924,  2.5527,\n",
      "          7.4164, 10.0898,  5.8167,  8.3114,  7.6772,  1.8828]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.0898]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-2.5837, -1.6360,  1.3262, -0.4705, -1.1930, -2.3249, -0.3324,  1.0970,\n",
      "         -0.3551, -2.1019,  0.7528, -2.6883, -3.8225, -2.5409, -4.4657, -4.7883,\n",
      "         -1.0003, -4.5232, -2.5249, -3.7720, -3.7961, -2.8905, -4.4077, -3.1080,\n",
      "         -4.6222, -1.0411, -0.9860,  1.3025,  8.2947,  6.3952,  2.6385,  1.1886,\n",
      "          8.8617,  7.5773,  6.7332, 12.5752,  5.5817,  4.5873]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[12.5752]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.2406e+00, -1.8905e+00, -7.0534e-01,  4.2139e-02, -2.4708e+00,\n",
      "         -2.7721e+00, -1.3868e+00,  4.4406e-01, -1.4384e+00, -7.5426e-02,\n",
      "          2.3656e+00, -2.6382e+00, -3.4472e+00, -8.6092e-01, -2.6863e+00,\n",
      "         -1.7704e+00, -1.3436e+00, -4.1274e+00, -2.7051e+00, -3.1927e+00,\n",
      "         -1.9996e+00, -1.9798e+00, -3.5614e+00, -2.7562e+00, -2.7407e+00,\n",
      "         -3.0798e+00, -8.5359e-01,  2.0846e+00,  2.9805e+00,  3.0947e+00,\n",
      "          5.1028e+00, -1.3503e-03,  8.5687e+00,  1.0334e+01,  7.8417e+00,\n",
      "          6.6572e+00,  5.7875e+00,  1.3970e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[10.3345]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-3.8581, -1.8492, -1.2088, -2.6315, -2.3970, -3.1639, -0.8034, -0.4990,\n",
      "         -3.5474, -1.5564,  0.4830, -3.4960, -3.8265, -1.8554, -3.5847, -3.4310,\n",
      "         -1.4507, -2.4993, -2.9051, -3.1550, -2.5725, -2.4377, -3.2901, -1.1680,\n",
      "         -2.7557,  0.3040, -1.7906,  1.5648,  5.7260,  3.8621,  3.2903, -0.7296,\n",
      "          8.8357,  5.9148,  6.2525,  8.6073,  3.2356,  3.4700]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.8357]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.4450, -1.0040, -0.5804, -1.5309, -2.8918, -2.0857,  1.5262, -0.7249,\n",
      "         -3.9460, -1.5193,  0.0854, -3.5045, -2.6492, -4.7828, -4.5488, -5.2045,\n",
      "         -0.4091, -3.3593, -1.4704, -3.9727, -3.9745, -3.5221, -4.5120, -1.3718,\n",
      "         -3.0211, -0.5400, -1.8992,  4.4029,  6.2809,  6.2354,  4.7401,  1.7927,\n",
      "          8.5513,  7.8245,  6.9024,  8.4448,  5.5291,  1.9016]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.5513]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.5301,  0.7735,  2.4277,  0.1279, -0.3678, -1.0522,  1.9236,  1.5714,\n",
      "         -1.5368, -1.6239,  1.5178, -3.2826, -2.4636, -3.9169, -5.1871, -6.7993,\n",
      "         -2.8094, -3.8860, -3.5308, -4.3458, -5.9535, -4.1661, -3.8051, -1.3393,\n",
      "         -2.0969,  1.4067, -0.9283,  2.9361,  8.6071,  6.5842,  4.5523,  2.2334,\n",
      "         10.7292,  9.6676,  8.2516, 10.0087,  4.5963,  4.9708]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.7292]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.5023, -0.6700,  2.2005,  1.1665, -0.0770, -0.4579,  0.9480,  1.1112,\n",
      "         -0.8459, -0.6192,  1.7357, -2.0411, -3.2032, -3.2903, -5.3596, -5.1433,\n",
      "         -1.4960, -5.5870, -1.7668, -4.2946, -3.3266, -3.8031, -4.0619, -3.0665,\n",
      "         -1.4968, -0.5041, -0.0510,  1.7519,  7.6449,  4.5175,  4.5544,  3.0757,\n",
      "          8.2532,  8.4725,  9.2219,  9.5553,  6.0410,  2.2598]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.5553]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.7457, -1.2926, -1.5915, -1.3263, -2.2927, -3.2017, -2.4217, -0.4161,\n",
      "         -2.8539,  0.9716,  0.4012, -2.0412, -3.9153, -0.4107, -1.8497, -3.5300,\n",
      "         -0.8660, -3.7553, -2.3155, -4.5568, -2.4758, -1.2515, -5.9131, -2.3406,\n",
      "         -1.8935, -3.7129,  0.6847,  0.1141,  4.3839,  0.9556,  7.6652,  1.4619,\n",
      "          6.4074,  7.3096,  6.3174,  5.3288,  5.7558, -0.1557]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[7.6652]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[30]], device='cuda:0')\n",
      "tensor([[-2.3647, -3.5372, -2.5868, -2.7304, -5.2962, -2.7346, -2.3229, -5.1217,\n",
      "         -5.9259, -3.6407, -2.3294, -4.5462, -5.0824, -4.6512, -7.7437, -8.2972,\n",
      "          0.2296, -4.0946, -0.0860, -2.8894, -2.2400, -3.9497, -3.5725,  0.2804,\n",
      "         -7.3077,  1.0966, -2.8839,  3.8343, 11.2731,  3.4651,  3.3629, -0.1610,\n",
      "          7.2526,  4.3155,  1.9924,  9.9445,  1.2781,  0.3691]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.2731]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[28]], device='cuda:0')\n",
      "tensor([[-0.6727, -1.2748, -1.1493, -1.2511, -3.7677, -1.5972,  0.7522, -3.1105,\n",
      "         -4.0046, -3.3785, -1.2189, -4.5651, -3.5428, -8.5657, -7.0574, -7.4824,\n",
      "         -1.3033, -3.2126, -1.6525, -4.1662, -5.1582, -5.0213, -2.3706,  1.1697,\n",
      "         -2.4858,  2.1953,  1.4089,  8.3834,  8.4261,  4.6743,  2.2839,  1.1457,\n",
      "          8.8063,  8.3648,  4.0035,  7.4185,  2.0738,  0.0589]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.8063]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[ 0.6327,  2.6912,  3.5780,  1.9300, -0.8248,  0.4518,  2.2811,  3.0283,\n",
      "          0.0704, -1.4764,  1.6872, -1.9634, -1.5227, -7.1366, -5.0732, -7.4255,\n",
      "         -4.8535, -5.6238, -5.6182, -3.8757, -6.5076, -5.2493, -0.8083, -1.0389,\n",
      "         -1.0737,  3.2844,  0.7735,  5.6467,  8.6366,  7.5437,  4.8080,  3.1972,\n",
      "         12.6135, 10.0248,  7.2564,  7.3703,  3.9611,  4.9521]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[12.6135]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[ 0.5677,  0.5481,  4.0276,  3.0219,  1.3272,  0.3199,  2.1991,  1.4560,\n",
      "          1.1921, -0.3084,  3.4942, -2.1552, -0.6354, -4.5649, -5.2961, -4.7463,\n",
      "         -2.8954, -5.5066, -3.6197, -2.0717, -4.4629, -4.2490, -1.3915, -3.7282,\n",
      "          0.3964,  0.9561,  1.4514,  2.0074,  5.9106,  4.9834,  3.1359,  5.3391,\n",
      "          9.6631,  9.2601,  7.8563, 10.1171,  6.9749,  2.1679]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.1171]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-1.1513, -0.0792,  0.5576,  0.5099,  0.6353, -0.8760,  0.5009,  1.2957,\n",
      "         -0.5297,  0.9389,  2.6336, -0.3551, -0.5308, -1.6176, -2.1263, -3.0323,\n",
      "         -3.9492, -3.5345, -3.1406, -2.5235, -4.4624, -2.0795, -2.0678, -2.2677,\n",
      "         -0.9374, -1.4189,  0.6815, -1.2820,  5.0576,  2.4678,  3.7798,  3.4872,\n",
      "          8.5587,  7.7051,  7.0733,  8.4390,  6.4668,  1.8124]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.5587]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-3.3254, -1.0500, -0.8393, -1.9775, -0.0314, -2.6423, -0.4014, -1.5120,\n",
      "         -2.9519, -0.9019, -0.1309, -2.3337, -2.8088, -1.8439, -2.7218, -3.5475,\n",
      "         -2.8011, -3.7494, -1.5393, -2.9107, -4.0245, -1.2566, -1.9636, -3.8621,\n",
      "         -3.9103, -1.0353,  0.7370, -0.5723,  7.4760,  3.0193,  1.8556,  1.7805,\n",
      "          7.4468,  4.7572,  5.0317, 10.8210,  5.5088,  3.4956]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10.8210]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.5105, -1.9594, -1.2048, -2.8255, -1.6403, -3.9798, -0.6382, -1.2774,\n",
      "         -4.1743, -0.5631,  0.1738, -2.8821, -3.9390, -2.4097, -2.5004, -1.6544,\n",
      "         -2.9377, -3.5260, -3.0614, -3.9086, -3.2361, -0.8930, -2.7259, -3.4742,\n",
      "         -4.4420, -0.0860, -0.1987,  2.9338,  5.3815,  4.8978,  2.9419, -0.0661,\n",
      "          7.6659,  6.1058,  7.2956,  8.0927,  4.8095,  3.0339]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.0927]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.8917, -1.2207, -0.2910, -1.7488, -1.5443, -3.3798, -0.5600, -0.5424,\n",
      "         -3.8121, -0.5093,  0.7171, -2.7840, -3.4148, -2.7682, -2.3827, -3.0815,\n",
      "         -2.9375, -4.0928, -3.7118, -3.7495, -3.8102, -2.1831, -4.3973, -1.7099,\n",
      "         -3.5917,  0.3875, -1.7097,  3.7210,  5.1188,  4.1748,  5.4416, -1.1499,\n",
      "          7.7746,  7.5062,  7.9580,  7.0695,  4.0773,  3.0048]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[7.9580]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-3.7472, -1.1295, -0.5771, -1.6734, -0.5194, -2.6260,  0.3713, -0.4641,\n",
      "         -3.7394,  0.5534, -0.0145, -2.4034, -3.7408, -2.5425, -3.4714, -5.0517,\n",
      "         -1.1272, -3.6341, -2.0424, -3.8207, -3.5914, -2.4072, -5.0559, -0.6750,\n",
      "         -2.6557, -0.6541, -2.2329,  0.9236,  5.4502,  3.6180,  4.9187,  0.7863,\n",
      "          7.6879,  6.9266,  6.4381,  8.7173,  5.9631,  3.6829]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.7173]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.3762, -0.2622,  0.7357, -0.9685,  0.5787, -2.5787,  0.3636, -0.1189,\n",
      "         -2.7500, -0.7910,  0.8101, -1.7611, -2.8429, -1.7689, -2.6295, -3.8113,\n",
      "         -1.8533, -4.0294, -1.5505, -3.7989, -4.3728, -1.4685, -5.2980, -3.2475,\n",
      "         -3.9567, -1.5147, -0.8543,  0.4982,  6.0741,  5.1859,  3.0384,  1.4029,\n",
      "          7.4633,  7.3961,  7.8228, 10.8197,  7.4106,  4.6829]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.8197]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.1705, -1.5048, -0.1853, -1.3558,  0.0340, -3.2722, -0.6092, -0.4591,\n",
      "         -3.2519, -1.4570,  0.8980, -2.6849, -3.2610, -1.4587, -1.8737, -1.6697,\n",
      "         -2.3809, -3.7859, -2.0750, -2.8994, -2.6650, -1.5084, -4.7593, -3.8209,\n",
      "         -3.6140, -1.5129, -0.2899,  1.6892,  4.2023,  4.3215,  4.1952, -0.7286,\n",
      "          8.0223,  7.3527,  7.8081,  8.6341,  6.1071,  3.3384]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.6341]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.8938, -0.9477, -0.5590, -1.5646, -0.9824, -3.2327, -0.6303,  0.0879,\n",
      "         -3.2034, -1.1311,  1.0888, -2.9414, -3.5999, -1.3191, -2.2861, -2.1243,\n",
      "         -3.2546, -3.1284, -3.0455, -3.0857, -2.6171, -2.1335, -3.1672, -2.4457,\n",
      "         -1.9460, -0.3833, -0.9563,  2.0485,  3.3124,  3.8805,  3.8288, -0.7430,\n",
      "         10.5522,  8.1951,  8.1084,  7.5013,  3.8493,  4.0276]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.5522]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.8386, -0.8182, -0.5714, -1.8957, -1.3689, -1.6949,  1.9250, -0.1267,\n",
      "         -3.1869, -0.5550, -0.1371, -3.3449, -3.2601, -3.2158, -3.7407, -5.4828,\n",
      "         -0.8180, -3.3447, -1.5725, -3.3901, -3.0385, -4.4890, -5.0284, -0.5206,\n",
      "         -1.7031, -0.1693, -1.2863,  1.4956,  5.2388,  4.3945,  5.6961,  2.0191,\n",
      "          8.2191,  6.9138,  5.5172,  8.2221,  5.9052,  2.3679]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.2221]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.4557,  1.2628,  1.7959, -0.2514,  0.5964, -1.6475,  1.2254,  1.1842,\n",
      "         -2.5588, -0.4008,  2.0339, -1.5490, -1.7594, -2.6386, -2.8788, -4.3741,\n",
      "         -3.7592, -3.6342, -2.8152, -4.6036, -5.4654, -2.2761, -4.1209, -2.4464,\n",
      "         -3.8094, -0.9514, -1.5401,  1.6475,  7.2868,  6.6747,  4.2983,  1.4141,\n",
      "          8.9736,  8.8955,  9.1918, 10.3408,  6.2406,  4.6365]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.3408]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.8067, -1.3899,  0.1711, -0.3096,  1.2405, -2.9463, -0.2713, -0.5862,\n",
      "         -2.1523, -1.3000,  1.0303, -1.6118, -2.9202, -0.4953, -2.4888, -2.9972,\n",
      "         -2.9258, -5.0454, -1.6154, -3.3045, -3.1618, -2.3796, -3.8309, -4.2823,\n",
      "         -3.9586, -1.5312, -0.7098, -1.1896,  6.3360,  4.1616,  3.5348,  0.4829,\n",
      "          9.1434,  6.9072,  6.9258, 11.0308,  6.1322,  3.7054]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.0308]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-5.3715, -2.0064, -2.4244, -3.1772, -0.8159, -4.1902, -1.4164, -1.5933,\n",
      "         -4.8648, -1.3074,  0.2157, -3.1006, -4.8068, -0.7468, -1.2559, -1.7538,\n",
      "         -1.6315, -2.5820, -1.6243, -3.0266, -1.4472, -0.9665, -4.2878, -2.9954,\n",
      "         -3.2671, -0.9565, -0.2324,  1.0187,  2.7719,  2.6146,  4.2156, -1.1159,\n",
      "          6.7029,  7.0511,  6.4328,  6.8428,  5.1320,  2.1467]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[7.0511]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-3.4915, -1.4465, -1.2020, -2.5065, -1.8585, -2.8730,  0.4353, -1.0565,\n",
      "         -4.3451, -2.0756, -0.3513, -3.9492, -3.3714, -3.7387, -4.0539, -4.1421,\n",
      "         -1.3621, -3.6621, -1.8892, -4.0345, -3.7630, -2.8277, -3.5697, -0.9171,\n",
      "         -3.4059,  0.1258, -1.2921,  2.9803,  6.5789,  5.4600,  3.3344,  0.0712,\n",
      "          8.6250,  6.4128,  5.6076,  8.8523,  3.8904,  2.9468]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.8523]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.5400e+00,  1.4240e+00,  1.5751e+00,  5.6091e-03,  2.7477e-01,\n",
      "         -1.6995e+00,  1.4054e+00,  1.4810e+00, -2.7086e+00,  1.8265e-01,\n",
      "          2.1865e+00, -2.3974e+00, -1.7787e+00, -2.6439e+00, -2.6853e+00,\n",
      "         -3.9803e+00, -4.0361e+00, -3.5864e+00, -4.0875e+00, -4.3846e+00,\n",
      "         -5.0464e+00, -2.4198e+00, -3.9920e+00, -1.7347e+00, -2.2761e+00,\n",
      "         -1.3411e-01, -1.9394e+00,  2.5118e+00,  5.5256e+00,  6.1229e+00,\n",
      "          5.4272e+00,  3.9025e-01,  9.8629e+00,  9.7905e+00,  9.6661e+00,\n",
      "          7.8983e+00,  5.4330e+00,  4.2903e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[9.8629]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.7051, -0.1963,  1.2903, -0.2198,  0.3637, -1.5158,  1.3374,  0.1921,\n",
      "         -1.9124, -1.2729,  0.4452, -3.2830, -3.1972, -2.1484, -4.8506, -5.8254,\n",
      "         -1.9248, -4.5961, -2.0982, -3.2995, -3.8102, -3.9808, -4.0822, -2.8124,\n",
      "         -2.1655,  0.1145, -0.0928,  0.0620,  7.2871,  5.0404,  3.1570,  2.9739,\n",
      "         10.1211,  7.1531,  6.9732, 11.3011,  5.8686,  4.4581]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.3011]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.7447,  0.2738,  0.9519, -0.3016,  1.0368, -2.1475,  0.3978,  0.2795,\n",
      "         -2.8443, -0.4560,  1.8274, -0.9097, -2.4434, -1.4516, -2.0097, -3.1343,\n",
      "         -3.5675, -3.6506, -2.0230, -3.9524, -4.4701, -1.8438, -4.1465, -3.5742,\n",
      "         -3.7814, -1.4612, -0.5263,  0.7245,  5.7822,  4.8762,  4.7184,  1.7510,\n",
      "          8.0152,  7.6401,  9.1042,  9.6484,  7.1027,  3.4372]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.6484]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.1825, -2.0578, -0.3688, -1.4055,  0.2787, -3.4997, -0.5588, -0.8871,\n",
      "         -2.6938, -2.0119,  0.2079, -2.3343, -3.6926, -0.9012, -2.7306, -2.3090,\n",
      "         -2.9308, -4.7367, -1.6575, -3.7044, -2.9115, -1.7925, -3.3627, -4.3101,\n",
      "         -4.0839, -1.1464,  0.4092, -0.4279,  7.1094,  3.8603,  2.1801, -0.2468,\n",
      "          9.0476,  5.5431,  5.8452, 11.2293,  4.4146,  3.8595]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[11.2293]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.4620, -1.7324, -1.6289, -2.6995, -1.2406, -3.9930, -0.8877, -1.2637,\n",
      "         -4.5982, -0.7846,  0.1342, -3.0278, -4.5637, -1.2462, -2.1186, -2.0870,\n",
      "         -2.0570, -2.9258, -2.9038, -3.7227, -2.1820, -1.4725, -3.8161, -2.1810,\n",
      "         -2.5856, -0.1314, -0.2189,  2.8947,  3.4542,  3.0150,  4.4433, -1.1743,\n",
      "          7.1023,  6.8862,  7.0323,  5.7221,  4.1278,  1.8180]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[7.1023]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.4495, -1.4567, -1.0579, -2.4555, -1.6335, -2.1859,  1.4121, -0.9317,\n",
      "         -4.5895, -2.0872, -1.0970, -3.8119, -3.3302, -4.5158, -4.1318, -5.5479,\n",
      "         -0.7501, -3.2786, -1.8425, -3.7957, -4.1123, -3.4249, -4.5226,  0.2470,\n",
      "         -2.6875,  0.6965, -0.9545,  2.9962,  7.1121,  4.6534,  4.1887,  1.0920,\n",
      "          7.8764,  5.9108,  4.8949,  8.1357,  4.2933,  2.2231]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.1357]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-1.9154,  1.9373,  2.3468,  0.5217,  0.7072, -1.1838,  1.9955,  1.6374,\n",
      "         -2.3422,  0.0185,  2.4216, -2.0721, -1.1990, -3.4199, -3.2342, -4.8502,\n",
      "         -4.1828, -3.7465, -3.4557, -4.6745, -5.9719, -2.5863, -4.0304, -1.8169,\n",
      "         -2.7477,  0.2631, -1.7179,  3.0752,  6.8071,  6.9934,  4.7305,  1.6989,\n",
      "          9.4510, 10.2585, 10.1834,  8.8963,  5.9745,  4.9597]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.2585]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-2.7601, -0.7245,  1.4128,  0.4083,  1.2476, -1.8860,  0.5602, -0.1432,\n",
      "         -1.5433, -1.0608,  0.9822, -2.1125, -2.7042, -1.2836, -4.3944, -4.5896,\n",
      "         -2.7042, -5.4458, -1.9910, -3.8454, -4.0659, -3.1169, -4.0672, -4.2311,\n",
      "         -3.0232, -0.4647, -0.1313, -1.0209,  7.8252,  4.5713,  3.0211,  2.5859,\n",
      "         10.0219,  7.0294,  7.7035, 12.3496,  6.2853,  4.6368]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[12.3496]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.2458, -1.1970, -1.0756, -1.9385,  0.3219, -3.4584, -0.9775, -1.0750,\n",
      "         -3.7871, -1.0706,  0.7521, -1.5406, -3.6723, -0.8394, -1.2036, -2.0227,\n",
      "         -2.7333, -3.5834, -1.0868, -3.1624, -3.0810, -1.1264, -4.7390, -3.9248,\n",
      "         -3.4466, -1.4754,  0.3624,  0.2573,  3.9598,  2.9482,  4.7588,  0.6427,\n",
      "          7.0320,  6.4047,  7.1874,  8.3093,  6.6124,  2.2844]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.3093]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.6330, -2.0185, -0.7629, -2.4306, -0.7829, -4.0373, -0.9753, -1.1347,\n",
      "         -3.5952, -1.8398,  0.0959, -2.9880, -4.3135, -1.4068, -2.8271, -1.9833,\n",
      "         -2.6049, -4.1724, -2.2778, -3.3928, -2.4610, -1.5662, -3.1897, -3.4347,\n",
      "         -3.9857, -0.2207, -0.0479,  1.3312,  5.9188,  3.8614,  2.2238, -1.2542,\n",
      "          8.7590,  5.7428,  5.5013,  9.2729,  3.3608,  3.3909]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.2729]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.4498, -0.9966, -0.7148, -2.0807, -1.3587, -3.3068, -0.2657, -0.4768,\n",
      "         -4.2296, -0.1063,  0.3634, -2.4327, -3.2747, -1.9609, -2.5834, -2.9666,\n",
      "         -2.8188, -3.3592, -3.6364, -4.2434, -3.3732, -2.1193, -3.7171, -1.0210,\n",
      "         -2.1786,  0.1719, -1.2960,  2.9708,  4.2063,  3.7948,  5.3658, -0.7708,\n",
      "          8.2106,  7.3267,  7.5031,  5.7348,  3.6187,  2.3293]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.2106]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.1816, -0.2360, -0.2982, -1.7685, -0.5931, -1.6535,  2.1922, -0.2640,\n",
      "         -3.5788, -1.2155, -0.5851, -3.5055, -3.1977, -4.2658, -4.5682, -6.7291,\n",
      "         -0.7807, -3.4850, -1.5927, -3.2348, -4.4266, -4.0834, -5.8375, -0.1371,\n",
      "         -1.8546,  0.9869, -0.5992,  2.3621,  6.6566,  4.2958,  4.5385,  2.8370,\n",
      "          8.7466,  7.0731,  4.9583,  8.7733,  5.7354,  2.9303]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.7733]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.1511,  1.2484,  1.9591,  0.0547,  0.8263, -1.6881,  1.3069,  0.9468,\n",
      "         -2.5557, -0.3773,  1.9481, -1.3372, -1.5976, -2.9402, -3.3605, -4.8681,\n",
      "         -4.0558, -4.1414, -2.6097, -4.7629, -5.7273, -2.4827, -3.6246, -2.4183,\n",
      "         -3.5479,  0.0715, -1.0287,  2.1760,  8.0371,  6.5897,  4.0010,  1.9231,\n",
      "          8.5980,  8.2825,  9.0624, 10.0494,  5.7417,  4.6057]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.0494]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.6509, -1.4011,  0.3123, -0.3317,  1.3491, -2.7394, -0.0939, -0.5659,\n",
      "         -2.4470, -1.1738,  0.9897, -1.4996, -2.7115, -0.9111, -2.7005, -3.0796,\n",
      "         -3.0114, -4.9786, -1.8770, -3.4878, -3.4074, -2.3393, -3.9458, -4.3316,\n",
      "         -3.5379, -1.1397,  0.1181, -0.3282,  6.4219,  4.1063,  3.7999,  0.9632,\n",
      "          8.6139,  6.5161,  7.0598, 10.7528,  6.2196,  3.3296]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.7528]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.8313, -1.8111, -1.6399, -2.9228, -0.7641, -3.9098, -1.0011, -1.0814,\n",
      "         -4.5881, -1.0983,  0.5927, -2.7674, -4.3072, -0.9175, -1.9181, -1.8211,\n",
      "         -1.9023, -2.7800, -1.8851, -3.4283, -2.0030, -1.3365, -4.0554, -2.9267,\n",
      "         -2.7266, -0.6879, -0.2164,  1.3886,  3.5896,  3.3414,  3.9288, -0.7474,\n",
      "          7.0241,  6.2888,  6.8203,  7.5071,  4.9209,  2.4159]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[7.5071]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.9851, -0.9175, -0.4274, -2.1870, -1.3501, -3.4451, -0.3862, -0.4024,\n",
      "         -3.7220, -1.1667,  0.6335, -3.1210, -3.1512, -2.3222, -3.3267, -2.4069,\n",
      "         -2.9397, -3.9284, -3.0379, -4.0140, -3.7396, -1.9129, -3.2526, -2.2470,\n",
      "         -3.0962,  0.2511, -1.1449,  2.7122,  5.4976,  5.1208,  3.3096, -0.7763,\n",
      "          9.6486,  7.0044,  6.9117,  8.1317,  3.2777,  3.7785]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.6486]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.3736, -0.7565, -0.1314, -1.7113, -0.7218, -1.7356,  1.8689,  0.2032,\n",
      "         -3.4373, -0.4772,  0.2435, -3.5163, -3.0528, -3.6513, -3.9506, -5.2568,\n",
      "         -1.1289, -3.7244, -2.3321, -3.9787, -3.6666, -4.1092, -5.1644, -0.4842,\n",
      "         -1.3920, -0.2150, -1.0404,  2.1353,  6.3241,  4.4772,  5.9554,  1.4612,\n",
      "          8.0012,  7.4186,  5.9610,  8.0870,  5.2171,  2.0199]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.0870]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.4542,  1.3336,  1.7729, -0.3871,  0.7359, -1.8018,  1.1717,  1.0452,\n",
      "         -2.6447, -0.2407,  2.1282, -1.7411, -2.0832, -2.6201, -3.0916, -4.7859,\n",
      "         -3.3566, -3.9459, -2.6116, -4.8228, -5.1807, -2.2391, -4.3590, -2.2251,\n",
      "         -2.7899, -0.4795, -1.3242,  1.5006,  6.9470,  6.2158,  4.0051,  1.6675,\n",
      "          8.6861,  8.6801,  8.9027,  9.9251,  6.0931,  4.5008]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.9251]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.7944, -1.1559,  0.2347, -0.6029,  1.1832, -3.1312, -0.3994, -0.7040,\n",
      "         -2.5906, -1.0807,  0.8765, -1.6033, -2.9963, -0.8000, -2.6158, -3.0541,\n",
      "         -3.0286, -4.9773, -1.4861, -3.5656, -3.1914, -2.1999, -3.7768, -4.3770,\n",
      "         -3.5475, -1.2560, -0.1272, -1.1616,  6.4436,  4.1564,  3.1325,  0.5631,\n",
      "          9.0664,  6.1132,  6.5870, 11.0315,  5.8263,  3.5954]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.0315]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.9729, -1.7030, -1.6999, -3.0027, -0.7783, -4.2098, -1.1542, -1.2220,\n",
      "         -4.9515, -0.9332,  0.3617, -2.8579, -4.6658, -0.9206, -1.7052, -1.9270,\n",
      "         -1.9915, -2.7053, -1.6087, -3.4120, -1.6418, -1.3424, -3.8842, -3.0661,\n",
      "         -2.6697, -0.7805,  0.0153,  1.5047,  3.1146,  3.3543,  3.8851, -0.9639,\n",
      "          6.8196,  6.1595,  6.9832,  6.9337,  5.1078,  2.1913]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[6.9832]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-3.4235, -0.8914, -0.1009, -2.0658, -1.3503, -2.9192,  0.1443, -0.0698,\n",
      "         -4.0301, -1.2289,  0.4905, -3.3468, -2.8896, -3.0167, -3.9000, -3.3195,\n",
      "         -2.4690, -3.4851, -2.6374, -4.0397, -3.7349, -2.7141, -3.3010, -1.7308,\n",
      "         -2.7437,  0.5320, -1.3760,  3.0744,  5.7622,  5.5721,  3.2421, -0.3403,\n",
      "          9.4291,  6.6748,  7.1143,  8.5619,  3.7076,  3.7428]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.4291]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.9610, -0.0731,  0.5429, -1.3539, -0.2905, -1.4728,  2.2763,  0.7379,\n",
      "         -3.0857, -0.5342,  0.3265, -3.4460, -2.5572, -4.0095, -4.2210, -5.1667,\n",
      "         -1.7142, -3.8617, -2.4440, -4.0083, -4.7241, -4.3814, -5.0910, -0.7515,\n",
      "         -1.1735,  0.4640, -0.5773,  3.1403,  6.7925,  4.8316,  5.6665,  1.9137,\n",
      "          8.6959,  8.2230,  6.3989,  7.9559,  5.0537,  2.5441]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.6959]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.8354,  0.9102,  1.8627, -0.5141,  0.5710, -0.7135,  2.5617,  0.5331,\n",
      "         -2.1695, -1.5365,  1.1464, -3.7503, -2.5147, -4.3017, -5.0989, -6.2264,\n",
      "         -2.0536, -4.5043, -2.1568, -4.0923, -5.8681, -3.2826, -4.6684, -2.1707,\n",
      "         -2.2836,  0.8980,  0.3792,  2.5270,  8.5765,  5.4620,  3.4637,  3.5541,\n",
      "          9.2454,  9.2133,  7.1666, 10.2401,  5.6722,  4.5222]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.2401]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.8626, -0.5247,  0.6779,  0.2660,  1.5797, -1.9001,  0.6194, -0.1864,\n",
      "         -2.1025, -0.8516,  1.7473, -0.8913, -2.0759, -2.0456, -2.9292, -3.5120,\n",
      "         -3.4290, -5.6160, -1.9352, -3.9886, -4.3502, -2.1953, -3.8947, -3.8198,\n",
      "         -3.3963, -0.6114,  0.2674,  0.1045,  6.9874,  4.0990,  4.2711,  1.9963,\n",
      "          8.3325,  7.0709,  7.1899, 10.3416,  6.5744,  3.0434]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.3416]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.4528, -2.0306, -1.2789, -1.9665,  0.0859, -3.4991, -0.8721, -1.1781,\n",
      "         -3.3919, -1.7437,  0.4354, -2.2145, -3.5339, -1.1712, -2.0761, -1.8169,\n",
      "         -2.4677, -4.3577, -1.3595, -3.4024, -2.9163, -1.3195, -3.9327, -3.9460,\n",
      "         -3.5005, -1.0694,  0.7161,  0.8056,  5.6606,  3.0530,  3.4133,  0.0292,\n",
      "          7.4107,  5.6418,  5.9639,  9.2370,  5.1754,  2.4564]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.2370]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.2161, -1.4945, -1.1717, -2.6154, -1.3607, -3.7198, -0.5782, -0.7582,\n",
      "         -4.2248, -1.1669,  0.3959, -2.8230, -3.5443, -1.9053, -2.6741, -1.6710,\n",
      "         -2.5139, -3.3877, -2.9893, -3.7919, -3.2413, -1.2800, -3.5727, -2.4299,\n",
      "         -2.7815,  0.2353, -0.5844,  3.0511,  4.7754,  3.9601,  3.5062, -0.9188,\n",
      "          8.1595,  6.3417,  6.9442,  7.1459,  3.6506,  2.8096]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.1595]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.5113, -1.9627, -1.8085, -2.6735, -1.9779, -2.2071,  1.4011, -1.4971,\n",
      "         -5.1796, -2.1636, -1.5076, -4.3372, -3.5294, -5.1305, -4.2073, -5.7265,\n",
      "         -0.2647, -4.0113, -2.3652, -4.6072, -4.2556, -3.1106, -4.9850,  0.8764,\n",
      "         -2.6815,  0.6872, -1.2147,  3.8124,  7.6026,  4.0523,  5.4437,  0.7123,\n",
      "          7.2065,  7.0220,  4.3916,  7.4805,  4.1119,  1.4097]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[7.6026]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[28]], device='cuda:0')\n",
      "tensor([[-1.2638, -0.9850, -1.9670, -2.2064, -2.4693, -1.6731,  1.3649, -2.3242,\n",
      "         -5.1544, -3.7250, -2.4348, -7.3886, -4.2071, -8.5228, -7.2443, -8.6516,\n",
      "         -0.5694, -4.0574, -1.2159, -4.3873, -5.5079, -4.9914, -5.3701,  1.5578,\n",
      "         -1.5173,  2.4379,  2.3854,  6.9460,  9.1061,  2.7380,  4.5547,  3.0354,\n",
      "          8.8035,  8.9139,  2.3447,  9.1209,  2.7513, -0.5233]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.1209]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[ 2.1682e-01,  2.6920e+00,  2.3223e+00,  1.4901e+00,  8.2189e-01,\n",
      "          1.7796e-01,  1.9039e+00,  2.1491e+00, -1.0957e-02, -1.4376e+00,\n",
      "          2.5687e+00, -2.8998e+00, -1.3370e+00, -4.4891e+00, -4.5791e+00,\n",
      "         -5.9200e+00, -5.0998e+00, -5.2436e+00, -4.9356e+00, -5.7993e+00,\n",
      "         -6.8112e+00, -2.7132e+00,  5.7337e-01, -1.0264e+00, -2.9359e+00,\n",
      "          6.0432e-01,  1.8681e+00,  4.0298e+00,  9.6846e+00,  7.2614e+00,\n",
      "          2.2612e+00,  5.3658e+00,  1.1360e+01,  8.8146e+00,  6.1228e+00,\n",
      "          9.1017e+00,  3.8571e+00,  5.3354e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[11.3602]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-0.8035, -0.1270,  2.8760,  1.9600,  0.7245,  0.0324,  2.1955,  0.5314,\n",
      "         -0.8504, -2.0543,  2.4192, -3.4577, -1.9047, -3.4441, -4.9105, -5.9096,\n",
      "         -3.1489, -4.8957, -4.0217, -3.5393, -4.5243, -3.7003, -2.3947, -3.3244,\n",
      "         -1.6058,  1.3117,  1.8317,  1.8679,  7.8669,  4.1800,  5.3377,  4.5167,\n",
      "          7.9557,  8.9107,  6.6997, 10.1360,  5.4759,  1.0187]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.1360]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-0.7324,  0.7594,  1.1558,  0.7934,  1.1087, -0.0903,  1.8825,  2.0387,\n",
      "         -0.7952,  0.0941,  2.2010, -1.1067, -2.1651, -2.9679, -2.5466, -3.3868,\n",
      "         -4.2406, -3.6346, -4.0669, -3.0346, -4.5465, -2.5914, -2.6416, -2.2710,\n",
      "         -1.4363, -0.3016,  0.7116,  1.6890,  5.8950,  3.6409,  4.4356,  4.0069,\n",
      "          8.7919,  7.6388,  7.7714,  8.5934,  5.7498,  2.3546]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.7919]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.8599, -0.6343,  0.5492, -0.5395, -0.1549, -1.8795,  1.0939, -0.4983,\n",
      "         -2.2435, -1.6992,  0.2688, -2.2500, -2.6720, -2.6016, -4.2231, -4.9893,\n",
      "         -2.6259, -4.0640, -3.1352, -2.4968, -4.5949, -3.0201, -3.0938, -2.8041,\n",
      "         -3.6106,  1.0183, -0.0527,  1.5638,  8.3998,  4.0874,  3.2547,  2.2683,\n",
      "          7.9394,  5.1599,  5.9839, 10.7870,  4.3980,  3.0726]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.7870]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.7713, -0.3389, -0.1664, -1.1286, -0.5198, -2.3320,  0.3831,  0.2168,\n",
      "         -3.5209, -0.3195,  0.7123, -2.3639, -2.8852, -2.2979, -2.3669, -2.5593,\n",
      "         -3.3742, -3.5351, -4.2914, -3.9445, -4.0303, -1.4846, -3.2243, -2.1598,\n",
      "         -3.1965, -0.2969, -0.1236,  3.5928,  5.3828,  5.1163,  4.4570,  1.0293,\n",
      "          7.3165,  7.1580,  8.4937,  6.6740,  5.2427,  2.6818]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.4937]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-3.2042, -0.9043,  0.3703, -1.2738, -0.4162, -2.5899,  0.3689, -0.3131,\n",
      "         -3.2803, -1.1948,  0.5998, -2.8388, -2.9885, -2.0693, -3.4331, -4.5506,\n",
      "         -1.9281, -3.9983, -3.1613, -3.7887, -4.0983, -2.8797, -4.2801, -2.2806,\n",
      "         -3.4033,  0.3938, -1.4799,  1.6800,  6.5698,  4.8983,  4.1894,  0.9623,\n",
      "          8.1316,  6.3581,  7.5722,  9.9490,  5.1988,  4.0593]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.9490]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.2079, -0.2777,  0.2701, -0.8415,  0.5123, -2.4641,  0.2893, -0.1286,\n",
      "         -3.0326, -0.2105,  1.0180, -1.7455, -2.7932, -1.2746, -2.0725, -3.3473,\n",
      "         -2.2759, -3.4983, -2.5826, -4.0188, -3.9841, -1.4026, -4.7036, -2.7010,\n",
      "         -2.8957, -1.3172, -0.8306,  1.0718,  4.6301,  4.0033,  4.3508,  1.2092,\n",
      "          7.1674,  7.3187,  8.5583,  8.5265,  6.8724,  3.6087]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.5583]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-3.7505, -1.2605,  0.1367, -1.4694, -0.0649, -2.8474,  0.0294, -0.7834,\n",
      "         -2.9089, -1.7231,  0.5732, -3.1274, -3.5063, -1.2295, -3.6646, -4.2216,\n",
      "         -1.3283, -3.8550, -2.3640, -3.6638, -3.3076, -2.2658, -4.5617, -2.8068,\n",
      "         -3.5638, -0.6725, -0.8966,  0.0520,  6.4080,  4.5688,  2.7950,  0.8523,\n",
      "          8.2542,  6.6280,  6.8962, 11.2641,  5.8515,  4.5551]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.2641]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.6914, -0.6639, -0.1687, -1.1177, -0.1505, -2.9467, -0.0533, -0.3124,\n",
      "         -3.3091, -1.0712,  1.1486, -2.6895, -3.0059, -1.4890, -2.2528, -2.3713,\n",
      "         -2.2424, -3.7105, -2.7719, -3.6048, -3.3334, -1.3703, -4.6916, -2.9721,\n",
      "         -3.3364, -1.1212, -0.9227,  1.8455,  4.3507,  4.5330,  4.4383, -0.1127,\n",
      "          7.9718,  8.0773,  8.5554,  8.0510,  5.9396,  3.5009]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.5554]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-3.4647, -1.3142, -0.0106, -1.3113, -0.4223, -2.7526, -0.1334, -0.2323,\n",
      "         -2.6898, -1.7389,  0.8122, -3.3083, -3.4307, -1.2858, -3.5876, -3.8532,\n",
      "         -2.0342, -3.8898, -2.8695, -3.5839, -3.1314, -2.7246, -3.6832, -2.3442,\n",
      "         -2.8821, -0.5289, -1.2641,  0.5069,  6.0021,  4.5553,  3.5949,  0.2744,\n",
      "          9.7920,  7.1327,  7.1923, 10.0653,  4.6713,  4.4670]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.0653]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.5091e+00,  5.6188e-02, -1.4157e-01, -1.1563e+00, -4.8844e-03,\n",
      "         -2.6614e+00,  1.3487e-01, -3.2618e-01, -3.2434e+00,  7.9723e-02,\n",
      "          1.3854e+00, -1.8558e+00, -3.3859e+00, -8.8033e-01, -1.7880e+00,\n",
      "         -3.6984e+00, -2.4776e+00, -3.2550e+00, -3.1216e+00, -3.9483e+00,\n",
      "         -3.1374e+00, -1.6130e+00, -4.1175e+00, -2.1431e+00, -2.1395e+00,\n",
      "         -1.3125e+00, -9.5722e-01,  1.9252e-01,  3.4743e+00,  3.8496e+00,\n",
      "          5.1621e+00,  1.0405e+00,  7.9883e+00,  8.0108e+00,  8.4698e+00,\n",
      "          7.2354e+00,  6.6624e+00,  2.8278e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[8.4698]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-3.4237e+00, -9.2506e-01,  2.3578e-01, -1.5455e+00, -9.8458e-02,\n",
      "         -2.7922e+00, -2.1532e-03, -8.5790e-01, -2.7090e+00, -1.9958e+00,\n",
      "          2.3795e-01, -2.7798e+00, -3.8899e+00, -1.4283e+00, -3.8473e+00,\n",
      "         -5.0346e+00, -1.7527e+00, -4.3768e+00, -2.1661e+00, -3.7803e+00,\n",
      "         -3.2004e+00, -2.4267e+00, -3.9552e+00, -2.5872e+00, -3.6428e+00,\n",
      "         -4.4817e-01, -9.4139e-01, -7.9745e-01,  7.3907e+00,  4.9252e+00,\n",
      "          2.7430e+00,  8.6659e-01,  8.7376e+00,  5.7444e+00,  6.0472e+00,\n",
      "          1.1436e+01,  5.1461e+00,  4.2906e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[11.4357]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.5903, -0.7434, -0.4012, -1.4191, -0.2562, -2.9153, -0.1581, -0.4847,\n",
      "         -3.6923, -1.1713,  0.8453, -2.9100, -3.5842, -1.8172, -2.0201, -2.4553,\n",
      "         -1.9959, -3.1574, -2.7233, -3.5611, -2.7175, -1.4869, -4.3485, -2.6268,\n",
      "         -2.7018, -1.0160, -0.3192,  2.8416,  3.5290,  4.4761,  4.6478, -0.1732,\n",
      "          7.1823,  7.6494,  8.4533,  6.8496,  6.0931,  2.6678]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.4533]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-3.2943, -1.1954,  0.1070, -1.4390, -0.6960, -2.6268, -0.0400, -0.1548,\n",
      "         -2.6704, -2.2120,  0.5453, -3.4507, -3.4367, -1.7186, -4.0600, -4.5189,\n",
      "         -1.7875, -3.8977, -2.8747, -3.8663, -3.4204, -2.9512, -3.8451, -2.0166,\n",
      "         -3.0138,  0.0957, -1.2574,  0.8939,  6.6893,  5.1604,  3.2420,  0.3033,\n",
      "          9.6403,  6.6644,  6.9714, 10.4310,  4.5253,  4.5989]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.4310]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.0855,  0.1117,  0.1913, -0.6844,  0.1329, -2.4208,  0.3059, -0.0160,\n",
      "         -2.7770, -0.3922,  1.3853, -2.0594, -2.8676, -1.3330, -1.9831, -3.2191,\n",
      "         -2.4709, -3.5196, -3.1275, -3.9747, -3.4317, -1.7878, -4.1308, -2.2861,\n",
      "         -2.3325, -1.1803, -0.9151,  1.4652,  3.8058,  4.1482,  5.0850,  0.5694,\n",
      "          8.1493,  8.4803,  8.8622,  7.0962,  6.2781,  3.1018]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.8622]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-3.4285, -1.1318,  0.2198, -1.4776, -0.1482, -2.7046, -0.1162, -0.5443,\n",
      "         -2.6134, -2.0705,  0.3229, -2.6500, -3.7177, -1.0648, -3.9348, -4.9355,\n",
      "         -1.6601, -4.2183, -2.5380, -3.7874, -3.3213, -2.7208, -4.0422, -2.2857,\n",
      "         -3.1257, -0.3135, -0.9861, -0.5561,  7.0122,  4.5855,  3.1916,  0.9762,\n",
      "          9.0543,  6.0009,  6.5411, 10.8387,  5.1391,  4.4858]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.8387]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.4901, -0.8040, -0.2428, -1.2307,  0.0131, -2.9675, -0.2207, -0.6245,\n",
      "         -3.6170, -1.1208,  0.7730, -2.4832, -3.4325, -1.6157, -2.1729, -2.6111,\n",
      "         -1.9212, -3.4666, -2.2464, -3.4105, -2.7841, -1.7103, -4.5130, -2.9552,\n",
      "         -2.9259, -0.9553, -0.3054,  2.2820,  3.8993,  4.2452,  4.5528,  0.0934,\n",
      "          7.0585,  7.3140,  8.1493,  7.3524,  6.2218,  2.5468]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.1493]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-3.3473e+00, -1.2528e+00,  1.2940e-01, -1.5061e+00, -5.4975e-01,\n",
      "         -2.7509e+00,  5.1416e-03, -3.2659e-01, -2.7244e+00, -2.1906e+00,\n",
      "          7.0259e-01, -3.2955e+00, -3.4413e+00, -1.7165e+00, -3.9179e+00,\n",
      "         -4.0501e+00, -1.6864e+00, -3.9836e+00, -2.6793e+00, -3.9385e+00,\n",
      "         -3.3175e+00, -2.7021e+00, -3.9115e+00, -2.2786e+00, -3.3206e+00,\n",
      "         -3.1055e-01, -1.0181e+00,  7.9854e-01,  6.9131e+00,  5.1644e+00,\n",
      "          2.9229e+00, -6.2099e-02,  9.0980e+00,  6.3720e+00,  6.6302e+00,\n",
      "          1.0642e+01,  4.5379e+00,  4.3157e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[10.6417]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.8860,  0.3706,  0.4711, -0.5537,  0.1567, -2.2579,  0.5384,  0.1703,\n",
      "         -2.7876, -0.3663,  1.4313, -1.7937, -2.5368, -1.5437, -2.3438, -3.4623,\n",
      "         -2.6869, -3.6570, -3.1310, -4.1252, -3.8534, -2.0956, -4.0536, -2.1422,\n",
      "         -2.1003, -0.8155, -1.1448,  1.5943,  4.3125,  4.4936,  5.1253,  0.8521,\n",
      "          8.5573,  8.3007,  9.0978,  7.2602,  6.0227,  3.2653]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.0978]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-2.9960, -0.5533,  0.8476, -0.8626,  0.6384, -2.3607,  0.3941, -0.3113,\n",
      "         -2.5347, -1.6904,  0.6523, -1.9910, -3.4248, -1.1845, -4.1537, -5.1679,\n",
      "         -1.9387, -4.6559, -2.1646, -4.0890, -3.7219, -2.7756, -4.2137, -2.9052,\n",
      "         -2.9661, -0.6887, -0.4611, -0.7716,  7.7586,  4.7775,  3.0930,  1.6200,\n",
      "          8.8262,  6.0890,  7.0394, 11.3082,  5.6926,  4.5142]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.3082]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.1327, -0.2587,  0.2549, -0.8555,  0.5072, -2.6964,  0.0346, -0.4873,\n",
      "         -3.3471, -0.6978,  0.9334, -1.8283, -3.2095, -1.4785, -2.2982, -3.4129,\n",
      "         -2.1799, -3.6150, -1.9677, -3.5244, -3.1086, -1.9183, -4.5825, -3.0707,\n",
      "         -2.7821, -1.0909, -0.4310,  1.2721,  4.3922,  4.2354,  4.6105,  0.9116,\n",
      "          7.2280,  7.2039,  8.3541,  8.0396,  6.7760,  2.8742]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.3541]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-3.3731, -1.4249,  0.0958, -1.5115, -0.3546, -2.8475,  0.0215, -0.5357,\n",
      "         -2.6854, -2.3333,  0.5281, -3.1912, -3.6144, -1.6195, -3.9911, -4.0580,\n",
      "         -1.5040, -4.2655, -2.4674, -4.0242, -3.1716, -2.6033, -4.0173, -2.5797,\n",
      "         -3.5648, -0.5221, -0.7116,  0.5087,  7.3964,  5.0233,  2.6400, -0.0260,\n",
      "          8.7282,  6.1068,  6.1813, 11.0592,  4.5527,  4.2227]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.0592]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.2163e+00, -8.5345e-02, -6.1988e-02, -9.3863e-01, -3.1290e-01,\n",
      "         -2.7142e+00,  7.5016e-02, -2.6665e-03, -3.3736e+00, -6.4326e-01,\n",
      "          1.2661e+00, -2.4709e+00, -2.9201e+00, -1.6809e+00, -2.1627e+00,\n",
      "         -2.6335e+00, -2.5873e+00, -3.3478e+00, -3.1218e+00, -3.9639e+00,\n",
      "         -3.3273e+00, -1.7965e+00, -3.9730e+00, -2.2342e+00, -2.1213e+00,\n",
      "         -7.8244e-01, -7.8197e-01,  2.7095e+00,  3.6468e+00,  4.5258e+00,\n",
      "          4.8263e+00,  5.8461e-02,  8.3412e+00,  8.3158e+00,  8.9858e+00,\n",
      "          6.5162e+00,  5.5695e+00,  3.0789e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[8.9858]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-2.9540, -0.2557,  0.6018, -1.0854,  0.3608, -2.1248,  0.4810, -0.0160,\n",
      "         -2.6126, -1.0666,  0.5052, -1.9799, -3.4153, -1.1482, -3.9549, -5.3861,\n",
      "         -1.9484, -4.3414, -2.3989, -3.5359, -3.5315, -3.2571, -4.2466, -2.1433,\n",
      "         -1.7376, -0.3477, -0.8448, -0.8426,  6.4444,  4.2552,  4.1127,  2.1383,\n",
      "          9.6554,  6.2198,  7.3823,  9.5360,  5.7986,  4.3320]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.6554]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.8650,  0.1902,  0.7847, -0.5569,  0.1467, -1.1944,  1.5409, -0.2889,\n",
      "         -2.9396, -1.2353,  0.4382, -2.8262, -2.5905, -3.0860, -4.1516, -5.7279,\n",
      "         -1.0915, -3.7718, -1.5807, -3.5194, -3.5218, -3.5926, -4.4942, -2.6779,\n",
      "         -2.2609, -0.5578,  0.0524,  1.6581,  6.5186,  5.2378,  3.9052,  2.8121,\n",
      "          7.9050,  6.8934,  7.2394,  9.8256,  6.4804,  2.5054]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.8256]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.9130, -0.0314,  0.6973, -0.4486,  0.1810, -2.2923,  0.6059,  0.7075,\n",
      "         -2.2609, -1.6298,  1.4993, -2.5225, -2.7631, -2.1673, -2.9611, -2.9751,\n",
      "         -3.8363, -4.4892, -3.3153, -4.0859, -4.3870, -2.1080, -3.4300, -2.7947,\n",
      "         -3.3944, -0.8030, -0.4423,  1.5243,  7.4614,  5.7835,  3.3046,  0.4886,\n",
      "         10.1103,  7.5443,  7.7647, 10.3975,  4.8234,  4.7182]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.3975]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.8740, -1.4829, -0.6516, -1.0375,  0.1708, -3.4207, -0.8872, -1.0086,\n",
      "         -2.9937, -0.8678,  0.7425, -2.5312, -3.8831, -0.6768, -1.8619, -2.3472,\n",
      "         -2.4983, -4.1866, -2.3128, -3.1046, -2.1022, -1.7685, -3.9504, -3.6828,\n",
      "         -2.6951, -1.4904,  0.3936,  1.0224,  3.8837,  3.1483,  4.3251,  0.2875,\n",
      "          8.1151,  7.5979,  7.2357,  7.4043,  5.7708,  2.4028]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.1151]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-3.3090, -2.1837, -1.7823, -3.0036, -2.1342, -2.8151,  0.0352, -1.7106,\n",
      "         -4.5134, -2.1443, -0.8959, -3.9623, -3.7477, -2.9016, -3.6851, -4.5934,\n",
      "         -0.1715, -2.5080, -2.1389, -3.9090, -2.8081, -3.0918, -4.3912, -0.4122,\n",
      "         -3.0570,  0.6988, -1.0050,  2.4982,  6.3951,  4.1289,  3.0476,  0.3313,\n",
      "          6.5011,  5.3443,  5.1710,  8.9431,  4.1474,  1.9820]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.9431]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.5619,  1.1231,  1.3200, -0.2405,  0.1414, -1.8728,  1.3280,  1.1421,\n",
      "         -3.5831, -0.5426,  1.3673, -2.4024, -1.1706, -3.1654, -3.2429, -3.6087,\n",
      "         -3.5860, -3.4936, -3.3597, -4.8735, -5.6082, -2.3404, -3.8514, -2.3068,\n",
      "         -3.3651, -0.0202, -1.9792,  3.8888,  6.7823,  7.1387,  4.4995,  1.0905,\n",
      "          9.3773,  9.4397, 10.3859,  8.5057,  5.7273,  4.7996]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.3859]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-2.9265, -0.7423,  1.0262,  0.1090,  1.1722, -2.3792,  0.4003,  0.1749,\n",
      "         -1.7985, -1.4402,  1.0962, -2.3823, -2.7181, -0.6834, -3.7963, -3.8766,\n",
      "         -2.6066, -5.2500, -2.2669, -3.9631, -3.8429, -3.0304, -4.2824, -4.2762,\n",
      "         -3.1449, -1.2969, -0.5297, -0.7633,  7.8403,  5.0140,  3.4871,  1.2717,\n",
      "          9.9564,  7.3625,  8.0761, 11.7183,  5.9375,  5.0810]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.7183]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.4971, -1.2573, -1.5070, -2.0919,  0.1403, -3.7602, -1.2686, -1.2951,\n",
      "         -4.1712, -0.6813,  0.5430, -2.0973, -4.2391, -0.3931, -1.0547, -2.4836,\n",
      "         -2.1685, -3.3857, -1.2191, -3.0783, -2.2328, -1.1232, -5.0614, -3.6635,\n",
      "         -2.4975, -1.7414,  0.6061,  0.1924,  2.8584,  2.3287,  4.9424,  0.6549,\n",
      "          6.6687,  6.9262,  7.2559,  7.0345,  6.8069,  2.0758]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[7.2559]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-4.2551, -1.9257, -1.0331, -2.7295, -1.3068, -3.8036, -0.6404, -1.5302,\n",
      "         -3.9978, -2.5833, -0.4692, -3.5965, -4.2641, -1.8420, -3.6103, -3.6903,\n",
      "         -1.4938, -4.2049, -2.1185, -3.6862, -2.7640, -1.9824, -3.7652, -2.4054,\n",
      "         -4.2655,  0.2697, -0.6484,  1.3374,  7.1111,  4.3087,  2.5021, -0.5266,\n",
      "          8.0118,  5.2692,  5.0666,  9.5785,  3.4614,  3.3910]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.5785]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.2111, -0.4990, -0.2014, -1.6453, -1.1770, -2.8357,  0.0790, -0.1119,\n",
      "         -3.8001, -1.1581,  0.4417, -3.1291, -3.0248, -3.0120, -2.6207, -2.3534,\n",
      "         -3.0429, -3.1475, -3.8746, -4.0658, -3.7615, -1.9125, -3.6427, -1.6686,\n",
      "         -2.1159,  0.2079, -0.8403,  5.3051,  4.1068,  4.9816,  5.0950, -0.7959,\n",
      "          8.7767,  8.2766,  8.8090,  5.6014,  4.0847,  3.0330]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.8090]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-2.6376, -0.3560,  0.9227, -1.3048,  0.1171, -1.9481,  0.5589,  0.7293,\n",
      "         -2.7844, -0.7510,  0.5673, -2.3866, -3.1904, -1.4661, -4.6544, -5.6771,\n",
      "         -1.9500, -4.1447, -3.1903, -3.4435, -4.3820, -3.8545, -4.5563, -0.9729,\n",
      "         -1.3590,  0.7829, -1.1676,  0.2665,  6.9717,  4.3690,  4.3266,  1.9299,\n",
      "          9.8025,  6.2798,  7.2713,  9.0098,  4.9378,  4.4018]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.8025]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.4620,  0.6270,  1.0370, -0.6868,  0.3592, -0.7396,  2.4369,  0.1797,\n",
      "         -2.3937, -0.5986,  0.4147, -3.4314, -2.5545, -3.7734, -4.6439, -5.9864,\n",
      "         -1.2445, -3.6811, -1.3980, -3.8311, -3.6744, -4.1336, -4.6578, -1.9346,\n",
      "         -0.9617, -0.3508,  0.6110,  2.0148,  6.3686,  3.6640,  4.9782,  3.4729,\n",
      "          7.4053,  7.5964,  7.0961,  8.7903,  6.0117,  1.9654]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.7903]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.6791, -0.4676,  1.1828, -0.5942,  0.1532, -1.9385,  0.9127,  0.1566,\n",
      "         -2.2924, -1.4132,  1.2005, -1.2493, -2.4364, -2.3466, -3.6188, -4.1529,\n",
      "         -3.8017, -5.0354, -2.4549, -4.5655, -4.7342, -2.5513, -3.2130, -3.0042,\n",
      "         -3.8225, -0.6967,  0.0185,  0.5525,  9.0617,  5.4404,  3.5052,  1.9624,\n",
      "          8.8749,  6.5316,  7.0232, 11.3512,  5.1307,  3.9579]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.3512]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.5838, -1.6327, -0.3221, -0.7022,  0.5477, -2.8511, -0.2209, -0.4863,\n",
      "         -3.1286, -1.2988,  0.6408, -2.3317, -3.1597, -1.2300, -2.2878, -2.2753,\n",
      "         -2.9815, -4.4926, -2.0066, -3.2778, -2.8823, -2.1114, -3.6717, -4.2364,\n",
      "         -3.1556, -1.3514,  0.8745,  1.6791,  5.6798,  3.8701,  4.3743,  0.6370,\n",
      "          7.9987,  6.6673,  7.5042,  8.5589,  5.8572,  2.5251]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[8.5589]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.5882, -1.3880, -1.3418, -2.8587, -1.2500, -3.8945, -0.8116, -0.6642,\n",
      "         -4.8284, -0.8255,  0.5501, -2.8414, -4.0918, -0.9035, -2.1369, -2.2624,\n",
      "         -2.3146, -2.4919, -2.5614, -3.7860, -2.3928, -1.6161, -3.7701, -2.1461,\n",
      "         -2.1588, -0.0435, -0.8140,  1.9143,  3.8373,  3.6152,  4.2445, -0.9795,\n",
      "          7.4528,  6.0951,  7.6693,  6.8853,  4.0876,  2.7428]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[7.6693]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[34]], device='cuda:0')\n",
      "tensor([[-2.9245, -0.3788,  0.0779, -1.7248, -0.6723, -2.4493,  1.0238, -0.1123,\n",
      "         -3.9088, -0.7044,  0.3941, -2.8682, -2.5209, -2.7380, -3.9518, -4.0836,\n",
      "         -1.9053, -3.5164, -2.1738, -4.1423, -4.2830, -2.9390, -4.0492, -1.8353,\n",
      "         -2.6260,  0.1576, -1.4403,  2.2338,  6.2433,  5.2466,  4.2050,  1.2753,\n",
      "          8.6749,  6.4615,  7.4928,  8.7923,  4.8518,  3.4564]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.7923]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.6144e+00,  1.0151e+00,  1.6007e+00, -2.9595e-01,  8.5913e-01,\n",
      "         -1.9805e+00,  1.1483e+00,  8.7527e-01, -2.7088e+00,  8.6757e-03,\n",
      "          2.1754e+00, -1.7425e+00, -2.1741e+00, -1.8338e+00, -3.1355e+00,\n",
      "         -4.2976e+00, -3.4056e+00, -3.8078e+00, -2.8591e+00, -4.3006e+00,\n",
      "         -4.8729e+00, -2.2896e+00, -4.1400e+00, -2.3006e+00, -2.4891e+00,\n",
      "         -8.9796e-01, -1.4018e+00,  5.3768e-01,  6.6111e+00,  5.5894e+00,\n",
      "          3.9713e+00,  1.2657e+00,  8.9844e+00,  7.9412e+00,  9.0340e+00,\n",
      "          9.8418e+00,  6.0303e+00,  4.6895e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[9.8418]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.8384, -1.0909, -0.0240, -0.8776,  0.9707, -3.2524, -0.5335, -0.8902,\n",
      "         -3.0986, -0.7922,  0.6915, -1.8109, -3.5995, -0.4469, -2.7753, -3.6447,\n",
      "         -2.3540, -4.6983, -1.3177, -3.6972, -3.0512, -1.8839, -4.7499, -4.0366,\n",
      "         -3.1406, -1.5206,  0.0773, -0.8447,  5.7950,  3.7364,  3.2759,  1.1553,\n",
      "          8.1073,  6.4120,  7.0183, 10.2433,  6.3248,  3.5419]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.2433]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.6386, -2.2983, -1.4727, -2.8070, -0.5726, -4.0410, -1.0578, -2.0068,\n",
      "         -4.6581, -2.5407, -0.5734, -2.7045, -4.2047, -0.9305, -2.4042, -2.5564,\n",
      "         -1.1410, -4.0289, -1.0066, -3.4485, -2.1261, -1.3172, -4.8643, -3.6963,\n",
      "         -4.2722, -0.7304,  0.3593,  1.3697,  5.4492,  3.4523,  3.7580, -0.2369,\n",
      "          5.7813,  5.4210,  5.7623,  8.2365,  5.0158,  2.0962]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.2365]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.9420, -1.6060, -0.5874, -2.2552, -1.2876, -3.4633, -0.3555, -0.4630,\n",
      "         -4.0226, -1.8607,  0.3268, -3.3384, -3.7427, -2.3588, -3.1369, -1.6695,\n",
      "         -2.6958, -3.4020, -2.9261, -3.8371, -2.7915, -1.9882, -3.4481, -2.7834,\n",
      "         -2.6158,  0.0543, -0.3283,  4.6157,  4.6975,  4.5552,  3.5726, -1.5409,\n",
      "          9.2207,  6.9931,  7.5274,  7.0145,  3.3547,  3.1259]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.2207]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.2766, -0.8442,  0.2519, -1.8137, -1.0388, -1.9409,  1.6545,  0.3611,\n",
      "         -3.4618, -0.5049,  0.1338, -3.5035, -2.9928, -3.2100, -4.8281, -5.3930,\n",
      "         -1.6737, -3.5473, -2.4541, -3.7253, -4.1968, -4.5668, -4.7816, -0.1283,\n",
      "         -1.1641,  0.8062, -0.7619,  2.3329,  6.7042,  4.3826,  5.4506,  1.7178,\n",
      "          8.4902,  6.6189,  6.3395,  7.8128,  4.5745,  2.5346]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.4902]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.7865,  1.0923,  1.8333, -1.0192,  0.0521, -1.0792,  2.9595,  0.4159,\n",
      "         -3.0513, -0.5498,  1.1944, -3.2436, -2.2580, -4.1837, -5.0995, -6.0294,\n",
      "         -2.3387, -3.3642, -1.5378, -4.1329, -5.3794, -3.9559, -4.5706, -1.6739,\n",
      "         -1.8341,  0.9871, -0.2401,  2.9545,  7.1975,  5.3984,  3.9400,  3.1684,\n",
      "          8.8357,  8.9019,  7.9447,  9.3493,  5.5521,  3.8769]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.3493]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.7926,  0.1523,  1.3230,  0.0813,  0.9655, -2.0946,  1.0469,  0.1688,\n",
      "         -2.3842, -0.5261,  2.1196, -1.0129, -1.9215, -1.9186, -3.8256, -4.0041,\n",
      "         -4.0584, -5.1932, -2.2249, -4.7765, -4.8734, -2.7630, -2.8129, -3.4318,\n",
      "         -3.2489, -0.1850, -0.2528,  0.2537,  8.5380,  5.1893,  3.7264,  1.9377,\n",
      "          9.0798,  6.5026,  8.0457, 10.8328,  5.4915,  4.1711]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.8328]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.2119, -1.7208, -0.9113, -1.4369,  0.4248, -3.5211, -0.5884, -1.0639,\n",
      "         -3.8688, -1.2693,  0.4625, -1.8862, -3.3858, -0.9113, -2.1390, -2.1936,\n",
      "         -2.9692, -4.2888, -1.1315, -3.3922, -2.9191, -1.6215, -3.9768, -4.1430,\n",
      "         -2.9232, -1.0259,  0.9749,  1.2802,  5.3275,  3.1540,  3.9872,  0.4597,\n",
      "          7.0887,  5.5705,  7.1296,  8.3521,  5.6838,  2.2511]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.3521]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.0480, -1.3480, -0.9950, -2.5227, -1.1783, -3.5254, -0.0733, -0.4619,\n",
      "         -4.3789, -1.2446,  0.5349, -2.7479, -3.5259, -1.6539, -2.8866, -2.0853,\n",
      "         -2.4643, -3.1383, -2.5752, -3.9555, -3.1473, -1.5558, -3.4275, -2.5148,\n",
      "         -2.5559,  0.2816, -0.7206,  2.6020,  5.5337,  4.1760,  3.3041, -0.7080,\n",
      "          7.8038,  5.4876,  7.5051,  7.7194,  3.6715,  2.9505]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[7.8038]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.1246, -1.1224, -0.2972, -1.8891, -1.8415, -2.0202,  1.6084, -0.7270,\n",
      "         -4.4553, -1.6459, -0.2803, -4.0792, -2.5746, -4.5002, -4.8130, -4.8000,\n",
      "         -1.4578, -3.3167, -2.2145, -4.2861, -4.5747, -3.3433, -3.8043, -0.5588,\n",
      "         -2.1047,  0.6656, -0.9015,  3.9027,  7.2363,  5.2407,  4.1915,  1.3954,\n",
      "          8.3760,  6.5479,  6.4447,  8.2662,  3.9995,  2.2620]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.3760]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.2765,  1.2838,  2.1159, -0.6258,  0.1516, -0.6573,  2.9296,  0.8568,\n",
      "         -2.4953, -0.5953,  1.2838, -3.2251, -1.7799, -4.5231, -5.0138, -6.1363,\n",
      "         -2.9434, -3.4719, -2.6266, -4.1982, -6.1285, -3.7491, -4.4665, -0.9060,\n",
      "         -1.3153,  1.3527, -0.5325,  3.5543,  7.4578,  5.6839,  4.4803,  2.9841,\n",
      "          9.8004, 10.0316,  7.9583,  8.4397,  4.7387,  4.0293]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.0316]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-2.2654,  0.5936,  1.9714,  0.1780,  0.8408, -1.2059,  1.9328,  0.2875,\n",
      "         -1.5453, -0.2629,  2.1503, -2.2636, -2.0009, -3.1767, -4.6053, -4.5165,\n",
      "         -3.6605, -5.3996, -1.9238, -4.8623, -5.5907, -2.3264, -3.0494, -3.2914,\n",
      "         -2.3143,  0.1993,  0.2231,  0.4467,  8.8041,  4.8649,  3.1648,  3.3007,\n",
      "          9.2354,  8.0103,  8.3156, 10.8620,  5.6028,  4.6509]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.8620]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.2625, -1.4162, -0.6658, -1.1356,  0.6567, -3.1999, -0.5478, -1.3403,\n",
      "         -3.2850, -1.1652,  1.0423, -1.0830, -2.9778, -1.4484, -2.3421, -3.0526,\n",
      "         -3.2252, -5.5193, -0.8263, -3.9376, -3.4716, -1.4741, -4.4332, -4.1319,\n",
      "         -3.1443, -0.5671,  0.9138, -0.4306,  5.7381,  2.7599,  4.0964,  1.3722,\n",
      "          7.5461,  5.9256,  6.4075,  9.9439,  6.2050,  2.1291]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.9439]], device='cuda:0', grad_fn=<TopkBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.5013, -2.0063, -1.2744, -2.5853, -0.6326, -4.0050, -0.8962, -1.3922,\n",
      "         -3.9868, -1.4550, -0.1360, -2.7113, -4.1909, -1.5327, -2.4193, -1.6319,\n",
      "         -2.7779, -4.1994, -1.7079, -3.5139, -2.7051, -0.9461, -3.3471, -3.8813,\n",
      "         -3.6448, -0.3192,  0.7003,  1.5058,  6.0442,  3.2433,  2.7368, -0.6261,\n",
      "          7.5909,  4.9516,  5.6011,  8.5762,  3.6552,  2.4378]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.5762]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.4818, -1.0764, -0.5115, -2.1130, -1.4691, -3.2472, -0.0321, -0.2611,\n",
      "         -4.1505, -0.6981,  0.5604, -2.6070, -2.8208, -2.2701, -3.0477, -2.2139,\n",
      "         -2.9381, -3.3564, -3.5430, -4.2801, -3.8230, -1.7484, -3.3048, -1.7471,\n",
      "         -2.1678,  0.5067, -1.0557,  3.5190,  5.0794,  4.3688,  3.9959, -0.5774,\n",
      "          8.6206,  6.6655,  7.7248,  6.5585,  3.3380,  2.8674]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.6206]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.7954, -0.6100,  0.0913, -1.8336, -0.7533, -1.4567,  1.8561, -0.1243,\n",
      "         -3.9137, -0.9569, -0.2151, -3.2176, -2.7872, -4.0753, -4.9222, -5.9585,\n",
      "         -1.3837, -3.6119, -2.2518, -3.6575, -4.6834, -3.6584, -5.1985,  0.1776,\n",
      "         -1.4736,  0.8454, -0.8435,  2.4096,  7.1370,  4.0268,  4.9536,  2.4453,\n",
      "          8.4750,  6.7571,  5.8532,  8.2188,  4.7085,  2.5121]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.4750]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.4841,  1.2140,  2.1361, -0.3452,  0.2229, -0.5124,  2.8517,  0.5762,\n",
      "         -2.2810, -0.8105,  1.3462, -3.3370, -1.6068, -4.7015, -4.8359, -5.7690,\n",
      "         -3.0058, -4.0507, -1.9231, -4.0507, -6.3716, -2.9573, -4.1285, -2.2878,\n",
      "         -2.2789,  0.6795,  0.1053,  2.8308,  8.2661,  6.0471,  3.3704,  3.6463,\n",
      "          9.7202,  9.3643,  8.2184,  9.9890,  5.5048,  4.6924]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.9890]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.3068, -0.3022,  0.6904, -0.0888,  1.2704, -2.4593,  0.6416, -0.2481,\n",
      "         -2.2698, -0.7184,  1.7352, -1.0897, -2.3246, -1.6825, -2.9489, -3.5465,\n",
      "         -3.9955, -5.8601, -1.9077, -4.1664, -4.4958, -1.7567, -3.7066, -3.8248,\n",
      "         -3.3489, -0.6765,  0.3458, -0.6563,  7.4924,  4.3162,  3.2004,  2.0544,\n",
      "          8.9464,  6.7773,  6.9590, 11.0352,  6.0804,  3.8322]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.0352]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.6477, -2.1129, -1.4195, -2.0475,  0.0314, -3.7929, -1.0468, -1.4989,\n",
      "         -3.8954, -1.6858,  0.2421, -2.4808, -3.8843, -1.0261, -1.9398, -1.9251,\n",
      "         -2.4314, -4.3330, -1.2392, -3.4945, -2.5603, -1.0137, -4.0380, -4.1851,\n",
      "         -3.3321, -1.1395,  1.0225,  0.8459,  5.2004,  2.8287,  3.7780,  0.1226,\n",
      "          7.0270,  5.7628,  6.1203,  8.5930,  5.2124,  2.0954]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.5930]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.1227, -1.3535, -0.8790, -2.6242, -1.2826, -3.5893, -0.3652, -0.4574,\n",
      "         -4.1859, -1.2084,  0.5086, -2.9457, -3.5106, -1.9906, -2.8133, -1.8011,\n",
      "         -2.5251, -3.2951, -2.9519, -3.9057, -3.3010, -1.2947, -3.4059, -2.4521,\n",
      "         -2.6130,  0.2866, -0.5991,  2.8149,  5.0652,  4.2921,  2.9155, -0.7501,\n",
      "          8.3668,  6.1382,  6.9617,  7.7945,  3.6011,  3.2349]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.3668]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.8962, -1.1657, -0.3527, -2.0103, -1.4891, -1.8010,  1.7450, -0.6012,\n",
      "         -4.4639, -1.5156, -0.5424, -3.9342, -2.7873, -4.5767, -4.6610, -5.0927,\n",
      "         -1.2821, -3.4943, -2.4768, -4.1833, -4.5202, -3.3967, -4.2804, -0.2661,\n",
      "         -1.8452,  0.5603, -0.8520,  3.5714,  7.2404,  4.8626,  4.7607,  1.5875,\n",
      "          8.3165,  6.7625,  5.9944,  8.1649,  4.1348,  2.1459]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.3165]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.3769,  1.3755,  2.2654, -0.5540,  0.3739, -0.5130,  3.0302,  0.8953,\n",
      "         -2.5277, -0.6264,  1.5186, -3.4140, -1.8670, -4.5722, -4.8905, -6.3222,\n",
      "         -2.8842, -3.6337, -2.6256, -4.1780, -6.2173, -3.3799, -4.5307, -1.1195,\n",
      "         -1.6900,  0.9127, -0.3678,  3.0531,  7.7304,  6.0694,  4.0626,  3.1559,\n",
      "          9.6961, 10.2627,  8.0299,  9.2944,  5.2262,  4.4605]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.2627]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-2.3777,  0.1282,  1.6649,  0.3665,  1.1559, -1.2827,  1.5741,  0.1938,\n",
      "         -1.5034, -0.5397,  2.0525, -2.0183, -2.0674, -2.8381, -4.3245, -4.2999,\n",
      "         -3.4063, -5.6714, -1.8695, -4.6183, -5.1442, -2.2543, -3.3985, -3.6916,\n",
      "         -2.7233, -0.1294,  0.3609,  0.1734,  8.4679,  4.7011,  3.1715,  2.9912,\n",
      "          9.1514,  7.8861,  7.8771, 11.2150,  6.0018,  4.3844]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.2150]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.5086, -1.6647, -1.1819, -1.5572,  0.4983, -3.4092, -0.9384, -1.4948,\n",
      "         -3.1454, -1.5770,  0.9404, -1.4493, -3.1854, -1.2635, -1.8939, -2.2642,\n",
      "         -2.9459, -5.1540, -0.8259, -3.7083, -3.5255, -0.9749, -4.3490, -4.2279,\n",
      "         -3.3740, -1.0030,  0.9970, -0.3534,  5.1455,  2.6155,  3.9348,  0.9176,\n",
      "          7.6047,  5.9967,  5.8631,  9.7004,  6.2243,  2.1896]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.7004]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.6395, -2.0173, -1.3599, -2.7807, -0.6919, -4.0238, -1.0367, -1.4540,\n",
      "         -3.8473, -1.4220, -0.0457, -2.7888, -4.2491, -1.4248, -2.3185, -1.3384,\n",
      "         -2.6548, -3.9595, -2.0587, -3.2866, -2.7179, -0.7047, -3.4014, -3.8067,\n",
      "         -3.5832, -0.2448,  0.5674,  1.6232,  5.4827,  3.0213,  2.5470, -0.7637,\n",
      "          7.7857,  5.4768,  5.5072,  8.2810,  3.6124,  2.5560]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.2810]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.4816, -1.1725, -0.6656, -2.2071, -1.4716, -3.2845, -0.1964, -0.4233,\n",
      "         -4.2427, -0.4829,  0.4254, -2.3985, -2.7844, -2.2187, -3.0187, -2.4571,\n",
      "         -2.8316, -3.5433, -3.6393, -4.2689, -3.8316, -1.7709, -3.4089, -1.3321,\n",
      "         -2.2913,  0.4230, -1.3761,  2.7532,  5.1129,  4.0832,  4.2955, -0.5616,\n",
      "          8.6235,  6.7246,  7.1060,  6.6183,  3.2683,  2.7341]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.6235]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.8066, -0.4997, -0.2666, -1.7920, -0.5542, -1.3182,  2.1248, -0.3049,\n",
      "         -4.2358, -0.7653, -0.3871, -3.5294, -3.1048, -4.4347, -4.6346, -6.2504,\n",
      "         -0.8444, -3.6334, -2.0560, -3.7230, -4.4114, -3.6877, -5.7002,  0.2961,\n",
      "         -1.5728,  0.3797, -0.7889,  2.4659,  6.9824,  4.1584,  5.2426,  2.5926,\n",
      "          8.3062,  7.3538,  5.3165,  8.3028,  5.2789,  2.3068]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.3062]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.4999,  1.1430,  2.0104, -0.2645,  0.2852, -0.6031,  2.7077,  0.6254,\n",
      "         -1.8914, -1.3411,  1.3483, -3.5126, -1.5950, -4.7702, -4.7113, -5.8098,\n",
      "         -2.8402, -4.2752, -2.0736, -3.9753, -6.7588, -2.7055, -4.3077, -2.5588,\n",
      "         -2.8136,  0.6241,  0.4792,  2.8909,  8.7964,  6.4666,  2.8908,  3.6913,\n",
      "          9.8512,  9.2516,  7.6182, 10.6847,  5.7603,  5.0904]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.6847]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.1275, -0.4673,  0.6284,  0.1934,  1.4735, -2.2583,  0.4899, -0.1893,\n",
      "         -2.0403, -0.7146,  1.7866, -1.2141, -2.3290, -1.6833, -2.7041, -3.2575,\n",
      "         -3.8822, -5.7657, -2.1794, -3.8221, -4.2961, -1.8262, -3.6960, -4.0697,\n",
      "         -3.4942, -0.8613,  0.3625, -0.3175,  6.8606,  4.2762,  3.8002,  1.9713,\n",
      "          8.9593,  7.2740,  7.0293, 10.5755,  6.5253,  3.5299]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.5755]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.7090e+00, -2.0864e+00, -1.4655e+00, -2.2298e+00, -1.3987e-03,\n",
      "         -3.8388e+00, -1.1372e+00, -1.4979e+00, -3.7704e+00, -1.5925e+00,\n",
      "          3.5168e-01, -2.5403e+00, -4.0283e+00, -9.4919e-01, -1.7981e+00,\n",
      "         -1.7951e+00, -2.2838e+00, -4.1049e+00, -1.4359e+00, -3.3085e+00,\n",
      "         -2.4688e+00, -9.6323e-01, -4.1296e+00, -4.0548e+00, -3.2633e+00,\n",
      "         -1.1469e+00,  8.5596e-01,  5.9006e-01,  4.7252e+00,  2.6927e+00,\n",
      "          3.6604e+00, -1.2133e-02,  7.1912e+00,  5.9352e+00,  5.9339e+00,\n",
      "          8.6961e+00,  5.2696e+00,  2.3550e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[8.6961]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.1339, -1.2964, -0.9251, -2.6273, -1.3265, -3.6266, -0.3973, -0.5246,\n",
      "         -4.2270, -1.2530,  0.4832, -2.9253, -3.5144, -1.9473, -2.8101, -1.8119,\n",
      "         -2.4781, -3.3514, -2.9901, -3.8383, -3.3364, -1.3035, -3.4510, -2.4085,\n",
      "         -2.7214,  0.2869, -0.6340,  2.7822,  5.0717,  4.2986,  2.9576, -0.6846,\n",
      "          8.3412,  6.1140,  6.8596,  7.7598,  3.6315,  3.1893]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.3412]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.8688, -1.0939, -0.3800, -1.9710, -1.4069, -1.7823,  1.7641, -0.5624,\n",
      "         -4.4384, -1.4168, -0.4926, -3.9110, -2.8068, -4.5573, -4.5018, -5.0940,\n",
      "         -1.1969, -3.4944, -2.4625, -4.1401, -4.4307, -3.4541, -4.3606, -0.2875,\n",
      "         -1.8779,  0.3480, -0.8940,  3.5034,  7.1016,  4.9921,  4.8289,  1.5606,\n",
      "          8.2867,  6.8755,  5.9179,  8.2807,  4.3289,  2.1603]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.2867]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.4198,  1.4479,  2.2389, -0.4106,  0.5005, -0.4836,  3.0585,  0.9459,\n",
      "         -2.3884, -0.7570,  1.4574, -3.5572, -1.9160, -4.6059, -4.6680, -6.4190,\n",
      "         -2.6488, -3.6796, -2.6925, -4.0745, -6.2613, -3.3931, -4.7954, -1.3150,\n",
      "         -1.8831,  0.7970, -0.2749,  3.1319,  7.7196,  6.2905,  3.8419,  3.2667,\n",
      "          9.7452, 10.4578,  7.8747,  9.6805,  5.6760,  4.7224]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.4578]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[33]], device='cuda:0')\n",
      "tensor([[-2.3273,  0.0962,  1.6897,  0.4534,  1.2376, -1.2729,  1.4831,  0.2158,\n",
      "         -1.3771, -0.6356,  2.1084, -2.0591, -2.0704, -2.7660, -4.3219, -4.3534,\n",
      "         -3.2858, -5.6977, -1.9167, -4.5866, -5.1437, -2.2370, -3.4859, -3.7267,\n",
      "         -2.8830, -0.1803,  0.2658,  0.2133,  8.3798,  4.7794,  3.1408,  3.0451,\n",
      "          9.2490,  8.0897,  7.7938, 11.3425,  6.1101,  4.4291]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.3425]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.6176, -1.6659, -1.2712, -1.6945,  0.5192, -3.5075, -1.0717, -1.5492,\n",
      "         -3.1995, -1.5822,  0.8925, -1.5277, -3.2910, -1.1553, -1.7143, -2.1470,\n",
      "         -2.8550, -4.9881, -0.8170, -3.6164, -3.4621, -0.8734, -4.4131, -4.2604,\n",
      "         -3.4288, -1.1180,  0.9784, -0.4044,  4.8633,  2.5762,  3.9288,  0.8519,\n",
      "          7.5296,  6.0748,  5.8226,  9.6400,  6.3318,  2.2102]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.6400]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.7648, -2.0898, -1.4748, -2.9055, -0.7165, -4.1089, -1.2125, -1.6126,\n",
      "         -3.8055, -1.4610, -0.0921, -2.8267, -4.3611, -1.3611, -2.2504, -1.2675,\n",
      "         -2.5880, -3.9183, -2.0906, -3.1698, -2.6331, -0.5963, -3.3709, -3.8482,\n",
      "         -3.7250, -0.2640,  0.5664,  1.4745,  5.3201,  2.8688,  2.4100, -0.8112,\n",
      "          7.8639,  5.6347,  5.2189,  8.3064,  3.5468,  2.6095]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.3064]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.6912, -1.5105, -1.0899, -2.5293, -1.6352, -3.4964, -0.3756, -0.8936,\n",
      "         -4.3791, -0.5123,  0.0140, -2.4968, -3.0825, -2.3455, -2.9387, -2.5507,\n",
      "         -2.5407, -3.6680, -3.6984, -4.3537, -3.7114, -1.6137, -3.5340, -1.0603,\n",
      "         -2.5333,  0.4483, -1.3687,  2.6966,  5.0986,  3.7379,  4.4198, -0.6977,\n",
      "          8.3130,  6.7069,  6.3360,  6.4881,  3.2044,  2.5196]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.3130]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-1.9251, -0.5786, -0.4298, -1.8820, -0.5228, -1.4267,  2.0875, -0.3768,\n",
      "         -4.3587, -0.6322, -0.5246, -3.6605, -3.3016, -4.5676, -4.4988, -6.3661,\n",
      "         -0.6144, -3.6647, -2.0655, -3.8512, -4.3016, -3.6703, -5.9018,  0.4241,\n",
      "         -1.6221,  0.3177, -0.8500,  2.5040,  6.8334,  4.1329,  5.3985,  2.4243,\n",
      "          8.1493,  7.5981,  4.9964,  8.2941,  5.4482,  2.2468]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.2941]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.1358,  1.2402,  1.9507, -0.0934,  0.6686, -1.6056,  1.3821,  0.8614,\n",
      "         -2.0210, -0.6792,  1.9960, -1.8954, -1.6862, -3.2852, -3.4171, -5.0246,\n",
      "         -3.6483, -4.6171, -2.3798, -4.6842, -6.0252, -1.9864, -4.0995, -2.3467,\n",
      "         -3.2590, -0.0589, -0.6363,  1.8597,  8.2455,  6.6319,  3.1322,  2.0801,\n",
      "          8.8671,  8.3875,  8.2639, 10.5202,  5.7610,  5.0463]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.5202]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.6586, -1.1993,  0.2321, -0.4438,  1.3694, -2.8212, -0.2709, -0.7088,\n",
      "         -2.6828, -1.0106,  1.0242, -1.7408, -2.7327, -1.0577, -2.5265, -2.8892,\n",
      "         -2.9458, -4.8611, -1.7326, -3.3179, -3.4231, -1.9150, -3.9735, -4.5440,\n",
      "         -3.2746, -1.2388,  0.5020, -0.3141,  6.0485,  3.9552,  3.6805,  1.0596,\n",
      "          8.3856,  6.4349,  6.9096, 10.3638,  6.4374,  3.0905]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.3638]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-4.7787, -1.8121, -1.6367, -3.0122, -0.8469, -3.8675, -0.9920, -1.0156,\n",
      "         -4.4322, -1.2432,  0.5360, -2.9085, -4.2692, -1.1447, -1.9939, -1.6603,\n",
      "         -1.9419, -2.8231, -1.9212, -3.4122, -2.1161, -1.1620, -3.8466, -3.0739,\n",
      "         -2.6444, -0.6864, -0.1032,  1.4209,  3.8086,  3.3480,  3.5685, -0.6843,\n",
      "          7.3096,  6.1499,  6.6325,  7.7355,  4.7505,  2.6664]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[7.7355]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.7414, -0.7614, -0.1693, -2.0490, -1.2917, -3.2701, -0.2412, -0.1498,\n",
      "         -3.7103, -1.0057,  0.7401, -3.0518, -3.0042, -2.3706, -3.4228, -2.4059,\n",
      "         -3.1025, -3.7868, -3.1492, -4.0262, -3.8629, -2.0106, -3.1302, -2.2626,\n",
      "         -2.7748,  0.3602, -1.1453,  2.8294,  5.4723,  5.1218,  3.2654, -0.6127,\n",
      "          9.8899,  7.0115,  7.2037,  8.1408,  3.2901,  3.8867]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[9.8899]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.5514e+00, -9.7659e-01, -3.1946e-01, -1.9783e+00, -6.4054e-01,\n",
      "         -1.8320e+00,  1.8221e+00,  2.1332e-02, -3.8104e+00, -8.7710e-02,\n",
      "          2.1769e-02, -3.5045e+00, -3.3468e+00, -3.6937e+00, -4.1227e+00,\n",
      "         -5.6632e+00, -9.5173e-01, -3.8990e+00, -2.5097e+00, -4.2950e+00,\n",
      "         -3.7033e+00, -3.8601e+00, -5.4309e+00, -2.8041e-03, -1.3851e+00,\n",
      "         -1.0447e-01, -1.1379e+00,  1.8160e+00,  6.3774e+00,  4.0224e+00,\n",
      "          6.2892e+00,  1.4568e+00,  7.6957e+00,  7.6335e+00,  5.4544e+00,\n",
      "          8.0392e+00,  5.3211e+00,  2.0392e+00]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward>)\n",
      "tensor([[8.0392]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-2.3853,  1.3048,  1.8691, -0.4102,  0.7231, -1.7904,  1.1863,  0.9899,\n",
      "         -2.6654, -0.0632,  2.0112, -1.8470, -2.0875, -2.7546, -3.1335, -4.8001,\n",
      "         -3.2352, -3.9447, -2.3895, -4.8210, -5.3240, -2.1119, -4.5190, -2.3940,\n",
      "         -2.8241, -0.5963, -1.1446,  1.4383,  7.1670,  6.3011,  3.6420,  1.7450,\n",
      "          8.5552,  8.5146,  8.7691, 10.3069,  6.2572,  4.6703]],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[10.3069]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-3.8179, -1.1793,  0.2360, -0.6985,  1.1704, -3.1714, -0.4009, -0.6898,\n",
      "         -2.6411, -1.0666,  0.8886, -1.6837, -2.9075, -0.8378, -2.5639, -2.8917,\n",
      "         -2.9660, -4.9236, -1.4547, -3.5794, -3.3563, -1.9712, -3.8967, -4.4938,\n",
      "         -3.5377, -1.3335,  0.0543, -1.0779,  6.4890,  4.1569,  2.9082,  0.5699,\n",
      "          8.9022,  5.9889,  6.4740, 11.1160,  5.9052,  3.6790]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[11.1160]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n",
      "tensor([[-5.0143, -1.8197, -1.7967, -3.0678, -0.8250, -4.1423, -1.2118, -1.2342,\n",
      "         -4.8212, -1.0181,  0.3469, -3.0169, -4.5847, -1.0341, -1.6697, -1.7059,\n",
      "         -1.8997, -2.7059, -1.6885, -3.3700, -1.7077, -1.1382, -4.0302, -3.1579,\n",
      "         -2.6516, -0.8490,  0.0481,  1.5804,  2.9927,  3.2032,  3.8462, -0.9803,\n",
      "          7.0143,  6.4489,  6.8933,  6.9957,  5.1388,  2.3620]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[7.0143]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[32]], device='cuda:0')\n",
      "tensor([[-2.8523, -1.7916, -1.4484, -2.6270, -2.1907, -2.5452,  0.9071, -1.5944,\n",
      "         -4.6527, -2.1961, -1.1460, -4.4396, -3.3957, -4.8270, -4.5882, -4.7177,\n",
      "         -0.7476, -3.5455, -1.6098, -4.3536, -3.9543, -3.1100, -3.7065, -0.3212,\n",
      "         -2.8258,  0.5114, -0.9763,  3.8693,  6.8605,  5.0327,  3.4337,  0.7917,\n",
      "          8.3458,  6.5691,  4.6945,  8.8956,  3.8875,  2.4306]],\n",
      "       device='cuda:0', grad_fn=<SliceBackward>)\n",
      "tensor([[8.8956]], device='cuda:0', grad_fn=<TopkBackward>)\n",
      "tensor([[35]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "predicted_notes_lst = prediction_with_influence(testing_data,int_to_note,initial_seq, max_midi_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_notes_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x219099ef7c8>]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO29ebwkR3Um+p3a7r1Vrd7UV0Jb0w3ad+RGyGIbJLFIBgQ2BmFj8AKy/WzLYA9jbPxGxjOeN9gyM8bPBsQzHmYGBDMsBsbYCINAmKVNS0hGC1rQhhak7pbUy93qVlW8PyJP5snIiMys1q26lbrn+/3613mzMjMiMyO/OPGdcyLIGAOFQqFQVA+11a6AQqFQKA4NSuAKhUJRUSiBKxQKRUWhBK5QKBQVhRK4QqFQVBSNcRa2ZcsWs23btnEWqVAoFJXHDTfcsMcYM+vuHyuBb9u2Dbt27RpnkQqFQlF5ENH9vv0qoSgUCkVFoQSuUCgUFYUSuEKhUFQUSuAKhUJRUSiBKxQKRUWhBK5QKBQVhRK4QqFQVBRjjQM/VLznC7fitof3r3Y1FAqF4pBw6tHrceWrTlvx66oFrlAoFBVFJSzwUfRcCoVCUXWoBa5QKBQVhRK4QqFQVBRK4AqFQlFRKIErFApFRaEErlAoFBWFErhCoVBUFErgCoVCUVEogSsUCkVFoQSuUCgUFYUSuEKhUFQUSuAKhUJRUSiBKxQKRUWhBK5QKBQVRSkCJ6J3ENGtRHQLEV1DRNNEdCER3UhENxHRPxPR8aOurEKhUCgSFBI4ER0D4AoAO4wxpwOoA7gMwAcA/Lwx5mwAHwfwh6OsqEKhUCjSKCuhNADMEFEDQBvAwwAMgPXR7xuifQqFQqEYEwoXdDDGPEREVwF4AMACgGuNMdcS0VsBfJGIFgDsB3Ce73wiuhzA5QCwdevWFau4QqFQrHWUkVA2AbgUwHYARwPoENGbALwDwCXGmGMB/C2A9/nON8ZcbYzZYYzZMTs7u3I1VygUijWOMhLKRQDuNcbsNsYsA/gMgOcDOMsYszM65pMAzh9RHRUKhULhQRkCfwDAeUTUJiICcCGA2wBsIKITo2NeCuD2EdVRoVAoFB6U0cB3EtGnANwIoAfgewCuBvAggE8T0QDAEwB+eZQVVSgUCkUapValN8ZcCeBKZ/dno38KhUKhWAVoJqZCoVBUFErgCoVCUVEogSsUCkVFoQSuUCgUFYUSuEKhUFQUSuAKhUJRUSiBKxQKRUWhBK5QKBQVhRK4QqFQVBRK4AqFQlFRKIErFApFRaEErlAoFBWFErhCoVBUFErgCoVCUVEogSsUCkVFoQSuUCgUFYUSuEKhUFQUSuAKhUJRUSiBKxQKRUWhBK5QKBQVhRK4QqFQVBRK4AqFQlFRKIErFApFRaEErlAoFBWFErhCoVBUFErgCoVCUVEogSsUCkVFUYrAiegdRHQrEd1CRNcQ0TRZ/AkR3UlEtxPRFaOurEKhUCgSNIoOIKJjAFwB4FRjzAIR/S8AlwEgAMcBONkYMyCiI0ZbVYVCoVBIFBK4OG6GiJYBtAE8DOA/Avg5Y8wAAIwxj42mijn48S3AnjuA038mfMyBR4FbPws871cBovzr/fA64LbPAc0Z4EXvBNqbs8fsuQt48LvA2T/nv8ZN1wA/2gm0Dwde8gdAbwnY+UHg/CuAunjcgwHwrfcDz/0VYOqw4nsFgH/9X8AzzgCOOMX+vfAEcON/B37yt4CaGEz1usC3/xI47zeA5nSy3xjg238FnPVGoHO43ffY7cC/fNg+m+f9GrDlhHSZt/4dsGkbcPTZ2fr0l+09uOUwfnwLsPsHwBmvS8oBbDmzJwK7PgI88q/J8c95E3DsDvu+7vl6sv/kVwInXATc/y2gOwec8FLgge8AN38iOeaYc4Bz3gzsuds+/+f8PPD4PcC3/xoY9Owxhx0FvPjfAXN7gG/8OdBbTNeXy7nna/a+Gc88Hzjz9elj+X5s87fP6AVvT36/7XPAxmfa53bHPwIzm4CtzwN++FWA6sCzXgzc9017Pye+DHjwBmD/Q8Cpr04/tz13RffzJuDJB4C7vmzbzMHHgO9/Cjjv14HFfcAN/822sd5C0t6MidrB/2XbNAA8sBO4+Rqg3gJe8A5g/VH2Pk66BNhwDHDDR4FtLwAOf7Ztb0eeDhx5qj13bg9w/VXJc5s6DHjJuwEY4Lo/AZYOpp+R+9y+8T57DwBANftNzp6UPufmT9h3yzj9Z4DtL7TbywvAdf8JWDpg/25M2++U23KZcvY9BHzzL4B+1/7d3mzvoVZPzl9eSN8Pl9PeDHztPwMHH02Xd8QptoxMe3sG8OLfAx7+HnDjR4EXv8s+7xVEIYEbYx4ioqsAPABgAcC1xphriegaAG8gotcC2A3gCmPMXe75RHQ5gMsBYOvWrStaeXz3w8AP/j6fwG//PPCPvwec9hr7QPPwrffbDwwAtp4HnHpp9pgbPwp85wOWBH0dwlf/g/0QAeDMNwCP/xD4ynuAbS8Ejntuctzu24F/uhLYvN1fjg9feDtwzi8AF7/X/n3ntcCX/z1w/EXAkaclx/1oJ/CVPwaOOsv+xnjyfuDadwNT64Cf+EW776aPAbv+xm5PbwQu/L/TZf7ju4DtLwZ++kPZ+jz4XVvO0ecAz35J9vfv/n+WyM54HXDTx5NyptYBF70H+Id3AbUG0OoA83ssMRy7w34kj98LTG+wndSeOy2xXn8VMLfbEvjODwG3/R3Q3mI/6Ns+Zwn8e/8D+NZf2g72lk/bNtKZBZYXge4Be8y91wM7PwDMbLblA7acvXfbcr71/wL3XGd/XzoA3PmlLIHffI29n84RwPI80D0InPs2ey8A8KV3WyJ87Qft+9+0Hdj6ceBr7wVgLIF//b2WiE98GfDN/wI8dKMl8J0fAG6NntuuvwW+89fAmZfZZ/i1/wc4/acteX/p94FTXmnv55+uBI6/0D63r/wxcNx5wGDZbh9xKnDSxbZe//Ih+1wAS8ynvRb44r+1ncBP/ibwhSuA5/+2fT+f/y3bzl/1X+3xP/xq8tzMAFh8Ejjx5ZawvvWXtpOqNe2xS/vt8fzcFp60z6G1Dmi2gbnH7PFue/un9wALjwNT64H5vZYsmcAf/p79Rqc3WmJeeNx+U/L7X3giKucw22m55dzxRfsM2lusAbK0Dzjj9cARJyfXeOhGez9uOcc+F/j6f7Z1a0QGS3fOdprnXm4ND19723u37WB/8rey38hTRKEGTkSbAFwKYDuAowF0iOhNAKYALBpjdgD4MICP+M43xlxtjNlhjNkxOzu7cjUH7EPinjR4zIL9vztXfL3uHLAuInnuRX3HDHrhcgc9a30DlpDi8h3rpL+c/r8IgwGwPJe2GnvRted2Z+so/8/b352zjXlqvSUiF925bN0ZfG+he+gt2hEIYP+f2gAcdjQwt9cSY38JeMnvA++8C9j8rOTeeouWyN55l+0YZL3l9WZPscc891fE/kXA9O3f3TlLKO+8O+n0eotJOb/2DXv+O++yZCvLP/a5dv/zLredizHOvUX38867gAv+MNnH6C8n11teENdeSN7X3J7stjF2u3vAnje3G4CxJBIfu1ds787Z3pNsy3pzG+8t2W+Iy5/fk2x3D9o6p86Njv3V64Ff/HtR7+i8X/5S8jzPfH36efD2RX9kf2/MZEdAgH1nP/GL9pijn+O092j7jZ8A3vaV9HXl+QDw8j+x12i2nWtEx19xI/Cav05f173Gmz4NvO2ryXl87k/9eXKfL3i77cwGveT3370TuORPk2vz/kYre79PEWWcmBcBuNcYs9sYswzgMwDOB/AggKgrx2cBnLnitStCfwkY9IuPAUoS+HwiZwwG4WPyrjfo20YDWJJnonfJ0UT1NoFyXDBZ90THwdv8ATGWmfCcMuO6z6f3NdvWWnHvyRi7z0fsQHJvoc6ut5Q8//4S0JgCOlvSRNOJOvXGdHI/va49FrD14vKX57LXA+z/fUHsQGQVzyfvgo+V76QhZJ/GdHJuX5TfmbV/L+3P3pssX5YNRJ1IN1tmr5sm1vm9ts3M7U7K8RJ8iKgFgaa2neNlvafXJ9v83PKuL8/lZ8Xvzfcu+Zi+7NDEufzMfEbQ8lwi98g2wc+Oz+XrZAg8aivyvctyZD34Gm49+PtpzqTLie9/StznVPr3esvKmfH+btJB1MV5K4QyBP4AgPOIqE1EBOBCALcD+DsAF0THvBjAnSteuyL0umHykMcAYRKSWJ5LCNwEOgZ+uaHrGUHg8qW7ZModRFH9GXy+76PIWOCC8Lx1n0vva7Vtnd176nft/bh1Z/C9he6hH72fQT8h5c5s2jrsbLH/11tpcubG3uykO56Y5AWB1qeScvhj7M4l9wZkPzQuk9FoCZIV5cdE5XSSkuT5WPluBv10p9IT97a0397L/B4ABph/PIe095TYDhG7h4T7S3a0xduxEZBzfXnP/Kx4lMnlUN1KDox6K0y+/L9Lvvw9NztJOd5OYEo88wD58nuvO+VwPeqtxCLO6wRSHT+3G0HEdccwqHvaRN+59xVEGQ18JxF9CsCNAHoAvgfgagAzAD5GRO8AcBDAW1e8dkXoLRZLENz7lbXANxxnt0OWvc+KlRgMEgtCDp9cMuUOoiyB8/mpxsjDX4fAlwN1zLPAaw3P8R7Cl4gJPEdC4eN6i/aj6cxax1zGAhcfmiTnVlt0PAvpa8aWWitdDj8DaYHXp7LHNJwPUV6br8kdzNxu69iT98YdQNAClwS+mD5m793J6OvAw1aD5nJSFvUw1rizze/FtcBbbQCUrlewLGmBC0uy3rBaOJfT2ZJ2pDfE80yd20qu4RKnl3yduvP++J278odrgbey30y9Zf1Xdc97A5Lvp9VJ6huSQmQ9eovJ3z6DYTUIHACMMVcCuNLZvQTgp1a8RsOALURjwhEmIQnDh+UFIaEEiHU5YN0yBj1HQmECX8geBxRLQAxumD5rIkTgbpm+ui/P24baa3gs9gX/dRh8b6F7kFYnW6ydLdby5Dq3I4JsTKXvMZZQ2ukRBZfVXwJmIotPWjvxiCeSflriQ5Z1AbIWeCx5CAu8LQjcvbfYAvdYcoN+9v7lMbt/kBy7+45k+8kHkvc091hal5akOu+TTfZYSQawv/d72br3lqxzmDtMfofzznUORkFlS/uSe+05z60zm5Qj5RPAPj/Tt8+hVg9IKC5xRu0sRb7SihcEGksbrgUeXYOdyXWnHGklN0S7SV3DY4H3uukOhJGSWLrp++P9cXtbHQllctErIBB5TMhillieS4aXQQmlwAI3fWGBO4QiMRjWAs+TUPamjw1JKF1hycrrNtuW6Nx7KrrXMhIKkDR+JvDleRsRAwgJJfrQBgNr0XFjb3USX0d3Xli1QsKQH5nssLtzYjjuaJn1qXSnLzVb+SGWkVB8WipLKMbY/VJfB4DHbkuODW3vvTt5tgcethEWQI7cEbKiRfvodyP5INKGpR+Fw+N6C8n7AZJOoS80XiDyZ0TltJ1QPndUEksowkJ1ybcrLF8gX0evNQCQh3xZv24nx7qdQJ7vQtaDR6ZUc6QQ4TvhzoyNNTnC4Dr3Fq0zvbbydFttAo8twBwZpawFzvpbbIEXSCghq3TQT6y+lXRi8vleJ2ZJCSWOiHEklFbbEp17T0USCt9bMApFWqARaTIhPna7jeKIP6ZWmoD5Q+cPcXGffc+DZUvy0kpueCzw5QX7Lx6Oyw9NkC+jLgil75NQHAKXOrlPS2UnZvyMHAv8sduTY8tsSyt934+SyKCDj5WTU2S9WUOWFrjp2w7DVzaf3+umrUjpkHYtcNe6dfVj6fNguOQb0tFZAvHp6HEnIKx4txNwR04+Hb0xYwmXpZaUFCJHbo5U0nDbRDdtbKwwqk3gRRYgkNZE88CNhz30IWItklBMP7H6pMa4Uk5Mn64YCiN079nrxJy39ZVas/zNV3e3/LwoFP6/17WNWhI4kyOQaNDuh84f4ry0IpfSH0Xd+YiAREJxo1CkHi/RaKV1atk5TG0ISCiOtSXfzaCf6KL8mzHJ/Q1L4KHtx+9JDJiDj1qHKJC2zOf3JO2N763hkFJeGTGBL6bJSzqkMxKK06m5OnAp8g3o6PK9h3T0Zsd/DEeKyOv4dHSuA5Do6D4pRDox5bVlm5CW+Qrj6UHgeY7MshJK3HjW2f+DFnggRA+wH6hxnZhsga+QEzMVEhUKIwyEOnqdmByFMhN2eg6W/c+Y7y0ooThOvMZ0QtpP3p8m8MZ0WmqINfDoQ3StyN6ifygc+xwiCaXlk1CERCLLZ+teWlJAYmm69+bqnfLdmH5aopDWON8/YJNMeLt9+FPb3vcjAMZuLzxhY8nbh9v3s/ikqPdUokFLcnvy/kQKkdvcvuQ9A5a0uRz5Lvl5AsMRuEu+3CYYLoH6dHRvJ+AQePzeQjr6fFKHuB5L2Q5EbjNR+9qE255WENUm8DgOOUcD7wcI1AWTXmEYYY4FzlY7k4Z0YmYs8H76/yJ4nZh87QNJQgaQY4HPZ/ezldrshC12eU2JWMIKWeBCkmDtVVpqqbjhljNMdSxwN5wt5IziMl0LPB4uL6UlEkbmd4fA510JpRuOQpHWrpQQXMKa2WzT+wGrtR5+fPLb7CnJ9hGnPvVtHsHwyCWWUBzy8p3LBC7vGUiTdlBCcSQkGWZXRL51N4JkKa0l+3R06YDkY1wnZjxyEu88VY+5tAXO8lphHLjPL7OkBB7EMBJKoQUeERQ7MX3E2hcZmL7r8Tk+J2ZIAy9L4MseApcNTxJMMIzQycTk+2lFEkp3Lp1x2HWI3kWhE1Na4FEjbsuP3pVQPMNUrwW+6EgYOWGEsQXuSijOB8W/8/wXdVcqcJ2YUkt1yIrfbSr+WyR0yOvyM+jMJiTY7ACbnpkcJ+cLOeKUQ9vm58fD+bjDdOo0K1LKN2+3x8pzUx1boDMGciSUVvJ/GfLlURGQ9V34dPTunH0fPLdJRkf3+E58YYT8Dcd1FVnfsm1IHV1Kc1LWk/6aFcbThMDzJJQVtMBd7dgFnyOjEkIEPmwUSky6HgscSBNcMJHHscBlxlmznQz74+OlBZ5D4EVOTCbm+lTiMAUcC9wZ0stMTCAdSeF+FL6QrdgCj85PfWjdsAXOGZdFEorXick+ARHqKN8RT8IUX1eQdmeLsx0R+8ymsJU+LIEbk0hZ9als/QBg43F2HhHAzvMiOy+p8QLhzhjwRKGw/CDki1AKu2/UxNdIhX5O+8k3pV+7OrrseHMSeaSEUp+CV96T98NtLyOhLPnb2wqh2gReNISXxxRZ4C6B+1LpXe3YBX+49aYd6kmnnFt+HIUyrAXuWBM8GZO0EMum0stwKbZUuwHS9nWApePAu2nNmj92SQBxWNtC8jfgl1A4AsMdrsqPbP5xACYdTgZktcq4/OhvOdMdozObpLzLe3PDCLlsfqeu1c3X5nfW2ZI8g7Yg7dD+9uEi8WkG2Cgmh5OWs5RfZgWBc0fbaCXyA3d4cZ1mk9n9uCOJnZgeDTzeDhB4X7QBIG2hZqI/XP3ao6Onpj9oFZOvL5We68YRJr5omEwnIDq7VBy4K5V4ZDW1wD3g+FqgIA68ZBhh1yVwT6fgascu+ByqJx9IqPyhLXBfHHjXTg4FpAkulIAjpwEwJp1xxkSXuscF/zYjz4kpIy5cfVBamozYAnYkDJ+EsuhYye5wVR4fJ3Q4YV2ZKBSWUPanjwcsiZpBEocN5IejyfYoJwLja/M7S1ngT2F7ekNipVMN2HJiUibLL3N70hE+7ognVKe2IHBXwsjTwN0sx4wT00O+cSKPSKXncuPypXzhI995j34tR63dLAH7koGaDoGzb4TqSE0L7fpO6p424XY8K4jqEniqV82TUEqm0ksyA/yWccg6ZbATs1ZP9LlQ+UMn8nAq/WKiU/eWgPVM4MICDybycJ2NbaRyyBpb4AGr2/f8YsnA8/zlO4mtEJHBJ/8HBIEeSP8dW+Di/vgYn4TCHysfzx9irWZHRfxOMnHgUd3czgHwx4LnOjHFO13cn93ecEx0XVcDFxZ4UFrxbQvSbW+JpnVtWNKY2WTnKJnbkxBVYypxEHKduR35ypgPSCjTG5NyOHqL4SNfwHFiuok8c/b6TJC+TiBFvgEdPUO+rgXuaNg+Z2qrkz4m5IwMWeBE0XmOv2aFUV0CT8Xc5kkoZS1wnoehYxulz6pfLpAU+JxaIxl2FSXyhGY9zNRPnM/k2Fuyw+rGtGOBByQUdwQhh6yxBR6SUDzPL282Qvl++k7jl6TFiCWM/em/mx4JJT7GE00QEzhb4O5Q2Ilgkb8BonNwnJjymrGW7BkBAOkcAql78/Z6JvA8QvYRu8c65u1Wx8oqnVnbWbWj6xEl8doyDE6m0lM9mSvfV7bUwCWBueVIuLHxvcW09epq04CffOVzdYnQq6P7yNeJJXdT4X3hjG49Yj9OYOTms7Q5C1QlFA9kz5sbhcJOxEDmJEN6wKkecGI6GYwu+ByqZXUzTgO/7j/ZuhxqKj2QDk1rTGWjJOKQw4V0ByGtaEngzU7i7AuRdp4T0xuxI97P0kEAxkPgHgll0ZEwWnkSihPP252z5cjjXT00tohCBO5cG8gS+KCXvh9p3QPp5yGnoV1yLfASure7f2aTbV+dLZbMpjbYTpwoup7QrzmOm9tHSkIRcfc8xQGXkarHbBJT70ahuOWknqcTY+0+c7aM3ainlvO+AKe9O2nsvizKFPlOp8txsyJDOrrb8cd+HEcKSSXsOAQfj8JH58Qsu6Ta5CE1dWcJAi+SUFLzH9T9pBTr5Bv8HUJsgdezXv7lOeD+b9pVWLaed+hOTL6nqcOSxjizMdFn+1G6+dQGOxHR8rxdAQewdeb93fl03C2HXcn76s6J6/gklJwoFF/0BTf2Z18A7L4z/eGHJJR6y3aoUn92reSGQ/5Acryrh7qjAfmbvIb84GY2pa8ZcmYxWcl36pNQtv4k8OAuu2hEa51dremZz7cyxrNeYlegWfcM4MRX2L+nDgNOfY1dXalWs6vknPAye62z3gAcFS13d8brEi37tNckjsmZTcAT9wkJpZUe3tdbwPEvtavmNKej9/MDS87y3l0Jwy1HQk4eBvjJjffzu3DJNyOh+PRrD/ludLIoZTluJ+Tq6DyHjRuFwkl5oeglnvfb7aQ4VHNEGnh1CbxXksBlZl4eloX+RnV/Kj1fo3N4IJGHLfB6os/Fc00MgH0PRvXtH7oTExDEGTVGmYTDHVVni4fA561FxYQsLXBiAnesdL6OzwLPiwKSH4Ubmrf9RfafhBvGx38TWavMZ8m6EkZq0QWT3BtDvpNCJ6b4ELkTiFcg6qbP4fqWtcDXHw380heT/b/4f5LtN4u1OH/uk8n26z+abPNKMgBwyZ8l2xf9UbL9onem6788n0gJcSp9N7FqT3qF/QfYDoSXMZO+EZ+WK8uR8OnXoQQY3valsANCR19Krx8bimRxyVeW4zqwXR09llJ90punA+PR1/K8/cbrTpsISS8rhApLKDIaI+DENCYtYeRBzh1dqwUs8OjltrfkJ/KkLHCZPv1AcpwZci4USaypuNgpZ9Ua7mS2pOvMq+vE++fTjZUbrDvRFVvJ3kQe1sCLLHBPZIeLkAYOpK2y1PUcJ6YbZw0MYYFzB+KMAGT5cSinM7c1H8/7pQXu08BHZI0Fwe1DzqZXdnjP0try/HBarjvBV4bAPSvqBMlX6OgZC9wTS+6Sb6oerpXs6OhuMhHgPCvP/Tem/fkDcQq+p72tEKpL4GUkFNYqqZ6EzoWwvJBYG0UaOOuCLpiUqZ44R9hJBABP3J/Ua3AITky+TmrpsZattxvbzTpmPA1t196T3C8bK3847mo90+uthZGbSu95VvLDdDVrH1wZpOGxgPn+3WNqdTuM54+Ij+N7i8sQzqiMBu52INKSaqafQRwSJx1WQkIpssBHZI0FwasayWxIbp++rNTUuaLzGmZWvTiZTYwWXetU/g5E0//KDEhXR3f1a18IoJtFKXT0Qd9+A3k6uptMxPXwSSTx7y1/u5UzbKoT00HKiRlKJIl61pmNiEPnQpD6W5EG3tmSn8hTYwkl+kB44YEnBYEPPZnVfHIdvi/+KJpy1Rohocg6d939c04iT8AC59+8FnjObIQpJyZbnjnEVXesaEly/F74/n1Wcn0q+YhmNmbP5eODqfSuE9X5XT4Dd8pbPl7KZQyfBj4iaywInmlSaveNFoBoVJZXHymh+JyYIcTWcw75AlkL3CuhOJJhXIYTAsgzQIYklPj+3ZGTUwfAMxvhYpiI61P+dhtr5wHiXwFUl8BTLy6Uyh01npnN9v88HVzqb1QPJPLMWUtvekNBIk8tLaFw+SyhmL5YkWcIDZyv0+8C/Z4lisZ0ejEG7qQ4ciIjrYj9fD+NVjiRp9XxT3QFJM93WCemD3mZkDGBb3au52iZvJ+PA7ISyvJC9NwCTsxQZyPX5vQ6MYWWOiiQUEZkjQXRbNt7lqMLeb955MLPfmm/batl616r2bYVIl8ZfsfI068Bj34djSJ4ZB2vxhPQ0bkuGZlD1sFJJuJ6xFKIxwhpTPnbLUs8g54SeAZlnJj8wtiTnheJIhtPLeDE5DTdVid6MY6VblwLPGo0XP6BR6L6Cidm6SiUueQ6bEUCEfn6nJgOgXcdAucoFHnPjWkn1HAuvFoPkO/E9GngeY2YP4yl/VEHKPzr/EHy/XtlDvER8XEA0HAmJQrJGJk4cJ8F7kooTvmlNfBxW+DRO+YoGo4D5zrlkTI/e56OdphwOJkF6Uaw+OZQDzkxg/p1NIpgA8KrX4tyfBa4GyfudWJGx3cP+p9VYyqggU/5J0dbQTy9CbznEHiuBS6cHyEJRa7g7rteKpEnIkMzSBMK1zd2YpYgcF4tiK/D07MCiCeI4pkEQ05MV1pZnsvO+SCdoUDyQflWrOd6he5BjpDKSAcyjM/9SLiTmTrMjo68eqP4iPg5NdvOQrvTYT0+Lw6cr+VOZxCy5GR7zIueGRe4vTKB85JqXKdcJ6ZL/kM4YBsOgXsXA3YtcEd7BtKhiCn9eir9u0zGk3Xgcop8F1wHwF+Pxf3+d1dvZfMXuH6h9hi5BbsAACAASURBVLRCqC6Bp+YEKUngeZEo0vkRcmKyJuxLegHSYYR1MaRvb04fN2wYIZNvygIXDqlmG0CUHeha2q4FzlElsQUuCVzIBIMB0FtIRhzDptIPLaGIKBCXULiTac6kh6tuRMKih8Al5DvJxPOKDsT3e0uMcoISisepm9HAyR83PUr4LPC8DjN1Lq+IFK30M4wlyXHXgMeJ6ZDvYBBp4E4WJSB0dCf8U5IzECBfUU5R+CcgciM8Uk5IbkpJKE6bCLWnFUJ1CbxMJqYroRRp4FJO8Frg0RqLLU/EBpBElNTqfotQ1neY+cBZl5ME3hfWhHQ0LTsSSqyNc8Ncl1jUcs1IIC0TuGn2vtFGniOWP5bWumzquw8yCsRn/QL2PuutHAnFed4th8DlO3FJqx4tXhv6vdkWU/oO4cRc2p/ME7IUWXBu2vmoEVvgLINMpyWrMhp4fO4Qowc5F3fGielGmDgr0suy+kuIF7v26uhOqLCPfFOrynt0dIa7LieQflZeCSUnjDDUnlYIFSbwEnOhxE7MEgQu5YRgGOFclHbuidgA0qn09SnEySQugZshLXAuJ5ZQnMYYx+rOeSxwnhdFaHsxgc+lnTVSJpDWjE8DT0lYvjDC6P1MrUf8HPIacfyb8ejPnaQuDfFcXSem+7zlvcXHR8d4h8Ly2k2nDqITkwkxsnzv1AImWSREpt+PEy2PhCKfd16dOKchPneI+ss4ba9+jeT3XPIVBosrUchr5JGv1MDzUundVYFkOTB+I0S2q0yoZE57WwFUl8Cl5zgYhcJhhGWcmGIKybwwwpQFHtLA6+kXlrHAhfVaxomZkVCcxhhr8gtJnXjOjDxCdp1GLY8zNI5CcZ5darKqnPUyZeZcGSemu8115rr4rDh3O2iBO9pnpg5TyXGulSwdxT3PUFxacu47bc7YdwGMPwIF8OjYU34tOYRWO31uWfAcIkA2BM+VUHzky1nRKcnQmQ8cSN6Hj3ylju7zXbip9O66nO7xPi3b1zFl9iuBp1FkAQLJi2ENOmSBu/pbXip9M8eJmUqlFy9semP6uFQiTwkCj/Xr6D7kRPwpCWUuPSWnJJ3U3N+dJJU+pYG3s8eHLPB+gYTFz74sgfs+bkasgbed44SVnJq/e3NyvIT74bpwU/PdOuQ5MWVCiPtOeQUc95xxIbbAhY4dIhofmp3k3KEklCGcmD7y5fJS+rXHAneny2gGdPTQHDZmkPjRuvNJaK08xr1e6j49dcrbv4J4ehB4cEkvllAiAg05MV39LS+VnteP9F0vlUovE1Gm0z33sKn0sQUe3YecZ7zuxHAvLySSiowqiZN2ZoQFPpcesvriybnDcpOgUh2ozwKP6je9PtmXK6E0AURWr0sS/EG22mmSlVZyasQTPaeWT0Lh430f4lT4N9m5BSezCljgvAKOW4dxgdvDwhPJc3PrngdpgQ/rxEwRuGfEFMsfHg2cy5Nhs+5UsHxtQBC4JxMzNWr1OUJFPTJ1KHhWoZGgb7SwwqgugZdJpc84MQMSiqu/BRN5okiVZoGE4lrgnC0p6xsn8gxhgXujUKayTkxJeD4JJdbAcyxwn2YupyJIWeA+DVw4MRl5jZhISBghC7yTEEgmEUdcOxSFUmiBt8K/8TMYDPxEIK1N93nw5FGAfwg+akgJpeHppIpIuXmIEoqM8HAnEIutZ44g8UgoXF5vKSBbOXHieTq6TOTxSjmiHpk6FDyrkPxX1N5WAKUInIjeQUS3EtEtRHQNEU2L3/6SiA7mnT8S9JYSXbEoDny6wAJ3G08tL4xQzp3tdAgmoIEzyfLLHzqMcD59H3KhiJQFPpfWtd0En8a0rRtHVGTCCEWkhSuhuFMRsMVCtbCEIokLKCYvbuTuRxJr4O3kGi6R+DTwPALPs6R8v/Ez7S2kI4Dk9XxRKID9qPOs+1GD6y5nxSvyB6TO7/hD8IogMyXd+WeC5OuTUAJZlGV0dJmy3xOyY149cn0nnjYckv/caJcRoJDAiegYAFcA2GGMOR1AHcBl0W87AGzMOX106HcTy6KIwNnRF9LAu87Qy7ciDyfJlHViusOuZhtYd2R0LRmCV8YCFw1TpvUCiFPp+T6kVd1yCNnd7849ISMtXCeme7/8MTQ7YSdmwyHwIksvZKVyvZszYStZEiN3dO6HWDQUDln3QPIMuvPJvbsxyYNeFO7m0cDzrPtRw5eU4ounLnP+UFEokV/AR/5uko7PecjlpeYx8bxDqaPzYiruMSknpodYpY7ujV5yrpe6z0C7miAJpQFghogaANoAHiaiOoA/A/DvRlKzIvQWI8KlHAlFZCtK61Li+58CvvQHdjvkxHzgO8CnfgnxKudxGGHAAs9IKC1LJp3ZpL6hVHpjgOuvAh6/N9knY7I5cyzVOQmClbp2ShIRTtpmB9j/UHI/jKaYIiBjgTv3yw2+1QkvqSYTRuqt4vhn3/Ce75vrF7JkZTny/lPXL3Aq5enU8Zzgc/be3PuRs+u5z6PeEtb9KljgPE2CLH+Y4b1vetYykJM5ueXUGgConAXuJq65dYn164h85XuR5fjqUXct8Dl/HdzjQ7+HIlJWS0IxxjwE4CoADwB4BMA+Y8y1AH4TwOeNMY/knU9ElxPRLiLatXv37rxDhwNbePVmCQs8ipV2V+8AgG/+BfDgd+2qJkeeZve5TsybrwFu+zwwe4pdTaUxZV/k4r70tUJOzMYUcNprgTN+NrLue2En5sFHga/+B+D2L4j74HlPxDzOMi5WEuziviTueHp9Uke5//gLgI1bk/thsMNxcV9y3vT65Dx5vz1J4IEl1aSEUqYB1wNW6jPOtKvEHP2cHAtclFNv2VVrnv0S5/qBj8st32uBi1GOd01N4ZTLODGn/NLFOMH1r3veR5koFMZQTswokcenX7PPg9sRt62p9elrZNq7h0zZUFvcl3aau+X0PJEsUmLha/jq4B6f+j0koRS0txVAYU4vEW0CcCmA7QCeBPC/iejNAH4WwL8pOt8YczWAqwFgx44dORNyDwl2itQaOVEogvhqAcfk3B67XNVr/irZ5yby9Ht20dff+E6yrzMLzO9NXytlgTu62fN/225/5Y/zNXBed9GdJhOwc1LH8zgLC7whJt2f32sJj+t4/7ft9vyeZB6U03/G/nPByT/ze+1zqbdsY473y1XZmcDbdpIfFxw2lkeKLkJOzPZm4Bc+G/0W0sDFuUTAaz/ouX6BlpmrgYtRjm+CfhkWFzuza8kqLasZhQLY+i88HrDAi5yYnjm6y4AXNPAl4QCRHBiR6vwe276nN2SvEdSvHfKdE208dQ1XR/fNp8L12AsccWq2Dm6Zqd+j+6o1nLl3hvD/HCLKSCgXAbjXGLPbGLMM4DMA3gPgeAB3E9F9ANpEdPdIahgCf0ShFeSBtKOv5rHUjUkTG8NN5Bn0svNXdLakF9oFnFT6gOearx3SwOUK4LJ8qtnGwZljUles1SyJd+dsneSq4vN7bRlzu9OrwPvA583tjj6G2WRVc1k3QEgo6wqcmDmk6CJE4BKheOoycdZlowlCUSgA4pVpQuX3l5LRFVuuKQt8FTRwIKm/730UkYtvZr4yqLtyn2cCsZh8d/tXt4919Dz9upu+hrceohPw6uiRs1V+P+4x7rluPUITpAEj67jLEPgDAM4jojYREYALAbzPGPMMY8w2Y8w2APPGmONHUsMQUgSeI6GwVllrZOOVl/bbl+++MNcCHyx7CHw2S+CZVPoI8kVyhEsokcdH4H1RfuzEdPS8Vhs48OPofmaTOsLYiYhC1ol7T0BE4LIjEMTOiMMEQxp49H5iUizRgH3Dexeh65Upp9CJmdMJSEexO6mSPEda4Gy5NkRHthpOTCCpf93znIpI2TdHdxnwQgg+/RpwCDzQPmMdPUdC4W+BjY5MPZjAF8NWcm/RjiR7i9lrFOYPBNreMDLVIaKMBr4TwKcA3Ajg+9E5V4+kNsOALTwfMTPkHMQ+S53J0n1htXp6qTOfBd7ekrZIASeV3lm2Kb42a+ABJ2YsoTiZjlx+vNKPEwnR7CQLRvCHwDMP7n/QdlZFBN6WFvju5O/pDXYEkyLw6KNpdfyzQTKBx6RYYggZD+9LkHDG2ilh6Ze2pHwWuJjAzE1KkedIDZwJXGY+rpoFLkYDQDqZZ1ROzMa0/TZ9zkMg0bcBv+XL5blOe7cu0ornNu9eg7M53fcmdXRu3xkLvEh6C3T8Y3BilprX0hhzJYArc35fF/ptZOD1IPOcmHKoW/dY6qEX5sY2D/rITG7UiQjcmGTYF6/I40go8qVzklCRBu5OFVBrJtfiyX3qrcSaaLWzBM4d0+470n+HwI1/bq+VlracGNWZkvtl8IfXDFjgHHMciizxITQUTR0TklBKECT/VmumrbD49xwJpeU4MUNRMLz2IpDo5qudSg8k9ffFrpcNI6w1rHFSFvxO2Efii92XFvjhnkF8TL6B6QsA+z66czZGP1dC8Y2cRDbnXOTTyljgcsqGITp+3k91pBYoWUFUOxOzPmUbVHA+cJE84HN2xgTuscBTTszlbMPtzNoGI0PrZCJPKKCfRwJDEbgon+fc6DmREM02cPDH6fvh/x+7zX+fLhotG0Mda+CiY3MJPJZQ2uH5wGUc+FBRKDlkH3IGlimniESLUumBxIkZikPvdYUF3k5+W20nphwNMMrq8twRDWtF8nVDC3qwvg2E5Y9YR/fE3vMoorcU/pZlOb7OKo4Tz7HA5WglV0I5BKPiKaK6BF5GA5fpuz4nZlBCceQWrxNT6MXxcXJFHlGutPb42rET08na48gWd6oAtgKknicbk0wfzhD47fb/doGEwuc8eb8lKvlcXM1fxoGbQfY+mOSGsTxLOTEDH8UwTswQiZZxYnbnkrbnK19a4DFpToIT05FQ5HYRMcuOaBjwPQfnYI/07W60OpRXQmEdnSPKPATcWwp/y7IcnwUudfTcTiDnWYVGmWPotJ8GBJ4XB76YDGN8U8TyS3eJLePE7CUSBsMXmcHRBzKM0JUDarVDkFCkBi4lFMcCZ/D98JSyTOBFGjjfV3x8DoHHYYSReuZq+WztDKP9lnJilggjLDw3INHkauBR0lgoCkUOxQeuBT6Vf+1xwHViyroUEbOUgoZBZqFqj+O5180nX16qru+JIAESHT1kPQNpHT3ouxDX8Bk6ee0raIEP4f85RFSXwDmZIjcKRWiVPmfn3G5gakO2YZVxYnJDkbHRsQVeExaXe+2Gk0ofigMXTsx+z5FQltLyEJBYe/J+ajWra+/7UVTnAgkFADryeNGQ21vSce+9JXsvPDJw5ak4lX6I6ItSJBwg+VISSoGTNM9KJkoWvHDlK3nN3lLWiZmKxlktC9yjgZclGJ/8Ugb8HEPrQnKc+HzAkOIyWb/2acmNaacTCMWBL/l9F1JCmd8LtA6zs4dm6pHTNovCCEeYfVtdAudEkVCCDpC2Un3OzpDn22uBuxq4J7TOl0rvfrCuEzMThcJhhM6KQzUpoXj0PLaS3Pth0q5PpefmDiFldTsaePegmBNbdKBcRwmWr4pIU2IoCSUwFM5dtq2A5It+5yXn+kue8gURuE7MYaWkUSC2oj0hcWVmIwSGrztf17fgL/9daIFPJWGzoSSafoH1zOX4fBf8N1vgoVFqXgccip4axv9ziKg2gden8qNQpFbps9TnA44TN5VeatCMtofAfZNZ+Sxwdz5wnqa1O5947N01P+M4cBFbm5pzPPrI3PuRsdxl1mJ0ZRN3e150MI1W0rG4z5Y/uJE5MUdhgRdYTLEFvjikBd4qT5ajgptKL7dH7cT0rRfJf6e05wD5Ava78C6mwNfYY+U8dx4TWY7XdyHeW16yW64TM9D21ImZA7ZAc1PpJYF7olVCyQOZVHpPIk+rbRtMSgOXq9I3sin1XA8ZhQIkZD7vCdMDsho4z0aYssCZwAMWeBn9Wx4PpK0Z12nLeiKPTDIEznPVrLATMz7mUOLAizTwAkmBl5xz5St5Dk8GBqSdmGVCJEeJlseKHtqJeagEfsBfDqfS5+rXQkf3xmCzFZ9nPbOO7iHwWs0aIf2lcCSMvJfcOPCABa4E7sCY8qn0cSJPSELxWeBuKn0/S+BANrROptIDEYH59PWeI9FE23ytWjNrgddlIk83m5TAUQYZC3zWvz8E/ghca8Z12nKcd66EMjVcIy4loQQ+lmHiwAsllBwLPJhKL2KSYwvcFwe+WhZ4ThRKWQt8aAnFIfC8CJJmJ7uCkjxn6YD/2bFBk2s9Cx09FMdd2AlMASA/D4TIvWyi1FNANQl80EO8enlRKn3Kiekk58zv9b90KjEXCpCNzJCJPABSiSwMdmK61wcSclx/dFoDD6XSS6IJWeBsRZcJIUwd72S0dTjJR6T6N6YSaUk+234022JKQilBXEOl0ocklJxyavVsklWq/IJOoCWdmAFrq7eUdOReCWW1NPCnIKE0PREsZVAYBz4lLF9PBqUsc3G//9mxvj2/J9zG85yYQGQULUR8ELgGf8s+GbIeaHts3asT00EcE9oaMpVekMzCE5ZkQhZ40VwoQDad3ogoFCAtIcTXjuoh68LncWew4dicVPopW+/ufPqDKKOBl0HIYvdJKNKJKWUsuV7nSsxGKPFULHD+/ZAt8E4yH3gwDryb1cAnwYnZDDkxA1alRGMqu1BCGaTiwD3lsGWcZz1LHd3rxBQaeB75cu6EtxOYAg4+Zr+zvHoE201O28s7bwVQUQIXU0seaip97Pn29PxlZiMEsjMSulKLtEAZbN0bJ0wRSK61/uicVHo5pJQaeEEUSmkJJXB8q2M7CTndbUMQuHxeffF+htF+n0osd9lyfO8kU35IA2/b5276nuFy5PNgDVxGIskyVzsO3J3XozFd7Nwmsh3A0Bq4o19nZhqMchrmHitB4Af85cfXyOsExJS0IZLd96DdzrtGsN3ktL289rYCGE2C/ihx15eBndFcz6zBSvL4p/cAe+602/OPJw/PdXbmZV355gP3zWXQmbVDN54PxfQT+QRIa8CMWj2d7AHYhvWFtwP3fQNozAAzm7Op9G489cFH0z170AIfksA5+cdnzXS22IUtnrgPeORmYPbkrAZ+15eBnR+y28OmkD+l2QhLluN7J275od+bbWD/w+FyWNOtN51IJCklrZYF7pFB5CRbRWi1DyEKJbr2gR8HokNaAAyw527gqLP81+AyD/wY2HCc5/cW8Pg9BdZzVM7c7jCBx/MFFUgoeXUMRckogQvc9DHg3uuBo88Bjn0ucM91CTEP+sA/vw/oHAGsOwI44mS7kguQdWKyY8VdfQMob4HPbLS/deeAqXWRpSwI/Ow3Aocd7Vy7AQzm0h3EE/cBN/wtsP5Yew5rdrJ81pq3nmdXpukvAye8NDnm2OcCJ12S/RCOPA046aeAbS/I1t+HWg049/LkuUmc+Qbgjn+w9T3sKODknxIEHr2Dmz4O3Pv16P2ca+dWOfMy4FkvLi5763nAKa8CNm0LH3P48cDJrwSOe156f9lyzvkF4MjT/b894/ToGZ7t//2ElwEP32Tf8TPPz/7O89QQ2Y5c3s/MJvsejjg5v36jwpYToud2brLvpIuzK9iEcM6bw88thMOOtu3o4GPpchnbXmjb62AAnHix/xrPOMO2o+V5295cnPgKa7DVm+E2LsvxtetTLwV+8Pc2T4IXQ3FxyiuTyd1c1GrAT/wScPyF2d+e8ybgyFOz+1cI1SPwQQ/Y/Gzg8uvs35KYeeh+3q8DL/yd9HmupS4Xe3BRJpUeSE9wNLXOyiLSAn/h72bP4SgUWRde0PWl7wHOeB3w1f+IeIJ5onQHcvTZwOVfy153/VHAG6/J7p9aB7zx49n9ebj4vf79F/yh/Sdx55fs//IdHH588n4A4Kc/VK7czduBN/zP/GNaHeCyj2X312rlynHrLzGzyf8MGae+2v4Lod6MHM6RBS7vpzkz/HtYSfie20kX239lkPfcQmi0kpWUfNh6HvCr1+dfY+NxwFu/HP79rDfYf3koKuclf2D/5cG3gpXEq/6rf/8F784/7ymiehr4oJ+WM6QTky1xN+kGiIhTSChMOKFjgSSaIGSBs+7MMxIO+v5pSlPXdlLpgSS7MTXMNoIUA+VPAvhZcYx96FmtBdSato25UppCMSJUj8DdpBppWccE7tOiHAkltsA9BM4fn5yvxDcPsrTA+fiiD5eiLM/BAAClz3dXDudojkkmRVcD73tC7NYK2AIv05ErFCuA6rUyl8xkdAlb2D6yi1fCidLWmex90gh/fHLGQB/Rxxb4fHJ80YT3ckUetrjZgndXLudom4kmcCcOvL/sf1ZrAUzgaoErxoSKErggCBldkqdru+FuRRo4lwX4U+kBYYHPJccXfbhyQQf2Xi+7Egpn9UWOzFAc+iTAdWKGOru1AHZilunIFYoVQEUJXHwcXgnFZ1U7Q/1YA/dZ60JCGQwAmIAGLpbZAqwTs9ACZydmL5FK+Hw3GYVDCV3df5Lg6xh9o5q1ADm6mtQOV/G0QkUJ3NXAXVIuQeBsgXslFEFKfLzXAueFboeUUMzAfuSxBR5Z8O6ETEzgoRHAJKDuPtflNayBswU+UAlFMRZUk8DrjoQycCSUXFJ2I1ZyJBQzyNfVOVV6GCdmbIEPkvtYXrD/u8kesYQyyRq4k0rfX57c0cKoITVwdWIqxoDqtTI3pE5GlxRFoQBpx6TcLyGdmHkW+KE4MXlBh1wnJlvg7MTsT64s4ToxB2vZAhdRKGqBK8aA6hG4TwPnRXVjAs/RtaWEUmv454GQYYRM+D6id52YpSxw6cSMiC7kxIzDCJeLO4bVQvxcVQOP48BDYacKxQqjogQuJRRBzIO80EBn7UbOmPNBklIcbuj5IHmWtqHDCKNEnrrjxHQnU5ISyqRGdmSkqcC8MWsBGkaoGDMqSOBuIo8YwpcKIyzhbEtZ4DkSCs/StiyiUEpr4EJCYQvenSq1CnHg7nzgKqHY0aBa4IoxYEJZIQeDftaJCVji6JeJQuknx4csxZQsEEksIWu91R4ylb6epNK7Tsx4TUjhxBwMovDECX1VGSemSijWAq+ebaSoHiaUFXLQd/RgScx56fF1d6ifk/JNjq4ry3HBy2wB5RJ5qC7qyU7M+WgCJLEQBBBNO5szApgEZOLAJ1juGTU0kUcxZpQyE4joHUR0KxHdQkTXENE0EX2MiO6I9n2EiMbz1fpS6Xl/rgbuSiiBGQaBhEilhBKy1lsdkcgzRBw4IBJ5DjoT7VeRwOVcKGuVwBvRcnKayKMYDwoJnIiOAXAFgB3GmNMB1AFcBuBjAE4GcAaAGQBvHWE9E/hS6QFrmeeFEfqG+kEJRVr1OXHgQGSBSwml4MNNdT6sgS+kJ9aXy3PlxaFPAqSEZcwa18CFBa5OTMUYUJYVGgBmiGgZQBvAw8aYa/lHIvoXAMeOoH5Z+DIxeX9uGKEbB17SicmTXwUJfCadSl/oxBR9ZkPMhSIXloidmGLlnkm1auXzj0cLE1rXUSPWwNWJqRgPCi1wY8xDAK4C8ACARwDsc8i7CeAXAPyj73wiupyIdhHRrt27d/sOGQ6ZOHARBTHIs8B9E1SVCCNkAg06MTuJE7LsfOCMOIxwzllslmcjXBSkOKGE4ButTGpnM2qkEnnUiakYPcpIKJsAXApgO4CjAXSI6E3ikL8GcL0x5hu+840xVxtjdhhjdszOllyXMQ+ZVHpBzLlTxHpS6UNE402lDxColFDKJvIw4o7GpDXweDbCbrGEs9qo1SxZ9ZfznchrAalU+gntcBVPK5QxEy4CcK8xZrcxZhnAZwCcDwBEdCWAWQC/k3P+ysEYjxNTxoHnWIDeeOUCJ2ZRKj0QhREOmUrPSOneYpvI/p1yYk4wKfKydrHDdw1r4JyJqRq4YgwoY9Y9AOA8ImoDWABwIYBdRPRWAC8HcKExHFYxYsRyRkgDz7EAM+FuQybyhMg+lcgzrAUuVqt2V66uT1UjCgVAPCPkpI8WRg3uZHtLaoErxoLCL80Ys5OIPgXgRgA9AN8DcDWAOQD3A/g22flEPmOM+eMR1tVPZnF0SckwQplKz5NRZY6VqfRlLPC5aHRQcj5wRt2je8d/RyvTF4UxTgJiAl/rEkr0jnqLaoErxoJSrGCMuRLAlYdy7orCF1LnjUIpOR94fZO/HLkiT5ETsdm2lne/Gw2dS2RiMkISCmAJvNethgXOy9qphGL/X15UC1wxFlTLVZ5ngQ+Wk4UPfDMM+lbkCWrgvrlQcqJQgMgKL5nIw8iVUFrWAq+CLMHL2sXzsU9wXUeJWEJRAleMBxUjcE9MtKuBh6w/14nJ08n6EF+zYEEHIL0yfZlEnjJOTMBKKr3F4jDGSQDPsJiXSLUWUBcErhKKYgyoFoH7pnaVxOxmaUr44sCDTkyZSu9xnErIRR2ekhPT1cBbjoQywYTgOjHXrAauFrhivKgWgfvkDP5Q+myBhwjcN51smUSeAieiXFbtKTkxnc6kzk7Mikgog2WRSLVWCTx6hxpGqBgTKkrgOU7MICk7CzrkxYHLMMIyc6EAlsDLronJ8M1/Ev/mODEnmRQzYYQTXNdRItUulcAVo8fTgMAdCaXQAhdx4KVS6YvCCIWEMmwqvSzfdWI2ppxU+gm2wOtNZzrftaqBy2Ssan1aimqiWq3MJ2e4TsxCUpap9EWJPIPiKBS5LuawGnitnvztS+Tpd0Uc+gRbdLV6NAdIBWLWRwnfFA8KxQhRLQL3yRl1V0IpGYVSdkWeIidiKyLw2AIfIpWeBIFnJJQKptKveQtcvCPVwBVjQLUIvJQGXhQaKMMIQxp4LTm2TCo9YC3wMvNAp2ZSrCXH+5yYvSo5MVUD9zrXFYoRomIE7omJlinygzxdm52Y0YLCZhC2FGtDODGlBT5sIk+tkWOBcxTKhM8HDmgYIcOXn6BQjBAVI3BPHLi7Kn2QlGsAkBXwKwAAD8xJREFUqHjhByDt8CyKA88k8gxhgVMdmXUwGQ13MqsJtujiVHol8BjqxFSMAdVqZV4JRc4HXrCgbs0lmiFmIwxmbdZtEk53pZ2YkQZeqVR6lVCS7QnucBVPG1STwOue8LveYvGCuvVmMmcKUCJiJUqlp7p/fhUGr0y/kk7M5kwURlgBUmS9fs2n0sswQiVwxehRLQL3Te3a4EzIhXwNnM8rs/RXKpW+V2z9Ntu2/GETeWr1sBOz2QZgrGXPdZ9UtKJViQYF0tTTHRpGqBgzqkXgPj24VrNk153Lz8Tk88rMW+2uiVmk6bbaQPdgtm7eawtyo1pyvDsXCicILe4vd93VBHdgGkaYbKsFrhgDKkrgDqGyhFFI4M0kWgUop4H3l4vJs9kGlg6kzw3WwbHAYwnFZ4EDWNwX/T7BEkqrY6Nw+hWIWR8lVANXjBkVI/CAQ4/XpRzkJPLweezs9F0nPs5J5CmSL1odYSkPkUpPdWGBOxo4hycu7c+eN2nghZ37XQC0dslLU+kVY0a1WlkopK/ZiQikrAZeIKGQS+AFFuWKWOCuE9OVUCaYwFttG1ffPWifaZ7D9+mMuic6SqEYISpG4IG5Npoz0RC+QEKpu9OelkjkKWWBCwIfJgql1ghb4DxNLUsok0zgcWezb+3q34BGoSjGjmoReCgmutVJnGhl4sCLwgjjVHom8CINvCMIvIBoXScmhSSUiBSX9hWHMa42WkKvn+SOZtTwZQgrFCNEtQg814k5Vyx3xJMuFYURkiVNtsBLRaGUlVACiTxBJ+b+yXZgAkldF56c/LqOEq48plCMGBUj8IAGzk7MQgu8nqzcA5QIOeQolBJx4PF5Q6xKX9aJOenWnIyYWcsSClFy/+rEVIwB1WplvrlQgMiJWSaMkFPpWUvPIZvYAi+xULEk8EN1YmY0cOHEnHRrLpZQnpz8zmbU4BHgpL8zxdMCFSPwwNSusRPR5JNynEofWeB5ZFOrJ6n0ZZyY8rw8ZFLpo7/dKBS+pulPfly1OjETcNtUJ6ZiDKgmgWfCCNvl4qUzqfRlLPCSqfTyvDy4GnjIiclTBLjnTCKk3LOWNXAguX+1wBVjQLUIPJSAwxEbQEEiT72cExOwWvagrBNTlD90Kn0jbYnL8rljmHQClx3Ymidw1sCVwBWjR7UIfNCDN9OvKazVYVLp84iRmOxX2gL3aODuPCjxdaP7mvTJoVId2BoncG4raoErxoBSBE5E7yCiW4noFiK6hoimiWg7Ee0koruI6JNENHrxM6RHl7UAMyvHFFjrpmQceIrACh6pjE5gy9udiZDB2nKlLPC1roFrFIpifChsZUR0DIArAOwwxpwOoA7gMgDvBfBfjDEnAHgCwK+MsqIAwnp0WQuwbCp9fOygfCq9PC8PRMJKizIxXQcmo1VFCWXC6zpqxBr4Gn8OirGgrJnQADBDRA0AbQCPALgAwKei3z8K4DUrXz0Hoaldy1qAcSp9wULFgHBiDhmFUkb75GNiCSVkgTOBT7gsUaslTtc1b4GrE1MxPhQSuDHmIQBXAXgAlrj3AbgBwJPGmIgJ8SCAY3znE9HlRLSLiHbt3r37qdU2NLVrq6QFGEsoHEZYxolZYpWd5hBOTK4HYIn8rDcC51/hP45HFlUgA9brJ72zGTVqGkaoGB/KSCibAFwKYDuAowF0AFzsOdT4zjfGXG2M2WGM2TE7O/tU6hqWUJplJRQ3lb5kGGGZVHp5XhGYkGs14ISXAue+zX8cW+BViOzgzkYlFPt/FTpdReVRRkK5CMC9xpjdxphlAJ8BcD6AjZGkAgDHAnh4RHVMENKjWyUllFojSqUvE0Y4olR6vnYZoq+KBg6IzkYlFABqgSvGgjIE/gCA84ioTUQE4EIAtwG4DsDromPeAuBzo6miQBkLPFdCqSer0tca+TP8jSqVHkicl0WoShQKIDqbCowWRok4lV6jUBSjRxkNfCess/JGAN+PzrkawO8B+B0iuhvA4QD+ZoT1tAiF9KXiwItS6SMNvIho4lT6EnHgjakkbKwMMVNJCzzWlStA4NzZVEHuGSU0kUcxRpRiBmPMlQCudHbfA+DcFa9RHkJ6dGoukqLQwGUroxQRDdXS1nrusWQJrHtgZS3wKkkorQrp9aMEjwBVA1eMAdUa54X06GbZVPooDnxQMGsh4CTylCDQmGxLOjFLWeAVklBUA7dQC1wxRlSLwEN6dKOV7C8bRlgooTSSMMIyVuUw85bUPHOf+BBbtRUg8FaFOptRoqaJPIrxoWIEnmMNxxpsURQKSygFliI7MUOx5y6YwMqkUJd2YlZIQlEL3KKuTkzF+FCtVpanR5eJgqg1IlLuFlu1KSfmMBZ4SQ28VBhhhaxa1cAtNIxQMUZUjMBz5IwySS9M2ssLJSzwWvlUeiAhsLKp9ENZ4BUgRY1CseB2pU5MxRhQMQLPmRmwjAXIRLw8Xy6MsMzKPYyhLPBhE3kqQAaaSm8hp0lQKEaMahF4XlZkGWuVf+stlggjrAM9nrVwCAIvm0pfRiOtVCp9heo6SmgqvWKMqBaB5+nRZchOWuBlwgh7i+nz8jBUGGFJDbxSTswSTuS1AA0jVIwRFSTwkBOzhAbL5Lq8UDzUpzrQW4rOK2OBjyAKpYpOzCrUdZTQVHrFGFGtVpangZcJY2NyX14oZ4H3mcBLyALDWOBUUwv86QqNQlGMEdUj8BDxlrEAazIKpYyEspRsF2GoRJ7GkLJMBQhcNXALXZFHMUZUo5V98Z3A4r4CJ2aneIZBPnfh8XISyuK+9Hl5iBN5ys6FUsaJWSEJpUqjhVFCwwgVY0Q1vraDjwKP3Z4/teuZPwusOyL/Os98PnDqa6xl/Zyfzz/2rMuspd5oAdtfVFzHky4GDvy4uA4AsOOXge7B4uMaLeCCPwRO9K2fMWE44lTgeb8ObH/xatdkdXHCy4Dnvx1Yd+Rq10SxBkDGeBfSGQl27Nhhdu3aNfyJf/+7wC2ftov/nvhy4NXvX/nKKRQKxYSCiG4wxuxw91dDA+/MAgtPAL0FHaIrFApFhIoQ+Bb7/+I+JXCFQqGIUBECF4shr/UoB4VCoYhQDQJvb0m21buvUCgUAKpC4NICX+uTJSkUCkWEihC4tMBVA1coFAqgKgQ+vTEhbiVwhUKhAFAVAq/VEh28CutDKhQKxRhQDQIHEhlFLXCFQqEAoASuUCgUlUWFCDyKRFECVygUCgBK4AqFQlFZVIjAVUJRKBQKiUICJ6KTiOgm8W8/Eb2diM4mou9E+3YR0bkjrWkchaKJPAqFQgGUmA/cGHMHgLMBgIjqAB4C8FkAHwbwHmPMPxDRJQD+FMC/GVlNVUJRKBSKFIaVUC4E8ENjzP0ADID10f4NAB5eyYplEBO4zoWiUCgUwPAr8lwG4Jpo++0AvkREV8F2BOf7TiCiywFcDgBbt249xGoCOOos4Pm/rSu+KBQKRYTSK/IQUQvWyj7NGPMoEb0fwNeNMZ8motcDuNwYc1HeNQ55RR6FQqFYw1iJFXkuBnCjMebR6O+3APhMtP2/AYzWialQKBSKFIYh8DcikU8Aa42znnEBgLtWqlIKhUKhKEYpDZyI2gBeCuBXxe63AfgLImoAWESkcysUCoViPChF4MaYeQCHO/v+GcBPjKJSCoVCoShGdTIxFQqFQpGCErhCoVBUFErgCoVCUVEogSsUCkVFUTqRZ0UKI9oN4P5DPH0LgD0rWJ2VwqTWC5jcumm9hoPWa3hMat0OtV7PNMbMujvHSuBPBUS0y5eJtNqY1HoBk1s3rddw0HoNj0mt20rXSyUUhUKhqCiUwBUKhaKiqBKBX73aFQhgUusFTG7dtF7DQes1PCa1bitar8po4AqFQqFIo0oWuEKhUCgElMAVCoWioqgEgRPRK4joDiK6m4jetYr1OI6IriOi24noViL67Wj/HxHRQ2Lh50tWoW73EdH3eZHpaN9mIvoyEd0V/b9pzHUKLYi9Ks+LiD5CRI8R0S1in/cZkcX7ozb3r0R0zpjr9WdE9IOo7M8S0cZo/zYiWhDP7oNjrlfw3RHR70fP6w4ievmY6/VJUaf7iOimaP84n1eIH0bXxowxE/0PQB3ADwE8C0ALwM0ATl2luhwF4Jxo+zAAdwI4FcAfAfi3q/yc7gOwxdn3pwDeFW2/C8B7V/k9/hjAM1freQF4EYBzANxS9IwAXALgHwAQgPMA7BxzvV4GoBFtv1fUa5s8bhWel/fdRd/BzQCmAGyPvtn6uOrl/P7nAP79KjyvED+MrI1VwQI/F8Ddxph7jDFdAJ8AcOlqVMQY84gx5sZo+wCA2wEcsxp1KYlLAXw02v4ogNesYl3kgtirAmPM9QAed3aHntGlAP67sfgOgI1EdNS46mWMudYY04v+/A6AY0dR9rD1ysGlAD5hjFkyxtwL4G6MaJWuvHoREQF4PdKLz4wFOfwwsjZWBQI/BsCPxN8PYgJIk4i2AXgOgJ3Rrt+MhkEfGbdUEcEAuJaIbiC7kDQAHGmMeQSwjQvAEatQL4ZcEBtY/efFCD2jSWp3vwxrqTG2E9H3iOjrRPTCVaiP791NyvN6IYBHjTFyhbCxPy+HH0bWxqpA4OTZt6qxj0S0DsCnAbzdGLMfwAcAPBvA2QAegR3CjRvPN8acA7t26W8Q0YtWoQ5ekF0Q+9Wwa6cCk/G8ijAR7Y6I3g2gB+Bj0a5HAGw1xjwHwO8A+DgRrR9jlULvbiKeF7JLP479eXn4IXioZ99Qz6wKBP4ggOPE38fCrse5KiCiJuzL+Zgx5jMAYIx51BjTN8YMAHwYq7DAszHm4ej/xwB8NqrDozwki/5/bNz1ipBaEHsSnpdA6BmtersjorcAeCWAnzeRaBpJFHuj7RtgteYTx1WnnHc3Cc+rAeCnAXyS9437efn4ASNsY1Ug8O8COIGItkeW3GUAPr8aFYn0tb8BcLsx5n1iv9StXgvgFvfcEderQ0SH8TasA+wW2Of0luiwtwD43DjrJZCyilb7eTkIPaPPA3hzFClwHoB9PAweB4joFQB+D8CrjV3SkPfPElE92n4WgBMA3DPGeoXe3ecBXEZEU0S0ParXv4yrXhEuAvADY8yDvGOczyvEDxhlGxuHd3YFvLuXwHp0fwjg3atYjxfADnH+FcBN0b9LAPwPAN+P9n8ewFFjrtezYCMAbgZwKz8j2HVMvwLgruj/zavwzNoA9gLYIPatyvOC7UQeAbAMa/38SugZwQ5v/ypqc98HsGPM9bobVh/ldvbB6Nifid7xzQBuBPCqMdcr+O4AvDt6XncAuHic9Yr2/zcAv+YcO87nFeKHkbUxTaVXKBSKiqIKEopCoVAoPFACVygUiopCCVyhUCgqCiVwhUKhqCiUwBUKhaKiUAJXKBSKikIJXKFQKCqK/x8kPlftMBsNaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(np.ones(200)*89))\n",
    "\n",
    "#plt.plot(list(np.ones(200)*50))\n",
    "#plt.plot(list(np.ones(20)*50))\n",
    "# plt.plot(testing_data_unnorm)\n",
    "plt.plot(predicted_notes_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({79: 11, 78: 1, 83: 49, 86: 94, 81: 4, 84: 22, 85: 18, 80: 1})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(predicted_notes_lst)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
