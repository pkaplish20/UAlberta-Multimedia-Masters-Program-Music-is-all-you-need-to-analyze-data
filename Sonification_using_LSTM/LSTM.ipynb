{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.tensorboard as tb\n",
    "from Preprocessing.preprocessing_sorted import PreprocessingTrainingData\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as  plt\n",
    "import os\n",
    "import logging\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static parameters\n",
    "train_batch_size = 170\n",
    "val_batch_size = 170\n",
    "sequence_length=50\n",
    "test_batch_size = 1\n",
    "input_size = 1\n",
    "hidden_size = 256\n",
    "num_layer = 2\n",
    "output_size = 38\n",
    "clip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[72 76 72 ... 79 69 81]\n",
      " [76 72 67 ... 69 81 71]\n",
      " [72 67 72 ... 81 71 72]\n",
      " ...\n",
      " [71 68 64 ... 56 57 59]\n",
      " [64 66 68 ... 76 78 79]\n",
      " [81 83 81 ... 83 81 83]]\n",
      "[71 72 74 ... 61 80 80]\n",
      "89\n",
      "50\n",
      "[50 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n",
      " 75 76 77 78 79 80 81 82 83 84 85 86 88 89]\n"
     ]
    }
   ],
   "source": [
    "#get data from preprocessing.py\n",
    "dataset_path = os.path.join(os.path.abspath('..'),'Dataset\\\\Clementi dataset\\\\Clementi dataset' )\n",
    "network_input,network_output,max_midi_number,min_midi_number,int_to_note = PreprocessingTrainingData().preprocess_notes(dataset_path)\n",
    "network_input, network_output = network_input.cuda(), network_output.cuda()\n",
    "\n",
    "# print(network_input)\n",
    "#print(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(network_output.max())\n",
    "print(network_output.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8617])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1., device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n",
      "89\n",
      "50\n",
      "{0: 50, 1: 52, 2: 53, 3: 54, 4: 55, 5: 56, 6: 57, 7: 58, 8: 59, 9: 60, 10: 61, 11: 62, 12: 63, 13: 64, 14: 65, 15: 66, 16: 67, 17: 68, 18: 69, 19: 70, 20: 71, 21: 72, 22: 73, 23: 74, 24: 75, 25: 76, 26: 77, 27: 78, 28: 79, 29: 80, 30: 81, 31: 82, 32: 83, 33: 84, 34: 85, 35: 86, 36: 88, 37: 89}\n"
     ]
    }
   ],
   "source": [
    "print(network_input.max())\n",
    "print(network_input.min())\n",
    "print(max_midi_number)\n",
    "print(min_midi_number)\n",
    "print(int_to_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8617, 50, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata is highly unbalanced\\n# '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data is highly unbalanced\n",
    "# '''\n",
    "# sns.distplot(torch.tensor(network_output).cpu())\n",
    "# xx = pd.DataFrame(torch.tensor(network_output).cpu())\n",
    "# xx.groupby(0).size().to_frame(name='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8500, 50, 1])\n",
      "torch.Size([8500])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "to make batch of equal sizes\n",
    "Quick Fix\n",
    "'''\n",
    "network_input = network_input[: -117]\n",
    "network_output = network_output[: -117]\n",
    "\n",
    "print(network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#network_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create Stacked LSTM model\n",
    "'''\n",
    "class Stacked_LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True, dropout = 0.5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden,batch_size):\n",
    "        \n",
    "        output, hidden = self.lstm(x, hidden)        \n",
    "        #print(output.shape)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        output = output.contiguous().view(-1, self.hidden_size)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        #print('Linear Output :-',output.shape)\n",
    "        \n",
    "        #output = F.softmax(output, dim = 1)\n",
    "        #print('SOFTMAX OUTPUT :--', output)\n",
    "        \n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1)\n",
    "        #print('Reshape to batch size first :-',output.shape)\n",
    "        \n",
    "        output = output[:, -self.output_size:] # get last batch of labels\n",
    "        #print('Final Output :-',output)\n",
    "        #print('RESHAPE SIZE :-', output.shape)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def hidden_init(self,batch_size):\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
    "          weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "#initialize the weights of LSTM using Xavier initialization    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Divide the dataset into train/val \n",
    "'''\n",
    "train_size = 0.8\n",
    "indices = list(range(len(network_input)))\n",
    "split = int(np.floor(train_size*len(network_input)))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SequentialSampler(train_idx)\n",
    "val_sampler = SequentialSampler(val_idx)\n",
    "\n",
    "dataset = TensorDataset(network_input,network_output)\n",
    "train_loader = DataLoader(dataset, batch_size= train_batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size= val_batch_size,sampler= val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm): LSTM(1, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=256, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optimizer\n",
    "\n",
    "model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer.AdamW(model.parameters())\n",
    "#optimizer = optimizer.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "#make sure to transfer model to GPU after initializing optimizer\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 3.2718702 \tVal Loss:3.0169748 \tTrain Acc: 8.32353% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from    inf to 3.016975, saving the model weights\n",
      "Epoch: 1\tTrain Loss: 3.1063371 \tVal Loss:2.9714865 \tTrain Acc: 8.529412% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 3.016975 to 2.971487, saving the model weights\n",
      "Epoch: 2\tTrain Loss: 3.0718069 \tVal Loss:2.9730795 \tTrain Acc: 8.720588% \tVal Acc: 11.4117651%\n",
      "Epoch: 3\tTrain Loss: 3.0509014 \tVal Loss:3.0414517 \tTrain Acc: 9.441177% \tVal Acc: 8.5882355%\n",
      "Epoch: 4\tTrain Loss: 3.0997689 \tVal Loss:2.9621521 \tTrain Acc: 9.25% \tVal Acc: 11.4117651%\n",
      "Validation Loss decreased from 2.971487 to 2.962152, saving the model weights\n",
      "Epoch: 5\tTrain Loss: 3.0059325 \tVal Loss:2.8767249 \tTrain Acc: 9.897059% \tVal Acc: 10.9411767%\n",
      "Validation Loss decreased from 2.962152 to 2.876725, saving the model weights\n",
      "Epoch: 6\tTrain Loss: 2.9369754 \tVal Loss:2.8751791 \tTrain Acc: 11.20588% \tVal Acc: 11.4117650%\n",
      "Validation Loss decreased from 2.876725 to 2.875179, saving the model weights\n",
      "Epoch: 7\tTrain Loss: 2.8863377 \tVal Loss:2.8491251 \tTrain Acc: 11.82353% \tVal Acc: 10.5882356%\n",
      "Validation Loss decreased from 2.875179 to 2.849125, saving the model weights\n",
      "Epoch: 8\tTrain Loss: 2.8078462 \tVal Loss:2.7487512 \tTrain Acc: 12.55882% \tVal Acc: 12.9411768%\n",
      "Validation Loss decreased from 2.849125 to 2.748751, saving the model weights\n",
      "Epoch: 9\tTrain Loss: 2.7397929 \tVal Loss:2.6866137 \tTrain Acc: 13.69118% \tVal Acc: 12.8235296%\n",
      "Validation Loss decreased from 2.748751 to 2.686614, saving the model weights\n",
      "Epoch: 10\tTrain Loss: 2.6938869 \tVal Loss:2.6429828 \tTrain Acc: 14.30882% \tVal Acc: 12.4117651%\n",
      "Validation Loss decreased from 2.686614 to 2.642983, saving the model weights\n",
      "Epoch: 11\tTrain Loss: 2.6503413 \tVal Loss:2.5831303 \tTrain Acc: 15.25% \tVal Acc: 13.2352947%\n",
      "Validation Loss decreased from 2.642983 to 2.583130, saving the model weights\n",
      "Epoch: 12\tTrain Loss: 2.6112508 \tVal Loss:2.5371011 \tTrain Acc: 15.20588% \tVal Acc: 14.0000004%\n",
      "Validation Loss decreased from 2.583130 to 2.537101, saving the model weights\n",
      "Epoch: 13\tTrain Loss: 2.5823163 \tVal Loss:2.5269485 \tTrain Acc: 15.67647% \tVal Acc: 13.8235298%\n",
      "Validation Loss decreased from 2.537101 to 2.526948, saving the model weights\n",
      "Epoch: 14\tTrain Loss: 2.5594267 \tVal Loss:2.5262170 \tTrain Acc: 16.02941% \tVal Acc: 14.0000003%\n",
      "Validation Loss decreased from 2.526948 to 2.526217, saving the model weights\n",
      "Epoch: 15\tTrain Loss: 2.5452705 \tVal Loss:2.4896936 \tTrain Acc: 16.22059% \tVal Acc: 13.8235297%\n",
      "Validation Loss decreased from 2.526217 to 2.489694, saving the model weights\n",
      "Epoch: 16\tTrain Loss: 2.5283468 \tVal Loss:2.4855034 \tTrain Acc: 16.69118% \tVal Acc: 14.0000003%\n",
      "Validation Loss decreased from 2.489694 to 2.485503, saving the model weights\n",
      "Epoch: 17\tTrain Loss: 2.5148821 \tVal Loss:2.4734897 \tTrain Acc: 15.89706% \tVal Acc: 14.1176474%\n",
      "Validation Loss decreased from 2.485503 to 2.473490, saving the model weights\n",
      "Epoch: 18\tTrain Loss: 2.5018172 \tVal Loss:2.4781141 \tTrain Acc: 17.01471% \tVal Acc: 14.2941180%\n",
      "Epoch: 19\tTrain Loss: 2.4921550 \tVal Loss:2.4643421 \tTrain Acc: 16.70588% \tVal Acc: 14.5294122%\n",
      "Validation Loss decreased from 2.473490 to 2.464342, saving the model weights\n",
      "Epoch: 20\tTrain Loss: 2.4861641 \tVal Loss:2.4465040 \tTrain Acc: 17.08824% \tVal Acc: 14.5294121%\n",
      "Validation Loss decreased from 2.464342 to 2.446504, saving the model weights\n",
      "Epoch: 21\tTrain Loss: 2.4716382 \tVal Loss:2.4452789 \tTrain Acc: 17.19118% \tVal Acc: 15.1764710%\n",
      "Validation Loss decreased from 2.446504 to 2.445279, saving the model weights\n",
      "Epoch: 22\tTrain Loss: 2.4671626 \tVal Loss:2.4325908 \tTrain Acc: 17.76471% \tVal Acc: 14.7058827%\n",
      "Validation Loss decreased from 2.445279 to 2.432591, saving the model weights\n",
      "Epoch: 23\tTrain Loss: 2.4604775 \tVal Loss:2.4207511 \tTrain Acc: 18.17647% \tVal Acc: 15.5882356%\n",
      "Validation Loss decreased from 2.432591 to 2.420751, saving the model weights\n",
      "Epoch: 24\tTrain Loss: 2.4552904 \tVal Loss:2.4370721 \tTrain Acc: 18.5% \tVal Acc: 15.5294121%\n",
      "Epoch: 25\tTrain Loss: 2.4446213 \tVal Loss:2.4338593 \tTrain Acc: 18.61765% \tVal Acc: 15.8235298%\n",
      "Epoch: 26\tTrain Loss: 2.4441745 \tVal Loss:2.4232625 \tTrain Acc: 18.11765% \tVal Acc: 15.7647063%\n",
      "Epoch: 27\tTrain Loss: 2.4319888 \tVal Loss:2.4220920 \tTrain Acc: 19.23529% \tVal Acc: 16.5882358%\n",
      "Epoch: 28\tTrain Loss: 2.4311779 \tVal Loss:2.4215453 \tTrain Acc: 19.22059% \tVal Acc: 16.1764710%\n",
      "Epoch: 29\tTrain Loss: 2.4225266 \tVal Loss:2.4131140 \tTrain Acc: 18.75% \tVal Acc: 17.1176475%\n",
      "Validation Loss decreased from 2.420751 to 2.413114, saving the model weights\n",
      "Epoch: 30\tTrain Loss: 2.4207433 \tVal Loss:2.4018354 \tTrain Acc: 18.55882% \tVal Acc: 17.0000004%\n",
      "Validation Loss decreased from 2.413114 to 2.401835, saving the model weights\n",
      "Epoch: 31\tTrain Loss: 2.4142523 \tVal Loss:2.3989208 \tTrain Acc: 19.57353% \tVal Acc: 17.3529416%\n",
      "Validation Loss decreased from 2.401835 to 2.398921, saving the model weights\n",
      "Epoch: 32\tTrain Loss: 2.4262247 \tVal Loss:2.3764876 \tTrain Acc: 18.22059% \tVal Acc: 18.0588239%\n",
      "Validation Loss decreased from 2.398921 to 2.376488, saving the model weights\n",
      "Epoch: 33\tTrain Loss: 2.4076335 \tVal Loss:2.3662371 \tTrain Acc: 19.55882% \tVal Acc: 19.7647062%\n",
      "Validation Loss decreased from 2.376488 to 2.366237, saving the model weights\n",
      "Epoch: 34\tTrain Loss: 2.3873066 \tVal Loss:2.3380715 \tTrain Acc: 19.80882% \tVal Acc: 20.8235297%\n",
      "Validation Loss decreased from 2.366237 to 2.338072, saving the model weights\n",
      "Epoch: 35\tTrain Loss: 2.3805290 \tVal Loss:2.3552976 \tTrain Acc: 20.05882% \tVal Acc: 20.8235297%\n",
      "Epoch: 36\tTrain Loss: 2.3768594 \tVal Loss:2.3536110 \tTrain Acc: 20.97059% \tVal Acc: 19.5294121%\n",
      "Epoch: 37\tTrain Loss: 2.3769257 \tVal Loss:2.3528163 \tTrain Acc: 21.07353% \tVal Acc: 20.5294120%\n",
      "Epoch: 38\tTrain Loss: 2.3682061 \tVal Loss:2.3071092 \tTrain Acc: 21.83824% \tVal Acc: 21.6470590%\n",
      "Validation Loss decreased from 2.338072 to 2.307109, saving the model weights\n",
      "Epoch: 39\tTrain Loss: 2.3565267 \tVal Loss:2.3155814 \tTrain Acc: 21.04412% \tVal Acc: 20.7647061%\n",
      "Epoch: 40\tTrain Loss: 2.3487323 \tVal Loss:2.2891046 \tTrain Acc: 21.91177% \tVal Acc: 23.1764710%\n",
      "Validation Loss decreased from 2.307109 to 2.289105, saving the model weights\n",
      "Epoch: 41\tTrain Loss: 2.3516141 \tVal Loss:2.2824235 \tTrain Acc: 21.61765% \tVal Acc: 21.4117652%\n",
      "Validation Loss decreased from 2.289105 to 2.282423, saving the model weights\n",
      "Epoch: 42\tTrain Loss: 2.3268092 \tVal Loss:2.2576631 \tTrain Acc: 22.47059% \tVal Acc: 24.5294125%\n",
      "Validation Loss decreased from 2.282423 to 2.257663, saving the model weights\n",
      "Epoch: 43\tTrain Loss: 2.2960265 \tVal Loss:2.2443863 \tTrain Acc: 23.73529% \tVal Acc: 25.2352948%\n",
      "Validation Loss decreased from 2.257663 to 2.244386, saving the model weights\n",
      "Epoch: 44\tTrain Loss: 2.2888352 \tVal Loss:2.2222876 \tTrain Acc: 23.69118% \tVal Acc: 25.6470595%\n",
      "Validation Loss decreased from 2.244386 to 2.222288, saving the model weights\n",
      "Epoch: 45\tTrain Loss: 2.2779297 \tVal Loss:2.2099126 \tTrain Acc: 24.76471% \tVal Acc: 26.8823539%\n",
      "Validation Loss decreased from 2.222288 to 2.209913, saving the model weights\n",
      "Epoch: 46\tTrain Loss: 2.2794308 \tVal Loss:2.2213888 \tTrain Acc: 24.22059% \tVal Acc: 25.5294125%\n",
      "Epoch: 47\tTrain Loss: 2.2788562 \tVal Loss:2.2444579 \tTrain Acc: 24.05882% \tVal Acc: 25.7647064%\n",
      "Epoch: 48\tTrain Loss: 2.2881101 \tVal Loss:2.3675862 \tTrain Acc: 25.89706% \tVal Acc: 21.0588242%\n",
      "Epoch: 49\tTrain Loss: 2.3189048 \tVal Loss:2.2706919 \tTrain Acc: 23.32353% \tVal Acc: 22.9411769%\n",
      "Epoch: 50\tTrain Loss: 2.2170601 \tVal Loss:2.1865278 \tTrain Acc: 26.76471% \tVal Acc: 27.1176480%\n",
      "Validation Loss decreased from 2.209913 to 2.186528, saving the model weights\n",
      "Epoch: 51\tTrain Loss: 2.1727505 \tVal Loss:2.1492542 \tTrain Acc: 28.70588% \tVal Acc: 27.9411770%\n",
      "Validation Loss decreased from 2.186528 to 2.149254, saving the model weights\n",
      "Epoch: 52\tTrain Loss: 2.1347630 \tVal Loss:2.1102758 \tTrain Acc: 29.19118% \tVal Acc: 29.0588245%\n",
      "Validation Loss decreased from 2.149254 to 2.110276, saving the model weights\n",
      "Epoch: 53\tTrain Loss: 2.1245022 \tVal Loss:2.0627512 \tTrain Acc: 30.16177% \tVal Acc: 32.0000011%\n",
      "Validation Loss decreased from 2.110276 to 2.062751, saving the model weights\n",
      "Epoch: 54\tTrain Loss: 2.0930641 \tVal Loss:2.0560469 \tTrain Acc: 31.57353% \tVal Acc: 32.4117658%\n",
      "Validation Loss decreased from 2.062751 to 2.056047, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 55\tTrain Loss: 2.0863531 \tVal Loss:2.0611022 \tTrain Acc: 30.94118% \tVal Acc: 33.0588245%\n",
      "Epoch: 56\tTrain Loss: 2.0759363 \tVal Loss:2.0195688 \tTrain Acc: 31.72059% \tVal Acc: 34.6470597%\n",
      "Validation Loss decreased from 2.056047 to 2.019569, saving the model weights\n",
      "Epoch: 57\tTrain Loss: 2.0597597 \tVal Loss:2.0235196 \tTrain Acc: 33.11765% \tVal Acc: 34.4117656%\n",
      "Epoch: 58\tTrain Loss: 2.0210873 \tVal Loss:1.9767666 \tTrain Acc: 33.82353% \tVal Acc: 36.7058831%\n",
      "Validation Loss decreased from 2.019569 to 1.976767, saving the model weights\n",
      "Epoch: 59\tTrain Loss: 1.9786724 \tVal Loss:1.9404528 \tTrain Acc: 36.10294% \tVal Acc: 38.0000007%\n",
      "Validation Loss decreased from 1.976767 to 1.940453, saving the model weights\n",
      "Epoch: 60\tTrain Loss: 1.9941925 \tVal Loss:1.9260906 \tTrain Acc: 34.98529% \tVal Acc: 38.5882360%\n",
      "Validation Loss decreased from 1.940453 to 1.926091, saving the model weights\n",
      "Epoch: 61\tTrain Loss: 1.9837612 \tVal Loss:1.9257414 \tTrain Acc: 35.72059% \tVal Acc: 38.5294124%\n",
      "Validation Loss decreased from 1.926091 to 1.925741, saving the model weights\n",
      "Epoch: 62\tTrain Loss: 1.9744824 \tVal Loss:1.8942328 \tTrain Acc: 35.52941% \tVal Acc: 40.1176479%\n",
      "Validation Loss decreased from 1.925741 to 1.894233, saving the model weights\n",
      "Epoch: 63\tTrain Loss: 1.9688110 \tVal Loss:1.9369095 \tTrain Acc: 35.67647% \tVal Acc: 37.7647066%\n",
      "Epoch: 64\tTrain Loss: 1.9873591 \tVal Loss:2.0973205 \tTrain Acc: 35.73529% \tVal Acc: 27.8823538%\n",
      "Epoch: 65\tTrain Loss: 1.9181848 \tVal Loss:1.9425012 \tTrain Acc: 38.91177% \tVal Acc: 36.6470596%\n",
      "Epoch: 66\tTrain Loss: 1.9296849 \tVal Loss:1.8685807 \tTrain Acc: 36.72059% \tVal Acc: 39.9411774%\n",
      "Validation Loss decreased from 1.894233 to 1.868581, saving the model weights\n",
      "Epoch: 67\tTrain Loss: 1.9037564 \tVal Loss:1.9174016 \tTrain Acc: 37.55882% \tVal Acc: 37.7647066%\n",
      "Epoch: 68\tTrain Loss: 1.8788318 \tVal Loss:1.9384169 \tTrain Acc: 40.01471% \tVal Acc: 37.8823537%\n",
      "Epoch: 69\tTrain Loss: 1.8200018 \tVal Loss:1.9175415 \tTrain Acc: 41.41177% \tVal Acc: 38.6470595%\n",
      "Epoch: 70\tTrain Loss: 1.7700315 \tVal Loss:1.7761606 \tTrain Acc: 43.10294% \tVal Acc: 44.7058827%\n",
      "Validation Loss decreased from 1.868581 to 1.776161, saving the model weights\n",
      "Epoch: 71\tTrain Loss: 1.7289677 \tVal Loss:1.7288873 \tTrain Acc: 44.0% \tVal Acc: 46.1764714%\n",
      "Validation Loss decreased from 1.776161 to 1.728887, saving the model weights\n",
      "Epoch: 72\tTrain Loss: 1.7138064 \tVal Loss:1.6117118 \tTrain Acc: 44.27941% \tVal Acc: 48.5882360%\n",
      "Validation Loss decreased from 1.728887 to 1.611712, saving the model weights\n",
      "Epoch: 73\tTrain Loss: 1.6540113 \tVal Loss:1.6301958 \tTrain Acc: 46.35294% \tVal Acc: 49.0000007%\n",
      "Epoch: 74\tTrain Loss: 1.6599727 \tVal Loss:1.7982307 \tTrain Acc: 46.80882% \tVal Acc: 43.1764713%\n",
      "Epoch: 75\tTrain Loss: 1.6154644 \tVal Loss:1.6751810 \tTrain Acc: 47.82353% \tVal Acc: 47.5294131%\n",
      "Epoch: 76\tTrain Loss: 1.5819735 \tVal Loss:1.5984601 \tTrain Acc: 48.79412% \tVal Acc: 49.3529418%\n",
      "Validation Loss decreased from 1.611712 to 1.598460, saving the model weights\n",
      "Epoch: 77\tTrain Loss: 1.5792320 \tVal Loss:1.5514614 \tTrain Acc: 49.22059% \tVal Acc: 52.0000005%\n",
      "Validation Loss decreased from 1.598460 to 1.551461, saving the model weights\n",
      "Epoch: 78\tTrain Loss: 1.6171346 \tVal Loss:1.5022427 \tTrain Acc: 48.11765% \tVal Acc: 54.4117656%\n",
      "Validation Loss decreased from 1.551461 to 1.502243, saving the model weights\n",
      "Epoch: 79\tTrain Loss: 1.5670229 \tVal Loss:1.7481562 \tTrain Acc: 50.54412% \tVal Acc: 45.1764712%\n",
      "Epoch: 80\tTrain Loss: 1.5735125 \tVal Loss:1.4961432 \tTrain Acc: 48.94118% \tVal Acc: 53.9411777%\n",
      "Validation Loss decreased from 1.502243 to 1.496143, saving the model weights\n",
      "Epoch: 81\tTrain Loss: 1.5630937 \tVal Loss:1.6296839 \tTrain Acc: 49.89706% \tVal Acc: 48.2941183%\n",
      "Epoch: 82\tTrain Loss: 1.5946357 \tVal Loss:1.4447870 \tTrain Acc: 48.58824% \tVal Acc: 55.6470600%\n",
      "Validation Loss decreased from 1.496143 to 1.444787, saving the model weights\n",
      "Epoch: 83\tTrain Loss: 1.5112286 \tVal Loss:1.4068330 \tTrain Acc: 51.70588% \tVal Acc: 56.2941179%\n",
      "Validation Loss decreased from 1.444787 to 1.406833, saving the model weights\n",
      "Epoch: 84\tTrain Loss: 1.4511905 \tVal Loss:1.3845767 \tTrain Acc: 53.36765% \tVal Acc: 57.1764714%\n",
      "Validation Loss decreased from 1.406833 to 1.384577, saving the model weights\n",
      "Epoch: 85\tTrain Loss: 1.3836066 \tVal Loss:1.2886004 \tTrain Acc: 55.85294% \tVal Acc: 60.6470588%\n",
      "Validation Loss decreased from 1.384577 to 1.288600, saving the model weights\n",
      "Epoch: 86\tTrain Loss: 1.4010800 \tVal Loss:1.4531474 \tTrain Acc: 55.86765% \tVal Acc: 55.2352950%\n",
      "Epoch: 87\tTrain Loss: 1.4054339 \tVal Loss:1.6203089 \tTrain Acc: 54.58824% \tVal Acc: 49.1764712%\n",
      "Epoch: 88\tTrain Loss: 1.4205381 \tVal Loss:1.2146476 \tTrain Acc: 54.41177% \tVal Acc: 63.5882351%\n",
      "Validation Loss decreased from 1.288600 to 1.214648, saving the model weights\n",
      "Epoch: 89\tTrain Loss: 1.3961476 \tVal Loss:1.1362092 \tTrain Acc: 54.73529% \tVal Acc: 66.1764711%\n",
      "Validation Loss decreased from 1.214648 to 1.136209, saving the model weights\n",
      "Epoch: 90\tTrain Loss: 1.3304612 \tVal Loss:1.1958926 \tTrain Acc: 57.5% \tVal Acc: 64.5294124%\n",
      "Epoch: 91\tTrain Loss: 1.2633670 \tVal Loss:1.2982935 \tTrain Acc: 59.51471% \tVal Acc: 61.1176479%\n",
      "Epoch: 92\tTrain Loss: 1.1974386 \tVal Loss:1.0343230 \tTrain Acc: 62.08824% \tVal Acc: 69.1764706%\n",
      "Validation Loss decreased from 1.136209 to 1.034323, saving the model weights\n",
      "Epoch: 93\tTrain Loss: 1.1320433 \tVal Loss:1.1139378 \tTrain Acc: 63.13235% \tVal Acc: 66.4705893%\n",
      "Epoch: 94\tTrain Loss: 1.0972138 \tVal Loss:0.9882312 \tTrain Acc: 65.36765% \tVal Acc: 70.9411755%\n",
      "Validation Loss decreased from 1.034323 to 0.988231, saving the model weights\n",
      "Epoch: 95\tTrain Loss: 1.0692341 \tVal Loss:0.9524081 \tTrain Acc: 65.58824% \tVal Acc: 72.9411763%\n",
      "Validation Loss decreased from 0.988231 to 0.952408, saving the model weights\n",
      "Epoch: 96\tTrain Loss: 1.0428391 \tVal Loss:0.9678124 \tTrain Acc: 66.77941% \tVal Acc: 71.8823534%\n",
      "Epoch: 97\tTrain Loss: 1.0315621 \tVal Loss:1.0700435 \tTrain Acc: 67.29412% \tVal Acc: 67.5882357%\n",
      "Epoch: 98\tTrain Loss: 1.0198002 \tVal Loss:0.9278931 \tTrain Acc: 68.17647% \tVal Acc: 71.9999999%\n",
      "Validation Loss decreased from 0.952408 to 0.927893, saving the model weights\n",
      "Epoch: 99\tTrain Loss: 1.0064828 \tVal Loss:0.8775032 \tTrain Acc: 67.80882% \tVal Acc: 75.2352947%\n",
      "Validation Loss decreased from 0.927893 to 0.877503, saving the model weights\n",
      "Epoch: 100\tTrain Loss: 0.9993196 \tVal Loss:0.8613892 \tTrain Acc: 68.58824% \tVal Acc: 75.5882353%\n",
      "Validation Loss decreased from 0.877503 to 0.861389, saving the model weights\n",
      "Epoch: 101\tTrain Loss: 0.9496856 \tVal Loss:0.7795434 \tTrain Acc: 69.55882% \tVal Acc: 77.7647054%\n",
      "Validation Loss decreased from 0.861389 to 0.779543, saving the model weights\n",
      "Epoch: 102\tTrain Loss: 0.8956300 \tVal Loss:0.7804322 \tTrain Acc: 71.83824% \tVal Acc: 78.8235289%\n",
      "Epoch: 103\tTrain Loss: 0.8774790 \tVal Loss:0.7760042 \tTrain Acc: 71.67647% \tVal Acc: 78.4705877%\n",
      "Validation Loss decreased from 0.779543 to 0.776004, saving the model weights\n",
      "Epoch: 104\tTrain Loss: 0.8648687 \tVal Loss:0.7142216 \tTrain Acc: 72.35294% \tVal Acc: 79.8823529%\n",
      "Validation Loss decreased from 0.776004 to 0.714222, saving the model weights\n",
      "Epoch: 105\tTrain Loss: 0.8791973 \tVal Loss:0.7378161 \tTrain Acc: 72.44118% \tVal Acc: 79.0588230%\n",
      "Epoch: 106\tTrain Loss: 0.8785656 \tVal Loss:0.7496688 \tTrain Acc: 71.58824% \tVal Acc: 78.7647057%\n",
      "Epoch: 107\tTrain Loss: 0.8418942 \tVal Loss:0.7373106 \tTrain Acc: 73.27941% \tVal Acc: 78.0588233%\n",
      "Epoch: 108\tTrain Loss: 0.8378302 \tVal Loss:0.7252814 \tTrain Acc: 73.32353% \tVal Acc: 79.7058809%\n",
      "Epoch: 109\tTrain Loss: 0.8273682 \tVal Loss:0.7502965 \tTrain Acc: 74.11765% \tVal Acc: 78.9411753%\n",
      "Epoch: 110\tTrain Loss: 0.8014770 \tVal Loss:0.7960326 \tTrain Acc: 75.01471% \tVal Acc: 76.5882355%\n",
      "Epoch: 111\tTrain Loss: 0.8140126 \tVal Loss:0.9213734 \tTrain Acc: 74.02941% \tVal Acc: 72.4117649%\n",
      "Epoch: 112\tTrain Loss: 0.8094015 \tVal Loss:1.2048027 \tTrain Acc: 74.60294% \tVal Acc: 64.5294118%\n",
      "Epoch: 113\tTrain Loss: 0.8369971 \tVal Loss:0.8246725 \tTrain Acc: 73.75% \tVal Acc: 73.9999992%\n",
      "Epoch: 114\tTrain Loss: 0.8660631 \tVal Loss:0.8039443 \tTrain Acc: 72.5% \tVal Acc: 76.1176461%\n",
      "Epoch: 115\tTrain Loss: 0.8172952 \tVal Loss:0.6296976 \tTrain Acc: 74.25% \tVal Acc: 82.8235281%\n",
      "Validation Loss decreased from 0.714222 to 0.629698, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 116\tTrain Loss: 0.7777662 \tVal Loss:0.5855895 \tTrain Acc: 75.80882% \tVal Acc: 83.9999992%\n",
      "Validation Loss decreased from 0.629698 to 0.585590, saving the model weights\n",
      "Epoch: 117\tTrain Loss: 0.7048485 \tVal Loss:0.5432622 \tTrain Acc: 77.95588% \tVal Acc: 85.6470597%\n",
      "Validation Loss decreased from 0.585590 to 0.543262, saving the model weights\n",
      "Epoch: 118\tTrain Loss: 0.6665400 \tVal Loss:0.5229259 \tTrain Acc: 78.82353% \tVal Acc: 86.2352931%\n",
      "Validation Loss decreased from 0.543262 to 0.522926, saving the model weights\n",
      "Epoch: 119\tTrain Loss: 0.6451150 \tVal Loss:0.4651992 \tTrain Acc: 79.42647% \tVal Acc: 87.1176469%\n",
      "Validation Loss decreased from 0.522926 to 0.465199, saving the model weights\n",
      "Epoch: 120\tTrain Loss: 0.6217405 \tVal Loss:0.4567693 \tTrain Acc: 80.17647% \tVal Acc: 88.6470586%\n",
      "Validation Loss decreased from 0.465199 to 0.456769, saving the model weights\n",
      "Epoch: 121\tTrain Loss: 0.6042930 \tVal Loss:0.4611342 \tTrain Acc: 81.27941% \tVal Acc: 87.8823525%\n",
      "Epoch: 122\tTrain Loss: 0.6007295 \tVal Loss:0.4612838 \tTrain Acc: 81.41176% \tVal Acc: 87.7058816%\n",
      "Epoch: 123\tTrain Loss: 0.5832383 \tVal Loss:0.4341132 \tTrain Acc: 81.69118% \tVal Acc: 87.8823525%\n",
      "Validation Loss decreased from 0.456769 to 0.434113, saving the model weights\n",
      "Epoch: 124\tTrain Loss: 0.5816085 \tVal Loss:0.4189151 \tTrain Acc: 81.97059% \tVal Acc: 89.7058821%\n",
      "Validation Loss decreased from 0.434113 to 0.418915, saving the model weights\n",
      "Epoch: 125\tTrain Loss: 0.5464565 \tVal Loss:0.4383379 \tTrain Acc: 83.20588% \tVal Acc: 88.2352936%\n",
      "Epoch: 126\tTrain Loss: 0.5918332 \tVal Loss:0.5408493 \tTrain Acc: 81.29412% \tVal Acc: 84.4117641%\n",
      "Epoch: 127\tTrain Loss: 0.6057063 \tVal Loss:0.6789506 \tTrain Acc: 80.73529% \tVal Acc: 80.5882359%\n",
      "Epoch: 128\tTrain Loss: 0.6404384 \tVal Loss:0.5625417 \tTrain Acc: 79.92647% \tVal Acc: 83.9411765%\n",
      "Epoch: 129\tTrain Loss: 0.6764474 \tVal Loss:0.6030938 \tTrain Acc: 77.85294% \tVal Acc: 82.5294113%\n",
      "Epoch: 130\tTrain Loss: 0.7024967 \tVal Loss:0.4952731 \tTrain Acc: 77.69118% \tVal Acc: 86.0588229%\n",
      "Epoch: 131\tTrain Loss: 0.6658963 \tVal Loss:0.4750594 \tTrain Acc: 78.77941% \tVal Acc: 87.5294113%\n",
      "Epoch: 132\tTrain Loss: 0.5829937 \tVal Loss:0.5300715 \tTrain Acc: 81.69118% \tVal Acc: 84.4705880%\n",
      "Epoch: 133\tTrain Loss: 0.5849498 \tVal Loss:0.4648766 \tTrain Acc: 81.35294% \tVal Acc: 86.9411761%\n",
      "Epoch: 134\tTrain Loss: 0.5723979 \tVal Loss:0.4754960 \tTrain Acc: 82.20588% \tVal Acc: 86.2352937%\n",
      "Epoch: 135\tTrain Loss: 0.5603835 \tVal Loss:0.5906976 \tTrain Acc: 82.32353% \tVal Acc: 81.9999999%\n",
      "Epoch: 136\tTrain Loss: 0.6399752 \tVal Loss:0.5291910 \tTrain Acc: 79.70588% \tVal Acc: 84.2941171%\n",
      "Epoch: 137\tTrain Loss: 0.6381998 \tVal Loss:0.7352975 \tTrain Acc: 79.73529% \tVal Acc: 77.8235292%\n",
      "Epoch: 138\tTrain Loss: 0.6403762 \tVal Loss:0.8385128 \tTrain Acc: 79.72059% \tVal Acc: 74.1764700%\n",
      "Epoch: 139\tTrain Loss: 0.6874508 \tVal Loss:0.6615698 \tTrain Acc: 78.91176% \tVal Acc: 80.7647055%\n",
      "Epoch: 140\tTrain Loss: 0.7123947 \tVal Loss:0.6226192 \tTrain Acc: 78.08824% \tVal Acc: 80.8235288%\n",
      "Epoch: 141\tTrain Loss: 0.6541345 \tVal Loss:0.5598541 \tTrain Acc: 78.98529% \tVal Acc: 83.5294122%\n",
      "Epoch: 142\tTrain Loss: 0.5827742 \tVal Loss:0.6276126 \tTrain Acc: 81.26471% \tVal Acc: 82.0588225%\n",
      "Epoch: 143\tTrain Loss: 0.5440919 \tVal Loss:0.6514128 \tTrain Acc: 82.77941% \tVal Acc: 80.9999996%\n",
      "Epoch: 144\tTrain Loss: 0.4987961 \tVal Loss:0.5890241 \tTrain Acc: 84.29412% \tVal Acc: 82.8235286%\n",
      "Epoch: 145\tTrain Loss: 0.4619771 \tVal Loss:0.4516373 \tTrain Acc: 85.51471% \tVal Acc: 87.2941172%\n",
      "Epoch: 146\tTrain Loss: 0.4603028 \tVal Loss:0.4400380 \tTrain Acc: 85.29412% \tVal Acc: 86.9999993%\n",
      "Epoch: 147\tTrain Loss: 0.4266070 \tVal Loss:0.3385928 \tTrain Acc: 86.66176% \tVal Acc: 90.8823520%\n",
      "Validation Loss decreased from 0.418915 to 0.338593, saving the model weights\n",
      "Epoch: 148\tTrain Loss: 0.4360794 \tVal Loss:0.2667772 \tTrain Acc: 86.2647% \tVal Acc: 93.4705871%\n",
      "Validation Loss decreased from 0.338593 to 0.266777, saving the model weights\n",
      "Epoch: 149\tTrain Loss: 0.3981504 \tVal Loss:0.2488709 \tTrain Acc: 87.58823% \tVal Acc: 93.5882348%\n",
      "Validation Loss decreased from 0.266777 to 0.248871, saving the model weights\n",
      "Epoch: 150\tTrain Loss: 0.4046572 \tVal Loss:0.2946286 \tTrain Acc: 87.02941% \tVal Acc: 92.2352922%\n",
      "Epoch: 151\tTrain Loss: 0.9019873 \tVal Loss:0.4045769 \tTrain Acc: 72.17647% \tVal Acc: 88.4117645%\n",
      "Epoch: 152\tTrain Loss: 0.5885247 \tVal Loss:0.3147274 \tTrain Acc: 81.23529% \tVal Acc: 91.8235284%\n",
      "Epoch: 153\tTrain Loss: 0.4711249 \tVal Loss:0.2808987 \tTrain Acc: 84.75% \tVal Acc: 92.7058816%\n",
      "Epoch: 154\tTrain Loss: 0.4145525 \tVal Loss:0.2703090 \tTrain Acc: 86.72059% \tVal Acc: 92.7647048%\n",
      "Epoch: 155\tTrain Loss: 0.3686261 \tVal Loss:0.2213683 \tTrain Acc: 88.39706% \tVal Acc: 94.4117635%\n",
      "Validation Loss decreased from 0.248871 to 0.221368, saving the model weights\n",
      "Epoch: 156\tTrain Loss: 0.3519155 \tVal Loss:0.2489183 \tTrain Acc: 89.39706% \tVal Acc: 93.6470580%\n",
      "Epoch: 157\tTrain Loss: 0.3195034 \tVal Loss:0.2308865 \tTrain Acc: 90.33823% \tVal Acc: 93.8235283%\n",
      "Epoch: 158\tTrain Loss: 0.3173756 \tVal Loss:0.2030842 \tTrain Acc: 90.55882% \tVal Acc: 95.2352929%\n",
      "Validation Loss decreased from 0.221368 to 0.203084, saving the model weights\n",
      "Epoch: 159\tTrain Loss: 0.3045302 \tVal Loss:0.1943163 \tTrain Acc: 90.94118% \tVal Acc: 95.3529400%\n",
      "Validation Loss decreased from 0.203084 to 0.194316, saving the model weights\n",
      "Epoch: 160\tTrain Loss: 0.2889156 \tVal Loss:0.1771319 \tTrain Acc: 91.91176% \tVal Acc: 95.4705876%\n",
      "Validation Loss decreased from 0.194316 to 0.177132, saving the model weights\n",
      "Epoch: 161\tTrain Loss: 0.2738698 \tVal Loss:0.1740454 \tTrain Acc: 91.48529% \tVal Acc: 95.8823514%\n",
      "Validation Loss decreased from 0.177132 to 0.174045, saving the model weights\n",
      "Epoch: 162\tTrain Loss: 0.2733003 \tVal Loss:0.1912634 \tTrain Acc: 91.38235% \tVal Acc: 95.4117632%\n",
      "Epoch: 163\tTrain Loss: 0.2639481 \tVal Loss:0.1835372 \tTrain Acc: 92.36765% \tVal Acc: 95.1176459%\n",
      "Epoch: 164\tTrain Loss: 0.2654195 \tVal Loss:0.1654918 \tTrain Acc: 92.0147% \tVal Acc: 96.2352931%\n",
      "Validation Loss decreased from 0.174045 to 0.165492, saving the model weights\n",
      "Epoch: 165\tTrain Loss: 0.2686204 \tVal Loss:0.1713689 \tTrain Acc: 91.76471% \tVal Acc: 95.8235282%\n",
      "Epoch: 166\tTrain Loss: 0.2559804 \tVal Loss:0.1648584 \tTrain Acc: 92.16176% \tVal Acc: 95.7058811%\n",
      "Validation Loss decreased from 0.165492 to 0.164858, saving the model weights\n",
      "Epoch: 167\tTrain Loss: 0.2508992 \tVal Loss:0.1668482 \tTrain Acc: 92.38235% \tVal Acc: 96.0588217%\n",
      "Epoch: 168\tTrain Loss: 0.2415438 \tVal Loss:0.1627321 \tTrain Acc: 92.72059% \tVal Acc: 96.2941164%\n",
      "Validation Loss decreased from 0.164858 to 0.162732, saving the model weights\n",
      "Epoch: 169\tTrain Loss: 0.2284051 \tVal Loss:0.1511121 \tTrain Acc: 93.41176% \tVal Acc: 96.4117634%\n",
      "Validation Loss decreased from 0.162732 to 0.151112, saving the model weights\n",
      "Epoch: 170\tTrain Loss: 0.2220328 \tVal Loss:0.1590703 \tTrain Acc: 93.25% \tVal Acc: 95.9999990%\n",
      "Epoch: 171\tTrain Loss: 0.2240551 \tVal Loss:0.1706906 \tTrain Acc: 93.39706% \tVal Acc: 95.2941173%\n",
      "Epoch: 172\tTrain Loss: 0.2303730 \tVal Loss:0.1656574 \tTrain Acc: 93.11765% \tVal Acc: 95.5882341%\n",
      "Epoch: 173\tTrain Loss: 0.2239113 \tVal Loss:0.1556059 \tTrain Acc: 93.38235% \tVal Acc: 96.0588223%\n",
      "Epoch: 174\tTrain Loss: 0.2195386 \tVal Loss:0.1831892 \tTrain Acc: 93.73529% \tVal Acc: 95.4117632%\n",
      "Epoch: 175\tTrain Loss: 0.2081662 \tVal Loss:0.1337411 \tTrain Acc: 93.95588% \tVal Acc: 96.7647052%\n",
      "Validation Loss decreased from 0.151112 to 0.133741, saving the model weights\n",
      "Epoch: 176\tTrain Loss: 0.2094770 \tVal Loss:0.1630813 \tTrain Acc: 93.82353% \tVal Acc: 95.7058823%\n",
      "Epoch: 177\tTrain Loss: 0.2005979 \tVal Loss:0.1534981 \tTrain Acc: 94.33823% \tVal Acc: 95.8235276%\n",
      "Epoch: 178\tTrain Loss: 0.2008568 \tVal Loss:0.1567127 \tTrain Acc: 94.0147% \tVal Acc: 95.5294102%\n",
      "Epoch: 179\tTrain Loss: 0.2011848 \tVal Loss:0.1637902 \tTrain Acc: 94.11765% \tVal Acc: 95.8823520%\n",
      "Epoch: 180\tTrain Loss: 0.1882421 \tVal Loss:0.1717957 \tTrain Acc: 94.47059% \tVal Acc: 95.5882335%\n",
      "Epoch: 181\tTrain Loss: 0.2034259 \tVal Loss:0.1725129 \tTrain Acc: 93.82353% \tVal Acc: 95.1764697%\n",
      "Epoch: 182\tTrain Loss: 0.2277692 \tVal Loss:0.1706091 \tTrain Acc: 92.97059% \tVal Acc: 95.4705864%\n",
      "Epoch: 183\tTrain Loss: 0.2067203 \tVal Loss:0.1531745 \tTrain Acc: 93.79412% \tVal Acc: 95.9411752%\n",
      "Epoch: 184\tTrain Loss: 0.2034381 \tVal Loss:0.2039490 \tTrain Acc: 93.72059% \tVal Acc: 94.3529397%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 185\tTrain Loss: 0.1939051 \tVal Loss:0.1764229 \tTrain Acc: 93.75% \tVal Acc: 95.1764691%\n",
      "Epoch: 186\tTrain Loss: 0.2031201 \tVal Loss:0.1838949 \tTrain Acc: 94.20588% \tVal Acc: 94.5294106%\n",
      "Epoch: 187\tTrain Loss: 0.2036566 \tVal Loss:0.1664778 \tTrain Acc: 93.76471% \tVal Acc: 95.0588232%\n",
      "Epoch: 188\tTrain Loss: 0.1855137 \tVal Loss:0.1429175 \tTrain Acc: 94.38235% \tVal Acc: 96.0588217%\n",
      "Epoch: 189\tTrain Loss: 0.1846832 \tVal Loss:0.1111345 \tTrain Acc: 94.23529% \tVal Acc: 97.1764690%\n",
      "Validation Loss decreased from 0.133741 to 0.111135, saving the model weights\n",
      "Epoch: 190\tTrain Loss: 0.1782926 \tVal Loss:0.1356066 \tTrain Acc: 94.66176% \tVal Acc: 96.0588229%\n",
      "Epoch: 191\tTrain Loss: 0.1719776 \tVal Loss:0.1890615 \tTrain Acc: 94.72059% \tVal Acc: 94.6470577%\n",
      "Epoch: 192\tTrain Loss: 0.2268788 \tVal Loss:0.1501766 \tTrain Acc: 92.95588% \tVal Acc: 96.4117640%\n",
      "Epoch: 193\tTrain Loss: 0.2289681 \tVal Loss:0.2147739 \tTrain Acc: 93.02941% \tVal Acc: 94.1176462%\n",
      "Epoch: 194\tTrain Loss: 0.2617825 \tVal Loss:0.2394354 \tTrain Acc: 91.64706% \tVal Acc: 93.4117627%\n",
      "Epoch: 195\tTrain Loss: 0.2821044 \tVal Loss:0.2016407 \tTrain Acc: 91.27941% \tVal Acc: 93.9999992%\n",
      "Epoch: 196\tTrain Loss: 0.2591795 \tVal Loss:0.1668726 \tTrain Acc: 91.86765% \tVal Acc: 95.5882335%\n",
      "Epoch: 197\tTrain Loss: 0.2447698 \tVal Loss:0.1652227 \tTrain Acc: 92.44118% \tVal Acc: 95.2941161%\n",
      "Epoch: 198\tTrain Loss: 0.2267632 \tVal Loss:0.1371354 \tTrain Acc: 92.86765% \tVal Acc: 96.1764699%\n",
      "Epoch: 199\tTrain Loss: 0.1850285 \tVal Loss:0.1080900 \tTrain Acc: 94.44118% \tVal Acc: 97.2941166%\n",
      "Validation Loss decreased from 0.111135 to 0.108090, saving the model weights\n",
      "Epoch: 200\tTrain Loss: 0.2035512 \tVal Loss:0.2170737 \tTrain Acc: 93.92647% \tVal Acc: 93.4117639%\n",
      "Epoch: 201\tTrain Loss: 0.2831901 \tVal Loss:0.1798237 \tTrain Acc: 91.25% \tVal Acc: 94.5882350%\n",
      "Epoch: 202\tTrain Loss: 0.3183305 \tVal Loss:0.1911104 \tTrain Acc: 90.30882% \tVal Acc: 94.5882344%\n",
      "Epoch: 203\tTrain Loss: 0.3022197 \tVal Loss:0.2236383 \tTrain Acc: 90.07353% \tVal Acc: 93.5882336%\n",
      "Epoch: 204\tTrain Loss: 0.2993712 \tVal Loss:0.1508076 \tTrain Acc: 90.61765% \tVal Acc: 95.1764697%\n",
      "Epoch: 205\tTrain Loss: 0.2639170 \tVal Loss:0.1537948 \tTrain Acc: 91.60294% \tVal Acc: 95.5882341%\n",
      "Epoch: 206\tTrain Loss: 0.2352924 \tVal Loss:0.1884920 \tTrain Acc: 93.08823% \tVal Acc: 94.1764694%\n",
      "Epoch: 207\tTrain Loss: 0.2323326 \tVal Loss:0.1519670 \tTrain Acc: 92.97059% \tVal Acc: 95.3529400%\n",
      "Epoch: 208\tTrain Loss: 0.1762805 \tVal Loss:0.1383715 \tTrain Acc: 94.77941% \tVal Acc: 96.2941164%\n",
      "Epoch: 209\tTrain Loss: 0.1534597 \tVal Loss:0.0997362 \tTrain Acc: 95.54412% \tVal Acc: 96.9999981%\n",
      "Validation Loss decreased from 0.108090 to 0.099736, saving the model weights\n",
      "Epoch: 210\tTrain Loss: 0.1379850 \tVal Loss:0.0988857 \tTrain Acc: 95.80882% \tVal Acc: 97.2352922%\n",
      "Validation Loss decreased from 0.099736 to 0.098886, saving the model weights\n",
      "Epoch: 211\tTrain Loss: 0.1338238 \tVal Loss:0.1088742 \tTrain Acc: 96.10294% \tVal Acc: 96.7647040%\n",
      "Epoch: 212\tTrain Loss: 0.1217255 \tVal Loss:0.1230935 \tTrain Acc: 96.72059% \tVal Acc: 96.7058814%\n",
      "Epoch: 213\tTrain Loss: 0.1257897 \tVal Loss:0.1244819 \tTrain Acc: 96.30882% \tVal Acc: 96.0588211%\n",
      "Epoch: 214\tTrain Loss: 0.1185450 \tVal Loss:0.1152054 \tTrain Acc: 96.39706% \tVal Acc: 96.6470581%\n",
      "Epoch: 215\tTrain Loss: 0.1127676 \tVal Loss:0.1121398 \tTrain Acc: 96.82353% \tVal Acc: 96.5294111%\n",
      "Epoch: 216\tTrain Loss: 0.1110259 \tVal Loss:0.1260370 \tTrain Acc: 96.97059% \tVal Acc: 96.4705867%\n",
      "Epoch: 217\tTrain Loss: 0.1120791 \tVal Loss:0.1017474 \tTrain Acc: 97.02941% \tVal Acc: 97.4117637%\n",
      "Epoch: 218\tTrain Loss: 0.1058291 \tVal Loss:0.1116446 \tTrain Acc: 96.7647% \tVal Acc: 96.8235278%\n",
      "Epoch: 219\tTrain Loss: 0.1055457 \tVal Loss:0.0922171 \tTrain Acc: 97.22059% \tVal Acc: 96.8823516%\n",
      "Validation Loss decreased from 0.098886 to 0.092217, saving the model weights\n",
      "Epoch: 220\tTrain Loss: 0.1035883 \tVal Loss:0.0818573 \tTrain Acc: 97.19118% \tVal Acc: 97.2941160%\n",
      "Validation Loss decreased from 0.092217 to 0.081857, saving the model weights\n",
      "Epoch: 221\tTrain Loss: 0.1017513 \tVal Loss:0.1184490 \tTrain Acc: 97.04412% \tVal Acc: 96.5294105%\n",
      "Epoch: 222\tTrain Loss: 0.1093149 \tVal Loss:0.1104038 \tTrain Acc: 96.7647% \tVal Acc: 96.8235284%\n",
      "Epoch: 223\tTrain Loss: 0.1057020 \tVal Loss:0.1221485 \tTrain Acc: 97.07353% \tVal Acc: 96.5294099%\n",
      "Epoch: 224\tTrain Loss: 0.1021806 \tVal Loss:0.1116821 \tTrain Acc: 97.38235% \tVal Acc: 96.8823510%\n",
      "Epoch: 225\tTrain Loss: 0.1037440 \tVal Loss:0.1345830 \tTrain Acc: 97.14706% \tVal Acc: 95.9411758%\n",
      "Epoch: 226\tTrain Loss: 0.0951791 \tVal Loss:0.0960123 \tTrain Acc: 97.30882% \tVal Acc: 97.2352928%\n",
      "Epoch: 227\tTrain Loss: 0.1018741 \tVal Loss:0.1345452 \tTrain Acc: 97.29412% \tVal Acc: 96.1764693%\n",
      "Epoch: 228\tTrain Loss: 0.0960104 \tVal Loss:0.1479655 \tTrain Acc: 97.23529% \tVal Acc: 95.9999985%\n",
      "Epoch: 229\tTrain Loss: 0.0997427 \tVal Loss:0.0936246 \tTrain Acc: 97.32353% \tVal Acc: 97.0588219%\n",
      "Epoch: 230\tTrain Loss: 0.1085400 \tVal Loss:0.1750687 \tTrain Acc: 96.95588% \tVal Acc: 95.5294096%\n",
      "Epoch: 231\tTrain Loss: 0.1110818 \tVal Loss:0.1257961 \tTrain Acc: 96.89706% \tVal Acc: 96.5882349%\n",
      "Epoch: 232\tTrain Loss: 0.1093899 \tVal Loss:0.1394040 \tTrain Acc: 96.7647% \tVal Acc: 96.2941158%\n",
      "Epoch: 233\tTrain Loss: 0.1026149 \tVal Loss:0.0971175 \tTrain Acc: 97.10294% \tVal Acc: 96.9999993%\n",
      "Epoch: 234\tTrain Loss: 0.1056770 \tVal Loss:0.1515924 \tTrain Acc: 97.20588% \tVal Acc: 95.5294102%\n",
      "Epoch: 235\tTrain Loss: 0.1154359 \tVal Loss:0.1561500 \tTrain Acc: 96.41176% \tVal Acc: 94.9999982%\n",
      "Epoch: 236\tTrain Loss: 0.1230193 \tVal Loss:0.1578463 \tTrain Acc: 96.20588% \tVal Acc: 95.1764691%\n",
      "Epoch: 237\tTrain Loss: 0.1629523 \tVal Loss:0.6328305 \tTrain Acc: 94.98529% \tVal Acc: 84.9411768%\n",
      "Epoch: 238\tTrain Loss: 0.5128639 \tVal Loss:0.5055452 \tTrain Acc: 84.35294% \tVal Acc: 85.4117638%\n",
      "Epoch: 239\tTrain Loss: 0.5225633 \tVal Loss:0.3781301 \tTrain Acc: 83.97059% \tVal Acc: 88.4117639%\n",
      "Epoch: 240\tTrain Loss: 0.3811307 \tVal Loss:0.2058768 \tTrain Acc: 87.95588% \tVal Acc: 94.2941159%\n",
      "Epoch: 241\tTrain Loss: 0.2681070 \tVal Loss:0.1872928 \tTrain Acc: 91.19118% \tVal Acc: 94.8823518%\n",
      "Epoch: 242\tTrain Loss: 0.1905190 \tVal Loss:0.0934113 \tTrain Acc: 94.42647% \tVal Acc: 97.5294113%\n",
      "Epoch: 243\tTrain Loss: 0.1443970 \tVal Loss:0.0979987 \tTrain Acc: 95.60294% \tVal Acc: 97.2941160%\n",
      "Epoch: 244\tTrain Loss: 0.1238295 \tVal Loss:0.0826596 \tTrain Acc: 96.33823% \tVal Acc: 97.8823519%\n",
      "Epoch: 245\tTrain Loss: 0.1172189 \tVal Loss:0.0854246 \tTrain Acc: 96.60294% \tVal Acc: 97.7058816%\n",
      "Epoch: 246\tTrain Loss: 0.1118536 \tVal Loss:0.0907885 \tTrain Acc: 96.92647% \tVal Acc: 97.4705875%\n",
      "Epoch: 247\tTrain Loss: 0.0983639 \tVal Loss:0.0776359 \tTrain Acc: 97.2647% \tVal Acc: 98.0588228%\n",
      "Validation Loss decreased from 0.081857 to 0.077636, saving the model weights\n",
      "Epoch: 248\tTrain Loss: 0.0897690 \tVal Loss:0.0817014 \tTrain Acc: 97.77941% \tVal Acc: 97.5294107%\n",
      "Epoch: 249\tTrain Loss: 0.0923666 \tVal Loss:0.0824324 \tTrain Acc: 97.44117% \tVal Acc: 97.7647048%\n",
      "Epoch: 250\tTrain Loss: 0.0888676 \tVal Loss:0.0793149 \tTrain Acc: 97.48529% \tVal Acc: 97.7647054%\n",
      "Epoch: 251\tTrain Loss: 0.0892920 \tVal Loss:0.0943569 \tTrain Acc: 97.45588% \tVal Acc: 97.3529398%\n",
      "Epoch: 252\tTrain Loss: 0.1004150 \tVal Loss:0.1006202 \tTrain Acc: 97.16176% \tVal Acc: 96.9999993%\n",
      "Epoch: 253\tTrain Loss: 0.0901378 \tVal Loss:0.1126288 \tTrain Acc: 97.57353% \tVal Acc: 96.8235278%\n",
      "Epoch: 254\tTrain Loss: 0.0862084 \tVal Loss:0.0912374 \tTrain Acc: 97.55882% \tVal Acc: 97.2352934%\n",
      "Epoch: 255\tTrain Loss: 0.0874970 \tVal Loss:0.0972724 \tTrain Acc: 97.54412% \tVal Acc: 97.1764696%\n",
      "Epoch: 256\tTrain Loss: 0.0820688 \tVal Loss:0.0954124 \tTrain Acc: 97.79412% \tVal Acc: 97.1176451%\n",
      "Epoch: 257\tTrain Loss: 0.0786493 \tVal Loss:0.0872530 \tTrain Acc: 97.77941% \tVal Acc: 97.5882345%\n",
      "Epoch: 258\tTrain Loss: 0.0750979 \tVal Loss:0.1041850 \tTrain Acc: 97.85294% \tVal Acc: 97.3529398%\n",
      "Epoch: 259\tTrain Loss: 0.0710639 \tVal Loss:0.1024662 \tTrain Acc: 97.97059% \tVal Acc: 97.1176463%\n",
      "Epoch: 260\tTrain Loss: 0.0752889 \tVal Loss:0.1040287 \tTrain Acc: 97.97059% \tVal Acc: 97.1176457%\n",
      "Epoch: 261\tTrain Loss: 0.0721799 \tVal Loss:0.1098840 \tTrain Acc: 98.05882% \tVal Acc: 97.2352928%\n",
      "Epoch: 262\tTrain Loss: 0.0727692 \tVal Loss:0.1100609 \tTrain Acc: 98.07353% \tVal Acc: 97.0588225%\n",
      "Epoch: 263\tTrain Loss: 0.0673299 \tVal Loss:0.1161039 \tTrain Acc: 98.13235% \tVal Acc: 96.7647052%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 264\tTrain Loss: 0.0676802 \tVal Loss:0.1000220 \tTrain Acc: 98.17647% \tVal Acc: 97.2352934%\n",
      "Epoch: 265\tTrain Loss: 0.0660821 \tVal Loss:0.0902366 \tTrain Acc: 98.07353% \tVal Acc: 97.1764690%\n",
      "Epoch: 266\tTrain Loss: 0.0718489 \tVal Loss:0.0805349 \tTrain Acc: 97.97059% \tVal Acc: 97.3529398%\n",
      "Epoch: 267\tTrain Loss: 0.0654994 \tVal Loss:0.0828257 \tTrain Acc: 98.23529% \tVal Acc: 97.4705869%\n",
      "Epoch: 268\tTrain Loss: 0.0816577 \tVal Loss:0.1166646 \tTrain Acc: 97.55882% \tVal Acc: 96.6470569%\n",
      "Epoch: 269\tTrain Loss: 0.1971746 \tVal Loss:0.2972804 \tTrain Acc: 93.72059% \tVal Acc: 91.2352931%\n",
      "Epoch: 270\tTrain Loss: 0.5047963 \tVal Loss:0.5105223 \tTrain Acc: 84.91176% \tVal Acc: 85.6470585%\n",
      "Epoch: 271\tTrain Loss: 0.5518170 \tVal Loss:0.2970934 \tTrain Acc: 83.88235% \tVal Acc: 90.9411758%\n",
      "Epoch: 272\tTrain Loss: 0.4754439 \tVal Loss:0.1740756 \tTrain Acc: 85.22059% \tVal Acc: 94.9411756%\n",
      "Epoch: 273\tTrain Loss: 0.3127927 \tVal Loss:0.1555692 \tTrain Acc: 90.22059% \tVal Acc: 95.6470573%\n",
      "Epoch: 274\tTrain Loss: 0.1874434 \tVal Loss:0.1247725 \tTrain Acc: 94.17647% \tVal Acc: 96.7058808%\n",
      "Epoch: 275\tTrain Loss: 0.1479344 \tVal Loss:0.0887699 \tTrain Acc: 95.32353% \tVal Acc: 97.5294095%\n",
      "Epoch: 276\tTrain Loss: 0.1237218 \tVal Loss:0.0966686 \tTrain Acc: 96.22059% \tVal Acc: 97.2352934%\n",
      "Epoch: 277\tTrain Loss: 0.1061887 \tVal Loss:0.0878155 \tTrain Acc: 97.0147% \tVal Acc: 97.2941166%\n",
      "Epoch: 278\tTrain Loss: 0.0917354 \tVal Loss:0.0909006 \tTrain Acc: 97.45588% \tVal Acc: 97.2941166%\n",
      "Epoch: 279\tTrain Loss: 0.0888734 \tVal Loss:0.0887867 \tTrain Acc: 97.36765% \tVal Acc: 97.5882339%\n",
      "Epoch: 280\tTrain Loss: 0.0796751 \tVal Loss:0.0968650 \tTrain Acc: 97.82353% \tVal Acc: 97.3529398%\n",
      "Epoch: 281\tTrain Loss: 0.0750316 \tVal Loss:0.0979990 \tTrain Acc: 98.0147% \tVal Acc: 96.9999987%\n",
      "Epoch: 282\tTrain Loss: 0.0766365 \tVal Loss:0.0914174 \tTrain Acc: 97.85294% \tVal Acc: 97.2352940%\n",
      "Epoch: 283\tTrain Loss: 0.0720844 \tVal Loss:0.0875333 \tTrain Acc: 98.10294% \tVal Acc: 97.1176451%\n",
      "Epoch: 284\tTrain Loss: 0.0721689 \tVal Loss:0.0880730 \tTrain Acc: 98.07353% \tVal Acc: 97.2352934%\n",
      "Epoch: 285\tTrain Loss: 0.0691514 \tVal Loss:0.0905498 \tTrain Acc: 98.13235% \tVal Acc: 97.2352928%\n",
      "Epoch: 286\tTrain Loss: 0.0641998 \tVal Loss:0.0898908 \tTrain Acc: 98.32353% \tVal Acc: 97.4705875%\n",
      "Epoch: 287\tTrain Loss: 0.0670033 \tVal Loss:0.0980093 \tTrain Acc: 98.02941% \tVal Acc: 97.5882339%\n",
      "Epoch: 288\tTrain Loss: 0.0607069 \tVal Loss:0.0913676 \tTrain Acc: 98.5% \tVal Acc: 97.3529404%\n",
      "Epoch: 289\tTrain Loss: 0.0576157 \tVal Loss:0.0884682 \tTrain Acc: 98.54412% \tVal Acc: 97.3529404%\n",
      "Epoch: 290\tTrain Loss: 0.0606829 \tVal Loss:0.0996108 \tTrain Acc: 98.5147% \tVal Acc: 97.1764690%\n",
      "Epoch: 291\tTrain Loss: 0.0535816 \tVal Loss:0.0981439 \tTrain Acc: 98.57353% \tVal Acc: 97.2352928%\n",
      "Epoch: 292\tTrain Loss: 0.0609068 \tVal Loss:0.0820371 \tTrain Acc: 98.41176% \tVal Acc: 97.7058810%\n",
      "Epoch: 293\tTrain Loss: 0.0564704 \tVal Loss:0.0926737 \tTrain Acc: 98.42647% \tVal Acc: 97.1764690%\n",
      "Epoch: 294\tTrain Loss: 0.0558074 \tVal Loss:0.0966219 \tTrain Acc: 98.54412% \tVal Acc: 97.4117637%\n",
      "Epoch: 295\tTrain Loss: 0.0618314 \tVal Loss:0.0796620 \tTrain Acc: 98.32353% \tVal Acc: 97.4117637%\n",
      "Epoch: 296\tTrain Loss: 0.0599659 \tVal Loss:0.0892864 \tTrain Acc: 98.32353% \tVal Acc: 97.4117631%\n",
      "Epoch: 297\tTrain Loss: 0.0576845 \tVal Loss:0.1077249 \tTrain Acc: 98.5% \tVal Acc: 96.8823522%\n",
      "Epoch: 298\tTrain Loss: 0.0572907 \tVal Loss:0.0940581 \tTrain Acc: 98.52941% \tVal Acc: 96.9999993%\n",
      "Epoch: 299\tTrain Loss: 0.0527662 \tVal Loss:0.0796623 \tTrain Acc: 98.63235% \tVal Acc: 97.0588219%\n",
      "Epoch: 300\tTrain Loss: 0.0549936 \tVal Loss:0.0874270 \tTrain Acc: 98.39706% \tVal Acc: 97.4705863%\n",
      "Epoch: 301\tTrain Loss: 0.0538296 \tVal Loss:0.1127140 \tTrain Acc: 98.55882% \tVal Acc: 96.9999993%\n",
      "Epoch: 302\tTrain Loss: 0.0576887 \tVal Loss:0.1040032 \tTrain Acc: 98.42647% \tVal Acc: 97.2352934%\n",
      "Epoch: 303\tTrain Loss: 0.0640533 \tVal Loss:0.0939646 \tTrain Acc: 98.25% \tVal Acc: 97.4705863%\n",
      "Epoch: 304\tTrain Loss: 0.0615789 \tVal Loss:0.1161144 \tTrain Acc: 98.35294% \tVal Acc: 96.8235278%\n",
      "Epoch: 305\tTrain Loss: 0.0600895 \tVal Loss:0.1178238 \tTrain Acc: 98.35294% \tVal Acc: 96.8235284%\n",
      "Epoch: 306\tTrain Loss: 0.0742737 \tVal Loss:0.0984281 \tTrain Acc: 97.92647% \tVal Acc: 97.2352928%\n",
      "Epoch: 307\tTrain Loss: 0.0781912 \tVal Loss:0.1692688 \tTrain Acc: 97.70588% \tVal Acc: 95.6470585%\n",
      "Epoch: 308\tTrain Loss: 0.0972819 \tVal Loss:0.1008720 \tTrain Acc: 97.0% \tVal Acc: 97.0588231%\n",
      "Epoch: 309\tTrain Loss: 0.0985330 \tVal Loss:0.1289048 \tTrain Acc: 96.82353% \tVal Acc: 96.2352937%\n",
      "Epoch: 310\tTrain Loss: 0.1193406 \tVal Loss:0.2130452 \tTrain Acc: 96.5% \tVal Acc: 94.2352933%\n",
      "Epoch: 311\tTrain Loss: 0.1897010 \tVal Loss:0.1881951 \tTrain Acc: 94.41176% \tVal Acc: 95.2941167%\n",
      "Epoch: 312\tTrain Loss: 0.1594860 \tVal Loss:0.1381742 \tTrain Acc: 94.91176% \tVal Acc: 95.7647049%\n",
      "Epoch: 313\tTrain Loss: 0.1486871 \tVal Loss:0.1647752 \tTrain Acc: 95.35294% \tVal Acc: 95.2352935%\n",
      "Epoch: 314\tTrain Loss: 0.1670846 \tVal Loss:0.1165587 \tTrain Acc: 94.89706% \tVal Acc: 96.3529396%\n",
      "Epoch: 315\tTrain Loss: 0.1674275 \tVal Loss:0.1831659 \tTrain Acc: 94.97059% \tVal Acc: 95.1764691%\n",
      "Epoch: 316\tTrain Loss: 0.1662143 \tVal Loss:0.1517515 \tTrain Acc: 95.47059% \tVal Acc: 95.4705858%\n",
      "Epoch: 317\tTrain Loss: 0.1253735 \tVal Loss:0.1080659 \tTrain Acc: 96.0147% \tVal Acc: 96.8235278%\n",
      "Epoch: 318\tTrain Loss: 0.1138454 \tVal Loss:0.1515261 \tTrain Acc: 96.58823% \tVal Acc: 96.0588217%\n",
      "Epoch: 319\tTrain Loss: 0.0935647 \tVal Loss:0.1317726 \tTrain Acc: 97.2647% \tVal Acc: 96.2352920%\n",
      "Epoch: 320\tTrain Loss: 0.0765357 \tVal Loss:0.0988467 \tTrain Acc: 97.75% \tVal Acc: 97.1764702%\n",
      "Epoch: 321\tTrain Loss: 0.0725843 \tVal Loss:0.1079268 \tTrain Acc: 97.97059% \tVal Acc: 97.3529404%\n",
      "Epoch: 322\tTrain Loss: 0.0647010 \tVal Loss:0.1066981 \tTrain Acc: 98.22059% \tVal Acc: 97.3529398%\n",
      "Epoch: 323\tTrain Loss: 0.0598387 \tVal Loss:0.0827586 \tTrain Acc: 98.20588% \tVal Acc: 97.8823525%\n",
      "Epoch: 324\tTrain Loss: 0.0550952 \tVal Loss:0.0842829 \tTrain Acc: 98.42647% \tVal Acc: 97.7058810%\n",
      "Epoch: 325\tTrain Loss: 0.0551078 \tVal Loss:0.0782461 \tTrain Acc: 98.44118% \tVal Acc: 97.5882345%\n",
      "Epoch: 326\tTrain Loss: 0.0546463 \tVal Loss:0.0832388 \tTrain Acc: 98.57353% \tVal Acc: 97.6470578%\n",
      "Epoch: 327\tTrain Loss: 0.0562674 \tVal Loss:0.0976664 \tTrain Acc: 98.20588% \tVal Acc: 96.9999999%\n",
      "Epoch: 328\tTrain Loss: 0.0527655 \tVal Loss:0.0868549 \tTrain Acc: 98.63235% \tVal Acc: 97.2941154%\n",
      "Epoch: 329\tTrain Loss: 0.0465221 \tVal Loss:0.0858358 \tTrain Acc: 98.92647% \tVal Acc: 97.7647042%\n",
      "Epoch: 330\tTrain Loss: 0.0527725 \tVal Loss:0.1166596 \tTrain Acc: 98.55882% \tVal Acc: 96.8823510%\n",
      "Epoch: 331\tTrain Loss: 0.0514243 \tVal Loss:0.0911055 \tTrain Acc: 98.67647% \tVal Acc: 97.2352922%\n",
      "Epoch: 332\tTrain Loss: 0.0442065 \tVal Loss:0.0890110 \tTrain Acc: 98.82353% \tVal Acc: 97.6470572%\n",
      "Epoch: 333\tTrain Loss: 0.0456833 \tVal Loss:0.0894128 \tTrain Acc: 98.86765% \tVal Acc: 97.2941160%\n",
      "Epoch: 334\tTrain Loss: 0.0595648 \tVal Loss:0.1190682 \tTrain Acc: 98.52941% \tVal Acc: 96.8823522%\n",
      "Epoch: 335\tTrain Loss: 0.1785831 \tVal Loss:0.1156821 \tTrain Acc: 94.66176% \tVal Acc: 96.2941170%\n",
      "Epoch: 336\tTrain Loss: 0.2193003 \tVal Loss:0.1613292 \tTrain Acc: 93.25% \tVal Acc: 95.7647049%\n",
      "Epoch: 337\tTrain Loss: 0.2362939 \tVal Loss:0.1476821 \tTrain Acc: 92.58823% \tVal Acc: 95.4117632%\n",
      "Epoch: 338\tTrain Loss: 0.2332280 \tVal Loss:0.3404907 \tTrain Acc: 92.23529% \tVal Acc: 90.1764697%\n",
      "Epoch: 339\tTrain Loss: 0.2202069 \tVal Loss:0.1787184 \tTrain Acc: 93.2647% \tVal Acc: 94.8823518%\n",
      "Epoch: 340\tTrain Loss: 0.1609292 \tVal Loss:0.0820845 \tTrain Acc: 95.11765% \tVal Acc: 97.0588219%\n",
      "Epoch: 341\tTrain Loss: 0.1273105 \tVal Loss:0.0872137 \tTrain Acc: 96.0147% \tVal Acc: 97.1176457%\n",
      "Epoch: 342\tTrain Loss: 0.0937795 \tVal Loss:0.0922135 \tTrain Acc: 97.17647% \tVal Acc: 97.2352934%\n",
      "Epoch: 343\tTrain Loss: 0.0775547 \tVal Loss:0.0849948 \tTrain Acc: 97.64706% \tVal Acc: 97.1176463%\n",
      "Epoch: 344\tTrain Loss: 0.0749329 \tVal Loss:0.0661064 \tTrain Acc: 98.0147% \tVal Acc: 98.1176460%\n",
      "Validation Loss decreased from 0.077636 to 0.066106, saving the model weights\n",
      "Epoch: 345\tTrain Loss: 0.0641906 \tVal Loss:0.0608529 \tTrain Acc: 98.02941% \tVal Acc: 97.9999983%\n",
      "Validation Loss decreased from 0.066106 to 0.060853, saving the model weights\n",
      "Epoch: 346\tTrain Loss: 0.0575110 \tVal Loss:0.0603479 \tTrain Acc: 98.38235% \tVal Acc: 97.9411757%\n",
      "Validation Loss decreased from 0.060853 to 0.060348, saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 347\tTrain Loss: 0.0532582 \tVal Loss:0.0547441 \tTrain Acc: 98.57353% \tVal Acc: 98.1176454%\n",
      "Validation Loss decreased from 0.060348 to 0.054744, saving the model weights\n",
      "Epoch: 348\tTrain Loss: 0.0531424 \tVal Loss:0.0701563 \tTrain Acc: 98.45588% \tVal Acc: 97.8235275%\n",
      "Epoch: 349\tTrain Loss: 0.0504142 \tVal Loss:0.0619176 \tTrain Acc: 98.61765% \tVal Acc: 97.8823525%\n",
      "Epoch: 350\tTrain Loss: 0.0462040 \tVal Loss:0.0648690 \tTrain Acc: 98.72059% \tVal Acc: 97.9999983%\n",
      "Epoch: 351\tTrain Loss: 0.0457462 \tVal Loss:0.0632131 \tTrain Acc: 98.75% \tVal Acc: 97.8823519%\n",
      "Epoch: 352\tTrain Loss: 0.0441373 \tVal Loss:0.0619055 \tTrain Acc: 98.77941% \tVal Acc: 97.9411751%\n",
      "Epoch: 353\tTrain Loss: 0.0451012 \tVal Loss:0.0697523 \tTrain Acc: 98.66176% \tVal Acc: 97.8823519%\n",
      "Epoch: 354\tTrain Loss: 0.0422443 \tVal Loss:0.0584874 \tTrain Acc: 98.89706% \tVal Acc: 97.8823513%\n",
      "Epoch: 355\tTrain Loss: 0.0411889 \tVal Loss:0.0547146 \tTrain Acc: 98.95588% \tVal Acc: 98.1764692%\n",
      "Validation Loss decreased from 0.054744 to 0.054715, saving the model weights\n",
      "Epoch: 356\tTrain Loss: 0.0397639 \tVal Loss:0.0515385 \tTrain Acc: 98.94118% \tVal Acc: 98.0588216%\n",
      "Validation Loss decreased from 0.054715 to 0.051539, saving the model weights\n",
      "Epoch: 357\tTrain Loss: 0.0396936 \tVal Loss:0.0689020 \tTrain Acc: 99.0147% \tVal Acc: 97.8235286%\n",
      "Epoch: 358\tTrain Loss: 0.0507304 \tVal Loss:0.0683718 \tTrain Acc: 98.75% \tVal Acc: 98.1176454%\n",
      "Epoch: 359\tTrain Loss: 0.0528933 \tVal Loss:0.0682619 \tTrain Acc: 98.77941% \tVal Acc: 97.6470578%\n",
      "Epoch: 360\tTrain Loss: 0.0501531 \tVal Loss:0.0629166 \tTrain Acc: 98.67647% \tVal Acc: 97.9411745%\n",
      "Epoch: 361\tTrain Loss: 0.0460092 \tVal Loss:0.0763906 \tTrain Acc: 98.80882% \tVal Acc: 97.5882334%\n",
      "Epoch: 362\tTrain Loss: 0.0416984 \tVal Loss:0.0789689 \tTrain Acc: 99.0147% \tVal Acc: 97.6470572%\n",
      "Epoch: 363\tTrain Loss: 0.0455222 \tVal Loss:0.0967265 \tTrain Acc: 98.86765% \tVal Acc: 97.0588225%\n",
      "Epoch: 364\tTrain Loss: 0.0567327 \tVal Loss:0.1036253 \tTrain Acc: 98.42647% \tVal Acc: 96.9411755%\n",
      "Epoch: 365\tTrain Loss: 0.0504482 \tVal Loss:0.0643395 \tTrain Acc: 98.64706% \tVal Acc: 97.8823513%\n",
      "Epoch: 366\tTrain Loss: 0.0454616 \tVal Loss:0.0731544 \tTrain Acc: 98.70588% \tVal Acc: 97.6470584%\n",
      "Epoch: 367\tTrain Loss: 0.0451186 \tVal Loss:0.0626857 \tTrain Acc: 98.63235% \tVal Acc: 97.9411751%\n",
      "Epoch: 368\tTrain Loss: 0.0485347 \tVal Loss:0.0771058 \tTrain Acc: 98.61765% \tVal Acc: 97.7058810%\n",
      "Epoch: 369\tTrain Loss: 0.0569249 \tVal Loss:0.0926094 \tTrain Acc: 98.39706% \tVal Acc: 97.1176457%\n",
      "Epoch: 370\tTrain Loss: 0.0850382 \tVal Loss:0.1247567 \tTrain Acc: 97.61765% \tVal Acc: 96.0588223%\n",
      "Epoch: 371\tTrain Loss: 0.1175505 \tVal Loss:0.1652490 \tTrain Acc: 96.44118% \tVal Acc: 95.3529406%\n",
      "Epoch: 372\tTrain Loss: 0.1712381 \tVal Loss:0.1477904 \tTrain Acc: 94.83823% \tVal Acc: 95.2352929%\n",
      "Epoch: 373\tTrain Loss: 0.1700213 \tVal Loss:0.1745967 \tTrain Acc: 94.57353% \tVal Acc: 95.6470579%\n",
      "Epoch: 374\tTrain Loss: 0.3761222 \tVal Loss:0.8599491 \tTrain Acc: 89.75% \tVal Acc: 78.7058818%\n",
      "Epoch: 375\tTrain Loss: 1.0695128 \tVal Loss:0.4874606 \tTrain Acc: 72.86765% \tVal Acc: 85.8823526%\n",
      "Epoch: 376\tTrain Loss: 0.7239983 \tVal Loss:0.3249741 \tTrain Acc: 78.52941% \tVal Acc: 90.5294108%\n",
      "Epoch: 377\tTrain Loss: 0.3424770 \tVal Loss:0.1337000 \tTrain Acc: 89.08823% \tVal Acc: 95.7647043%\n",
      "Epoch: 378\tTrain Loss: 0.1750772 \tVal Loss:0.0974360 \tTrain Acc: 94.33823% \tVal Acc: 96.7647040%\n",
      "Epoch: 379\tTrain Loss: 0.1198914 \tVal Loss:0.0931987 \tTrain Acc: 96.20588% \tVal Acc: 96.8235278%\n",
      "Epoch: 380\tTrain Loss: 0.0987292 \tVal Loss:0.0817153 \tTrain Acc: 97.0147% \tVal Acc: 97.2941166%\n",
      "Epoch: 381\tTrain Loss: 0.0867667 \tVal Loss:0.0780710 \tTrain Acc: 97.44118% \tVal Acc: 97.4117637%\n",
      "Epoch: 382\tTrain Loss: 0.0752740 \tVal Loss:0.0777349 \tTrain Acc: 97.88235% \tVal Acc: 97.1764702%\n",
      "Epoch: 383\tTrain Loss: 0.0709522 \tVal Loss:0.0866546 \tTrain Acc: 97.85294% \tVal Acc: 97.1176463%\n",
      "Epoch: 384\tTrain Loss: 0.0635198 \tVal Loss:0.0838777 \tTrain Acc: 98.54412% \tVal Acc: 97.3529398%\n",
      "Epoch: 385\tTrain Loss: 0.0594053 \tVal Loss:0.0929565 \tTrain Acc: 98.32353% \tVal Acc: 97.6470572%\n",
      "Epoch: 386\tTrain Loss: 0.0581219 \tVal Loss:0.0926849 \tTrain Acc: 98.38235% \tVal Acc: 97.3529398%\n",
      "Epoch: 387\tTrain Loss: 0.0635900 \tVal Loss:0.0827449 \tTrain Acc: 98.32353% \tVal Acc: 97.8235286%\n",
      "Epoch: 388\tTrain Loss: 0.0616016 \tVal Loss:0.0911330 \tTrain Acc: 98.10294% \tVal Acc: 97.3529404%\n",
      "Epoch: 389\tTrain Loss: 0.0495239 \tVal Loss:0.0821171 \tTrain Acc: 98.63235% \tVal Acc: 97.5294107%\n",
      "Epoch: 390\tTrain Loss: 0.0588852 \tVal Loss:0.0816113 \tTrain Acc: 98.48529% \tVal Acc: 97.7647048%\n",
      "Epoch: 391\tTrain Loss: 0.0553736 \tVal Loss:0.0762959 \tTrain Acc: 98.5147% \tVal Acc: 97.5294101%\n",
      "Epoch: 392\tTrain Loss: 0.0471885 \tVal Loss:0.0737030 \tTrain Acc: 98.79412% \tVal Acc: 97.7058816%\n",
      "Epoch: 393\tTrain Loss: 0.0481054 \tVal Loss:0.0801540 \tTrain Acc: 98.79412% \tVal Acc: 97.5882339%\n",
      "Epoch: 394\tTrain Loss: 0.0427103 \tVal Loss:0.0779396 \tTrain Acc: 98.82353% \tVal Acc: 97.6470572%\n",
      "Epoch: 395\tTrain Loss: 0.0447465 \tVal Loss:0.0817373 \tTrain Acc: 98.70588% \tVal Acc: 97.5882345%\n",
      "Epoch: 396\tTrain Loss: 0.0466740 \tVal Loss:0.0675920 \tTrain Acc: 98.67647% \tVal Acc: 97.4117637%\n",
      "Epoch: 397\tTrain Loss: 0.0410095 \tVal Loss:0.0847132 \tTrain Acc: 98.77941% \tVal Acc: 97.4117631%\n",
      "Epoch: 398\tTrain Loss: 0.0427485 \tVal Loss:0.0846710 \tTrain Acc: 98.85294% \tVal Acc: 97.2941160%\n",
      "Epoch: 399\tTrain Loss: 0.0392629 \tVal Loss:0.0859051 \tTrain Acc: 98.98529% \tVal Acc: 97.4705869%\n",
      "Epoch: 400\tTrain Loss: 0.0384219 \tVal Loss:0.0824915 \tTrain Acc: 99.0147% \tVal Acc: 97.7058816%\n",
      "Epoch: 401\tTrain Loss: 0.0460977 \tVal Loss:0.0846275 \tTrain Acc: 98.70588% \tVal Acc: 97.5882345%\n",
      "Epoch: 402\tTrain Loss: 0.0382534 \tVal Loss:0.0963623 \tTrain Acc: 98.98529% \tVal Acc: 97.4705875%\n",
      "Epoch: 403\tTrain Loss: 0.0385665 \tVal Loss:0.0881385 \tTrain Acc: 98.97059% \tVal Acc: 97.3529404%\n",
      "Epoch: 404\tTrain Loss: 0.0387669 \tVal Loss:0.0902273 \tTrain Acc: 99.07353% \tVal Acc: 97.4117637%\n",
      "Epoch: 405\tTrain Loss: 0.0364894 \tVal Loss:0.0942559 \tTrain Acc: 99.05882% \tVal Acc: 97.5882339%\n",
      "Epoch: 406\tTrain Loss: 0.0428805 \tVal Loss:0.0840645 \tTrain Acc: 98.86765% \tVal Acc: 97.5882345%\n",
      "Epoch: 407\tTrain Loss: 0.0373415 \tVal Loss:0.0791053 \tTrain Acc: 98.85294% \tVal Acc: 97.4117637%\n",
      "Epoch: 408\tTrain Loss: 0.0363961 \tVal Loss:0.0866822 \tTrain Acc: 98.97059% \tVal Acc: 97.3529398%\n",
      "Epoch: 409\tTrain Loss: 0.0386538 \tVal Loss:0.0803026 \tTrain Acc: 98.89706% \tVal Acc: 97.7647042%\n",
      "Epoch: 410\tTrain Loss: 0.0331419 \tVal Loss:0.0854049 \tTrain Acc: 99.14706% \tVal Acc: 97.4117631%\n",
      "Epoch: 411\tTrain Loss: 0.0365724 \tVal Loss:0.0750553 \tTrain Acc: 98.98529% \tVal Acc: 97.9411757%\n",
      "Epoch: 412\tTrain Loss: 0.0322684 \tVal Loss:0.0730006 \tTrain Acc: 99.17647% \tVal Acc: 97.9411757%\n",
      "Epoch: 413\tTrain Loss: 0.0391784 \tVal Loss:0.0675678 \tTrain Acc: 98.85294% \tVal Acc: 97.9411757%\n",
      "Epoch: 414\tTrain Loss: 0.0391622 \tVal Loss:0.0753764 \tTrain Acc: 98.95588% \tVal Acc: 97.3529404%\n",
      "Epoch: 415\tTrain Loss: 0.0350632 \tVal Loss:0.0847556 \tTrain Acc: 99.08823% \tVal Acc: 97.5294107%\n",
      "Epoch: 416\tTrain Loss: 0.0328199 \tVal Loss:0.0800061 \tTrain Acc: 99.16176% \tVal Acc: 97.4117637%\n",
      "Epoch: 417\tTrain Loss: 0.0324928 \tVal Loss:0.0817360 \tTrain Acc: 99.14706% \tVal Acc: 97.5294113%\n",
      "Epoch: 418\tTrain Loss: 0.0370095 \tVal Loss:0.0763736 \tTrain Acc: 98.98529% \tVal Acc: 97.5882334%\n",
      "Epoch: 419\tTrain Loss: 0.0345604 \tVal Loss:0.0782213 \tTrain Acc: 99.02941% \tVal Acc: 97.7058810%\n",
      "Epoch: 420\tTrain Loss: 0.0299731 \tVal Loss:0.0983755 \tTrain Acc: 99.19118% \tVal Acc: 97.1764696%\n",
      "Epoch: 421\tTrain Loss: 0.0316185 \tVal Loss:0.0894397 \tTrain Acc: 99.05882% \tVal Acc: 97.2941172%\n",
      "Epoch: 422\tTrain Loss: 0.0560420 \tVal Loss:0.1375972 \tTrain Acc: 98.36765% \tVal Acc: 95.9999985%\n",
      "Epoch: 423\tTrain Loss: 0.8269521 \tVal Loss:1.0689274 \tTrain Acc: 80.11765% \tVal Acc: 72.7058822%\n",
      "Epoch: 424\tTrain Loss: 0.8310996 \tVal Loss:0.3136916 \tTrain Acc: 77.82353% \tVal Acc: 89.8235285%\n",
      "Epoch: 425\tTrain Loss: 0.3291461 \tVal Loss:0.1891684 \tTrain Acc: 89.52941% \tVal Acc: 94.2352927%\n",
      "Epoch: 426\tTrain Loss: 0.1729652 \tVal Loss:0.1304454 \tTrain Acc: 94.20588% \tVal Acc: 96.7647040%\n",
      "Epoch: 427\tTrain Loss: 0.1048638 \tVal Loss:0.0809399 \tTrain Acc: 96.77941% \tVal Acc: 97.6470578%\n",
      "Epoch: 428\tTrain Loss: 0.0865234 \tVal Loss:0.0867225 \tTrain Acc: 97.44118% \tVal Acc: 97.6470578%\n",
      "Epoch: 429\tTrain Loss: 0.0746260 \tVal Loss:0.0951659 \tTrain Acc: 97.77941% \tVal Acc: 97.1176463%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 430\tTrain Loss: 0.0720854 \tVal Loss:0.0978017 \tTrain Acc: 97.80882% \tVal Acc: 97.1764690%\n",
      "Epoch: 431\tTrain Loss: 0.0621757 \tVal Loss:0.0808729 \tTrain Acc: 98.32353% \tVal Acc: 97.2941166%\n",
      "Epoch: 432\tTrain Loss: 0.0576547 \tVal Loss:0.0804682 \tTrain Acc: 98.45588% \tVal Acc: 97.8235286%\n",
      "Epoch: 433\tTrain Loss: 0.0556770 \tVal Loss:0.0954394 \tTrain Acc: 98.52941% \tVal Acc: 97.2352928%\n",
      "Epoch: 434\tTrain Loss: 0.0514082 \tVal Loss:0.0999173 \tTrain Acc: 98.58823% \tVal Acc: 97.1176463%\n",
      "Epoch: 435\tTrain Loss: 0.0449707 \tVal Loss:0.0994262 \tTrain Acc: 98.98529% \tVal Acc: 97.2352928%\n",
      "Epoch: 436\tTrain Loss: 0.0475972 \tVal Loss:0.0995978 \tTrain Acc: 98.83823% \tVal Acc: 97.4705863%\n",
      "Epoch: 437\tTrain Loss: 0.0434320 \tVal Loss:0.1069952 \tTrain Acc: 98.79412% \tVal Acc: 97.1176451%\n",
      "Epoch: 438\tTrain Loss: 0.0411274 \tVal Loss:0.1065052 \tTrain Acc: 99.0% \tVal Acc: 97.2941166%\n",
      "Epoch: 439\tTrain Loss: 0.0378271 \tVal Loss:0.1105581 \tTrain Acc: 99.04412% \tVal Acc: 97.4117637%\n",
      "Epoch: 440\tTrain Loss: 0.0443448 \tVal Loss:0.1084617 \tTrain Acc: 98.77941% \tVal Acc: 97.2352928%\n",
      "Epoch: 441\tTrain Loss: 0.0409834 \tVal Loss:0.1020342 \tTrain Acc: 99.0147% \tVal Acc: 97.3529398%\n",
      "Epoch: 442\tTrain Loss: 0.0429093 \tVal Loss:0.1030516 \tTrain Acc: 98.82353% \tVal Acc: 97.1764684%\n",
      "Epoch: 443\tTrain Loss: 0.0357096 \tVal Loss:0.0973970 \tTrain Acc: 99.0147% \tVal Acc: 97.2941160%\n",
      "Epoch: 444\tTrain Loss: 0.0389330 \tVal Loss:0.1101360 \tTrain Acc: 99.0147% \tVal Acc: 97.2352916%\n",
      "Epoch: 445\tTrain Loss: 0.0329883 \tVal Loss:0.0945625 \tTrain Acc: 99.22059% \tVal Acc: 97.4705863%\n",
      "Epoch: 446\tTrain Loss: 0.0377297 \tVal Loss:0.1156748 \tTrain Acc: 99.10294% \tVal Acc: 97.1176457%\n",
      "Epoch: 447\tTrain Loss: 0.0341212 \tVal Loss:0.1033075 \tTrain Acc: 99.08823% \tVal Acc: 97.4705875%\n",
      "Epoch: 448\tTrain Loss: 0.0392575 \tVal Loss:0.1046309 \tTrain Acc: 98.94117% \tVal Acc: 97.4117637%\n",
      "Epoch: 449\tTrain Loss: 0.0380058 \tVal Loss:0.1017491 \tTrain Acc: 99.10294% \tVal Acc: 97.5294101%\n",
      "Epoch: 450\tTrain Loss: 0.0348217 \tVal Loss:0.0990633 \tTrain Acc: 99.19118% \tVal Acc: 97.2941160%\n",
      "Epoch: 451\tTrain Loss: 0.0343381 \tVal Loss:0.0938765 \tTrain Acc: 99.05882% \tVal Acc: 97.4705875%\n",
      "Epoch: 452\tTrain Loss: 0.0334611 \tVal Loss:0.0969519 \tTrain Acc: 99.08823% \tVal Acc: 97.2352934%\n",
      "Epoch: 453\tTrain Loss: 0.0333791 \tVal Loss:0.0977983 \tTrain Acc: 99.0147% \tVal Acc: 97.4117631%\n",
      "Epoch: 454\tTrain Loss: 0.0332088 \tVal Loss:0.0884886 \tTrain Acc: 99.10294% \tVal Acc: 97.1764690%\n",
      "Epoch: 455\tTrain Loss: 0.0305407 \tVal Loss:0.0873789 \tTrain Acc: 99.22059% \tVal Acc: 97.4705869%\n",
      "Epoch: 456\tTrain Loss: 0.0327202 \tVal Loss:0.0817413 \tTrain Acc: 99.19118% \tVal Acc: 97.4117631%\n",
      "Epoch: 457\tTrain Loss: 0.0294427 \tVal Loss:0.0852278 \tTrain Acc: 99.23529% \tVal Acc: 97.7647042%\n",
      "Epoch: 458\tTrain Loss: 0.0339451 \tVal Loss:0.0897130 \tTrain Acc: 99.04412% \tVal Acc: 97.4117631%\n",
      "Epoch: 459\tTrain Loss: 0.0310970 \tVal Loss:0.0884934 \tTrain Acc: 99.08823% \tVal Acc: 97.5294107%\n",
      "Epoch: 460\tTrain Loss: 0.0293421 \tVal Loss:0.0935619 \tTrain Acc: 99.22059% \tVal Acc: 97.2941160%\n",
      "Epoch: 461\tTrain Loss: 0.0292906 \tVal Loss:0.0952921 \tTrain Acc: 99.29412% \tVal Acc: 97.7058816%\n",
      "Epoch: 462\tTrain Loss: 0.0320037 \tVal Loss:0.0894514 \tTrain Acc: 99.07353% \tVal Acc: 97.6470584%\n",
      "Epoch: 463\tTrain Loss: 0.0303267 \tVal Loss:0.0974780 \tTrain Acc: 99.22059% \tVal Acc: 97.4705869%\n",
      "Epoch: 464\tTrain Loss: 0.0371745 \tVal Loss:0.1053187 \tTrain Acc: 98.97059% \tVal Acc: 97.2941166%\n",
      "Epoch: 465\tTrain Loss: 0.0385577 \tVal Loss:0.0781511 \tTrain Acc: 98.98529% \tVal Acc: 97.6470572%\n",
      "Epoch: 466\tTrain Loss: 0.0426343 \tVal Loss:0.0897559 \tTrain Acc: 99.0147% \tVal Acc: 97.4117637%\n",
      "Epoch: 467\tTrain Loss: 0.0437545 \tVal Loss:0.0727747 \tTrain Acc: 98.80882% \tVal Acc: 97.8235281%\n",
      "Epoch: 468\tTrain Loss: 0.0451065 \tVal Loss:0.0970917 \tTrain Acc: 98.55882% \tVal Acc: 97.3529398%\n",
      "Epoch: 469\tTrain Loss: 0.0406144 \tVal Loss:0.1174901 \tTrain Acc: 98.69118% \tVal Acc: 96.7647046%\n",
      "Epoch: 470\tTrain Loss: 0.0478154 \tVal Loss:0.1388076 \tTrain Acc: 98.63235% \tVal Acc: 96.4117640%\n",
      "Epoch: 471\tTrain Loss: 0.0571620 \tVal Loss:0.0673028 \tTrain Acc: 98.48529% \tVal Acc: 97.7058810%\n",
      "Epoch: 472\tTrain Loss: 0.0540862 \tVal Loss:0.1156385 \tTrain Acc: 98.47059% \tVal Acc: 96.9999981%\n",
      "Epoch: 473\tTrain Loss: 0.0546997 \tVal Loss:0.1016635 \tTrain Acc: 98.54412% \tVal Acc: 96.9999987%\n",
      "Epoch: 474\tTrain Loss: 0.0734458 \tVal Loss:0.1664096 \tTrain Acc: 97.55882% \tVal Acc: 95.3529406%\n",
      "Epoch: 475\tTrain Loss: 0.1337499 \tVal Loss:0.1909821 \tTrain Acc: 96.0% \tVal Acc: 94.9999988%\n",
      "Epoch: 476\tTrain Loss: 0.1312809 \tVal Loss:0.2311134 \tTrain Acc: 96.0147% \tVal Acc: 93.9999992%\n",
      "Epoch: 477\tTrain Loss: 0.1266920 \tVal Loss:0.1374103 \tTrain Acc: 96.13235% \tVal Acc: 96.1176455%\n",
      "Epoch: 478\tTrain Loss: 0.2712410 \tVal Loss:0.3060478 \tTrain Acc: 92.30882% \tVal Acc: 91.6470581%\n",
      "Epoch: 479\tTrain Loss: 0.5075172 \tVal Loss:0.4844553 \tTrain Acc: 86.44118% \tVal Acc: 88.0588228%\n",
      "Epoch: 480\tTrain Loss: 0.4155146 \tVal Loss:0.2464944 \tTrain Acc: 87.92647% \tVal Acc: 92.7647054%\n",
      "Epoch: 481\tTrain Loss: 0.2289225 \tVal Loss:0.1715817 \tTrain Acc: 92.85294% \tVal Acc: 94.8823518%\n",
      "Epoch: 482\tTrain Loss: 0.1357368 \tVal Loss:0.0851346 \tTrain Acc: 95.73529% \tVal Acc: 97.4705863%\n",
      "Epoch: 483\tTrain Loss: 0.0891570 \tVal Loss:0.0764491 \tTrain Acc: 97.41176% \tVal Acc: 97.7058810%\n",
      "Epoch: 484\tTrain Loss: 0.0724899 \tVal Loss:0.0975601 \tTrain Acc: 97.91176% \tVal Acc: 97.2941160%\n",
      "Epoch: 485\tTrain Loss: 0.0606850 \tVal Loss:0.0943879 \tTrain Acc: 98.2647% \tVal Acc: 97.3529404%\n",
      "Epoch: 486\tTrain Loss: 0.0520867 \tVal Loss:0.0924962 \tTrain Acc: 98.69118% \tVal Acc: 97.4117637%\n",
      "Epoch: 487\tTrain Loss: 0.0489428 \tVal Loss:0.1019563 \tTrain Acc: 98.67647% \tVal Acc: 97.1764690%\n",
      "Epoch: 488\tTrain Loss: 0.0419350 \tVal Loss:0.0985954 \tTrain Acc: 98.77941% \tVal Acc: 97.5294107%\n",
      "Epoch: 489\tTrain Loss: 0.0435407 \tVal Loss:0.1168994 \tTrain Acc: 98.77941% \tVal Acc: 97.2352934%\n",
      "Epoch: 490\tTrain Loss: 0.0403759 \tVal Loss:0.0907428 \tTrain Acc: 98.98529% \tVal Acc: 97.2941166%\n",
      "Epoch: 491\tTrain Loss: 0.0385403 \tVal Loss:0.0792072 \tTrain Acc: 99.14706% \tVal Acc: 97.5294101%\n",
      "Epoch: 492\tTrain Loss: 0.0378589 \tVal Loss:0.0941830 \tTrain Acc: 98.94118% \tVal Acc: 97.4117637%\n",
      "Epoch: 493\tTrain Loss: 0.0330934 \tVal Loss:0.0813555 \tTrain Acc: 99.13235% \tVal Acc: 97.8235281%\n",
      "Epoch: 494\tTrain Loss: 0.0348418 \tVal Loss:0.0760628 \tTrain Acc: 99.17647% \tVal Acc: 97.8235286%\n",
      "Epoch: 495\tTrain Loss: 0.0404843 \tVal Loss:0.0933719 \tTrain Acc: 98.82353% \tVal Acc: 97.4117631%\n",
      "Epoch: 496\tTrain Loss: 0.0351337 \tVal Loss:0.0903216 \tTrain Acc: 99.0147% \tVal Acc: 97.4705869%\n",
      "Epoch: 497\tTrain Loss: 0.0308560 \tVal Loss:0.0765419 \tTrain Acc: 99.20588% \tVal Acc: 97.7058810%\n",
      "Epoch: 498\tTrain Loss: 0.0308174 \tVal Loss:0.0855307 \tTrain Acc: 99.36765% \tVal Acc: 97.6470572%\n",
      "Epoch: 499\tTrain Loss: 0.0338880 \tVal Loss:0.0981292 \tTrain Acc: 99.05882% \tVal Acc: 97.2352934%\n"
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    \n",
    "    hidden = model.hidden_init(train_batch_size)    \n",
    "    #print('hidden[0].shape:- ',hidden[0].shape)\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        '''\n",
    "        Creating new variables for the hidden state, otherwise\n",
    "        we'd backprop through the entire training history\n",
    "        '''\n",
    "        h = tuple([each.data for each in hidden])\n",
    "        \n",
    "\n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "       \n",
    "        # get the output from the model\n",
    "        output, h = model.forward(inputs, h,train_batch_size)\n",
    "        #print('OUTPUT', output)\n",
    "        \n",
    "        \n",
    "        #print('Labels Shape :-', (torch.max(labels, 1)[1]).shape)\n",
    "    \n",
    "        # calculate the loss and perform backprop\n",
    "        #print('Labels Long :-', labels.long())\n",
    "        loss = criterion(output,labels.long())\n",
    "        #print('LOSS IS :-', loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #calculate training accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        logging.debug(' top probab {} top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        #print(train_loss)\n",
    "              \n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "                \n",
    "        val_h = tuple([each.data for each in hidden])\n",
    "        \n",
    "        output, hidden = model.forward(inputs, val_h,val_batch_size)\n",
    "       \n",
    "        loss = criterion(output,labels.long())\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        #calculate validation accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        \n",
    "        #logging.debug(output)\n",
    "        #logging.debug('VALIDATION top probab {} VALIDATION top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        #print('Top Class:- ',top_class)\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        #print('Equals:- ', equals)\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    #Averaging losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    val_accuracy = val_accuracy/len(val_loader)\n",
    "    train_accuracy = train_accuracy/len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}\\tTrain Loss: {:.7f} \\tVal Loss:{:.7f} \\tTrain Acc: {:.7}% \\tVal Acc: {:.7f}%'.format(e, train_loss, val_loss, train_accuracy*100,val_accuracy*100))\n",
    "    \n",
    "    #saving the model if validation loss is decreased\n",
    "    if val_loss <= min_val_loss:\n",
    "        print('Validation Loss decreased from {:6f} to {:6f}, saving the model weights'.format(min_val_loss, val_loss))\n",
    "        torch.save(model.state_dict(), 'lstm_state_256_hidden_size.pt')\n",
    "        min_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Music Genaration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load weights\n",
    "test_model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "test_model.load_state_dict(torch.load('lstm_state_256_hidden_size.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population database\n",
    "#testing_data = np.ones(200)*0\n",
    "testing_data = list(range(50,90))\n",
    "testing_data.extend(testing_data[::-1])\n",
    "testing_data = np.asarray(testing_data)\n",
    "testing_data = testing_data.reshape(testing_data.shape[0],1)\n",
    "\n",
    "initial_seq = [network_input[0][1:].cpu().numpy().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting\n",
    "def prediction_with_influence(influence,int2note,initial_seq, max_note, test_batch_size = 1):\n",
    "\n",
    "    predicted_notes = []\n",
    "    initial_seq[0].extend([[0]]*len(testing_data))\n",
    "    test_seq = torch.Tensor(initial_seq).cuda()\n",
    "    \n",
    "    for i in range(len(influence)):\n",
    "        \n",
    "        test_seq[0][sequence_length - 1 + i][0] = float(influence[i])\n",
    "        \n",
    "        test_slice = test_seq[0][i : i + sequence_length]        \n",
    "        test_slice = test_slice.view(1, test_slice.shape[0], test_slice.shape[1])\n",
    "        \n",
    "        test_hidden = model.hidden_init(test_batch_size)\n",
    "        test_output,_ = model.forward(test_slice, test_hidden, test_batch_size)\n",
    "    \n",
    "        top_p, top_class = test_output.topk(1,dim =1)\n",
    "        test_seq[0][sequence_length - 1 + i][0] = int2note[top_class.item()]/max_note\n",
    "        \n",
    "        predicted_notes.append(int2note[top_class.item()])\n",
    "        \n",
    "    return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_notes_lst = prediction_with_influence(testing_data,int_to_note,initial_seq, max_midi_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicted_notes_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22f23fa6b00>]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 369
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(list(np.ones(200)*50))\n",
    "#plt.plot(list(np.ones(20)*50))\n",
    "plt.plot(testing_data)\n",
    "plt.plot(predicted_notes_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({72: 5,\n",
       "         74: 37,\n",
       "         76: 9,\n",
       "         81: 16,\n",
       "         79: 21,\n",
       "         69: 66,\n",
       "         67: 24,\n",
       "         71: 10,\n",
       "         66: 1,\n",
       "         62: 4,\n",
       "         73: 2,\n",
       "         78: 2,\n",
       "         64: 2,\n",
       "         83: 1})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(predicted_notes_lst)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
