{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.tensorboard as tb\n",
    "import preprocessing\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static parameters\n",
    "train_batch_size = 170\n",
    "val_batch_size = 170\n",
    "input_size = 38\n",
    "hidden_size = 128\n",
    "num_layer = 2\n",
    "output_size = 38\n",
    "clip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from preprocessing.py\n",
    "dataset_path = os.path.join(os.path.abspath('..'),'Dataset\\\\Clementi dataset\\\\Clementi dataset' )\n",
    "network_input, network_output = preprocessing.preprocess_notes(dataset_path)\n",
    "network_input, network_output = network_input.cuda(), network_output.cuda()\n",
    "#print(network_input)\n",
    "#print(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata is highly unbalanced\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data is highly unbalanced\n",
    "'''\n",
    "#sns.distplot(torch.tensor(network_output).cpu())\n",
    "#xx = pd.DataFrame(torch.tensor(network_output).cpu())\n",
    "#xx.groupby(0).size().to_frame(name='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8500, 50, 38])\n",
      "torch.Size([8500])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "to make batch of equal sizes\n",
    "Quick Fix\n",
    "'''\n",
    "network_input = network_input[: -117]\n",
    "network_output = network_output[: -117]\n",
    "\n",
    "print(network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create Stacked LSTM model\n",
    "'''\n",
    "class Stacked_LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True, dropout = 0.5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden,batch_size):\n",
    "        \n",
    "        output, hidden = self.lstm(x, hidden)        \n",
    "        #print(output.shape)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        output = output.contiguous().view(-1, self.hidden_size)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        #print('Linear Output :-',output.shape)\n",
    "        \n",
    "        #output = F.softmax(output, dim = 1)\n",
    "        #print('SOFTMAX OUTPUT :--', output)\n",
    "        \n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1)\n",
    "        #print('Reshape to batch size first :-',output.shape)\n",
    "        \n",
    "        output = output[:, -x.shape[2]:] # get last batch of labels\n",
    "        #print('Final Output :-',output)\n",
    "\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def hidden_init(self,batch_size):\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
    "          weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "#initialize the weights of LSTM using Xavier initialization    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Divide the dataset into train/val \n",
    "'''\n",
    "train_size = 0.8\n",
    "indices = list(range(len(network_input)))\n",
    "split = int(np.floor(train_size*len(network_input)))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SequentialSampler(train_idx)\n",
    "val_sampler = SequentialSampler(val_idx)\n",
    "\n",
    "dataset = TensorDataset(network_input,network_output)\n",
    "train_loader = DataLoader(dataset, batch_size= train_batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size= val_batch_size,sampler= val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm): LSTM(38, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optimizer\n",
    "\n",
    "model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer.AdamW(model.parameters())\n",
    "#optimizer = optimizer.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "#make sure to transfer model to GPU after initializing optimizer\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "C:\\Users\\utkar\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\ipykernel_launcher.py:63: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 3.3189146 \tVal Loss:3.0111136 \tTrain Acc: 8.985294% \tVal Acc: 11.4117651%\n",
      "Epoch: 1\tTrain Loss: 3.0822378 \tVal Loss:2.9015777 \tTrain Acc: 9.117647% \tVal Acc: 13.1764709%\n",
      "Epoch: 2\tTrain Loss: 2.9454238 \tVal Loss:2.7799143 \tTrain Acc: 11.89706% \tVal Acc: 12.3529414%\n",
      "Epoch: 3\tTrain Loss: 2.8156076 \tVal Loss:2.6406058 \tTrain Acc: 13.72059% \tVal Acc: 15.1176476%\n",
      "Epoch: 4\tTrain Loss: 2.7097604 \tVal Loss:2.5547369 \tTrain Acc: 15.20588% \tVal Acc: 16.4117652%\n",
      "Epoch: 5\tTrain Loss: 2.6227233 \tVal Loss:2.4986680 \tTrain Acc: 15.72059% \tVal Acc: 14.7058827%\n",
      "Epoch: 6\tTrain Loss: 2.5657228 \tVal Loss:2.4337704 \tTrain Acc: 16.61765% \tVal Acc: 17.1176475%\n",
      "Epoch: 7\tTrain Loss: 2.5113175 \tVal Loss:2.4043114 \tTrain Acc: 17.95588% \tVal Acc: 18.2352945%\n",
      "Epoch: 8\tTrain Loss: 2.4833560 \tVal Loss:2.3491976 \tTrain Acc: 19.14706% \tVal Acc: 20.0588240%\n",
      "Epoch: 9\tTrain Loss: 2.4363530 \tVal Loss:2.3246825 \tTrain Acc: 20.55882% \tVal Acc: 23.0588243%\n",
      "Epoch: 10\tTrain Loss: 2.4142076 \tVal Loss:2.2550890 \tTrain Acc: 21.13235% \tVal Acc: 26.1176479%\n",
      "Epoch: 11\tTrain Loss: 2.3651524 \tVal Loss:2.2124434 \tTrain Acc: 23.23529% \tVal Acc: 29.7058833%\n",
      "Epoch: 12\tTrain Loss: 2.3342113 \tVal Loss:2.1572484 \tTrain Acc: 25.22059% \tVal Acc: 32.8823537%\n",
      "Epoch: 13\tTrain Loss: 2.2731750 \tVal Loss:2.0997744 \tTrain Acc: 26.67647% \tVal Acc: 34.9411772%\n",
      "Epoch: 14\tTrain Loss: 2.2285354 \tVal Loss:2.0367272 \tTrain Acc: 29.60294% \tVal Acc: 37.4705890%\n",
      "Epoch: 15\tTrain Loss: 2.1769733 \tVal Loss:1.9899098 \tTrain Acc: 31.05882% \tVal Acc: 41.4705890%\n",
      "Epoch: 16\tTrain Loss: 2.1302333 \tVal Loss:1.9407124 \tTrain Acc: 32.47059% \tVal Acc: 42.1764711%\n",
      "Epoch: 17\tTrain Loss: 2.0807878 \tVal Loss:1.9117254 \tTrain Acc: 34.91177% \tVal Acc: 42.8823537%\n",
      "Epoch: 18\tTrain Loss: 2.0428626 \tVal Loss:1.8654877 \tTrain Acc: 35.70588% \tVal Acc: 44.5294124%\n",
      "Epoch: 19\tTrain Loss: 2.0128437 \tVal Loss:1.7996031 \tTrain Acc: 38.07353% \tVal Acc: 44.8235297%\n",
      "Epoch: 20\tTrain Loss: 1.9580006 \tVal Loss:1.7561547 \tTrain Acc: 39.07353% \tVal Acc: 47.7647060%\n",
      "Epoch: 21\tTrain Loss: 1.9375978 \tVal Loss:1.7362182 \tTrain Acc: 39.47059% \tVal Acc: 47.4705893%\n",
      "Epoch: 22\tTrain Loss: 1.8982214 \tVal Loss:1.7321572 \tTrain Acc: 40.57353% \tVal Acc: 48.1176481%\n",
      "Epoch: 23\tTrain Loss: 1.8716580 \tVal Loss:1.6865085 \tTrain Acc: 42.02941% \tVal Acc: 49.2941186%\n",
      "Epoch: 24\tTrain Loss: 1.8451441 \tVal Loss:1.6452849 \tTrain Acc: 42.67647% \tVal Acc: 49.6470600%\n",
      "Epoch: 25\tTrain Loss: 1.8093295 \tVal Loss:1.6172073 \tTrain Acc: 44.30882% \tVal Acc: 51.5294123%\n",
      "Epoch: 26\tTrain Loss: 1.7803079 \tVal Loss:1.6573133 \tTrain Acc: 45.27941% \tVal Acc: 50.6470597%\n",
      "Epoch: 27\tTrain Loss: 1.7704765 \tVal Loss:1.6035299 \tTrain Acc: 45.61765% \tVal Acc: 52.8823543%\n",
      "Epoch: 28\tTrain Loss: 1.7348789 \tVal Loss:1.5717193 \tTrain Acc: 46.13235% \tVal Acc: 53.6470598%\n",
      "Epoch: 29\tTrain Loss: 1.7031952 \tVal Loss:1.5669273 \tTrain Acc: 46.79412% \tVal Acc: 53.5882363%\n",
      "Epoch: 30\tTrain Loss: 1.6818265 \tVal Loss:1.5343736 \tTrain Acc: 47.63235% \tVal Acc: 54.1176489%\n",
      "Epoch: 31\tTrain Loss: 1.6691649 \tVal Loss:1.4923412 \tTrain Acc: 47.89706% \tVal Acc: 56.0588253%\n",
      "Epoch: 32\tTrain Loss: 1.6479463 \tVal Loss:1.4686162 \tTrain Acc: 48.70588% \tVal Acc: 54.8235297%\n",
      "Epoch: 33\tTrain Loss: 1.6209529 \tVal Loss:1.4400085 \tTrain Acc: 50.27941% \tVal Acc: 56.2941188%\n",
      "Epoch: 34\tTrain Loss: 1.5771192 \tVal Loss:1.4244245 \tTrain Acc: 51.26471% \tVal Acc: 57.4117664%\n",
      "Epoch: 35\tTrain Loss: 1.5533902 \tVal Loss:1.3968666 \tTrain Acc: 52.20588% \tVal Acc: 58.0588242%\n",
      "Epoch: 36\tTrain Loss: 1.5529652 \tVal Loss:1.3827243 \tTrain Acc: 52.02941% \tVal Acc: 56.4117661%\n",
      "Epoch: 37\tTrain Loss: 1.5117616 \tVal Loss:1.3403162 \tTrain Acc: 53.17647% \tVal Acc: 58.7058836%\n",
      "Epoch: 38\tTrain Loss: 1.5028453 \tVal Loss:1.3338082 \tTrain Acc: 52.92647% \tVal Acc: 58.8823530%\n",
      "Epoch: 39\tTrain Loss: 1.4653774 \tVal Loss:1.3005942 \tTrain Acc: 54.42647% \tVal Acc: 59.1176477%\n",
      "Epoch: 40\tTrain Loss: 1.4373298 \tVal Loss:1.2637140 \tTrain Acc: 55.26471% \tVal Acc: 59.8823532%\n",
      "Epoch: 41\tTrain Loss: 1.4280410 \tVal Loss:1.2644991 \tTrain Acc: 55.26471% \tVal Acc: 59.8235300%\n",
      "Epoch: 42\tTrain Loss: 1.4082758 \tVal Loss:1.2263398 \tTrain Acc: 56.07353% \tVal Acc: 63.1176478%\n",
      "Epoch: 43\tTrain Loss: 1.3871926 \tVal Loss:1.1971150 \tTrain Acc: 56.44118% \tVal Acc: 63.2941186%\n",
      "Epoch: 44\tTrain Loss: 1.3801918 \tVal Loss:1.1613792 \tTrain Acc: 56.82353% \tVal Acc: 64.4705892%\n",
      "Epoch: 45\tTrain Loss: 1.3646748 \tVal Loss:1.1722931 \tTrain Acc: 58.14706% \tVal Acc: 63.2941186%\n",
      "Epoch: 46\tTrain Loss: 1.3333864 \tVal Loss:1.1497742 \tTrain Acc: 58.70588% \tVal Acc: 63.0588239%\n",
      "Epoch: 47\tTrain Loss: 1.3150394 \tVal Loss:1.1326564 \tTrain Acc: 58.98529% \tVal Acc: 65.8235294%\n",
      "Epoch: 48\tTrain Loss: 1.2684602 \tVal Loss:1.1467370 \tTrain Acc: 59.75% \tVal Acc: 63.5882360%\n",
      "Epoch: 49\tTrain Loss: 1.2604699 \tVal Loss:1.0472753 \tTrain Acc: 61.29412% \tVal Acc: 66.5882361%\n",
      "Epoch: 50\tTrain Loss: 1.2221481 \tVal Loss:1.0442092 \tTrain Acc: 61.60294% \tVal Acc: 66.1176467%\n",
      "Epoch: 51\tTrain Loss: 1.2038310 \tVal Loss:1.0062568 \tTrain Acc: 62.5% \tVal Acc: 67.8823543%\n",
      "Epoch: 52\tTrain Loss: 1.1720817 \tVal Loss:1.0059004 \tTrain Acc: 62.25% \tVal Acc: 68.0000007%\n",
      "Epoch: 53\tTrain Loss: 1.1644393 \tVal Loss:0.9743954 \tTrain Acc: 63.55882% \tVal Acc: 69.1176480%\n",
      "Epoch: 54\tTrain Loss: 1.1283186 \tVal Loss:0.9141021 \tTrain Acc: 64.19118% \tVal Acc: 71.5294117%\n",
      "Epoch: 55\tTrain Loss: 1.0885781 \tVal Loss:0.9132757 \tTrain Acc: 66.16176% \tVal Acc: 70.9411776%\n",
      "Epoch: 56\tTrain Loss: 1.0988779 \tVal Loss:0.8833743 \tTrain Acc: 65.41177% \tVal Acc: 74.3529415%\n",
      "Epoch: 57\tTrain Loss: 1.0729274 \tVal Loss:0.8601863 \tTrain Acc: 66.86765% \tVal Acc: 73.6470592%\n",
      "Epoch: 58\tTrain Loss: 1.0438238 \tVal Loss:0.8568528 \tTrain Acc: 66.58824% \tVal Acc: 73.0588245%\n",
      "Epoch: 59\tTrain Loss: 1.0265292 \tVal Loss:0.7866539 \tTrain Acc: 67.42647% \tVal Acc: 76.1764705%\n",
      "Epoch: 60\tTrain Loss: 1.0232550 \tVal Loss:0.7752657 \tTrain Acc: 67.44118% \tVal Acc: 75.6470591%\n",
      "Epoch: 61\tTrain Loss: 0.9864924 \tVal Loss:0.7477358 \tTrain Acc: 69.19118% \tVal Acc: 76.5294105%\n",
      "Epoch: 62\tTrain Loss: 0.9756495 \tVal Loss:0.7225307 \tTrain Acc: 69.30882% \tVal Acc: 77.5882351%\n",
      "Epoch: 63\tTrain Loss: 0.9476502 \tVal Loss:0.7138762 \tTrain Acc: 70.17647% \tVal Acc: 77.0588231%\n",
      "Epoch: 64\tTrain Loss: 0.9482952 \tVal Loss:0.6758759 \tTrain Acc: 70.05882% \tVal Acc: 79.1176462%\n",
      "Epoch: 65\tTrain Loss: 0.9188422 \tVal Loss:0.6512759 \tTrain Acc: 70.77941% \tVal Acc: 81.0588229%\n",
      "Epoch: 66\tTrain Loss: 0.8869712 \tVal Loss:0.6232633 \tTrain Acc: 72.35294% \tVal Acc: 81.3529414%\n",
      "Epoch: 67\tTrain Loss: 0.8695893 \tVal Loss:0.6240596 \tTrain Acc: 72.10294% \tVal Acc: 81.4117646%\n",
      "Epoch: 68\tTrain Loss: 0.8744962 \tVal Loss:0.6298150 \tTrain Acc: 73.08824% \tVal Acc: 80.3529406%\n",
      "Epoch: 69\tTrain Loss: 0.8389592 \tVal Loss:0.6027762 \tTrain Acc: 73.27941% \tVal Acc: 81.7058825%\n",
      "Epoch: 70\tTrain Loss: 0.8213992 \tVal Loss:0.5731161 \tTrain Acc: 74.55882% \tVal Acc: 82.5294107%\n",
      "Epoch: 71\tTrain Loss: 0.8039699 \tVal Loss:0.5629356 \tTrain Acc: 74.54412% \tVal Acc: 82.9999995%\n",
      "Epoch: 72\tTrain Loss: 0.7977004 \tVal Loss:0.5271169 \tTrain Acc: 74.70588% \tVal Acc: 84.9999994%\n",
      "Epoch: 73\tTrain Loss: 0.7945559 \tVal Loss:0.5295713 \tTrain Acc: 74.76471% \tVal Acc: 84.5294118%\n",
      "Epoch: 74\tTrain Loss: 0.7707365 \tVal Loss:0.4976845 \tTrain Acc: 75.33824% \tVal Acc: 86.0588223%\n",
      "Epoch: 75\tTrain Loss: 0.7476918 \tVal Loss:0.5079231 \tTrain Acc: 76.66176% \tVal Acc: 85.0588238%\n",
      "Epoch: 76\tTrain Loss: 0.7476082 \tVal Loss:0.4865191 \tTrain Acc: 75.83824% \tVal Acc: 86.4117646%\n",
      "Epoch: 77\tTrain Loss: 0.7423999 \tVal Loss:0.4777526 \tTrain Acc: 76.38235% \tVal Acc: 85.6470579%\n",
      "Epoch: 78\tTrain Loss: 0.7073422 \tVal Loss:0.4295913 \tTrain Acc: 77.36765% \tVal Acc: 87.5294113%\n",
      "Epoch: 79\tTrain Loss: 0.6952709 \tVal Loss:0.4396962 \tTrain Acc: 77.91176% \tVal Acc: 87.2941166%\n",
      "Epoch: 80\tTrain Loss: 0.6899803 \tVal Loss:0.4194252 \tTrain Acc: 77.75% \tVal Acc: 88.0588233%\n",
      "Epoch: 81\tTrain Loss: 0.6837054 \tVal Loss:0.3994583 \tTrain Acc: 78.01471% \tVal Acc: 89.2941165%\n",
      "Epoch: 82\tTrain Loss: 0.6572220 \tVal Loss:0.3922882 \tTrain Acc: 79.0% \tVal Acc: 88.6470586%\n",
      "Epoch: 83\tTrain Loss: 0.6547164 \tVal Loss:0.4001814 \tTrain Acc: 79.0% \tVal Acc: 87.8235292%\n",
      "Epoch: 84\tTrain Loss: 0.6508588 \tVal Loss:0.3704944 \tTrain Acc: 78.69118% \tVal Acc: 89.7058815%\n",
      "Epoch: 85\tTrain Loss: 0.6323046 \tVal Loss:0.3591707 \tTrain Acc: 79.70588% \tVal Acc: 89.0588230%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 86\tTrain Loss: 0.6248749 \tVal Loss:0.3505402 \tTrain Acc: 80.23529% \tVal Acc: 90.2941173%\n",
      "Epoch: 87\tTrain Loss: 0.6132146 \tVal Loss:0.3720383 \tTrain Acc: 80.76471% \tVal Acc: 88.4705877%\n",
      "Epoch: 88\tTrain Loss: 0.6009349 \tVal Loss:0.3334596 \tTrain Acc: 81.32353% \tVal Acc: 90.4705870%\n",
      "Epoch: 89\tTrain Loss: 0.5866326 \tVal Loss:0.3221521 \tTrain Acc: 81.91176% \tVal Acc: 91.8823516%\n",
      "Epoch: 90\tTrain Loss: 0.5728702 \tVal Loss:0.3039346 \tTrain Acc: 81.66176% \tVal Acc: 92.1176463%\n",
      "Epoch: 91\tTrain Loss: 0.5574482 \tVal Loss:0.2946312 \tTrain Acc: 82.0% \tVal Acc: 91.6470575%\n",
      "Epoch: 92\tTrain Loss: 0.5505330 \tVal Loss:0.2923189 \tTrain Acc: 82.86765% \tVal Acc: 92.6470578%\n",
      "Epoch: 93\tTrain Loss: 0.5423165 \tVal Loss:0.2950038 \tTrain Acc: 83.10294% \tVal Acc: 91.8235278%\n",
      "Epoch: 94\tTrain Loss: 0.5298859 \tVal Loss:0.2772740 \tTrain Acc: 83.17647% \tVal Acc: 93.4117627%\n",
      "Epoch: 95\tTrain Loss: 0.5423090 \tVal Loss:0.2837212 \tTrain Acc: 82.60294% \tVal Acc: 92.7647048%\n",
      "Epoch: 96\tTrain Loss: 0.5201515 \tVal Loss:0.2633501 \tTrain Acc: 83.30882% \tVal Acc: 93.2941175%\n",
      "Epoch: 97\tTrain Loss: 0.5201091 \tVal Loss:0.2551905 \tTrain Acc: 83.07353% \tVal Acc: 93.1764692%\n",
      "Epoch: 98\tTrain Loss: 0.5052314 \tVal Loss:0.2534461 \tTrain Acc: 84.32353% \tVal Acc: 93.0588233%\n",
      "Epoch: 99\tTrain Loss: 0.4978976 \tVal Loss:0.2485443 \tTrain Acc: 83.79412% \tVal Acc: 93.8235283%\n",
      "Epoch: 100\tTrain Loss: 0.5053265 \tVal Loss:0.2521021 \tTrain Acc: 83.95588% \tVal Acc: 93.6470574%\n",
      "Epoch: 101\tTrain Loss: 0.4867016 \tVal Loss:0.2265189 \tTrain Acc: 84.51471% \tVal Acc: 93.9411753%\n",
      "Epoch: 102\tTrain Loss: 0.4750719 \tVal Loss:0.2161745 \tTrain Acc: 84.91176% \tVal Acc: 95.0588220%\n",
      "Epoch: 103\tTrain Loss: 0.4552474 \tVal Loss:0.2210637 \tTrain Acc: 85.32353% \tVal Acc: 93.8823515%\n",
      "Epoch: 104\tTrain Loss: 0.4683693 \tVal Loss:0.2012208 \tTrain Acc: 84.88235% \tVal Acc: 94.8823518%\n",
      "Epoch: 105\tTrain Loss: 0.4598614 \tVal Loss:0.1938897 \tTrain Acc: 85.41176% \tVal Acc: 95.7058811%\n",
      "Epoch: 106\tTrain Loss: 0.4403654 \tVal Loss:0.1977288 \tTrain Acc: 85.72059% \tVal Acc: 95.2352929%\n",
      "Epoch: 107\tTrain Loss: 0.4438449 \tVal Loss:0.2023312 \tTrain Acc: 85.75% \tVal Acc: 94.9411750%\n",
      "Epoch: 108\tTrain Loss: 0.4352652 \tVal Loss:0.1860073 \tTrain Acc: 86.44118% \tVal Acc: 95.1764691%\n",
      "Epoch: 109\tTrain Loss: 0.4223397 \tVal Loss:0.1702634 \tTrain Acc: 86.47059% \tVal Acc: 95.8235276%\n",
      "Epoch: 110\tTrain Loss: 0.4218615 \tVal Loss:0.1781634 \tTrain Acc: 86.35294% \tVal Acc: 95.7647043%\n",
      "Epoch: 111\tTrain Loss: 0.4259877 \tVal Loss:0.2040205 \tTrain Acc: 85.92647% \tVal Acc: 94.4705874%\n",
      "Epoch: 112\tTrain Loss: 0.4286108 \tVal Loss:0.1915122 \tTrain Acc: 86.35294% \tVal Acc: 95.8235288%\n",
      "Epoch: 113\tTrain Loss: 0.4215596 \tVal Loss:0.1670889 \tTrain Acc: 86.58823% \tVal Acc: 95.1176453%\n",
      "Epoch: 114\tTrain Loss: 0.4090077 \tVal Loss:0.1608949 \tTrain Acc: 86.52941% \tVal Acc: 96.3529402%\n",
      "Epoch: 115\tTrain Loss: 0.3786470 \tVal Loss:0.1691570 \tTrain Acc: 87.79412% \tVal Acc: 95.1176453%\n",
      "Epoch: 116\tTrain Loss: 0.3834599 \tVal Loss:0.1601123 \tTrain Acc: 87.67647% \tVal Acc: 95.9999996%\n",
      "Epoch: 117\tTrain Loss: 0.3795645 \tVal Loss:0.1639484 \tTrain Acc: 87.7647% \tVal Acc: 95.6470573%\n",
      "Epoch: 118\tTrain Loss: 0.3738536 \tVal Loss:0.1477868 \tTrain Acc: 87.64706% \tVal Acc: 96.5294111%\n",
      "Epoch: 119\tTrain Loss: 0.3836101 \tVal Loss:0.1592768 \tTrain Acc: 87.86765% \tVal Acc: 96.0588217%\n",
      "Epoch: 120\tTrain Loss: 0.3787420 \tVal Loss:0.1369050 \tTrain Acc: 87.76471% \tVal Acc: 96.9999993%\n",
      "Epoch: 121\tTrain Loss: 0.3604203 \tVal Loss:0.1367279 \tTrain Acc: 88.52941% \tVal Acc: 96.5294111%\n",
      "Epoch: 122\tTrain Loss: 0.3580985 \tVal Loss:0.1372202 \tTrain Acc: 88.54412% \tVal Acc: 97.0588219%\n",
      "Epoch: 123\tTrain Loss: 0.3503980 \tVal Loss:0.1315125 \tTrain Acc: 88.36765% \tVal Acc: 96.5294111%\n",
      "Epoch: 124\tTrain Loss: 0.3548675 \tVal Loss:0.1343558 \tTrain Acc: 88.86765% \tVal Acc: 96.7647046%\n",
      "Epoch: 125\tTrain Loss: 0.3524659 \tVal Loss:0.1227924 \tTrain Acc: 88.63235% \tVal Acc: 97.4117643%\n",
      "Epoch: 126\tTrain Loss: 0.3423472 \tVal Loss:0.1238629 \tTrain Acc: 89.2647% \tVal Acc: 97.0588219%\n",
      "Epoch: 127\tTrain Loss: 0.3416634 \tVal Loss:0.1212822 \tTrain Acc: 88.89706% \tVal Acc: 96.8823522%\n",
      "Epoch: 128\tTrain Loss: 0.3409410 \tVal Loss:0.1030105 \tTrain Acc: 89.05882% \tVal Acc: 97.7647048%\n",
      "Epoch: 129\tTrain Loss: 0.3344678 \tVal Loss:0.1121373 \tTrain Acc: 88.86765% \tVal Acc: 97.5882339%\n",
      "Epoch: 130\tTrain Loss: 0.3247229 \tVal Loss:0.1238954 \tTrain Acc: 89.91176% \tVal Acc: 96.9999993%\n",
      "Epoch: 131\tTrain Loss: 0.3307355 \tVal Loss:0.1176555 \tTrain Acc: 89.39706% \tVal Acc: 97.1764702%\n",
      "Epoch: 132\tTrain Loss: 0.3215057 \tVal Loss:0.1180823 \tTrain Acc: 89.95588% \tVal Acc: 97.1176457%\n",
      "Epoch: 133\tTrain Loss: 0.3422966 \tVal Loss:0.1067698 \tTrain Acc: 88.94118% \tVal Acc: 97.4117631%\n",
      "Epoch: 134\tTrain Loss: 0.3168240 \tVal Loss:0.1067299 \tTrain Acc: 89.83823% \tVal Acc: 97.5294107%\n",
      "Epoch: 135\tTrain Loss: 0.3045192 \tVal Loss:0.1025580 \tTrain Acc: 90.63235% \tVal Acc: 97.5294107%\n",
      "Epoch: 136\tTrain Loss: 0.2954101 \tVal Loss:0.0991241 \tTrain Acc: 90.63235% \tVal Acc: 97.4705863%\n",
      "Epoch: 137\tTrain Loss: 0.2974415 \tVal Loss:0.0956359 \tTrain Acc: 90.22059% \tVal Acc: 97.6470572%\n",
      "Epoch: 138\tTrain Loss: 0.2967092 \tVal Loss:0.0864183 \tTrain Acc: 90.64706% \tVal Acc: 98.1176460%\n",
      "Epoch: 139\tTrain Loss: 0.2894690 \tVal Loss:0.0799871 \tTrain Acc: 90.86765% \tVal Acc: 98.3529401%\n",
      "Epoch: 140\tTrain Loss: 0.3075984 \tVal Loss:0.0988033 \tTrain Acc: 90.0147% \tVal Acc: 97.4117631%\n",
      "Epoch: 141\tTrain Loss: 0.2916915 \tVal Loss:0.0969970 \tTrain Acc: 90.58823% \tVal Acc: 97.5294101%\n",
      "Epoch: 142\tTrain Loss: 0.2905429 \tVal Loss:0.0979468 \tTrain Acc: 90.79412% \tVal Acc: 97.1764696%\n",
      "Epoch: 143\tTrain Loss: 0.2895803 \tVal Loss:0.1146358 \tTrain Acc: 90.60294% \tVal Acc: 96.7647052%\n",
      "Epoch: 144\tTrain Loss: 0.2936074 \tVal Loss:0.0923444 \tTrain Acc: 90.57353% \tVal Acc: 97.7647042%\n",
      "Epoch: 145\tTrain Loss: 0.2770108 \tVal Loss:0.0805362 \tTrain Acc: 90.82353% \tVal Acc: 97.9999995%\n",
      "Epoch: 146\tTrain Loss: 0.2675889 \tVal Loss:0.0747149 \tTrain Acc: 91.5% \tVal Acc: 98.4705859%\n",
      "Epoch: 147\tTrain Loss: 0.2570258 \tVal Loss:0.0823947 \tTrain Acc: 92.0% \tVal Acc: 97.5882334%\n",
      "Epoch: 148\tTrain Loss: 0.2665981 \tVal Loss:0.0736731 \tTrain Acc: 91.5147% \tVal Acc: 98.1176460%\n",
      "Epoch: 149\tTrain Loss: 0.2772768 \tVal Loss:0.0741047 \tTrain Acc: 91.05882% \tVal Acc: 97.9999983%\n",
      "Epoch: 150\tTrain Loss: 0.2575764 \tVal Loss:0.0687122 \tTrain Acc: 91.52941% \tVal Acc: 98.1764698%\n",
      "Epoch: 151\tTrain Loss: 0.2478548 \tVal Loss:0.0673483 \tTrain Acc: 92.33823% \tVal Acc: 98.4117639%\n",
      "Epoch: 152\tTrain Loss: 0.2561496 \tVal Loss:0.0781420 \tTrain Acc: 91.70588% \tVal Acc: 98.2941163%\n",
      "Epoch: 153\tTrain Loss: 0.2595951 \tVal Loss:0.0960069 \tTrain Acc: 91.41176% \tVal Acc: 97.4705875%\n",
      "Epoch: 154\tTrain Loss: 0.2562958 \tVal Loss:0.0743670 \tTrain Acc: 91.88235% \tVal Acc: 98.4117633%\n",
      "Epoch: 155\tTrain Loss: 0.2597253 \tVal Loss:0.0691711 \tTrain Acc: 91.82353% \tVal Acc: 98.5882348%\n",
      "Epoch: 156\tTrain Loss: 0.2510924 \tVal Loss:0.0716991 \tTrain Acc: 91.73529% \tVal Acc: 98.5882336%\n",
      "Epoch: 157\tTrain Loss: 0.2537994 \tVal Loss:0.0725651 \tTrain Acc: 91.77941% \tVal Acc: 98.2352930%\n",
      "Epoch: 158\tTrain Loss: 0.2394494 \tVal Loss:0.0657410 \tTrain Acc: 92.32353% \tVal Acc: 98.4117633%\n",
      "Epoch: 159\tTrain Loss: 0.2387590 \tVal Loss:0.0560544 \tTrain Acc: 92.48529% \tVal Acc: 98.8823521%\n",
      "Epoch: 160\tTrain Loss: 0.2287576 \tVal Loss:0.0480913 \tTrain Acc: 93.02941% \tVal Acc: 99.1764700%\n",
      "Epoch: 161\tTrain Loss: 0.2085533 \tVal Loss:0.0507330 \tTrain Acc: 93.29412% \tVal Acc: 98.8235283%\n",
      "Epoch: 162\tTrain Loss: 0.2151423 \tVal Loss:0.0606172 \tTrain Acc: 93.20588% \tVal Acc: 98.4705877%\n",
      "Epoch: 163\tTrain Loss: 0.2231805 \tVal Loss:0.0525179 \tTrain Acc: 92.82353% \tVal Acc: 98.9999992%\n",
      "Epoch: 164\tTrain Loss: 0.2249649 \tVal Loss:0.0538794 \tTrain Acc: 92.86765% \tVal Acc: 98.9999992%\n",
      "Epoch: 165\tTrain Loss: 0.2345957 \tVal Loss:0.0573214 \tTrain Acc: 92.5% \tVal Acc: 98.6470574%\n",
      "Epoch: 166\tTrain Loss: 0.2245028 \tVal Loss:0.0512568 \tTrain Acc: 92.67647% \tVal Acc: 98.8823515%\n",
      "Epoch: 167\tTrain Loss: 0.2208236 \tVal Loss:0.0509489 \tTrain Acc: 93.11765% \tVal Acc: 98.8235283%\n",
      "Epoch: 168\tTrain Loss: 0.2178443 \tVal Loss:0.0468010 \tTrain Acc: 93.02941% \tVal Acc: 98.9411747%\n",
      "Epoch: 169\tTrain Loss: 0.2103558 \tVal Loss:0.0557023 \tTrain Acc: 93.23529% \tVal Acc: 98.5882330%\n",
      "Epoch: 170\tTrain Loss: 0.2159343 \tVal Loss:0.0445730 \tTrain Acc: 93.08823% \tVal Acc: 98.9411753%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 171\tTrain Loss: 0.2128841 \tVal Loss:0.0519821 \tTrain Acc: 93.25% \tVal Acc: 98.7647045%\n",
      "Epoch: 172\tTrain Loss: 0.2103501 \tVal Loss:0.0461004 \tTrain Acc: 93.25% \tVal Acc: 98.8235283%\n",
      "Epoch: 173\tTrain Loss: 0.2154471 \tVal Loss:0.0433420 \tTrain Acc: 93.41176% \tVal Acc: 99.1764688%\n",
      "Epoch: 174\tTrain Loss: 0.2170243 \tVal Loss:0.0819967 \tTrain Acc: 93.04412% \tVal Acc: 97.7647042%\n",
      "Epoch: 175\tTrain Loss: 0.2160470 \tVal Loss:0.0579155 \tTrain Acc: 93.11765% \tVal Acc: 98.4117639%\n",
      "Epoch: 176\tTrain Loss: 0.2158372 \tVal Loss:0.0570730 \tTrain Acc: 93.07353% \tVal Acc: 98.5882342%\n",
      "Epoch: 177\tTrain Loss: 0.2064410 \tVal Loss:0.0520751 \tTrain Acc: 93.32353% \tVal Acc: 98.8823515%\n",
      "Epoch: 178\tTrain Loss: 0.1983982 \tVal Loss:0.0553749 \tTrain Acc: 93.73529% \tVal Acc: 98.9999986%\n",
      "Epoch: 179\tTrain Loss: 0.1945203 \tVal Loss:0.0403278 \tTrain Acc: 93.58823% \tVal Acc: 98.9411753%\n",
      "Epoch: 180\tTrain Loss: 0.2013926 \tVal Loss:0.0385233 \tTrain Acc: 93.5% \tVal Acc: 99.2352921%\n",
      "Epoch: 181\tTrain Loss: 0.2038730 \tVal Loss:0.0396917 \tTrain Acc: 93.20588% \tVal Acc: 99.1176462%\n",
      "Epoch: 182\tTrain Loss: 0.1842825 \tVal Loss:0.0405138 \tTrain Acc: 94.13235% \tVal Acc: 99.1764694%\n",
      "Epoch: 183\tTrain Loss: 0.1942578 \tVal Loss:0.0360301 \tTrain Acc: 94.0147% \tVal Acc: 99.1176456%\n",
      "Epoch: 184\tTrain Loss: 0.1834804 \tVal Loss:0.0341458 \tTrain Acc: 94.13235% \tVal Acc: 99.2941171%\n",
      "Epoch: 185\tTrain Loss: 0.1806696 \tVal Loss:0.0434793 \tTrain Acc: 94.33823% \tVal Acc: 98.9999992%\n",
      "Epoch: 186\tTrain Loss: 0.1802149 \tVal Loss:0.0428657 \tTrain Acc: 94.10294% \tVal Acc: 98.7647051%\n",
      "Epoch: 187\tTrain Loss: 0.1906374 \tVal Loss:0.0448745 \tTrain Acc: 93.98529% \tVal Acc: 98.9411747%\n",
      "Epoch: 188\tTrain Loss: 0.1893477 \tVal Loss:0.0355936 \tTrain Acc: 93.60294% \tVal Acc: 99.1176456%\n",
      "Epoch: 189\tTrain Loss: 0.1982899 \tVal Loss:0.0416261 \tTrain Acc: 93.58823% \tVal Acc: 99.0588224%\n",
      "Epoch: 190\tTrain Loss: 0.1907476 \tVal Loss:0.0367867 \tTrain Acc: 93.60294% \tVal Acc: 99.1176456%\n",
      "Epoch: 191\tTrain Loss: 0.1907858 \tVal Loss:0.0479050 \tTrain Acc: 93.80882% \tVal Acc: 98.5882342%\n",
      "Epoch: 192\tTrain Loss: 0.1798260 \tVal Loss:0.0401865 \tTrain Acc: 94.52941% \tVal Acc: 99.1176462%\n",
      "Epoch: 193\tTrain Loss: 0.1811724 \tVal Loss:0.0355995 \tTrain Acc: 94.33823% \tVal Acc: 99.0588224%\n",
      "Epoch: 194\tTrain Loss: 0.1786382 \tVal Loss:0.0372135 \tTrain Acc: 94.27941% \tVal Acc: 99.1176450%\n",
      "Epoch: 195\tTrain Loss: 0.1748949 \tVal Loss:0.0409324 \tTrain Acc: 94.55882% \tVal Acc: 98.7647039%\n",
      "Epoch: 196\tTrain Loss: 0.1696986 \tVal Loss:0.0472994 \tTrain Acc: 94.80882% \tVal Acc: 98.7058800%\n",
      "Epoch: 197\tTrain Loss: 0.1712761 \tVal Loss:0.0507265 \tTrain Acc: 94.5147% \tVal Acc: 98.7058806%\n",
      "Epoch: 198\tTrain Loss: 0.1711740 \tVal Loss:0.0341103 \tTrain Acc: 94.63235% \tVal Acc: 99.1764694%\n",
      "Epoch: 199\tTrain Loss: 0.1788410 \tVal Loss:0.0381765 \tTrain Acc: 94.2647% \tVal Acc: 99.0588224%\n"
     ]
    }
   ],
   "source": [
    "epochs = 200\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    hidden = model.hidden_init(train_batch_size)    \n",
    "    #print('hidden[0].shape:- ',hidden[0].shape)\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        '''\n",
    "        Creating new variables for the hidden state, otherwise\n",
    "        we'd backprop through the entire training history\n",
    "        '''\n",
    "        h = tuple([each.data for each in hidden])\n",
    "        \n",
    "\n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = model.forward(inputs, h,train_batch_size)\n",
    "        #print(output)\n",
    "        \n",
    "        #print('Labels Shape :-', (torch.max(labels, 1)[1]).shape)\n",
    "    \n",
    "        # calculate the loss and perform backprop\n",
    "        #print('Labels Long :-', labels.long())\n",
    "        loss = criterion(output,labels.long())\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #calculate training accuracy\n",
    "        output = F.softmax(output)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        logging.debug(' top probab {} top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        #print(train_loss)\n",
    "              \n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "                \n",
    "        val_h = tuple([each.data for each in hidden])\n",
    "        \n",
    "        output, hidden = model.forward(inputs, val_h,val_batch_size)\n",
    "        loss = criterion(output,labels.long())\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        #calculate validation accuracy\n",
    "        output = F.softmax(output)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        \n",
    "        #logging.debug(output)\n",
    "        #logging.debug('VALIDATION top probab {} VALIDATION top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        #print('Top Class:- ',top_class)\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        #print('Equals:- ', equals)\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    #Averaging losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    val_accuracy = val_accuracy/len(val_loader)\n",
    "    train_accuracy = train_accuracy/len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}\\tTrain Loss: {:.7f} \\tVal Loss:{:.7f} \\tTrain Acc: {:.7}% \\tVal Acc: {:.7f}%'.format(e, train_loss, val_loss, train_accuracy*100,val_accuracy*100))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Music Genaration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting\n",
    "test_hidden = model.hidden_init(test_seq.shape[0])\n",
    "test_output,_ = model.forward(test_seq, test_hidden,test_seq.shape[0])\n",
    "\n",
    "top_p, top_class = test_output.topk(1,dim =1)\n",
    "top_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
