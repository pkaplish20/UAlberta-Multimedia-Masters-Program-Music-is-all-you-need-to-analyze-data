{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.tensorboard as tb\n",
    "from Preprocessing import preprocessing\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as  plt\n",
    "import os\n",
    "import logging\n",
    "torch.set_printoptions(profile=\"full\")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#static parameters\n",
    "train_batch_size = 170\n",
    "val_batch_size = 170\n",
    "sequence_length=50\n",
    "test_batch_size = 1\n",
    "input_size = 1\n",
    "hidden_size = 128\n",
    "num_layer = 2\n",
    "output_size = 38\n",
    "clip = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data from preprocessing.py\n",
    "dataset_path = os.path.join(os.path.abspath('..'),'Dataset\\\\Clementi dataset\\\\Clementi dataset' )\n",
    "network_input, network_output = preprocessing.preprocess_notes(dataset_path)\n",
    "network_input, network_output = network_input.cuda(), network_output.cuda()\n",
    "\n",
    "# print(network_input)\n",
    "#print(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(37, device='cuda:0')\n",
      "tensor(0, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(network_output.max())\n",
    "print(network_output.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9737, device='cuda:0')\n",
      "tensor(0., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(network_input.max())\n",
    "print(network_input.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8617, 50, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndata is highly unbalanced\\n# '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "data is highly unbalanced\n",
    "# '''\n",
    "# sns.distplot(torch.tensor(network_output).cpu())\n",
    "# xx = pd.DataFrame(torch.tensor(network_output).cpu())\n",
    "# xx.groupby(0).size().to_frame(name='values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8500, 50, 1])\n",
      "torch.Size([8500])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "to make batch of equal sizes\n",
    "Quick Fix\n",
    "'''\n",
    "network_input = network_input[: -117]\n",
    "network_output = network_output[: -117]\n",
    "\n",
    "print(network_input.shape)\n",
    "print(network_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "create Stacked LSTM model\n",
    "'''\n",
    "class Stacked_LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = num_layers, batch_first = True, dropout = 0.5)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, x, hidden,batch_size):\n",
    "        \n",
    "        output, hidden = self.lstm(x, hidden)        \n",
    "        #print(output.shape)\n",
    "        \n",
    "        # stack up lstm outputs\n",
    "        output = output.contiguous().view(-1, self.hidden_size)\n",
    "        #print(output.shape)\n",
    "        \n",
    "        output = self.dropout(output)\n",
    "        output = self.linear(output)\n",
    "        #print('Linear Output :-',output.shape)\n",
    "        \n",
    "        #output = F.softmax(output, dim = 1)\n",
    "        #print('SOFTMAX OUTPUT :--', output)\n",
    "        \n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        output = output.view(batch_size, -1)\n",
    "        #print('Reshape to batch size first :-',output.shape)\n",
    "        \n",
    "        output = output[:, -self.output_size:] # get last batch of labels\n",
    "        #print('Final Output :-',output)\n",
    "        #print('RESHAPE SIZE :-', output.shape)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def hidden_init(self,batch_size):\n",
    "        \n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
    "          weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "        return hidden\n",
    "\n",
    "#initialize the weights of LSTM using Xavier initialization    \n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Divide the dataset into train/val \n",
    "'''\n",
    "train_size = 0.8\n",
    "indices = list(range(len(network_input)))\n",
    "split = int(np.floor(train_size*len(network_input)))\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "train_sampler = SequentialSampler(train_idx)\n",
    "val_sampler = SequentialSampler(val_idx)\n",
    "\n",
    "dataset = TensorDataset(network_input,network_output)\n",
    "train_loader = DataLoader(dataset, batch_size= train_batch_size, sampler=train_sampler)\n",
    "val_loader = DataLoader(dataset, batch_size= val_batch_size,sampler= val_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Stacked_LSTM(\n",
       "  (lstm): LSTM(1, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (linear): Linear(in_features=128, out_features=38, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optimizer\n",
    "\n",
    "model = Stacked_LSTM(input_size,hidden_size,num_layer,output_size)\n",
    "model.apply(init_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optimizer.AdamW(model.parameters())\n",
    "#optimizer = optimizer.RMSprop(model.parameters(), lr = 0.001, weight_decay = 0.01)\n",
    "\n",
    "\n",
    "#make sure to transfer model to GPU after initializing optimizer\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    eps: 1e-08\n",
       "    lr: 0.001\n",
       "    weight_decay: 0.01\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\tTrain Loss: 3.3177888 \tVal Loss:3.0410884 \tTrain Acc: 8.470588% \tVal Acc: 11.4117651%\n",
      "Validation Loss is decreased from    inf to 3.041088, so saving the model weights\n",
      "Epoch: 1\tTrain Loss: 3.1162264 \tVal Loss:2.9926653 \tTrain Acc: 8.441177% \tVal Acc: 11.4117651%\n",
      "Validation Loss is decreased from 3.041088 to 2.992665, so saving the model weights\n",
      "Epoch: 2\tTrain Loss: 3.1024166 \tVal Loss:2.9727437 \tTrain Acc: 8.75% \tVal Acc: 11.4117651%\n",
      "Validation Loss is decreased from 2.992665 to 2.972744, so saving the model weights\n",
      "Epoch: 3\tTrain Loss: 3.0821394 \tVal Loss:2.9607683 \tTrain Acc: 8.779412% \tVal Acc: 11.4117651%\n",
      "Validation Loss is decreased from 2.972744 to 2.960768, so saving the model weights\n",
      "Epoch: 4\tTrain Loss: 3.0779367 \tVal Loss:2.9549859 \tTrain Acc: 8.632353% \tVal Acc: 11.4117651%\n",
      "Validation Loss is decreased from 2.960768 to 2.954986, so saving the model weights\n",
      "Epoch: 5\tTrain Loss: 3.0637497 \tVal Loss:2.9466888 \tTrain Acc: 9.558824% \tVal Acc: 11.4117651%\n",
      "Validation Loss is decreased from 2.954986 to 2.946689, so saving the model weights\n",
      "Epoch: 6\tTrain Loss: 3.0598744 \tVal Loss:2.9427535 \tTrain Acc: 9.941177% \tVal Acc: 11.4117651%\n",
      "Validation Loss is decreased from 2.946689 to 2.942753, so saving the model weights\n",
      "Epoch: 7\tTrain Loss: 3.0563996 \tVal Loss:2.9395054 \tTrain Acc: 9.455883% \tVal Acc: 11.4117651%\n",
      "Validation Loss is decreased from 2.942753 to 2.939505, so saving the model weights\n",
      "Epoch: 8\tTrain Loss: 3.0554515 \tVal Loss:2.9381618 \tTrain Acc: 9.485294% \tVal Acc: 11.4117651%\n",
      "Validation Loss is decreased from 2.939505 to 2.938162, so saving the model weights\n",
      "Epoch: 9\tTrain Loss: 3.0472643 \tVal Loss:2.9501348 \tTrain Acc: 10.17647% \tVal Acc: 10.6470592%\n",
      "Epoch: 10\tTrain Loss: 3.0463360 \tVal Loss:2.9338184 \tTrain Acc: 9.911765% \tVal Acc: 11.4117651%\n",
      "Validation Loss is decreased from 2.938162 to 2.933818, so saving the model weights\n",
      "Epoch: 11\tTrain Loss: 3.0444033 \tVal Loss:2.9303878 \tTrain Acc: 10.61765% \tVal Acc: 11.4117651%\n",
      "Validation Loss is decreased from 2.933818 to 2.930388, so saving the model weights\n",
      "Epoch: 12\tTrain Loss: 3.0386137 \tVal Loss:2.9290393 \tTrain Acc: 11.27941% \tVal Acc: 11.6470592%\n",
      "Validation Loss is decreased from 2.930388 to 2.929039, so saving the model weights\n",
      "Epoch: 13\tTrain Loss: 3.0345154 \tVal Loss:2.9245686 \tTrain Acc: 10.57353% \tVal Acc: 12.0588239%\n",
      "Validation Loss is decreased from 2.929039 to 2.924569, so saving the model weights\n",
      "Epoch: 14\tTrain Loss: 3.0310040 \tVal Loss:2.9229231 \tTrain Acc: 10.92647% \tVal Acc: 11.8823534%\n",
      "Validation Loss is decreased from 2.924569 to 2.922923, so saving the model weights\n",
      "Epoch: 15\tTrain Loss: 3.0283104 \tVal Loss:2.9095278 \tTrain Acc: 10.86765% \tVal Acc: 11.5882356%\n",
      "Validation Loss is decreased from 2.922923 to 2.909528, so saving the model weights\n",
      "Epoch: 16\tTrain Loss: 3.0229453 \tVal Loss:2.9185185 \tTrain Acc: 10.32353% \tVal Acc: 11.4117651%\n",
      "Epoch: 17\tTrain Loss: 3.0296224 \tVal Loss:2.9045549 \tTrain Acc: 10.29412% \tVal Acc: 11.4117651%\n",
      "Validation Loss is decreased from 2.909528 to 2.904555, so saving the model weights\n",
      "Epoch: 18\tTrain Loss: 3.0158715 \tVal Loss:2.8895553 \tTrain Acc: 11.63235% \tVal Acc: 11.8823534%\n",
      "Validation Loss is decreased from 2.904555 to 2.889555, so saving the model weights\n",
      "Epoch: 19\tTrain Loss: 3.0053384 \tVal Loss:2.8851430 \tTrain Acc: 12.13235% \tVal Acc: 14.0588240%\n",
      "Validation Loss is decreased from 2.889555 to 2.885143, so saving the model weights\n",
      "Epoch: 20\tTrain Loss: 2.9950626 \tVal Loss:2.8754738 \tTrain Acc: 12.20588% \tVal Acc: 14.6470591%\n",
      "Validation Loss is decreased from 2.885143 to 2.875474, so saving the model weights\n",
      "Epoch: 21\tTrain Loss: 2.9897212 \tVal Loss:2.8608111 \tTrain Acc: 12.47059% \tVal Acc: 15.2941180%\n",
      "Validation Loss is decreased from 2.875474 to 2.860811, so saving the model weights\n",
      "Epoch: 22\tTrain Loss: 2.9835004 \tVal Loss:2.8467026 \tTrain Acc: 12.51471% \tVal Acc: 15.2941179%\n",
      "Validation Loss is decreased from 2.860811 to 2.846703, so saving the model weights\n",
      "Epoch: 23\tTrain Loss: 2.9728861 \tVal Loss:2.8329097 \tTrain Acc: 13.44118% \tVal Acc: 15.7058828%\n",
      "Validation Loss is decreased from 2.846703 to 2.832910, so saving the model weights\n",
      "Epoch: 24\tTrain Loss: 2.9620878 \tVal Loss:2.8326321 \tTrain Acc: 13.39706% \tVal Acc: 16.2352945%\n",
      "Validation Loss is decreased from 2.832910 to 2.832632, so saving the model weights\n",
      "Epoch: 25\tTrain Loss: 2.9579349 \tVal Loss:2.8146810 \tTrain Acc: 12.88235% \tVal Acc: 16.3529415%\n",
      "Validation Loss is decreased from 2.832632 to 2.814681, so saving the model weights\n",
      "Epoch: 26\tTrain Loss: 2.9588009 \tVal Loss:2.8164924 \tTrain Acc: 13.64706% \tVal Acc: 15.9411769%\n",
      "Epoch: 27\tTrain Loss: 2.9482706 \tVal Loss:2.8073540 \tTrain Acc: 14.22059% \tVal Acc: 16.0000005%\n",
      "Validation Loss is decreased from 2.814681 to 2.807354, so saving the model weights\n",
      "Epoch: 28\tTrain Loss: 2.9392606 \tVal Loss:2.7948379 \tTrain Acc: 13.83824% \tVal Acc: 16.4705887%\n",
      "Validation Loss is decreased from 2.807354 to 2.794838, so saving the model weights\n",
      "Epoch: 29\tTrain Loss: 2.9377956 \tVal Loss:2.7820508 \tTrain Acc: 14.35294% \tVal Acc: 17.7058827%\n",
      "Validation Loss is decreased from 2.794838 to 2.782051, so saving the model weights\n",
      "Epoch: 30\tTrain Loss: 2.9303438 \tVal Loss:2.7827603 \tTrain Acc: 14.42647% \tVal Acc: 18.2941180%\n",
      "Epoch: 31\tTrain Loss: 2.9179585 \tVal Loss:2.7596668 \tTrain Acc: 15.20588% \tVal Acc: 19.2352944%\n",
      "Validation Loss is decreased from 2.782051 to 2.759667, so saving the model weights\n",
      "Epoch: 32\tTrain Loss: 2.9212674 \tVal Loss:2.7681539 \tTrain Acc: 15.17647% \tVal Acc: 18.4117651%\n",
      "Epoch: 33\tTrain Loss: 2.9140510 \tVal Loss:2.7540384 \tTrain Acc: 15.04412% \tVal Acc: 20.0000004%\n",
      "Validation Loss is decreased from 2.759667 to 2.754038, so saving the model weights\n",
      "Epoch: 34\tTrain Loss: 2.9050348 \tVal Loss:2.7390952 \tTrain Acc: 15.5% \tVal Acc: 19.1764710%\n",
      "Validation Loss is decreased from 2.754038 to 2.739095, so saving the model weights\n",
      "Epoch: 35\tTrain Loss: 2.8859611 \tVal Loss:2.7069658 \tTrain Acc: 15.51471% \tVal Acc: 20.4705887%\n",
      "Validation Loss is decreased from 2.739095 to 2.706966, so saving the model weights\n",
      "Epoch: 36\tTrain Loss: 2.8697144 \tVal Loss:2.6865293 \tTrain Acc: 16.42647% \tVal Acc: 20.2941181%\n",
      "Validation Loss is decreased from 2.706966 to 2.686529, so saving the model weights\n",
      "Epoch: 37\tTrain Loss: 2.8519170 \tVal Loss:2.6866088 \tTrain Acc: 16.76471% \tVal Acc: 20.4705887%\n",
      "Epoch: 38\tTrain Loss: 2.8474273 \tVal Loss:2.6955663 \tTrain Acc: 17.05882% \tVal Acc: 21.4117652%\n",
      "Epoch: 39\tTrain Loss: 2.8286374 \tVal Loss:2.6668360 \tTrain Acc: 17.54412% \tVal Acc: 22.0588242%\n",
      "Validation Loss is decreased from 2.686529 to 2.666836, so saving the model weights\n",
      "Epoch: 40\tTrain Loss: 2.8106939 \tVal Loss:2.6147619 \tTrain Acc: 18.29412% \tVal Acc: 23.1176475%\n",
      "Validation Loss is decreased from 2.666836 to 2.614762, so saving the model weights\n",
      "Epoch: 41\tTrain Loss: 2.7806969 \tVal Loss:2.6853115 \tTrain Acc: 18.79412% \tVal Acc: 22.7058832%\n",
      "Epoch: 42\tTrain Loss: 2.7930633 \tVal Loss:2.6846309 \tTrain Acc: 19.01471% \tVal Acc: 22.8235301%\n",
      "Epoch: 43\tTrain Loss: 2.7715042 \tVal Loss:2.7247232 \tTrain Acc: 19.76471% \tVal Acc: 22.4117653%\n",
      "Epoch: 44\tTrain Loss: 2.7677746 \tVal Loss:2.6827407 \tTrain Acc: 19.76471% \tVal Acc: 23.7058833%\n",
      "Epoch: 45\tTrain Loss: 2.7664898 \tVal Loss:2.6536357 \tTrain Acc: 20.27941% \tVal Acc: 23.5294124%\n",
      "Epoch: 46\tTrain Loss: 2.7762349 \tVal Loss:2.6168562 \tTrain Acc: 19.52941% \tVal Acc: 23.7058830%\n",
      "Epoch: 47\tTrain Loss: 2.7476246 \tVal Loss:2.5787679 \tTrain Acc: 21.10294% \tVal Acc: 24.4705887%\n",
      "Validation Loss is decreased from 2.614762 to 2.578768, so saving the model weights\n",
      "Epoch: 48\tTrain Loss: 2.7179577 \tVal Loss:2.5579401 \tTrain Acc: 21.51471% \tVal Acc: 25.2941185%\n",
      "Validation Loss is decreased from 2.578768 to 2.557940, so saving the model weights\n",
      "Epoch: 49\tTrain Loss: 2.6997949 \tVal Loss:2.5423428 \tTrain Acc: 21.76471% \tVal Acc: 26.1764716%\n",
      "Validation Loss is decreased from 2.557940 to 2.542343, so saving the model weights\n",
      "Epoch: 50\tTrain Loss: 2.6771584 \tVal Loss:2.4973371 \tTrain Acc: 22.08824% \tVal Acc: 26.2941186%\n",
      "Validation Loss is decreased from 2.542343 to 2.497337, so saving the model weights\n",
      "Epoch: 51\tTrain Loss: 2.6427986 \tVal Loss:2.4743790 \tTrain Acc: 22.45588% \tVal Acc: 27.5882362%\n",
      "Validation Loss is decreased from 2.497337 to 2.474379, so saving the model weights\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 52\tTrain Loss: 2.6041932 \tVal Loss:2.4579236 \tTrain Acc: 24.19118% \tVal Acc: 28.2352950%\n",
      "Validation Loss is decreased from 2.474379 to 2.457924, so saving the model weights\n",
      "Epoch: 53\tTrain Loss: 2.5910762 \tVal Loss:2.4796409 \tTrain Acc: 24.02941% \tVal Acc: 28.5882361%\n",
      "Epoch: 54\tTrain Loss: 2.5778684 \tVal Loss:2.4241627 \tTrain Acc: 24.23529% \tVal Acc: 28.5294126%\n",
      "Validation Loss is decreased from 2.457924 to 2.424163, so saving the model weights\n",
      "Epoch: 55\tTrain Loss: 2.5656474 \tVal Loss:2.4191251 \tTrain Acc: 24.97059% \tVal Acc: 28.3529420%\n",
      "Validation Loss is decreased from 2.424163 to 2.419125, so saving the model weights\n",
      "Epoch: 56\tTrain Loss: 2.5386767 \tVal Loss:2.5292156 \tTrain Acc: 25.08824% \tVal Acc: 25.7058832%\n",
      "Epoch: 57\tTrain Loss: 2.5549498 \tVal Loss:2.4437256 \tTrain Acc: 25.17647% \tVal Acc: 26.2941185%\n",
      "Epoch: 58\tTrain Loss: 2.5364429 \tVal Loss:2.3769479 \tTrain Acc: 25.60294% \tVal Acc: 29.5294125%\n",
      "Validation Loss is decreased from 2.419125 to 2.376948, so saving the model weights\n",
      "Epoch: 59\tTrain Loss: 2.5349195 \tVal Loss:2.3262092 \tTrain Acc: 25.73529% \tVal Acc: 32.0000008%\n",
      "Validation Loss is decreased from 2.376948 to 2.326209, so saving the model weights\n",
      "Epoch: 60\tTrain Loss: 2.4687817 \tVal Loss:2.3332397 \tTrain Acc: 26.85294% \tVal Acc: 31.0588244%\n",
      "Epoch: 61\tTrain Loss: 2.4619124 \tVal Loss:2.2909850 \tTrain Acc: 27.32353% \tVal Acc: 31.1764717%\n",
      "Validation Loss is decreased from 2.326209 to 2.290985, so saving the model weights\n",
      "Epoch: 62\tTrain Loss: 2.4384527 \tVal Loss:2.3077322 \tTrain Acc: 27.55882% \tVal Acc: 30.7647070%\n",
      "Epoch: 63\tTrain Loss: 2.4040333 \tVal Loss:2.2423120 \tTrain Acc: 28.42647% \tVal Acc: 31.8235303%\n",
      "Validation Loss is decreased from 2.290985 to 2.242312, so saving the model weights\n",
      "Epoch: 64\tTrain Loss: 2.3590707 \tVal Loss:2.1778862 \tTrain Acc: 29.02941% \tVal Acc: 33.8823539%\n",
      "Validation Loss is decreased from 2.242312 to 2.177886, so saving the model weights\n",
      "Epoch: 65\tTrain Loss: 2.3052090 \tVal Loss:2.1320647 \tTrain Acc: 31.82353% \tVal Acc: 35.5294129%\n",
      "Validation Loss is decreased from 2.177886 to 2.132065, so saving the model weights\n",
      "Epoch: 66\tTrain Loss: 2.2718177 \tVal Loss:2.1434307 \tTrain Acc: 32.01471% \tVal Acc: 35.6470597%\n",
      "Epoch: 67\tTrain Loss: 2.2478660 \tVal Loss:2.1524589 \tTrain Acc: 32.33824% \tVal Acc: 34.7058834%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-084314b214ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mtop_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' top probab {} top class {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtop_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtop_p\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mequals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtop_class\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mtop_class\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__ipow__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;31m# characters to replace unicode characters with.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encoding'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[0mtensor_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                 \u001b[0mtensor_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_tensor_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayout\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrided\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m_tensor_str\u001b[1;34m(self, indent)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat16\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbfloat16\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m     \u001b[0mformatter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_summarized_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0msummarize\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_tensor_str_with_formatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msummarize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\_tensor_str.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating_dtype\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtensor_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0mvalue_str\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'{}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_width\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue_str\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch_env\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__format__\u001b[1;34m(self, format_spec)\u001b[0m\n\u001b[0;32m    338\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__format__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat_spec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 500\n",
    "min_val_loss = np.Inf\n",
    "\n",
    "for e in range(epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "    train_accuracy = 0\n",
    "    val_accuracy = 0\n",
    "    \n",
    "    \n",
    "    hidden = model.hidden_init(train_batch_size)    \n",
    "    #print('hidden[0].shape:- ',hidden[0].shape)\n",
    "    \n",
    "    for inputs,labels in train_loader:\n",
    "        #print(inputs.shape)\n",
    "\n",
    "        '''\n",
    "        Creating new variables for the hidden state, otherwise\n",
    "        we'd backprop through the entire training history\n",
    "        '''\n",
    "        h = tuple([each.data for each in hidden])\n",
    "        \n",
    "\n",
    "        # zero accumulated gradients\n",
    "        model.zero_grad()\n",
    "       \n",
    "        # get the output from the model\n",
    "        output, h = model.forward(inputs, h,train_batch_size)\n",
    "        #print('OUTPUT', output)\n",
    "        \n",
    "        \n",
    "        #print('Labels Shape :-', (torch.max(labels, 1)[1]).shape)\n",
    "    \n",
    "        # calculate the loss and perform backprop\n",
    "        #print('Labels Long :-', labels.long())\n",
    "        loss = criterion(output,labels.long())\n",
    "        #print('LOSS IS :-', loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        #calculate training accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        logging.debug(' top probab {} top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        train_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        #print(train_loss)\n",
    "              \n",
    "    model.eval()\n",
    "    for inputs, labels in val_loader:\n",
    "                \n",
    "        val_h = tuple([each.data for each in hidden])\n",
    "        \n",
    "        output, hidden = model.forward(inputs, val_h,val_batch_size)\n",
    "       \n",
    "        loss = criterion(output,labels.long())\n",
    "        \n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        #calculate validation accuracy\n",
    "        output = F.softmax(output, dim = 1)\n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        \n",
    "        #logging.debug(output)\n",
    "        #logging.debug('VALIDATION top probab {} VALIDATION top class {}'.format(top_p.view(-1, top_p.shape[0]), top_class.view(-1, top_p.shape[0])))\n",
    "\n",
    "        #print('Top Class:- ',top_class)\n",
    "        equals = top_class == labels.long().view(*top_class.shape)\n",
    "        #print('Equals:- ', equals)\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n",
    "        \n",
    "    model.train()\n",
    "    \n",
    "    #Averaging losses\n",
    "    train_loss = train_loss/len(train_loader)\n",
    "    val_loss = val_loss/len(val_loader)\n",
    "    val_accuracy = val_accuracy/len(val_loader)\n",
    "    train_accuracy = train_accuracy/len(train_loader)\n",
    "    \n",
    "    print('Epoch: {}\\tTrain Loss: {:.7f} \\tVal Loss:{:.7f} \\tTrain Acc: {:.7}% \\tVal Acc: {:.7f}%'.format(e, train_loss, val_loss, train_accuracy*100,val_accuracy*100))\n",
    "    \n",
    "    #saving the model if validation loss is decreased\n",
    "    if val_loss <= min_val_loss:\n",
    "        print('Validation Loss is decreased from {:6f} to {:6f}, so saving the model weights'.format(min_val_loss, val_loss))\n",
    "        torch.save(model.state_dict(), 'lstm_state.pt')\n",
    "        min_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Music Genaration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load population database\n",
    "testing_data = np.random.rand(1000)\n",
    "testing_data = testing_data.reshape(testing_data.shape[0],1)\n",
    "initial_seq = np.random.rand(49,1)\n",
    "initial_seq = initial_seq.reshape(starting_notes.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting\n",
    "def prediction_with_influence(influence,int2note):\n",
    "    \n",
    "    predicted_notes = []\n",
    "    test_seq = initial_seq\n",
    "    for i in range(len(testing_data)):\n",
    "        test_seq.append(testing_data[i])\n",
    "        test_seq = test_seq[-sequence_length:]\n",
    "\n",
    "        test_hidden = model.hidden_init(test_batch_size)\n",
    "        test_output,_ = model.forward(test_seq, test_hidden, test_batch_size)\n",
    "\n",
    "        top_p, top_class = test_output.topk(1,dim =1)\n",
    "        test_seq[sequence_length-1] = int2note[top_class]/max_note\n",
    "        predicted_notes.append(int2note[top_class]/max_note)\n",
    "        \n",
    "        return predicted_notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
